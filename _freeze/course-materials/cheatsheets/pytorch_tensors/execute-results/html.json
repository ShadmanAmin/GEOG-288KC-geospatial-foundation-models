{
  "hash": "abe6251f4352e6c157fe25751c8dc93b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"PyTorch Tensors & GPU Operations\"\nsubtitle: \"Essential tensor operations for geospatial AI\"\njupyter: geoai\nformat: html\n---\n\n\n## Creating Tensors\n\n### Basic tensor creation\n\n::: {#f0cf9379 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport numpy as np\n\n# From Python lists\ntensor_from_list = torch.tensor([[1, 2], [3, 4]])\nprint(f\"From list: {tensor_from_list}\")\n\n# From NumPy arrays\nnumpy_array = np.array([[1, 2], [3, 4]])\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(f\"From numpy: {tensor_from_numpy}\")\n\n# Common initialization patterns\nzeros_tensor = torch.zeros(3, 3)\nones_tensor = torch.ones(2, 4)\nrandom_tensor = torch.randn(2, 3)  # Normal distribution\n\nprint(f\"Zeros: {zeros_tensor}\")\nprint(f\"Random: {random_tensor}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFrom list: tensor([[1, 2],\n        [3, 4]])\nFrom numpy: tensor([[1, 2],\n        [3, 4]])\nZeros: tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nRandom: tensor([[ 0.7016,  0.0190,  0.9809],\n        [-0.8297, -0.5010,  1.3637]])\n```\n:::\n:::\n\n\n### Tensor Properties\n\n::: {#06ca2202 .cell execution_count=2}\n``` {.python .cell-code}\n# Check tensor properties\nsample_tensor = torch.randn(3, 4, 5)\n\nprint(f\"Shape: {sample_tensor.shape}\")\nprint(f\"Size: {sample_tensor.size()}\")\nprint(f\"Data type: {sample_tensor.dtype}\")\nprint(f\"Device: {sample_tensor.device}\")\nprint(f\"Number of dimensions: {sample_tensor.ndim}\")\nprint(f\"Total elements: {sample_tensor.numel()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape: torch.Size([3, 4, 5])\nSize: torch.Size([3, 4, 5])\nData type: torch.float32\nDevice: cpu\nNumber of dimensions: 3\nTotal elements: 60\n```\n:::\n:::\n\n\n## GPU Operations\n\n### Check GPU availability\n\n::: {#ee502acf .cell execution_count=3}\n``` {.python .cell-code}\n# Check if CUDA is available\ncuda_available = torch.cuda.is_available()\nprint(f\"CUDA available: {cuda_available}\")\n\nif cuda_available:\n    gpu_count = torch.cuda.device_count()\n    gpu_name = torch.cuda.get_device_name(0)\n    print(f\"Number of GPUs: {gpu_count}\")\n    print(f\"GPU name: {gpu_name}\")\nelse:\n    print(\"Running on CPU\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCUDA available: False\nRunning on CPU\n```\n:::\n:::\n\n\n### Moving tensors to GPU\n\n::: {#867abaf7 .cell execution_count=4}\n``` {.python .cell-code}\n# Create a tensor\ncpu_tensor = torch.randn(3, 3)\nprint(f\"Original device: {cpu_tensor.device}\")\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpu_tensor = cpu_tensor.to(device)\nprint(f\"After moving to device: {gpu_tensor.device}\")\n\n# Alternative methods\nif torch.cuda.is_available():\n    # Method 2: .cuda() method\n    gpu_tensor2 = cpu_tensor.cuda()\n    print(f\"Using .cuda(): {gpu_tensor2.device}\")\n    \n    # Method 3: Create directly on GPU\n    direct_gpu = torch.randn(3, 3, device='cuda')\n    print(f\"Created on GPU: {direct_gpu.device}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal device: cpu\nAfter moving to device: cpu\n```\n:::\n:::\n\n\n## Basic Tensor Operations\n\n### Mathematical operations\n\n::: {#b8f89337 .cell execution_count=5}\n``` {.python .cell-code}\n# Create sample tensors\na = torch.randn(3, 3)\nb = torch.randn(3, 3)\n\n# Element-wise operations\naddition = a + b\nsubtraction = a - b  \nmultiplication = a * b\ndivision = a / b\n\nprint(f\"Addition shape: {addition.shape}\")\nprint(f\"Subtraction mean: {subtraction.mean():.3f}\")\n\n# Matrix operations\nmatmul = torch.matmul(a, b)  # Matrix multiplication\ntranspose = a.t()  # Transpose\n\nprint(f\"Matrix multiplication shape: {matmul.shape}\")\nprint(f\"Transpose shape: {transpose.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAddition shape: torch.Size([3, 3])\nSubtraction mean: -0.073\nMatrix multiplication shape: torch.Size([3, 3])\nTranspose shape: torch.Size([3, 3])\n```\n:::\n:::\n\n\n### Useful tensor methods\n\n::: {#00080df0 .cell execution_count=6}\n``` {.python .cell-code}\n# Statistical operations\ndata = torch.randn(4, 5)\n\nprint(f\"Mean: {data.mean():.3f}\")\nprint(f\"Standard deviation: {data.std():.3f}\")\nprint(f\"Min: {data.min():.3f}\")\nprint(f\"Max: {data.max():.3f}\")\n\n# Reshaping\nreshaped = data.view(2, 10)  # Reshape to 2x10\nflattened = data.flatten()   # Flatten to 1D\n\nprint(f\"Original shape: {data.shape}\")\nprint(f\"Reshaped: {reshaped.shape}\")\nprint(f\"Flattened: {flattened.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean: -0.380\nStandard deviation: 1.090\nMin: -2.545\nMax: 1.585\nOriginal shape: torch.Size([4, 5])\nReshaped: torch.Size([2, 10])\nFlattened: torch.Size([20])\n```\n:::\n:::\n\n\n## Memory Management\n\n### GPU memory monitoring\n\n::: {#e7a8ddd3 .cell execution_count=7}\n``` {.python .cell-code}\nif torch.cuda.is_available():\n    # Check current memory usage\n    allocated = torch.cuda.memory_allocated() / (1024**2)  # Convert to MB\n    reserved = torch.cuda.memory_reserved() / (1024**2)\n    \n    print(f\"GPU memory allocated: {allocated:.1f} MB\")\n    print(f\"GPU memory reserved: {reserved:.1f} MB\")\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    print(\"GPU cache cleared\")\nelse:\n    print(\"GPU memory monitoring not available (running on CPU)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGPU memory monitoring not available (running on CPU)\n```\n:::\n:::\n\n\n### Best practices\n\n::: {#aa2f1bca .cell execution_count=8}\n``` {.python .cell-code}\n# Use context managers for temporary operations\ndef memory_efficient_operation():\n    with torch.no_grad():  # Disable gradient computation\n        large_tensor = torch.randn(1000, 1000)\n        result = large_tensor.mean()\n        return result\n\nresult = memory_efficient_operation()\nprint(f\"Result: {result:.3f}\")\n\n# The large_tensor is automatically garbage collected\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResult: -0.001\n```\n:::\n:::\n\n\n## Converting Between Formats\n\n### PyTorch â†” NumPy\n\n::: {#97ad8a9b .cell execution_count=9}\n``` {.python .cell-code}\n# PyTorch to NumPy\ntorch_tensor = torch.randn(2, 3)\nnumpy_array = torch_tensor.numpy()  # Only works on CPU tensors\n\nprint(f\"PyTorch tensor: {torch_tensor}\")\nprint(f\"NumPy array: {numpy_array}\")\n\n# NumPy to PyTorch\nnew_torch = torch.from_numpy(numpy_array)\nprint(f\"Back to PyTorch: {new_torch}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPyTorch tensor: tensor([[-1.9366,  0.4574, -0.2636],\n        [ 0.3120,  0.1357, -1.4863]])\nNumPy array: [[-1.9365865   0.45741966 -0.26364863]\n [ 0.31196123  0.13574891 -1.4862579 ]]\nBack to PyTorch: tensor([[-1.9366,  0.4574, -0.2636],\n        [ 0.3120,  0.1357, -1.4863]])\n```\n:::\n:::\n\n\n### Handling GPU tensors\n\n::: {#fc379a17 .cell execution_count=10}\n``` {.python .cell-code}\nif torch.cuda.is_available():\n    # GPU tensor must be moved to CPU first\n    gpu_tensor = torch.randn(2, 3).cuda()\n    cpu_numpy = gpu_tensor.cpu().numpy()\n    print(f\"GPU tensor converted to NumPy: {cpu_numpy.shape}\")\nelse:\n    print(\"GPU conversion example not available (running on CPU)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGPU conversion example not available (running on CPU)\n```\n:::\n:::\n\n\n",
    "supporting": [
      "pytorch_tensors_files"
    ],
    "filters": [],
    "includes": {}
  }
}