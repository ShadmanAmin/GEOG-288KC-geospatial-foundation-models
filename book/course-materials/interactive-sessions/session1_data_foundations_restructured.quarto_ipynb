{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Building a GeoTIFF to Embeddings Pipeline\"\n",
        "subtitle: \"From raw satellite data to model-ready tensors\"\n",
        "jupyter: geoai\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    code-fold: true\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Today we're building a complete pipeline that transforms raw satellite imagery into model-ready embeddings. This mirrors how Large Language Models process text: **raw text → tokens → embeddings**. Our geospatial version: **raw GeoTIFF → patches → embeddings**.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By building this pipeline, you will:\n",
        "- **Implement** GeoTIFF loading and preprocessing functions\n",
        "- **Create** patch extraction with spatial metadata\n",
        "- **Build** tensor normalization and encoding functions  \n",
        "- **Construct** a PyTorch DataLoader for model training\n",
        "- **Connect** to a simple embedding layer to verify end-to-end functionality\n",
        "\n",
        "## Session Roadmap\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "flowchart TD\n",
        "    A[\"Setup & GeoTIFF Loading\"] --> B[\"Geo Preprocessing Functions\"]\n",
        "    B --> C[\"Patch Extraction with Metadata\"] \n",
        "    C --> D[\"Tensor Operations & Normalization\"]\n",
        "    D --> E[\"DataLoader Construction\"]\n",
        "    E --> F[\"Embedding Layer Integration\"]\n",
        "    F --> G[\"End-to-End Pipeline Test\"]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Setting Up\n",
        "\n",
        "Let's establish our development environment and define the core constants we'll use throughout.\n",
        "\n",
        "### Imports and Configuration\n"
      ],
      "id": "fc50460a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import rasterio as rio\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Pipeline constants\n",
        "PATCH_SIZE = 64\n",
        "STRIDE = 32  # 50% overlap\n",
        "BATCH_SIZE = 8\n",
        "EMBEDDING_DIM = 256\n",
        "\n",
        "print(f\"✓ Environment setup complete\")\n",
        "print(f\"✓ Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
        "print(f\"✓ Stride: {STRIDE} (overlap: {(PATCH_SIZE-STRIDE)/PATCH_SIZE*100:.0f}%)\")"
      ],
      "id": "c609f3b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation\n"
      ],
      "id": "1d93d5af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set up data paths\n",
        "DATA_DIR = Path(\"../../data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "SAMPLE_PATH = DATA_DIR / \"landcover_sample.tif\"\n",
        "\n",
        "# Download sample if missing\n",
        "if not SAMPLE_PATH.exists():\n",
        "    import urllib.request\n",
        "    url = \"https://raw.githubusercontent.com/kellycaylor/geoAI/main/data/landcover_sample.tif\"\n",
        "    print(\"📥 Downloading sample data...\")\n",
        "    urllib.request.urlretrieve(url, SAMPLE_PATH)\n",
        "\n",
        "print(f\"✓ Data ready: {SAMPLE_PATH.name}\")\n",
        "print(f\"✓ File size: {SAMPLE_PATH.stat().st_size / 1024:.1f} KB\")"
      ],
      "id": "34334bc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 1: GeoTIFF Loading and Inspection\n",
        "\n",
        "**Goal**: Build a function that loads and extracts essential information from any GeoTIFF.\n",
        "\n",
        "### 🛠️ Build It: GeoTIFF Loader Function\n",
        "\n",
        "Your task: Complete this function to load a GeoTIFF and return both the data and metadata.\n"
      ],
      "id": "f8777ea8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_geotiff(file_path: Path) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Load a GeoTIFF and extract data + metadata.\n",
        "    \n",
        "    Returns:\n",
        "        data: (bands, height, width) array\n",
        "        metadata: dict with CRS, transform, resolution, etc.\n",
        "    \"\"\"\n",
        "    with rio.open(file_path) as src:\n",
        "        # TODO: Load the data array\n",
        "        data = src.read()  # YOUR CODE: Load raster data\n",
        "        \n",
        "        # TODO: Extract metadata\n",
        "        metadata = {\n",
        "            'crs': src.crs,  # YOUR CODE: Get coordinate reference system\n",
        "            'transform': src.transform,  # YOUR CODE: Get geospatial transform\n",
        "            'shape': data.shape,  # YOUR CODE: Get array dimensions\n",
        "            'dtype': data.dtype,  # YOUR CODE: Get data type\n",
        "            'resolution': src.res,  # YOUR CODE: Get pixel resolution\n",
        "            'bounds': src.bounds,  # YOUR CODE: Get spatial bounds\n",
        "        }\n",
        "    \n",
        "    return data, metadata\n",
        "\n",
        "# Test your function\n",
        "data, metadata = load_geotiff(SAMPLE_PATH)\n",
        "print(f\"✓ Loaded shape: {data.shape}\")\n",
        "print(f\"✓ Data type: {metadata['dtype']}\")\n",
        "print(f\"✓ Resolution: {metadata['resolution']}\")\n",
        "print(f\"✓ CRS: {metadata['crs']}\")"
      ],
      "id": "08ebe5f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔍 Verify It: Inspect Your Data\n"
      ],
      "id": "4b5ecff7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Examine the data you loaded\n",
        "bands, height, width = data.shape\n",
        "print(f\"Image dimensions: {height}×{width} pixels\")\n",
        "print(f\"Number of bands: {bands}\")\n",
        "print(f\"Value ranges per band:\")\n",
        "for i, band in enumerate(data):\n",
        "    print(f\"  Band {i+1}: {band.min():.0f} to {band.max():.0f}\")\n",
        "\n",
        "# Quick visualization\n",
        "fig, axes = plt.subplots(1, bands, figsize=(12, 4))\n",
        "if bands == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, band in enumerate(data):\n",
        "    axes[i].imshow(band, cmap='viridis')\n",
        "    axes[i].set_title(f'Band {i+1}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Raw Satellite Bands')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "4ddeb700",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Geo Preprocessing Functions\n",
        "\n",
        "**Goal**: Build preprocessing functions that operate on the full image before patch extraction.\n",
        "\n",
        "### 🛠️ Build It: Normalization Functions\n",
        "\n",
        "We'll create two normalization functions that can work with either **local statistics** (calculated from the input data) or **global statistics** (pre-computed from a training dataset). Global statistics ensure consistent normalization across different image tiles and are crucial for foundation model training.\n",
        "\n",
        "**Why use global statistics?** When training on multiple images, each tile might have different value ranges. Using global statistics ensures that the same pixel value represents the same relative intensity across all training data.\n",
        "\n",
        "#### Min-Max Normalization Function\n"
      ],
      "id": "d8496e17"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def minmax_normalize(data: np.ndarray, \n",
        "                    global_min: np.ndarray = None, \n",
        "                    global_max: np.ndarray = None) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Min-max normalize spectral bands to [0,1] range.\n",
        "    \n",
        "    Args:\n",
        "        data: (bands, height, width) array\n",
        "        global_min: Optional (bands,) array of global minimums per band\n",
        "        global_max: Optional (bands,) array of global maximums per band\n",
        "    \n",
        "    Returns:\n",
        "        normalized: (bands, height, width) array with values in [0,1]\n",
        "        stats: Dictionary containing the min/max values used\n",
        "    \"\"\"\n",
        "    bands, height, width = data.shape\n",
        "    normalized = np.zeros_like(data, dtype=np.float32)\n",
        "    \n",
        "    # Use global stats if provided, otherwise calculate from data\n",
        "    if global_min is None or global_max is None:\n",
        "        # Calculate per-band statistics from this data\n",
        "        mins = np.array([data[i].min() for i in range(bands)])\n",
        "        maxs = np.array([data[i].max() for i in range(bands)])\n",
        "        stats_source = \"local (calculated from input)\"\n",
        "    else:\n",
        "        # Use provided global statistics\n",
        "        mins = global_min\n",
        "        maxs = global_max\n",
        "        stats_source = \"global (provided)\"\n",
        "    \n",
        "    # Apply normalization per band\n",
        "    for i in range(bands):\n",
        "        band_range = maxs[i] - mins[i]\n",
        "        if band_range > 0:  # Avoid division by zero\n",
        "            normalized[i] = (data[i] - mins[i]) / band_range\n",
        "        else:\n",
        "            normalized[i] = 0  # Handle constant bands\n",
        "    \n",
        "    # Package statistics for inspection\n",
        "    stats = {\n",
        "        'source': stats_source,\n",
        "        'mins': mins,\n",
        "        'maxs': maxs,\n",
        "        'output_range': (normalized.min(), normalized.max())\n",
        "    }\n",
        "    \n",
        "    return normalized, stats"
      ],
      "id": "d1787e91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Z-Score Normalization Function\n"
      ],
      "id": "ab009a88"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def zscore_normalize(data: np.ndarray,\n",
        "                    global_mean: np.ndarray = None,\n",
        "                    global_std: np.ndarray = None) -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Z-score normalize spectral bands to mean=0, std=1.\n",
        "    \n",
        "    Args:\n",
        "        data: (bands, height, width) array\n",
        "        global_mean: Optional (bands,) array of global means per band\n",
        "        global_std: Optional (bands,) array of global standard deviations per band\n",
        "    \n",
        "    Returns:\n",
        "        normalized: (bands, height, width) standardized array\n",
        "        stats: Dictionary containing the mean/std values used\n",
        "    \"\"\"\n",
        "    bands, height, width = data.shape\n",
        "    normalized = np.zeros_like(data, dtype=np.float32)\n",
        "    \n",
        "    # Use global stats if provided, otherwise calculate from data\n",
        "    if global_mean is None or global_std is None:\n",
        "        # Calculate per-band statistics from this data\n",
        "        means = np.array([data[i].mean() for i in range(bands)])\n",
        "        stds = np.array([data[i].std() for i in range(bands)])\n",
        "        stats_source = \"local (calculated from input)\"\n",
        "    else:\n",
        "        # Use provided global statistics\n",
        "        means = global_mean\n",
        "        stds = global_std\n",
        "        stats_source = \"global (provided)\"\n",
        "    \n",
        "    # Apply normalization per band\n",
        "    for i in range(bands):\n",
        "        if stds[i] > 0:  # Avoid division by zero\n",
        "            normalized[i] = (data[i] - means[i]) / stds[i]\n",
        "        else:\n",
        "            normalized[i] = 0  # Handle constant bands\n",
        "    \n",
        "    # Package statistics for inspection\n",
        "    stats = {\n",
        "        'source': stats_source,\n",
        "        'means': means,\n",
        "        'stds': stds,\n",
        "        'output_mean': normalized.mean(),\n",
        "        'output_std': normalized.std()\n",
        "    }\n",
        "    \n",
        "    return normalized, stats\n",
        "\n",
        "print(\"✓ Normalization functions created\")\n",
        "print(\"  - minmax_normalize: scales to [0,1] range\")\n",
        "print(\"  - zscore_normalize: standardizes to mean=0, std=1\")"
      ],
      "id": "fd9ce8c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Both Functions with Local Statistics\n"
      ],
      "id": "9351381d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test min-max normalization with local statistics\n",
        "minmax_data, minmax_stats = minmax_normalize(data)\n",
        "print(\"📊 Min-Max Normalization (local stats):\")\n",
        "print(f\"  Source: {minmax_stats['source']}\")\n",
        "print(f\"  Original range: {data.min():.0f} to {data.max():.0f}\")\n",
        "print(f\"  Normalized range: {minmax_stats['output_range'][0]:.3f} to {minmax_stats['output_range'][1]:.3f}\")\n",
        "print(f\"  Per-band mins: {minmax_stats['mins']}\")\n",
        "print(f\"  Per-band maxs: {minmax_stats['maxs']}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test z-score normalization with local statistics  \n",
        "zscore_data, zscore_stats = zscore_normalize(data)\n",
        "print(\"📊 Z-Score Normalization (local stats):\")\n",
        "print(f\"  Source: {zscore_stats['source']}\")\n",
        "print(f\"  Output mean: {zscore_stats['output_mean']:.6f}\")\n",
        "print(f\"  Output std: {zscore_stats['output_std']:.6f}\")\n",
        "print(f\"  Per-band means: {zscore_stats['means']}\")\n",
        "print(f\"  Per-band stds: {zscore_stats['stds']}\")"
      ],
      "id": "9f7ce944",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test with Global Statistics\n"
      ],
      "id": "87b3e08e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate global statistics from a larger dataset\n",
        "# In practice, these would be pre-computed from your entire training corpus\n",
        "global_mins = np.array([100, 150, 200])  # Example global minimums per band\n",
        "global_maxs = np.array([1500, 2000, 2500])  # Example global maximums per band\n",
        "global_means = np.array([800, 1200, 1600])  # Example global means per band\n",
        "global_stds = np.array([300, 400, 500])  # Example global standard deviations per band\n",
        "\n",
        "print(\"🌍 Testing with Global Statistics:\")\n",
        "print(f\"  Global mins: {global_mins}\")\n",
        "print(f\"  Global maxs: {global_maxs}\")\n",
        "print(f\"  Global means: {global_means}\")\n",
        "print(f\"  Global stds: {global_stds}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Test with global statistics\n",
        "minmax_global, minmax_global_stats = minmax_normalize(data, global_mins, global_maxs)\n",
        "zscore_global, zscore_global_stats = zscore_normalize(data, global_means, global_stds)\n",
        "\n",
        "print(\"📊 Min-Max with Global Stats:\")\n",
        "print(f\"  Source: {minmax_global_stats['source']}\")\n",
        "print(f\"  Output range: {minmax_global_stats['output_range'][0]:.3f} to {minmax_global_stats['output_range'][1]:.3f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"📊 Z-Score with Global Stats:\")\n",
        "print(f\"  Source: {zscore_global_stats['source']}\")\n",
        "print(f\"  Output mean: {zscore_global_stats['output_mean']:.3f}\")\n",
        "print(f\"  Output std: {zscore_global_stats['output_std']:.3f}\")"
      ],
      "id": "a3e9b23a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What to notice:** When using global statistics, the output ranges and distributions differ from local normalization. This is expected and ensures consistency across different image tiles in your dataset.\n",
        "\n",
        "### 🛠️ Build It: Spatial Cropping Function\n"
      ],
      "id": "bfdd20a0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def crop_to_patches(data: np.ndarray, patch_size: int, stride: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Crop image to dimensions that allow complete patch extraction.\n",
        "    \n",
        "    Args:\n",
        "        data: (bands, height, width) array\n",
        "        patch_size: size of patches to extract\n",
        "        stride: step size between patches\n",
        "        \n",
        "    Returns:\n",
        "        cropped: (bands, new_height, new_width) array\n",
        "    \"\"\"\n",
        "    bands, height, width = data.shape\n",
        "    \n",
        "    # TODO: Calculate how many complete patches fit\n",
        "    patches_h = (height - patch_size) // stride + 1\n",
        "    patches_w = (width - patch_size) // stride + 1\n",
        "    \n",
        "    # TODO: Calculate the required dimensions\n",
        "    new_height = (patches_h - 1) * stride + patch_size\n",
        "    new_width = (patches_w - 1) * stride + patch_size\n",
        "    \n",
        "    # TODO: Crop the data\n",
        "    cropped = data[:, :new_height, :new_width]\n",
        "    \n",
        "    print(f\"✓ Cropped from {height}×{width} to {new_height}×{new_width}\")\n",
        "    print(f\"✓ Will generate {patches_h}×{patches_w} = {patches_h*patches_w} patches\")\n",
        "    \n",
        "    return cropped\n",
        "\n",
        "# Test your cropping function"
      ],
      "id": "727bc5e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: Patch Extraction with Metadata\n",
        "\n",
        "**Goal**: Extract patches while preserving spatial context information.\n",
        "\n",
        "### 🛠️ Build It: Patch Extraction Function\n"
      ],
      "id": "85fd8964"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_patches_with_metadata(\n",
        "    data: np.ndarray, \n",
        "    transform,\n",
        "    patch_size: int, \n",
        "    stride: int\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extract patches with their spatial coordinates.\n",
        "    \n",
        "    Args:\n",
        "        data: (bands, height, width) normalized array\n",
        "        transform: rasterio transform object\n",
        "        patch_size: size of patches\n",
        "        stride: step between patches\n",
        "        \n",
        "    Returns:\n",
        "        patches: (n_patches, bands, patch_size, patch_size) array\n",
        "        coordinates: (n_patches, 4) array of [min_x, min_y, max_x, max_y]\n",
        "    \"\"\"\n",
        "    bands, height, width = data.shape\n",
        "    patches = []\n",
        "    coordinates = []\n",
        "    \n",
        "    # TODO: Iterate through patch positions\n",
        "    for row in range(0, height - patch_size + 1, stride):\n",
        "        for col in range(0, width - patch_size + 1, stride):\n",
        "            # TODO: Extract patch from all bands\n",
        "            patch = data[:, row:row+patch_size, col:col+patch_size]\n",
        "            patches.append(patch)\n",
        "            \n",
        "            # TODO: Calculate real-world coordinates using transform\n",
        "            min_x, max_y = transform * (col, row)  # Top-left\n",
        "            max_x, min_y = transform * (col + patch_size, row + patch_size)  # Bottom-right\n",
        "            coordinates.append([min_x, min_y, max_x, max_y])\n",
        "    \n",
        "    patches = np.array(patches)\n",
        "    coordinates = np.array(coordinates)\n",
        "    \n",
        "    print(f\"✓ Extracted {len(patches)} patches\")\n",
        "    print(f\"✓ Patch shape: {patches.shape}\")\n",
        "    print(f\"✓ Coordinate shape: {coordinates.shape}\")\n",
        "    \n",
        "    return patches, coordinates\n",
        "\n",
        "# Test your patch extraction\n",
        "patches, coords = extract_patches_with_metadata(\n",
        "    cropped_data, metadata['transform'], PATCH_SIZE, STRIDE\n",
        ")\n",
        "\n",
        "# Visualize a few patches\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "for i in range(8):\n",
        "    row, col = i // 4, i % 4\n",
        "    # Show first band of each patch\n",
        "    axes[row, col].imshow(patches[i, 0], cmap='viridis')\n",
        "    axes[row, col].set_title(f'Patch {i}')\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "plt.suptitle('Sample Extracted Patches (Band 1)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "42ac4ee4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4: Tensor Operations & Metadata Encoding\n",
        "\n",
        "**Goal**: Convert numpy arrays to PyTorch tensors and encode metadata.\n",
        "\n",
        "### 🛠️ Build It: Metadata Encoder\n"
      ],
      "id": "0e09b6e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def encode_metadata(coordinates: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Encode spatial metadata as features.\n",
        "    \n",
        "    Args:\n",
        "        coordinates: (n_patches, 4) array of [min_x, min_y, max_x, max_y]\n",
        "        \n",
        "    Returns:\n",
        "        encoded: (n_patches, n_features) array\n",
        "    \"\"\"\n",
        "    # TODO: Calculate spatial features\n",
        "    center_x = (coordinates[:, 0] + coordinates[:, 2]) / 2\n",
        "    center_y = (coordinates[:, 1] + coordinates[:, 3]) / 2\n",
        "    width = coordinates[:, 2] - coordinates[:, 0]\n",
        "    height = coordinates[:, 3] - coordinates[:, 1]\n",
        "    area = width * height\n",
        "    \n",
        "    # TODO: Normalize spatial features\n",
        "    features = np.column_stack([\n",
        "        (center_x - center_x.mean()) / center_x.std(),  # Normalized center X\n",
        "        (center_y - center_y.mean()) / center_y.std(),  # Normalized center Y\n",
        "        (area - area.mean()) / area.std(),              # Normalized area\n",
        "        width / height,                                 # Aspect ratio\n",
        "    ])\n",
        "    \n",
        "    print(f\"✓ Encoded metadata shape: {features.shape}\")\n",
        "    print(f\"✓ Feature statistics:\")\n",
        "    feature_names = ['center_x', 'center_y', 'area', 'aspect_ratio']\n",
        "    for i, name in enumerate(feature_names):\n",
        "        print(f\"  {name}: mean={features[:, i].mean():.3f}, std={features[:, i].std():.3f}\")\n",
        "    \n",
        "    return features.astype(np.float32)\n",
        "\n",
        "# Test metadata encoding\n",
        "encoded_metadata = encode_metadata(coords)"
      ],
      "id": "cfcd412f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🛠️ Build It: Tensor Conversion\n"
      ],
      "id": "ad816e4d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def create_tensors(patches: np.ndarray, metadata: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Convert numpy arrays to PyTorch tensors.\n",
        "    \n",
        "    Args:\n",
        "        patches: (n_patches, bands, height, width) array\n",
        "        metadata: (n_patches, n_features) array\n",
        "        \n",
        "    Returns:\n",
        "        patch_tensors: (n_patches, bands, height, width) tensor\n",
        "        metadata_tensors: (n_patches, n_features) tensor\n",
        "    \"\"\"\n",
        "    # TODO: Convert to tensors with appropriate dtypes\n",
        "    patch_tensors = torch.from_numpy(patches).float()\n",
        "    metadata_tensors = torch.from_numpy(metadata).float()\n",
        "    \n",
        "    print(f\"✓ Patch tensors: {patch_tensors.shape}, dtype: {patch_tensors.dtype}\")\n",
        "    print(f\"✓ Metadata tensors: {metadata_tensors.shape}, dtype: {metadata_tensors.dtype}\")\n",
        "    \n",
        "    return patch_tensors, metadata_tensors\n",
        "\n",
        "# Create tensors\n",
        "patch_tensors, metadata_tensors = create_tensors(patches, encoded_metadata)"
      ],
      "id": "2c6a1743",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5: DataLoader Construction\n",
        "\n",
        "**Goal**: Build a PyTorch Dataset and DataLoader for training.\n",
        "\n",
        "### 🛠️ Build It: Custom Dataset Class\n"
      ],
      "id": "4b75bd95"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class GeospatialDataset(Dataset):\n",
        "    \"\"\"Dataset for geospatial patches with metadata.\"\"\"\n",
        "    \n",
        "    def __init__(self, patch_tensors: torch.Tensor, metadata_tensors: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patch_tensors: (n_patches, bands, height, width)\n",
        "            metadata_tensors: (n_patches, n_features)\n",
        "        \"\"\"\n",
        "        self.patches = patch_tensors\n",
        "        self.metadata = metadata_tensors\n",
        "        \n",
        "        # TODO: Create dummy labels for demonstration (in real use, load from file)\n",
        "        self.labels = torch.randint(0, 5, (len(patch_tensors),))  # 5 land cover classes\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return number of patches.\"\"\"\n",
        "        return len(self.patches)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get a single item.\n",
        "        \n",
        "        Returns:\n",
        "            patch: (bands, height, width) tensor\n",
        "            metadata: (n_features,) tensor  \n",
        "            label: scalar tensor\n",
        "        \"\"\"\n",
        "        return self.patches[idx], self.metadata[idx], self.labels[idx]\n",
        "\n",
        "# Test your dataset\n",
        "dataset = GeospatialDataset(patch_tensors, metadata_tensors)\n",
        "print(f\"✓ Dataset length: {len(dataset)}\")\n",
        "\n",
        "# Test getting an item\n",
        "sample_patch, sample_metadata, sample_label = dataset[0]\n",
        "print(f\"✓ Sample patch shape: {sample_patch.shape}\")\n",
        "print(f\"✓ Sample metadata shape: {sample_metadata.shape}\")\n",
        "print(f\"✓ Sample label: {sample_label.item()}\")"
      ],
      "id": "36d7a867",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🛠️ Build It: DataLoader\n"
      ],
      "id": "9040c33e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Create DataLoader with appropriate batch size and shuffling\n",
        "dataloader = DataLoader(\n",
        "    dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True,\n",
        "    num_workers=0,  # Set to 0 for compatibility\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"✓ DataLoader created with batch size {BATCH_SIZE}\")\n",
        "print(f\"✓ Number of batches: {len(dataloader)}\")\n",
        "\n",
        "# Test the DataLoader\n",
        "for batch_idx, (patches, metadata, labels) in enumerate(dataloader):\n",
        "    print(f\"✓ Batch {batch_idx}:\")\n",
        "    print(f\"  Patches: {patches.shape}\")\n",
        "    print(f\"  Metadata: {metadata.shape}\")\n",
        "    print(f\"  Labels: {labels.shape}\")\n",
        "    if batch_idx == 1:  # Show first two batches\n",
        "        break"
      ],
      "id": "ab2d93f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 6: Embedding Layer Integration\n",
        "\n",
        "**Goal**: Connect to a simple embedding layer to verify end-to-end functionality.\n",
        "\n",
        "### 🛠️ Build It: Simple GFM Embedding Layer\n"
      ],
      "id": "820756bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SimpleGFMEmbedding(nn.Module):\n",
        "    \"\"\"Simple embedding layer for geospatial patches.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_channels: int, metadata_features: int, embed_dim: int):\n",
        "        super().__init__()\n",
        "        \n",
        "        # TODO: Build patch encoder (simple CNN)\n",
        "        self.patch_encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2), \n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        \n",
        "        # TODO: Build metadata encoder\n",
        "        self.metadata_encoder = nn.Sequential(\n",
        "            nn.Linear(metadata_features, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "        )\n",
        "        \n",
        "        # TODO: Build fusion layer\n",
        "        # Calculate patch encoder output size\n",
        "        with torch.no_grad():\n",
        "            dummy_patch = torch.randn(1, input_channels, PATCH_SIZE, PATCH_SIZE)\n",
        "            patch_feat_size = self.patch_encoder(dummy_patch).shape[1]\n",
        "        \n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(patch_feat_size + 32, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "        )\n",
        "        \n",
        "    def forward(self, patches: torch.Tensor, metadata: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patches: (batch, channels, height, width)\n",
        "            metadata: (batch, n_features)\n",
        "            \n",
        "        Returns:\n",
        "            embeddings: (batch, embed_dim)\n",
        "        \"\"\"\n",
        "        # TODO: Encode patches and metadata\n",
        "        patch_features = self.patch_encoder(patches)\n",
        "        metadata_features = self.metadata_encoder(metadata)\n",
        "        \n",
        "        # TODO: Fuse features\n",
        "        combined = torch.cat([patch_features, metadata_features], dim=1)\n",
        "        embeddings = self.fusion(combined)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "# Create and test the model\n",
        "model = SimpleGFMEmbedding(\n",
        "    input_channels=bands, \n",
        "    metadata_features=encoded_metadata.shape[1], \n",
        "    embed_dim=EMBEDDING_DIM\n",
        ")\n",
        "\n",
        "print(f\"✓ Model created\")\n",
        "print(f\"✓ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "id": "95dbfa48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 7: End-to-End Pipeline Test\n",
        "\n",
        "**Goal**: Run the complete pipeline and verify everything works together.\n",
        "\n",
        "### 🛠️ Build It: Complete Pipeline Function\n"
      ],
      "id": "2417aff3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def geotiff_to_embeddings_pipeline(\n",
        "    file_path: Path,\n",
        "    patch_size: int = 64,\n",
        "    stride: int = 32,\n",
        "    batch_size: int = 8,\n",
        "    embed_dim: int = 256\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Complete pipeline from GeoTIFF to embeddings.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to GeoTIFF file\n",
        "        patch_size: Size of patches to extract\n",
        "        stride: Step between patches  \n",
        "        batch_size: Batch size for processing\n",
        "        embed_dim: Embedding dimension\n",
        "        \n",
        "    Returns:\n",
        "        all_embeddings: (n_patches, embed_dim) tensor\n",
        "    \"\"\"\n",
        "    print(\"🚀 Starting GeoTIFF → Embeddings Pipeline\")\n",
        "    \n",
        "    # Step 1: Load data\n",
        "    print(\"📁 Loading GeoTIFF...\")\n",
        "    data, metadata = load_geotiff(file_path)\n",
        "    \n",
        "    # Step 2: Preprocess\n",
        "    print(\"🔧 Preprocessing...\")\n",
        "    norm_data = normalize_bands(data, method='minmax')\n",
        "    cropped_data = crop_to_patches(norm_data, patch_size, stride)\n",
        "    \n",
        "    # Step 3: Extract patches\n",
        "    print(\"✂️ Extracting patches...\")\n",
        "    patches, coords = extract_patches_with_metadata(cropped_data, metadata['transform'], patch_size, stride)\n",
        "    \n",
        "    # Step 4: Encode metadata\n",
        "    print(\"📊 Encoding metadata...\")\n",
        "    encoded_meta = encode_metadata(coords)\n",
        "    \n",
        "    # Step 5: Create tensors\n",
        "    print(\"🔢 Creating tensors...\")\n",
        "    patch_tensors, meta_tensors = create_tensors(patches, encoded_meta)\n",
        "    \n",
        "    # Step 6: Create dataset and dataloader\n",
        "    print(\"📦 Creating DataLoader...\")\n",
        "    dataset = GeospatialDataset(patch_tensors, meta_tensors)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    # Step 7: Create model and generate embeddings\n",
        "    print(\"🧠 Generating embeddings...\")\n",
        "    model = SimpleGFMEmbedding(\n",
        "        input_channels=data.shape[0],\n",
        "        metadata_features=encoded_meta.shape[1], \n",
        "        embed_dim=embed_dim\n",
        "    )\n",
        "    model.eval()\n",
        "    \n",
        "    all_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for patches_batch, meta_batch, _ in dataloader:\n",
        "            embeddings = model(patches_batch, meta_batch)\n",
        "            all_embeddings.append(embeddings)\n",
        "    \n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "    print(f\"✅ Pipeline complete! Generated {len(all_embeddings)} embeddings\")\n",
        "    \n",
        "    return all_embeddings\n",
        "\n",
        "# Run the complete pipeline\n",
        "embeddings = geotiff_to_embeddings_pipeline(SAMPLE_PATH)\n",
        "print(f\"\\n🎉 Final Result:\")\n",
        "print(f\"✓ Embeddings shape: {embeddings.shape}\")\n",
        "print(f\"✓ Embedding statistics:\")\n",
        "print(f\"  Mean: {embeddings.mean().item():.4f}\")\n",
        "print(f\"  Std: {embeddings.std().item():.4f}\")\n",
        "print(f\"  Min: {embeddings.min().item():.4f}\")\n",
        "print(f\"  Max: {embeddings.max().item():.4f}\")"
      ],
      "id": "0d411c4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔍 Verify It: Pipeline Output Analysis\n"
      ],
      "id": "ffce926a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize embedding similarities\n",
        "print(\"🔍 Analyzing embedding relationships...\")\n",
        "\n",
        "# Compute pairwise cosine similarities for first 10 embeddings\n",
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "sample_embeddings = embeddings[:10]\n",
        "similarity_matrix = torch.zeros(10, 10)\n",
        "\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        if i != j:\n",
        "            sim = cosine_similarity(sample_embeddings[i:i+1], sample_embeddings[j:j+1])\n",
        "            similarity_matrix[i, j] = sim.item()\n",
        "\n",
        "# Plot similarity matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(similarity_matrix.numpy(), cmap='viridis', vmin=-1, vmax=1)\n",
        "plt.colorbar(label='Cosine Similarity')\n",
        "plt.title('Embedding Similarity Matrix (First 10 Patches)')\n",
        "plt.xlabel('Patch Index')\n",
        "plt.ylabel('Patch Index')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Average similarity: {similarity_matrix.mean().item():.4f}\")\n",
        "print(f\"✓ Similarity range: {similarity_matrix.min().item():.4f} to {similarity_matrix.max().item():.4f}\")"
      ],
      "id": "2bbe1e5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "🎉 **Congratulations!** You've successfully built a complete pipeline that transforms raw satellite imagery into model-ready embeddings. \n",
        "\n",
        "### What You Built:\n",
        "\n",
        "1. **GeoTIFF Loader**: Extracts both pixel data and spatial metadata\n",
        "2. **Preprocessing Functions**: Normalization and spatial cropping  \n",
        "3. **Patch Extractor**: Creates patches while preserving spatial context\n",
        "4. **Metadata Encoder**: Transforms coordinates into learned features\n",
        "5. **PyTorch Integration**: Dataset, DataLoader, and model components\n",
        "6. **Embedding Generator**: Simple CNN that produces vector representations\n",
        "\n",
        "### Key Insights:\n",
        "\n",
        "- **Spatial Context Matters**: Each patch carries location information\n",
        "- **Preprocessing is Critical**: Normalization ensures stable training  \n",
        "- **Modular Design**: Each step can be optimized independently\n",
        "- **End-to-End Testing**: Verify the complete pipeline works\n",
        "\n",
        "### What's Next:\n",
        "\n",
        "In the following sessions, you'll enhance each component:\n",
        "- **Week 2**: Advanced attention mechanisms for spatial relationships\n",
        "- **Week 3**: Complete GFM architecture with transformer blocks\n",
        "- **Week 4**: Pretraining strategies and masked autoencoding\n",
        "\n",
        "The pipeline you built today forms the foundation for everything that follows! 🚀\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [PyTorch DataLoader Documentation](https://pytorch.org/docs/stable/data.html)\n",
        "- [Rasterio User Guide](https://rasterio.readthedocs.io/)\n",
        "- [Geospatial Foundation Model Examples](../examples/normalization_comparison.qmd)"
      ],
      "id": "34ebdfb1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "geoai",
      "language": "python",
      "display_name": "GeoAI",
      "path": "/Users/kellycaylor/Library/Jupyter/kernels/geoai"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}