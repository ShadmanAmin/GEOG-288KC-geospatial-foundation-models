{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Working with Geospatial Raster Data\"\n",
        "subtitle: \"From raw rasters to model‚Äëready embeddings, step by step\"\n",
        "editor_options: \n",
        "  chunk_output_type: console\n",
        "jupyter: geoai\n",
        "format: \n",
        "    html:\n",
        "        toc: true\n",
        "        toc-depth: 2\n",
        "        code-fold: show\n",
        "---\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "In this session, we learn how to transform raw geospatial raster data into model‚Äëready embeddings. We‚Äôll work with a tiny multi‚Äëband GeoTIFF, build small, deterministic processing steps, and show visible outputs after every step. The goal is to build intuition that mirrors LLM pipelines (tokenization ‚Üí embeddings) while grounding everything in images (patches ‚Üí embeddings).\n",
        "\n",
        "## Learning Objectives\n",
        "- Load and inspect small multi‚Äëband rasters (CRS, resolution, dtype, bands)\n",
        "- Normalize and organize bands for consistent modeling\n",
        "- Extract fixed‚Äësize patches\n",
        "- Add contextual features (location/time ‚Äútokens‚Äù)\n",
        "- Sample training patches correctly and create patch embeddings with shapes you can verify\n",
        "\n",
        "## Session Roadmap\n",
        "\n",
        "This session follows a complete data processing pipeline organized into three logical phases that transform raw satellite imagery into model-ready embeddings:\n",
        "\n",
        "\n",
        "```{mermaid}\n",
        "flowchart TD\n",
        "    subgraph Phase1 [\"üî¨ Phase 1: Pixel-Level Pre-processing\"]\n",
        "        direction LR\n",
        "        A[\"üõ∞Ô∏è Raw Data<br/>Inspection<br/>Section 1.1\"] --> B[\"üìê Band<br/>Normalization<br/>Section 1.2\"]\n",
        "        B --> C[\"üî¨ Dimensionality<br/>Reduction<br/>Section 1.3\"]\n",
        "    end\n",
        "    \n",
        "    subgraph Phase2 [\"üó∫Ô∏è Phase 2: Spatial-Structure Processing\"]\n",
        "        direction LR\n",
        "        D[\"‚úÇÔ∏è Patch<br/>Extraction<br/>Section 2.1\"] --> E[\"üè∑Ô∏è Patch<br/>Metadata<br/>Section 2.2\"]\n",
        "        E --> F[\"üåç Spatial-Temporal<br/>Context<br/>Section 2.3\"]\n",
        "        F --> G[\"üìä Training<br/>Sampling<br/>Section 2.4\"]\n",
        "    end\n",
        "    \n",
        "    subgraph Phase3 [\"üß† Phase 3: Model-Ready Processing\"]\n",
        "        direction LR\n",
        "        H[\"üß† Patch<br/>Embeddings<br/>Section 3.1\"] --> I[\"üìç Positional<br/>Encoding<br/>Section 3.2\"]\n",
        "    end\n",
        "    \n",
        "    Phase1 --> Phase2\n",
        "    Phase2 --> Phase3\n",
        "    \n",
        "    style Phase1 fill:#e3f2fd\n",
        "    style Phase2 fill:#fff3e0  \n",
        "    style Phase3 fill:#e8f5e8\n",
        "```\n",
        "\n",
        "\n",
        "## Setting Up\n",
        "\n",
        "Before we get started, let's setup our computational environment by loading necessary libraries and seeding our computations.\n",
        "\n",
        "\n",
        "### Load required libraries\n",
        "Why these imports: file paths (Path), randomness (random, numpy), and plotting (matplotlib) are used throughout the session.\n"
      ],
      "id": "6cf39911"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import os, random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Imports OK\")"
      ],
      "id": "37950ae3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Random Number Generation (RNG) seeds\n",
        "\n",
        "We use fixed seeds to ensure that our randomness is consistent every time we run our example code. This allows your results to match the rendered output on the course site.\n"
      ],
      "id": "246d893c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "RNG_SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(RNG_SEED)\n",
        "random.seed(RNG_SEED)\n",
        "np.random.seed(RNG_SEED)\n",
        "print(\"Seeds set:\", RNG_SEED)"
      ],
      "id": "4429da70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define data paths and constants\n",
        "We‚Äôll store the sample raster in a local `data/` directory and set a default patch size.\n"
      ],
      "id": "4fd2f18e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "DATA_DIR = Path(\"../../data\"); DATA_DIR.mkdir(exist_ok=True)\n",
        "DATA_PATH = DATA_DIR / \"landcover_sample.tif\"\n",
        "PATCH_SIZE = 64\n",
        "print(\"DATA_PATH:\", DATA_PATH)\n",
        "print(\"PATCH_SIZE:\", PATCH_SIZE)"
      ],
      "id": "154b245d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the file is missing, we‚Äôll download a small sample.\n"
      ],
      "id": "8b00b17b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import urllib.request\n",
        "if not DATA_PATH.exists():\n",
        "    url = \"https://raw.githubusercontent.com/kellycaylor/geoAI/main/data/landcover_sample.tif\"\n",
        "    print(\"Downloading sample raster‚Ä¶\")\n",
        "    urllib.request.urlretrieve(url, DATA_PATH)\n",
        "print(\"Exists:\", DATA_PATH.exists(), \"\\nSize (KB):\", round(DATA_PATH.stat().st_size/1024,1))"
      ],
      "id": "e78e7a7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# üî¨ Phase 1: Pixel-Level Pre-processing\n",
        "\n",
        "*In this phase, we work with the full image and operate on individual pixels. We'll develop code to read our raw data, normalize it for consistent model training, and perform dimensionality reduction before moving to spatial operations.*\n",
        "\n",
        "## 1.1 üõ∞Ô∏è Raw Data Inspection\n",
        "\n",
        "### Reading Geospatial Data\n",
        "\n",
        "Before we start working with the data in Python, it‚Äôs important to understand what kinds of information a geospatial raster dataset contains.\n",
        "\n",
        "A single raster file stores **two broad categories of information**:\n",
        "\n",
        "1. **Pixel values** ‚Äì the actual measurements.  \n",
        "   - Each pixel has one or more **bands** (channels) that store numeric values.  \n",
        "   - Bands might represent reflectance in different wavelengths, elevation, temperature, or derived indices like NDVI.  \n",
        "   - Pixel values are stored in a numeric data type (e.g., integers or floats) and have a defined range (min/max).\n",
        "\n",
        "2. **Spatial metadata** ‚Äì information that describes where and how those measurements fit on Earth.  \n",
        "   - **Location reference** ‚Äî how pixel positions map to real-world coordinates.  \n",
        "   - **Pixel resolution** ‚Äî the width and height of each pixel in ground units (e.g., meters).  \n",
        "   - **Grid layout and orientation** ‚Äî how rows and columns are aligned in space.  \n",
        "   - **Coordinate system** ‚Äî the mathematical model that defines positions on Earth (projection).  \n",
        "   - **Transform** ‚Äî a mapping from pixel coordinates (row/col) to real-world coordinates (x/y).\n",
        "\n",
        "When we open a raster with `rasterio`, we can access **both** the measurement values and the spatial metadata.  This lets us work with the data not just as an image, but as a set of geographically-anchored measurements we can relate to location, time, and other datasets. Let's take a look at our sample image. \n",
        "\n",
        "### Rasterio image reading and metadata inspection\n"
      ],
      "id": "25c9964c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: true\n",
        "import rasterio as rio\n",
        "import pyproj\n",
        "\n",
        "with rio.open(DATA_PATH) as src:\n",
        "    arr = src.read()                          # (bands, height, width)\n",
        "    band_count, height, width = arr.shape\n",
        "    dtype = arr.dtype\n",
        "\n",
        "    # CRS and EPSG\n",
        "    crs = src.crs\n",
        "    epsg = crs.to_epsg()\n",
        "    transform = src.transform  # Save transform for later use\n",
        "\n",
        "    # Get a human-readable CRS name\n",
        "    crs_info = pyproj.CRS.from_user_input(crs)\n",
        "    crs_name = crs_info.name\n",
        "\n",
        "    # Resolution in CRS units\n",
        "    res_x, res_y = src.res\n",
        "\n",
        "# --- Print summary ---\n",
        "print(f\"Shape: {arr.shape}\")\n",
        "print(f\"Bands: {band_count}  |  Height: {height} px  Width: {width} px |  Data type: {dtype}\")\n",
        "print(f\"CRS: {crs_name}\" + (f\" (EPSG:{epsg})\" if epsg else \"\"))\n",
        "print(f\"Resolution: {res_x:.2f} √ó {res_y:.2f} ground units per pixel\")\n",
        "print(\"Per-band min/max:\")\n",
        "for i, b in enumerate(arr, start=1):\n",
        "    print(f\"  Band {i}: {float(b.min()):.3f} ‚Äì {float(b.max()):.3f}\")"
      ],
      "id": "28a367a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the output gives us the fundamental \"identity\" of our geospatial data. The `arr.shape` tells us we have a 3D array(bands √ó height √ó width), the Coordinate Reference System (CRS) anchors this data to a specific location on Earth, and the resolution information tells us the real-world size of each pixel. Finally, the per-band min/max values reveal the dynamic range of our spectral measurements. \n",
        "\n",
        "Although the pixel values in this image are stored as `uint8`, which supports a range from 0 to 255, all three bands in our dataset have maximum values of `254`. This usually happens because the largest possible value in the data type is often reserved as a `NoData` flag, marking pixels with no valid measurement. This upper limit is specific to this dataset, and the use of `NoData` flags is dataset dependent.\n",
        "\n",
        "Image metadata is crucial because satellite imagery comes with precise spatial and spectral calibration. Each pixel value either represents actual physical measurements (like surface reflectance) at a specific geographic location, or a specific category that maps to a classification based on the dataset. Our model needs to understand not just \"what values doe this pixel contain?\" but also \"where is this pixel?\" and \"what does this pixel represent?\"\n",
        "\n",
        ":::{.callout-note}\n",
        "## Digital Numbers\n",
        "The raw pixel values in many geospatial images are stored as integers to reduce file size and improve processing speed.  These integer values are called **Digital Numbers** (`DN`s).  \n",
        "\n",
        "A `DN` is an instrument-recorded value that may need to be scaled or converted to physical units (such as reflectance or temperature) before analysis.\n",
        ":::\n",
        "\n",
        "### DN visualization\n"
      ],
      "id": "6af87f3d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: true\n",
        "# | code-fold: true\n",
        "\n",
        "# Display each band's raw DNs\n",
        "num_bands = arr.shape[0]\n",
        "fig, axes = plt.subplots(1, num_bands, figsize=(4*num_bands, 4))\n",
        "\n",
        "# Handle case where we only have one band\n",
        "if num_bands == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i in range(num_bands):\n",
        "    band = arr[i]\n",
        "    data_min, data_max = float(band.min()), float(band.max())\n",
        "    \n",
        "    im = axes[i].imshow(band, cmap='viridis', vmin=data_min, vmax=data_max)\n",
        "    axes[i].set_title(f\"Band {i+1}: Raw Values\\n(Range: {data_min:.0f} - {data_max:.0f})\")\n",
        "    axes[i].axis('off')\n",
        "    \n",
        "    # Add colorbar for each band\n",
        "    plt.colorbar(im, ax=axes[i], label='Digital Number (DN)', shrink=0.8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "b988cc55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1.2  üìê Band Normalization\n",
        "\n",
        "*Scaling bands to consistent ranges for stable model training*\n",
        "\n",
        "Now that we've examined our raw data, we need to prepare it for neural network processing. While the raw digital numbers are meaningful for interpretation, they present several challenges when used directly as input to models. Different spectral bands often have vastly different value ranges, and **neural networks are sensitive to the scale of input features**.\n",
        "\n",
        "### Why normalize geospatial data?\n",
        "\n",
        "Normalization serves several critical purposes in geospatial machine learning:\n",
        "\n",
        "**Scale Invariance**: Different spectral bands capture different physical phenomena and often have dramatically different value ranges. For example, near-infrared reflectance might range from 0-4000 DN, while visible bands range from 0-255 DN. Without normalization, the model would be dominated by the larger-valued bands.\n",
        "\n",
        "**Training Stability**: Neural networks use gradient-based optimization, which works best when input features have similar scales. Large differences in feature magnitude can cause unstable gradients, slow convergence, or poor performance.\n",
        "\n",
        "**Sensor Consistency**: When working with data from multiple sensors or time periods, normalization helps remove acquisition-specific biases while preserving the relative patterns that matter for learning.\n",
        "\n",
        "**Activation Function Compatibility**: Many neural network activation functions (like `sigmoid` or `tanh`) work optimally when inputs are in specific ranges. Normalized inputs help ensure we're operating in the most effective part of these functions.\n",
        "\n",
        "### Common normalization approaches\n",
        "\n",
        "Let's examine the most common normalization strategies for geospatial data:\n",
        "\n",
        "#### Min-Max Normalization (what we'll use)\n",
        "Scales each band to a fixed range, typically [0,1]:\n",
        "```\n",
        "normalized = (value - min) / (max - min)\n",
        "```\n",
        "\n",
        "**Pros**: Preserves the distribution shape, guarantees output range  \n",
        "**Cons**: Sensitive to outliers, range depends on specific dataset\n",
        "\n",
        "#### Z-Score Standardization  \n",
        "Centers data around zero with unit variance:\n",
        "```\n",
        "standardized = (value - mean) / std\n",
        "```\n",
        "\n",
        "**Pros**: Removes mean/variance effects, handles outliers better  \n",
        "**Cons**: Output range is unbounded, can lose interpretability\n",
        "\n",
        "#### Robust Scaling\n",
        "Uses median and interquartile range instead of mean/std:\n",
        "```\n",
        "robust = (value - median) / (75th_percentile - 25th_percentile)\n",
        "```\n",
        "\n",
        "**Pros**: Very robust to outliers  \n",
        "**Cons**: More complex, can compress dynamic range\n",
        "\n",
        "For this exercise, we'll use min-max normalization because it's intuitive and preserves the relative structure of our data while ensuring all bands contribute equally to the model.\n"
      ],
      "id": "9e9e1c89"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: true\n",
        "# Convert to float32 for numerical precision in calculations\n",
        "arr = arr.astype(np.float32)\n",
        "print(f\"Data type after conversion: {arr.dtype}\")\n",
        "\n",
        "# Calculate per-band statistics\n",
        "# Reshape to (bands, pixels) for easier computation\n",
        "pixel_values = arr.reshape(arr.shape[0], -1)\n",
        "mins = pixel_values.min(axis=1)\n",
        "maxs = pixel_values.max(axis=1)\n",
        "ranges = maxs - mins\n",
        "\n",
        "print(\"Per-band statistics before normalization:\")\n",
        "for i in range(len(mins)):\n",
        "    print(f\"  Band {i+1}: min={mins[i]:.1f}, max={maxs[i]:.1f}, range={ranges[i]:.1f}\")"
      ],
      "id": "7fc388ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: true\n",
        "# Apply min-max normalization: (x - min) / (max - min)\n",
        "# Add small epsilon to prevent division by zero\n",
        "eps = 1e-8\n",
        "arr_norm = (arr - mins[:, None, None]) / (np.maximum(ranges, eps)[:, None, None])\n",
        "\n",
        "print(\"\\nNormalized to [0,1] range:\")\n",
        "print(\"Per-band min/max after normalization:\")\n",
        "for i, band in enumerate(arr_norm):\n",
        "    print(f\"  Band {i+1}: {float(band.min()):.3f} to {float(band.max()):.3f}\")"
      ],
      "id": "fbe7643c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the normalization effect\n",
        "\n",
        "Let's examine how normalization changes the distribution of pixel values. This visualization helps us understand both what we've gained (comparable scales) and what we've preserved (relative patterns within each band).\n"
      ],
      "id": "3e026bf3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: true\n",
        "# Compare distributions before and after normalization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
        "\n",
        "for i in range(min(3, arr.shape[0])):\n",
        "    # Raw data histogram\n",
        "    axes[0, i].hist(arr[i].ravel(), bins=50, color='gray', alpha=0.7, edgecolor='black')\n",
        "    axes[0, i].set_title(f\"Band {i+1}: Raw DNs\")\n",
        "    axes[0, i].set_xlabel(\"Digital Number\")\n",
        "    axes[0, i].set_ylabel(\"Frequency\")\n",
        "    \n",
        "    # Normalized data histogram  \n",
        "    axes[1, i].hist(arr_norm[i].ravel(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "    axes[1, i].set_title(f\"Band {i+1}: Normalized [0,1]\")\n",
        "    axes[1, i].set_xlabel(\"Normalized Value\")\n",
        "    axes[1, i].set_ylabel(\"Frequency\")\n",
        "    axes[1, i].set_xlim(0, 1)  # Fix scale to show [0,1] range\n",
        "\n",
        "plt.suptitle(\"Distribution Comparison: Raw vs. Normalized\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "f1aa6c2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how the histograms show that while the scales have changed dramatically, the shape of each distribution‚Äîthe relative patterns of light and dark areas‚Äîremains exactly the same. We've made the bands comparable without losing the information that distinguishes different materials and land cover types.\n",
        "\n",
        ":::{.callout-tip}\n",
        "## Historgram equalization\n",
        "\n",
        "**What to notice**\n",
        "\n",
        "- The raw data shows very different scales across bands (different x-axis ranges)\n",
        "- After normalization, all bands span [0,1] but keep their unique distribution shapes\n",
        "- This transformation makes bands equally \"visible\" to the neural network while preserving the spectral signatures that matter for classification\n",
        ":::\n",
        "\n",
        "Why this matters: Proper normalization is essential for stable training and ensures all spectral bands contribute meaningfully to the model's understanding of the landscape.\n",
        "\n",
        ":::{.callout-tip}\n",
        "## Deeper Dive: Normalization Method Comparison\n",
        "For a comprehensive analysis of different normalization approaches used in state-of-the-art geospatial foundation models (including Prithvi, SatMAE, and Clay), see our detailed [Normalization Methods Comparison](../examples/normalization_comparison.qmd). This example compares computational performance, robustness to outliers, and practical trade-offs between five different normalization strategies.\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## 1.3 Tiling and patch extraction ‚úÇÔ∏è\n",
        "\n",
        "*This section covers the patch extraction step in our roadmap: splitting large images into manageable \"tokens\" for model processing.*\n",
        "\n",
        "### Why do we need patches?\n",
        "\n",
        "When working with satellite imagery, we face a fundamental challenge: **scale mismatch**. A typical satellite image might be 10,000√ó10,000 pixels or larger, but neural networks work best with much smaller, consistent input sizes. But there's more to it than just computational limits. Patches serve several critical functions in geospatial machine learning:\n",
        "\n",
        "**üß† Cognitive Focus**: Just as humans focus on local areas when interpreting landscapes, neural networks learn more effectively when they can concentrate on coherent spatial neighborhoods rather than trying to process vast areas simultaneously.\n",
        "\n",
        "**‚ö° Computational Efficiency**: Smaller patches fit in GPU memory and enable parallel processing. We can train on batches of patches rather than one massive image at a time.\n",
        "\n",
        "**üéØ Consistent Learning**: Fixed-size patches ensure the model sees consistent input dimensions, enabling it to learn spatial patterns at a specific scale.\n",
        "\n",
        "**üîÑ Data Augmentation**: From one large image, we can extract hundreds or thousands of training patches, dramatically increasing our dataset size.\n",
        "\n",
        "### The tokenization analogy\n",
        "\n",
        "The relationship between patches and tokenization runs deeper than you might first think. Let's explore this analogy:\n",
        "\n",
        "| Aspect | Text Processing | Geospatial Processing |\n",
        "|--------|----------------|----------------------|\n",
        "| **Raw Input** | Documents, articles, books | Large satellite images, map tiles |\n",
        "| **Atomic Unit** | Words/subwords ‚Üí tokens | Pixels ‚Üí patches |\n",
        "| **Why Split?** | Models can't process infinite text | Models can't process arbitrarily large images |\n",
        "| **Context Window** | Limited token context (e.g., 4K tokens) | Limited spatial context (e.g., 64√ó64 pixels) |\n",
        "| **Semantic Coherence** | Tokens preserve word meaning | Patches preserve local spatial patterns |\n",
        "| **Position Matters** | Word order affects meaning | Spatial arrangement affects interpretation |\n",
        "| **Overlapping Context** | Sliding windows for long documents | Overlapping patches for spatial continuity |\n",
        "\n",
        "### Visualizing the patch extraction process\n",
        "\n",
        "Now we'll implement the patch extraction step from our roadmap. This transforms our normalized image into a collection of spatial \"tokens\" that our model can process.\n",
        "\n",
        "### Understanding patch size trade-offs\n",
        "\n",
        "Before we extract patches, it's important to understand how patch size affects what our model can learn:\n",
        "\n",
        "| Patch Size | Pros | Cons | Best Use Cases |\n",
        "|------------|------|------|----------------|\n",
        "| **Small (16√ó16, 32√ó32)** | Fine detail, many samples, fast processing | Limited context, may miss large features | Urban analysis, crop monitoring |\n",
        "| **Medium (64√ó64, 128√ó128)** | Good balance of detail and context | Moderate computational cost | General purpose, land cover mapping |\n",
        "| **Large (256√ó256, 512√ó512)** | Rich spatial context, captures large features | Fewer samples, more memory intensive | Landscape analysis, climate modeling |\n",
        "\n",
        "For our exercise, we'll use 64√ó64 patches‚Äîa sweet spot that captures meaningful spatial patterns while remaining computationally manageable.\n",
        "\n",
        "### Step-by-step patch extraction\n",
        "\n",
        "Now let's implement patch extraction with detailed visualization of each step:"
      ],
      "id": "0a50db20"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# First, let's examine our normalized image dimensions\n",
        "C, H, W = arr_norm.shape\n",
        "print(f\"Input image shape: {C} bands √ó {H} height √ó {W} width pixels\")\n",
        "print(f\"Using patch size: {PATCH_SIZE}√ó{PATCH_SIZE} pixels\")\n",
        "\n",
        "# Calculate how many complete patches we can extract\n",
        "ph = H // PATCH_SIZE  # patches vertically\n",
        "pw = W // PATCH_SIZE  # patches horizontally\n",
        "print(f\"Complete patches: {ph} rows √ó {pw} columns = {ph * pw} total patches\")\n",
        "\n",
        "# Calculate the cropped dimensions (we may lose edge pixels)\n",
        "Hc, Wc = ph * PATCH_SIZE, pw * PATCH_SIZE\n",
        "print(f\"Cropped image size: {Hc}√ó{Wc} (original: {H}√ó{W})\")\n",
        "print(f\"Edge pixels lost: {H-Hc} height, {W-Wc} width\")"
      ],
      "id": "09c221d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Step 1: Crop the image to fit complete patches\n",
        "arr_c = arr_norm[:, :Hc, :Wc]\n",
        "print(f\"Cropped array shape: {arr_c.shape}\")\n",
        "\n",
        "# Step 2: Visualize the grid overlay on the original image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Show original image with patch grid overlay\n",
        "if C >= 3:\n",
        "    rgb = np.transpose(arr_norm[:3], (1, 2, 0))\n",
        "else:\n",
        "    rgb = np.stack([arr_norm[0]]*3, axis=-1)\n",
        "\n",
        "ax1.imshow(np.clip(rgb, 0, 1))\n",
        "ax1.set_title(\"Original Image with Patch Grid\")\n",
        "\n",
        "# Draw grid lines to show patch boundaries\n",
        "for i in range(ph + 1):\n",
        "    ax1.axhline(y=i * PATCH_SIZE - 0.5, color='red', linewidth=1, alpha=0.7)\n",
        "for j in range(pw + 1):\n",
        "    ax1.axvline(x=j * PATCH_SIZE - 0.5, color='red', linewidth=1, alpha=0.7)\n",
        "\n",
        "# Show cropped version\n",
        "if C >= 3:\n",
        "    rgb_cropped = np.transpose(arr_c[:3], (1, 2, 0))\n",
        "else:\n",
        "    rgb_cropped = np.stack([arr_c[0]]*3, axis=-1)\n",
        "\n",
        "ax2.imshow(np.clip(rgb_cropped, 0, 1))\n",
        "ax2.set_title(\"Cropped Image (Complete Patches Only)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "725f6180",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Step 3: The magical reshape operation that extracts patches\n",
        "# This is the core of patch extraction - let's break it down step by step\n",
        "\n",
        "print(\"üîÑ Patch extraction transformation:\")\n",
        "print(f\"1. Input shape: {arr_c.shape} ‚Üí (bands, height, width)\")\n",
        "\n",
        "# Reshape to separate patch rows and columns\n",
        "reshaped = arr_c.reshape(C, ph, PATCH_SIZE, pw, PATCH_SIZE)\n",
        "print(f\"2. After reshape: {reshaped.shape} ‚Üí (bands, patch_rows, patch_height, patch_cols, patch_width)\")\n",
        "\n",
        "# Transpose to group patches together\n",
        "transposed = reshaped.transpose(1, 3, 0, 2, 4)\n",
        "print(f\"3. After transpose: {transposed.shape} ‚Üí (patch_rows, patch_cols, bands, patch_height, patch_width)\")\n",
        "\n",
        "# Final reshape to get individual patches\n",
        "patches = transposed.reshape(ph * pw, C, PATCH_SIZE, PATCH_SIZE)\n",
        "print(f\"4. Final patches: {patches.shape} ‚Üí (num_patches, bands, patch_height, patch_width)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Successfully extracted {patches.shape[0]} patches!\")"
      ],
      "id": "c88946dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Step 4: Visualize sample patches to see what we've extracted\n",
        "nshow = min(6, len(patches))\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(nshow):\n",
        "    p = patches[i]\n",
        "    \n",
        "    # Create RGB visualization (handle cases with fewer than 3 bands)\n",
        "    if C >= 3:\n",
        "        rgb = np.transpose(p[:3], (1, 2, 0))\n",
        "    else:\n",
        "        rgb = np.stack([p[0]]*3, axis=-1)\n",
        "    \n",
        "    axes[i].imshow(np.clip(rgb, 0, 1))\n",
        "    \n",
        "    # Calculate patch position in the grid\n",
        "    row = i // pw\n",
        "    col = i % pw\n",
        "    axes[i].set_title(f\"Patch {i}\\nGrid position: ({row}, {col})\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle(\"Sample Patches: Our Image 'Tokens'\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "70a37c8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding what we've created\n",
        "\n",
        "Each patch is now a self-contained \"token\" that represents a local region of our satellite image. Just like how words in a sentence carry meaning individually but also depend on context, these patches contain local spatial patterns while maintaining relationships to their neighboring patches.\n",
        "\n",
        ":::{.callout-note}\n",
        "## Key Transformation\n",
        "We've transformed one large image `(3, H, W)` into `{ph √ó pw}` smaller images, each of shape `(3, 64, 64)`. This is exactly analogous to how tokenization transforms one long text into many smaller tokens that a model can process efficiently.\n",
        ":::\n",
        "\n",
        "Why this matters: Patches become our fundamental unit of analysis‚Äîeach patch can be processed independently while preserving spatial relationships for the model to learn landscape patterns.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 1.4 Assigning patch IDs & metadata üè∑Ô∏è\n",
        "\n",
        "*This section covers the metadata step in our roadmap: giving each patch an identity and spatial context.*\n",
        "\n",
        "\"Every patch needs an identity ‚Äî not just for bookkeeping, but to remember where and when it came from.\"\n",
        "\n",
        "- Generate unique IDs for patches.\n",
        "- Store spatial metadata (bounding box, CRS) and acquisition info.\n",
        "- Print one sample metadata record.\n"
      ],
      "id": "934da015"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from rasterio.transform import xy\n",
        "\n",
        "# Compute patch bounding boxes (min lon/lat, max lon/lat approx)\n",
        "ids = []\n",
        "meta = []\n",
        "idx = 0\n",
        "for i in range(ph):\n",
        "    for j in range(pw):\n",
        "        pid = f\"patch_{i:03d}_{j:03d}\"\n",
        "        # top-left pixel (row=i*P, col=j*P)\n",
        "        r0, c0 = i*PATCH_SIZE, j*PATCH_SIZE\n",
        "        r1, c1 = r0 + PATCH_SIZE - 1, j * PATCH_SIZE + PATCH_SIZE - 1\n",
        "        (x0, y0) = xy(transform, r0, c0)\n",
        "        (x1, y1) = xy(transform, r1, c1)\n",
        "        bbox = (min(x0,x1), min(y0,y1), max(x0,x1), max(y0,y1))\n",
        "        ids.append(pid)\n",
        "        meta.append({\"id\": pid, \"bbox\": bbox, \"crs\": str(crs)})\n",
        "        idx += 1\n",
        "\n",
        "print(\"total ids:\", len(ids))\n",
        "print(\"sample record:\", meta[0])"
      ],
      "id": "7aebeabd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why this matters: metadata lets us track location/time and avoid leakage when splitting data.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.5 Adding special context tokens (location, time, sensor) üåç\n",
        "\n",
        "*This section covers the spatial-temporal context step in our roadmap: encoding where and when each patch was captured.*\n",
        "\n",
        "\"In addition to pixel values, our model benefits from knowing where and when the data was collected, and by which sensor.\"\n",
        "\n",
        "- Encode lat/lon as continuous features or sinusoidal embeddings.\n",
        "- Encode acquisition date as day‚Äëof‚Äëyear + year.\n",
        "- Show how these features attach to patches.\n"
      ],
      "id": "9f779143"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import datetime as dt\n",
        "\n",
        "# Example acquisition date (placeholder metadata)\n",
        "acq_date = dt.date(2023, 7, 15)\n",
        "doy = acq_date.timetuple().tm_yday\n",
        "year = acq_date.year\n",
        "\n",
        "# Use patch centers for location features\n",
        "centers = []\n",
        "for i in range(ph):\n",
        "    for j in range(pw):\n",
        "        r, c = i*PATCH_SIZE + PATCH_SIZE//2, j*PATCH_SIZE + PATCH_SIZE//2\n",
        "        x_c, y_c = xy(transform, r, c)\n",
        "        centers.append((x_c, y_c))\n",
        "centers = np.array(centers, dtype=np.float32)\n",
        "\n",
        "# Simple scaled features (you can swap for sinusoidal if desired)\n",
        "loc_feat = (centers - centers.mean(0)) / (centers.std(0) + 1e-6)\n",
        "time_feat = np.array([doy/366.0, (year-2000)/50.0], dtype=np.float32)  # scale to ~[0,1]\n",
        "print(\"loc_feat shape:\", loc_feat.shape, \"time_feat:\", time_feat.tolist())"
      ],
      "id": "2ebfa706",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why this matters: models need context to learn spatial/temporal patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.6 Dimensionality reduction for spectral bands (analogy to BPE)\n",
        "\n",
        "‚ÄúSometimes we have more bands than the model needs. We can reduce the input size while keeping the most useful information.‚Äù\n",
        "\n",
        "- Demonstrate PCA or band selection.\n",
        "- Compare original vs. reduced band profiles.\n"
      ],
      "id": "2ac75867"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# PCA via SVD on (pixels, bands) for speed; sample a subset of pixels\n",
        "C, Hc, Wc = arr_c.shape\n",
        "pixels = arr_c.reshape(C, -1).T  # (N, C)\n",
        "N = pixels.shape[0]\n",
        "sub = min(5000, N)\n",
        "idx = np.random.default_rng(RNG_SEED).choice(N, sub, replace=False)\n",
        "Xsub = pixels[idx]\n",
        "\n",
        "# Center\n",
        "mu = Xsub.mean(0, keepdims=True)\n",
        "Xc = Xsub - mu\n",
        "U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
        "explained = (S**2) / (S**2).sum()\n",
        "keep = min(3, C)\n",
        "Wred = Vt[:keep].T  # (C, keep)\n",
        "\n",
        "print(\"bands:\", C, \"keep:\", keep, \"explained_var@keep:\", float(explained[:keep].sum()))\n",
        "\n",
        "# Project full image to reduced components per pixel\n",
        "red_pixels = (pixels - mu).dot(Wred)  # (N, keep)\n",
        "red_stack = red_pixels.T.reshape(keep, Hc, Wc)\n",
        "print(\"reduced stack shape:\", red_stack.shape)"
      ],
      "id": "c36bc25d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why this matters: reducing input size speeds training and may denoise bands.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.7 Sampling training patches (sliding windows, spatial splits)\n",
        "\n",
        "‚ÄúRather than using the whole dataset at once, we‚Äôll sample patches in a way that keeps training efficient and avoids data leakage.‚Äù\n",
        "\n",
        "- Show random vs. sliding‚Äëwindow sampling.\n",
        "- Highlight spatial/temporal separation for validation/test.\n"
      ],
      "id": "40b4d1e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "num_patches = patches.shape[0]\n",
        "all_idx = np.arange(num_patches)\n",
        "# Simple spatial split by rows: top 80% train, bottom 20% val\n",
        "rows = np.repeat(np.arange(ph), pw)\n",
        "train_mask = rows < int(0.8*ph)\n",
        "train_idx = all_idx[train_mask]\n",
        "val_idx = all_idx[~train_mask]\n",
        "\n",
        "rng = np.random.default_rng(RNG_SEED)\n",
        "rng.shuffle(train_idx)\n",
        "print(\"num_patches:\", num_patches, \"train:\", len(train_idx), \"val:\", len(val_idx))\n",
        "print(\"example train idx:\", train_idx[:8].tolist())"
      ],
      "id": "1a00b605",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why this matters: proper splits reduce spatial leakage and inflated metrics.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.8 Creating patch embeddings üß†\n",
        "\n",
        "*This section covers the patch embeddings step in our roadmap: transforming spatial patches into the vector space where models operate.*\n",
        "\n",
        "\"This is where pixels meet the model: we project each patch into an embedding space where the model can learn patterns.\"\n",
        "\n",
        "- Flatten + linear layer (minimal version).\n",
        "- Print embedding tensor shapes.\n",
        "- Show a few example embedding vectors.\n"
      ],
      "id": "b78646c0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(RNG_SEED)\n",
        "    B, C, P, _ = patches.shape\n",
        "    input_dim = C * P * P\n",
        "    embed_dim = 64\n",
        "    proj = torch.nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "    # take a tiny batch of patches\n",
        "    sample = torch.from_numpy(patches[:4]).reshape(4, -1).float()\n",
        "    emb = proj(sample)\n",
        "    print(\"input_dim:\", input_dim, \"embed_dim:\", embed_dim, \"emb shape:\", tuple(emb.shape))\n",
        "    print(\"first row (rounded):\", [float(x) for x in emb[0][:8].detach().numpy().round(3)])\n",
        "except Exception as e:\n",
        "    print(\"Torch not available; skipping embeddings\", e)"
      ],
      "id": "79fd8fe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why this matters: embeddings are the model‚Äôs operating space for learning patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## 1.9 Encoding spatial & temporal positions üìç\n",
        "\n",
        "*This section covers the final step in our roadmap: adding positional encodings so models understand spatial and temporal relationships.*\n",
        "\n",
        "\"Models don't know where a patch sits in space or time unless we tell them ‚Äî positional encodings add this crucial context.\"\n",
        "\n",
        "- Create 2D sinusoidal spatial encodings (row/col).\n",
        "- Add temporal encodings (day‚Äëof‚Äëyear).\n",
        "- Combine with patch embeddings and inspect results.\n"
      ],
      "id": "c6bc4575"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# 2D sinusoidal positional encodings for (row, col)\n",
        "rows_grid = np.arange(ph)[:,None].repeat(pw,1)\n",
        "cols_grid = np.arange(pw)[None,:].repeat(ph,0)\n",
        "pos2d = np.stack([rows_grid.ravel(), cols_grid.ravel()], axis=1).astype(np.float32)\n",
        "\n",
        "# Scale rows/cols to [0,1]\n",
        "pos2d = (pos2d - pos2d.min(0)) / (pos2d.ptp(0) + 1e-6)\n",
        "\n",
        "# Simple sin/cos features\n",
        "pos_feat = np.concatenate([np.sin(2*np.pi*pos2d), np.cos(2*np.pi*pos2d)], axis=1)\n",
        "# time features from earlier (same for all patches here)\n",
        "time_feat_full = np.repeat(time_feat[None,:], len(pos_feat), axis=0)\n",
        "\n",
        "print(\"pos_feat shape:\", pos_feat.shape, \"time_feat_full shape:\", time_feat_full.shape)"
      ],
      "id": "385c94c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Combine: embedding + positional + time features (toy example)\n",
        "try:\n",
        "    comb = np.concatenate([\n",
        "        emb.detach().numpy(),         # (4, 64) toy batch\n",
        "        pos_feat[:4],                 # (4, 4)\n",
        "        time_feat_full[:4]            # (4, 2)\n",
        "    ], axis=1)\n",
        "    print(\"combined features shape:\", comb.shape)\n",
        "except Exception as e:\n",
        "    print(\"Combine skipped:\", e)"
      ],
      "id": "ebaca8fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why this matters: spatial/temporal encodings provide essential context alongside pixel content.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "- We inspected raster metadata (CRS, resolution, bands) and normalized bands\n",
        "- We extracted fixed‚Äësize patches, attached IDs/metadata, and added location/time features\n",
        "- We created minimal patch embeddings and combined them with positional/temporal encodings\n",
        "\n",
        "## Resources\n",
        "- Quarto: https://quarto.org/docs\n",
        "- Rasterio: https://rasterio.readthedocs.io/\n",
        "- NumPy: https://numpy.org/doc/\n",
        "- PyTorch: https://pytorch.org/docs/stable/index.html\n",
        "- Vision Transformers (ViT): https://arxiv.org/abs/2010.11929"
      ],
      "id": "31f3e090"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "geoai",
      "language": "python",
      "display_name": "GeoAI",
      "path": "/Users/kellycaylor/Library/Jupyter/kernels/geoai"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}