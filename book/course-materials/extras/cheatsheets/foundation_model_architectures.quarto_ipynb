{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Foundation Model Architectures\"\n",
        "subtitle: \"LLMs vs. Geospatial Foundation Models (GFMs)\"\n",
        "jupyter: geoai\n",
        "format: html\n",
        "---\n",
        "\n",
        "\n",
        "## Introduction to Foundation Model Architectures\n",
        "\n",
        "Foundation models are large-scale models trained on diverse data that can be adapted to various downstream tasks. This cheatsheet compares Language Model (LLM) and Geospatial Foundation Model (GFM) development pipelines.\n"
      ],
      "id": "d4ddda15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModel, AutoConfig\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "id": "307dc07a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evolution from AI to Transformers\n",
        "\n",
        "### Key Historical Milestones"
      ],
      "id": "859559d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Timeline of key developments\n",
        "timeline = {\n",
        "    \"1950s-1990s\": \"Symbolic AI, early neural networks\",\n",
        "    \"2012\": \"Deep learning breakthrough (ImageNet)\",\n",
        "    \"2017\": \"Transformers ('Attention Is All You Need')\",\n",
        "    \"2018-2020\": \"BERT/GPT families emerge\",\n",
        "    \"2021-2024\": \"Scaling laws, instruction tuning, multimodality\"\n",
        "}\n",
        "\n",
        "print(\"AI/ML â†’ Transformers Timeline:\")\n",
        "for year, development in timeline.items():\n",
        "    print(f\"{year}: {development}\")"
      ],
      "id": "7d6e26c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Architecture Essentials"
      ],
      "id": "826d63bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SimpleTransformerBlock(nn.Module):\n",
        "    \"\"\"Simplified transformer block to illustrate key components\"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim=768, num_heads=8, mlp_ratio=4):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        # Multi-head attention\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        \n",
        "        # Feed-forward network\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim)\n",
        "        )\n",
        "        \n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        attn_out, _ = self.attention(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        \n",
        "        # MLP with residual connection\n",
        "        mlp_out = self.mlp(x)\n",
        "        x = self.norm2(x + mlp_out)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Example transformer block\n",
        "transformer_block = SimpleTransformerBlock(embed_dim=768, num_heads=8)\n",
        "sample_input = torch.randn(2, 100, 768)  # [batch_size, seq_len, embed_dim]\n",
        "output = transformer_block(sample_input)\n",
        "\n",
        "print(f\"Input shape: {sample_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in transformer_block.parameters()):,}\")"
      ],
      "id": "20d31ded",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9-Step Development Pipeline Comparison\n",
        "\n",
        "### LLM Development Pipeline"
      ],
      "id": "b4778038"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "llm_pipeline = {\n",
        "    \"1. Data Preparation\": \"Text corpora, deduplication, quality filtering, mixing ratios\",\n",
        "    \"2. Tokenization\": \"BPE, vocabulary construction, special tokens\",\n",
        "    \"3. Architecture\": \"GPT/BERT variants, depth/width scaling, context length\",\n",
        "    \"4. Pretraining Objective\": \"Next-token prediction, masked language modeling\",\n",
        "    \"5. Training Loop\": \"Optimizers, LR schedules, mixed precision, gradient clipping\",\n",
        "    \"6. Evaluation\": \"Perplexity, downstream task probing, benchmarks\",\n",
        "    \"7. Pretrained Weights\": \"Model hubs, tokenizer alignment, loading utilities\",\n",
        "    \"8. Finetuning\": \"Task-specific heads, PEFT methods, instruction tuning\",\n",
        "    \"9. Deployment\": \"API serving, KV caching, inference optimization\"\n",
        "}\n",
        "\n",
        "print(\"LLM Development Pipeline:\")\n",
        "for step, description in llm_pipeline.items():\n",
        "    print(f\"{step}: {description}\")"
      ],
      "id": "225cbe43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GFM Development Pipeline"
      ],
      "id": "3ff7fe49"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gfm_pipeline = {\n",
        "    \"1. Data Preparation\": \"Multi-spectral data, georegistration, tiling, cloud masking\",\n",
        "    \"2. Tokenization\": \"Patch-based, continuous embeddings, 2D/temporal positions\",\n",
        "    \"3. Architecture\": \"ViT encoders, spatial/temporal attention, memory constraints\",\n",
        "    \"4. Pretraining Objective\": \"Masked patch reconstruction, contrastive learning\",\n",
        "    \"5. Training Loop\": \"Cloud masks, mixed precision, gradient accumulation\",\n",
        "    \"6. Evaluation\": \"Reconstruction metrics, linear probing, generalization\",\n",
        "    \"7. Pretrained Weights\": \"Prithvi, SatMAE, adapter loading, band alignment\",\n",
        "    \"8. Finetuning\": \"Task heads, PEFT, few-shot learning for limited labels\",\n",
        "    \"9. Deployment\": \"Tiling inference, geospatial APIs, batch processing\"\n",
        "}\n",
        "\n",
        "print(\"\\nGFM Development Pipeline:\")\n",
        "for step, description in gfm_pipeline.items():\n",
        "    print(f\"{step}: {description}\")"
      ],
      "id": "be95e922",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step-by-Step Detailed Comparison\n",
        "\n",
        "### 1. Data Preparation Differences"
      ],
      "id": "3ad14984"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demonstrate_data_preparation():\n",
        "    \"\"\"Show key differences in data preparation\"\"\"\n",
        "    \n",
        "    # LLM data preparation simulation\n",
        "    print(\"LLM Data Preparation:\")\n",
        "    print(\"- Text scraping from web, books, code\")\n",
        "    print(\"- Deduplication algorithms\")\n",
        "    print(\"- Quality filtering (language detection, toxicity)\")\n",
        "    print(\"- Data mixing ratios optimization\")\n",
        "    \n",
        "    # Simulate text preprocessing\n",
        "    sample_texts = [\n",
        "        \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"Machine learning is transforming many industries.\",\n",
        "        \"Climate change requires urgent global action.\"\n",
        "    ]\n",
        "    \n",
        "    # Basic tokenization simulation\n",
        "    vocab = set()\n",
        "    for text in sample_texts:\n",
        "        vocab.update(text.lower().replace('.', '').split())\n",
        "    \n",
        "    print(f\"\\nSample vocabulary size: {len(vocab)}\")\n",
        "    print(f\"Sample tokens: {list(vocab)[:10]}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    \n",
        "    # GFM data preparation simulation\n",
        "    print(\"GFM Data Preparation:\")\n",
        "    print(\"- Sensor calibration and atmospheric correction\")\n",
        "    print(\"- Georegistration and projection alignment\")\n",
        "    print(\"- Cloud masking and quality assessment\")\n",
        "    print(\"- Temporal compositing and gap filling\")\n",
        "    \n",
        "    # Simulate satellite data preprocessing\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Simulate multi-spectral satellite patch\n",
        "    patch_size = 64\n",
        "    num_bands = 6\n",
        "    satellite_patch = np.random.randint(0, 4096, (num_bands, patch_size, patch_size))\n",
        "    \n",
        "    # Simulate cloud mask\n",
        "    cloud_mask = np.random.random((patch_size, patch_size)) > 0.8\n",
        "    \n",
        "    # Apply atmospheric correction (simplified)\n",
        "    corrected_patch = satellite_patch.astype(np.float32) / 4095.0\n",
        "    corrected_patch[cloud_mask] = np.nan  # Mask cloudy pixels\n",
        "    \n",
        "    print(f\"\\nSatellite patch shape: {satellite_patch.shape}\")\n",
        "    print(f\"Cloud coverage: {cloud_mask.sum() / cloud_mask.size * 100:.1f}%\")\n",
        "    print(f\"Valid pixels: {(~np.isnan(corrected_patch[0])).sum():,}\")\n",
        "\n",
        "demonstrate_data_preparation()"
      ],
      "id": "aaad5e12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Tokenization Approaches"
      ],
      "id": "99efd55e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compare_tokenization():\n",
        "    \"\"\"Compare LLM vs GFM tokenization approaches\"\"\"\n",
        "    \n",
        "    print(\"LLM Tokenization (Discrete):\")\n",
        "    # Simulate BPE tokenization\n",
        "    vocab_size, embed_dim = 50000, 768\n",
        "    \n",
        "    # Sample token sequence\n",
        "    token_ids = torch.tensor([1, 15, 234, 5678, 2])  # [CLS, word1, word2, word3, SEP]\n",
        "    \n",
        "    # Embedding lookup\n",
        "    embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "    token_embeddings = embedding_layer(token_ids)\n",
        "    \n",
        "    print(f\"Token IDs: {token_ids.tolist()}\")\n",
        "    print(f\"Token embeddings shape: {token_embeddings.shape}\")\n",
        "    print(f\"Vocabulary size: {vocab_size:,}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*30)\n",
        "    \n",
        "    print(\"GFM Tokenization (Continuous Patches):\")\n",
        "    # Simulate patch-based tokenization\n",
        "    patch_size = 16\n",
        "    num_bands = 6\n",
        "    embed_dim = 768\n",
        "    \n",
        "    # Create sample patches\n",
        "    num_patches = 4\n",
        "    patch_dim = patch_size * patch_size * num_bands\n",
        "    patches = torch.randn(num_patches, patch_dim)\n",
        "    \n",
        "    # Linear projection (continuous \"tokenization\")\n",
        "    patch_projection = nn.Linear(patch_dim, embed_dim)\n",
        "    patch_embeddings = patch_projection(patches)\n",
        "    \n",
        "    print(f\"Patch dimensions: {patch_size}x{patch_size}x{num_bands} = {patch_dim}\")\n",
        "    print(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n",
        "    print(f\"No discrete vocabulary - continuous projection\")\n",
        "    \n",
        "    return token_embeddings, patch_embeddings\n",
        "\n",
        "token_emb, patch_emb = compare_tokenization()"
      ],
      "id": "d235ee82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Architecture Comparison"
      ],
      "id": "ea096654"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class LLMArchitecture(nn.Module):\n",
        "    \"\"\"Simplified LLM architecture (GPT-style)\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size=50000, embed_dim=768, num_layers=12, num_heads=12):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = nn.Embedding(2048, embed_dim)  # Max sequence length\n",
        "        \n",
        "        self.layers = nn.ModuleList([\n",
        "            SimpleTransformerBlock(embed_dim, num_heads) \n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.ln_final = nn.LayerNorm(embed_dim)\n",
        "        self.output_head = nn.Linear(embed_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, input_ids):\n",
        "        seq_len = input_ids.shape[1]\n",
        "        positions = torch.arange(seq_len, device=input_ids.device)\n",
        "        \n",
        "        # Token + positional embeddings\n",
        "        x = self.embedding(input_ids) + self.positional_encoding(positions)\n",
        "        \n",
        "        # Transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        \n",
        "        x = self.ln_final(x)\n",
        "        logits = self.output_head(x)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "class GFMArchitecture(nn.Module):\n",
        "    \"\"\"Simplified GFM architecture (ViT-style)\"\"\"\n",
        "    \n",
        "    def __init__(self, patch_size=16, num_bands=6, embed_dim=768, num_layers=12, num_heads=12):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_bands = num_bands\n",
        "        \n",
        "        # Patch embedding\n",
        "        patch_dim = patch_size * patch_size * num_bands\n",
        "        self.patch_embedding = nn.Linear(patch_dim, embed_dim)\n",
        "        \n",
        "        # 2D positional embedding\n",
        "        self.pos_embed_h = nn.Embedding(100, embed_dim // 2)  # Height positions\n",
        "        self.pos_embed_w = nn.Embedding(100, embed_dim // 2)  # Width positions\n",
        "        \n",
        "        self.layers = nn.ModuleList([\n",
        "            SimpleTransformerBlock(embed_dim, num_heads) \n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.ln_final = nn.LayerNorm(embed_dim)\n",
        "    \n",
        "    def forward(self, patches, patch_positions):\n",
        "        batch_size, num_patches, patch_dim = patches.shape\n",
        "        \n",
        "        # Patch embeddings\n",
        "        x = self.patch_embedding(patches)\n",
        "        \n",
        "        # 2D positional embeddings\n",
        "        pos_h, pos_w = patch_positions[:, :, 0], patch_positions[:, :, 1]\n",
        "        pos_emb = torch.cat([\n",
        "            self.pos_embed_h(pos_h),\n",
        "            self.pos_embed_w(pos_w)\n",
        "        ], dim=-1)\n",
        "        \n",
        "        x = x + pos_emb\n",
        "        \n",
        "        # Transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        \n",
        "        x = self.ln_final(x)\n",
        "        return x\n",
        "\n",
        "# Compare architectures\n",
        "llm_model = LLMArchitecture(vocab_size=10000, embed_dim=384, num_layers=6, num_heads=6)\n",
        "gfm_model = GFMArchitecture(patch_size=16, num_bands=6, embed_dim=384, num_layers=6, num_heads=6)\n",
        "\n",
        "llm_params = sum(p.numel() for p in llm_model.parameters())\n",
        "gfm_params = sum(p.numel() for p in gfm_model.parameters())\n",
        "\n",
        "print(\"Architecture Comparison:\")\n",
        "print(f\"LLM parameters: {llm_params:,}\")\n",
        "print(f\"GFM parameters: {gfm_params:,}\")\n",
        "\n",
        "# Test forward passes\n",
        "sample_tokens = torch.randint(0, 10000, (2, 50))  # [batch_size, seq_len]\n",
        "sample_patches = torch.randn(2, 16, 16*16*6)  # [batch_size, num_patches, patch_dim]\n",
        "sample_positions = torch.randint(0, 10, (2, 16, 2))  # [batch_size, num_patches, 2]\n",
        "\n",
        "llm_output = llm_model(sample_tokens)\n",
        "gfm_output = gfm_model(sample_patches, sample_positions)\n",
        "\n",
        "print(f\"\\nLLM output shape: {llm_output.shape}\")\n",
        "print(f\"GFM output shape: {gfm_output.shape}\")"
      ],
      "id": "0474a9cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Pretraining Objectives"
      ],
      "id": "823df152"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demonstrate_pretraining_objectives():\n",
        "    \"\"\"Compare pretraining objectives for LLMs vs GFMs\"\"\"\n",
        "    \n",
        "    print(\"LLM Pretraining Objectives:\")\n",
        "    print(\"1. Next-Token Prediction (GPT-style)\")\n",
        "    print(\"2. Masked Language Modeling (BERT-style)\")\n",
        "    \n",
        "    # Simulate next-token prediction\n",
        "    sequence = torch.tensor([[1, 2, 3, 4, 5]])\n",
        "    targets = torch.tensor([[2, 3, 4, 5, 6]])  # Shifted by one\n",
        "    \n",
        "    # Mock logits\n",
        "    vocab_size = 1000\n",
        "    logits = torch.randn(1, 5, vocab_size)\n",
        "    \n",
        "    # Cross-entropy loss\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    next_token_loss = ce_loss(logits.view(-1, vocab_size), targets.view(-1))\n",
        "    \n",
        "    print(f\"Next-token prediction loss: {next_token_loss.item():.4f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*30)\n",
        "    \n",
        "    print(\"GFM Pretraining Objectives:\")\n",
        "    print(\"1. Masked Patch Reconstruction (MAE-style)\")\n",
        "    print(\"2. Contrastive Learning (optional)\")\n",
        "    \n",
        "    # Simulate masked patch reconstruction\n",
        "    batch_size, num_patches, patch_dim = 2, 64, 768\n",
        "    original_patches = torch.randn(batch_size, num_patches, patch_dim)\n",
        "    \n",
        "    # Random masking\n",
        "    mask_ratio = 0.75\n",
        "    num_masked = int(num_patches * mask_ratio)\n",
        "    \n",
        "    # Create random mask\n",
        "    mask = torch.zeros(batch_size, num_patches, dtype=torch.bool)\n",
        "    for i in range(batch_size):\n",
        "        masked_indices = torch.randperm(num_patches)[:num_masked]\n",
        "        mask[i, masked_indices] = True\n",
        "    \n",
        "    # Reconstruction loss (simplified)\n",
        "    reconstructed_patches = torch.randn_like(original_patches)\n",
        "    reconstruction_loss = nn.MSELoss()(\n",
        "        reconstructed_patches[mask], \n",
        "        original_patches[mask]\n",
        "    )\n",
        "    \n",
        "    print(f\"Mask ratio: {mask_ratio:.1%}\")\n",
        "    print(f\"Masked patches per sample: {num_masked}\")\n",
        "    print(f\"Reconstruction loss: {reconstruction_loss.item():.4f}\")\n",
        "\n",
        "demonstrate_pretraining_objectives()"
      ],
      "id": "8de525d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scaling and Evolution\n",
        "\n",
        "### Parameter Scaling Comparison"
      ],
      "id": "39a83b2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compare_scaling_trends():\n",
        "    \"\"\"Compare scaling trends between LLMs and GFMs\"\"\"\n",
        "    \n",
        "    # LLM scaling milestones\n",
        "    llm_milestones = {\n",
        "        'GPT-1': 117e6,\n",
        "        'BERT-Base': 110e6,\n",
        "        'GPT-2': 1.5e9,\n",
        "        'GPT-3': 175e9,\n",
        "        'PaLM': 540e9,\n",
        "        'GPT-4': 1000e9  # Estimated\n",
        "    }\n",
        "    \n",
        "    # GFM scaling examples\n",
        "    gfm_milestones = {\n",
        "        'SatMAE-Base': 86e6,\n",
        "        'Prithvi-100M': 100e6,\n",
        "        'Clay-v0.1': 139e6,\n",
        "        'SatLas-Base': 300e6,\n",
        "        'Scale-MAE': 600e6\n",
        "    }\n",
        "    \n",
        "    # Visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # LLM scaling\n",
        "    models = list(llm_milestones.keys())\n",
        "    params = [llm_milestones[m]/1e9 for m in models]\n",
        "    \n",
        "    ax1.bar(models, params, color='skyblue', alpha=0.7)\n",
        "    ax1.set_yscale('log')\n",
        "    ax1.set_ylabel('Parameters (Billions)')\n",
        "    ax1.set_title('LLM Parameter Scaling')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # GFM scaling\n",
        "    models = list(gfm_milestones.keys())\n",
        "    params = [gfm_milestones[m]/1e6 for m in models]\n",
        "    \n",
        "    ax2.bar(models, params, color='lightcoral', alpha=0.7)\n",
        "    ax2.set_ylabel('Parameters (Millions)')\n",
        "    ax2.set_title('GFM Parameter Scaling')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Context/Input scaling\n",
        "    print(\"Context/Input Scaling:\")\n",
        "    print(\"\\nLLMs:\")\n",
        "    print(\"- Context length: 512 â†’ 2K â†’ 8K â†’ 128K+ tokens\")\n",
        "    print(\"- Training data: Web text, books, code (curated)\")\n",
        "    print(\"- Focus: Language understanding and generation\")\n",
        "    \n",
        "    print(\"\\nGFMs:\")\n",
        "    print(\"- Input bands: 3 (RGB) â†’ 6+ (multispectral) â†’ hyperspectral\")\n",
        "    print(\"- Spatial resolution: Various (10m to 0.3m)\")\n",
        "    print(\"- Temporal dimension: Single â†’ time series â†’ multi-temporal\")\n",
        "    print(\"- Focus: Earth observation and environmental monitoring\")\n",
        "\n",
        "compare_scaling_trends()"
      ],
      "id": "0c49ba1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Requirements and Constraints"
      ],
      "id": "fe87ca2e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compare_data_requirements():\n",
        "    \"\"\"Compare data requirements and constraints\"\"\"\n",
        "    \n",
        "    comparison = {\n",
        "        \"Data Volume\": {\n",
        "            \"LLMs\": \"Terabytes of text (web crawls, books, code)\",\n",
        "            \"GFMs\": \"Petabytes of satellite imagery (constrained by storage/IO)\"\n",
        "        },\n",
        "        \"Data Quality\": {\n",
        "            \"LLMs\": \"Deduplication, toxicity filtering, language detection\",\n",
        "            \"GFMs\": \"Cloud masking, atmospheric correction, sensor calibration\"\n",
        "        },\n",
        "        \"Preprocessing\": {\n",
        "            \"LLMs\": \"Tokenization, sequence packing, attention masks\",\n",
        "            \"GFMs\": \"Patch extraction, normalization, spatial alignment\"\n",
        "        },\n",
        "        \"Storage Format\": {\n",
        "            \"LLMs\": \"Compressed text files, tokenized sequences\",\n",
        "            \"GFMs\": \"Cloud-optimized formats (COG, Zarr), tiled storage\"\n",
        "        },\n",
        "        \"Access Patterns\": {\n",
        "            \"LLMs\": \"Sequential text processing, random sampling\",\n",
        "            \"GFMs\": \"Spatial/temporal queries, patch sampling, tiling\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(\"Data Requirements Comparison:\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    for aspect, details in comparison.items():\n",
        "        print(f\"\\n{aspect}:\")\n",
        "        print(f\"  LLMs: {details['LLMs']}\")\n",
        "        print(f\"  GFMs: {details['GFMs']}\")\n",
        "\n",
        "compare_data_requirements()"
      ],
      "id": "68d88e3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation Examples\n",
        "\n",
        "### Embedding Creation"
      ],
      "id": "0770c391"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demonstrate_embeddings():\n",
        "    \"\"\"Show embedding creation for both domains\"\"\"\n",
        "    \n",
        "    print(\"Text Embeddings (LLM):\")\n",
        "    # Simulate text tokenization and embedding\n",
        "    text = \"The forest shows signs of deforestation.\"\n",
        "    tokens = text.lower().replace('.', '').split()\n",
        "    \n",
        "    # Create simple vocabulary\n",
        "    vocab = {word: i for i, word in enumerate(set(tokens))}\n",
        "    vocab['<PAD>'] = len(vocab)\n",
        "    vocab['<UNK>'] = len(vocab)\n",
        "    \n",
        "    # Convert tokens to IDs\n",
        "    token_ids = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "    token_tensor = torch.tensor(token_ids).unsqueeze(0)\n",
        "    \n",
        "    # Embedding layer\n",
        "    embed_layer = nn.Embedding(len(vocab), 256)\n",
        "    text_embeddings = embed_layer(token_tensor)\n",
        "    \n",
        "    print(f\"Original text: {text}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Token IDs: {token_ids}\")\n",
        "    print(f\"Embeddings shape: {text_embeddings.shape}\")\n",
        "    \n",
        "    print(\"\\n\" + \"-\"*40)\n",
        "    \n",
        "    print(\"Patch Embeddings (GFM):\")\n",
        "    # Simulate satellite patch processing\n",
        "    patch_size = 16\n",
        "    num_bands = 6\n",
        "    \n",
        "    # Create sample satellite patch\n",
        "    satellite_patch = torch.randn(1, num_bands, patch_size, patch_size)\n",
        "    \n",
        "    # Reshape to patch format\n",
        "    patch_flat = satellite_patch.view(1, num_bands * patch_size * patch_size)\n",
        "    \n",
        "    # Linear projection to embedding space\n",
        "    patch_projection = nn.Linear(num_bands * patch_size * patch_size, 256)\n",
        "    patch_embeddings = patch_projection(patch_flat)\n",
        "    \n",
        "    print(f\"Original patch shape: {satellite_patch.shape}\")\n",
        "    print(f\"Flattened patch shape: {patch_flat.shape}\")\n",
        "    print(f\"Patch embeddings shape: {patch_embeddings.shape}\")\n",
        "    \n",
        "    return text_embeddings, patch_embeddings\n",
        "\n",
        "text_emb, patch_emb = demonstrate_embeddings()"
      ],
      "id": "c31cb6d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional Encoding Comparison"
      ],
      "id": "71ccf3ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compare_positional_encodings():\n",
        "    \"\"\"Compare positional encoding strategies\"\"\"\n",
        "    \n",
        "    print(\"1D Positional Encoding (LLM):\")\n",
        "    \n",
        "    def sinusoidal_positional_encoding(seq_len, embed_dim):\n",
        "        \"\"\"Create sinusoidal positional encodings\"\"\"\n",
        "        pe = torch.zeros(seq_len, embed_dim)\n",
        "        position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
        "        \n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * \n",
        "                           -(np.log(10000.0) / embed_dim))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        return pe\n",
        "    \n",
        "    seq_len, embed_dim = 100, 256\n",
        "    pos_encoding_1d = sinusoidal_positional_encoding(seq_len, embed_dim)\n",
        "    \n",
        "    print(f\"1D positional encoding shape: {pos_encoding_1d.shape}\")\n",
        "    \n",
        "    # Visualize positional encoding\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(pos_encoding_1d[:50, :50].T, cmap='RdBu', aspect='auto')\n",
        "    plt.title('1D Positional Encoding (LLM)')\n",
        "    plt.xlabel('Sequence Position')\n",
        "    plt.ylabel('Embedding Dimension')\n",
        "    plt.colorbar()\n",
        "    \n",
        "    print(\"\\n2D Positional Encoding (GFM):\")\n",
        "    \n",
        "    def create_2d_positional_encoding(height, width, embed_dim):\n",
        "        \"\"\"Create 2D positional encodings for spatial data\"\"\"\n",
        "        pe = torch.zeros(height, width, embed_dim)\n",
        "        \n",
        "        # Create position grids\n",
        "        y_pos = torch.arange(height).unsqueeze(1).repeat(1, width).float()\n",
        "        x_pos = torch.arange(width).unsqueeze(0).repeat(height, 1).float()\n",
        "        \n",
        "        # Encode positions\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim//2, 2).float() * \n",
        "                           -(np.log(10000.0) / (embed_dim//2)))\n",
        "        \n",
        "        # Y (height) encoding\n",
        "        pe[:, :, 0:embed_dim//4:2] = torch.sin(y_pos.unsqueeze(-1) * div_term)\n",
        "        pe[:, :, 1:embed_dim//4:2] = torch.cos(y_pos.unsqueeze(-1) * div_term)\n",
        "        \n",
        "        # X (width) encoding\n",
        "        pe[:, :, embed_dim//2:3*embed_dim//4:2] = torch.sin(x_pos.unsqueeze(-1) * div_term)\n",
        "        pe[:, :, embed_dim//2+1:3*embed_dim//4:2] = torch.cos(x_pos.unsqueeze(-1) * div_term)\n",
        "        \n",
        "        return pe\n",
        "    \n",
        "    height, width = 8, 8\n",
        "    pos_encoding_2d = create_2d_positional_encoding(height, width, embed_dim)\n",
        "    \n",
        "    print(f\"2D positional encoding shape: {pos_encoding_2d.shape}\")\n",
        "    \n",
        "    # Visualize 2D positional encoding\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # Show first 64 dimensions as an 8x8 grid\n",
        "    pos_2d_viz = pos_encoding_2d[:, :, :64].mean(dim=-1)\n",
        "    plt.imshow(pos_2d_viz, cmap='RdBu', aspect='equal')\n",
        "    plt.title('2D Positional Encoding (GFM)')\n",
        "    plt.xlabel('Width')\n",
        "    plt.ylabel('Height')\n",
        "    plt.colorbar()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "compare_positional_encodings()"
      ],
      "id": "b216ef82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Course Mapping and Applications\n",
        "\n",
        "### Weekly Course Structure"
      ],
      "id": "9ebdaf98"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "course_mapping = {\n",
        "    \"Weeks 1-3\": {\n",
        "        \"Focus\": \"Data â†’ Attention â†’ Architecture\",\n",
        "        \"LLM Topics\": [\"Text preprocessing\", \"Tokenization\", \"Transformer blocks\"],\n",
        "        \"GFM Topics\": [\"Satellite data\", \"Patch embedding\", \"Spatial attention\"]\n",
        "    },\n",
        "    \"Weeks 4-7\": {\n",
        "        \"Focus\": \"Pretraining â†’ Training â†’ Evaluation â†’ Integration\",\n",
        "        \"LLM Topics\": [\"Language modeling\", \"Training loops\", \"Perplexity\"],\n",
        "        \"GFM Topics\": [\"Masked reconstruction\", \"Cloud handling\", \"Linear probing\"]\n",
        "    },\n",
        "    \"Weeks 8-10\": {\n",
        "        \"Focus\": \"Finetuning â†’ Deployment â†’ Synthesis\",\n",
        "        \"LLM Topics\": [\"Instruction tuning\", \"PEFT\", \"API deployment\"],\n",
        "        \"GFM Topics\": [\"Task heads\", \"Few-shot learning\", \"Geospatial inference\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Course Structure Mapping:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for period, details in course_mapping.items():\n",
        "    print(f\"\\n{period} - {details['Focus']}\")\n",
        "    print(f\"  LLM Focus: {', '.join(details['LLM Topics'])}\")\n",
        "    print(f\"  GFM Focus: {', '.join(details['GFM Topics'])}\")"
      ],
      "id": "263a8b50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaways"
      ],
      "id": "8644b39b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "key_differences = {\n",
        "    \"Data Nature\": {\n",
        "        \"LLMs\": \"Discrete text tokens with semantic consistency\",\n",
        "        \"GFMs\": \"Continuous pixel values requiring contextual interpretation\"\n",
        "    },\n",
        "    \"Tokenization\": {\n",
        "        \"LLMs\": \"Vocabulary-based discrete mapping\",\n",
        "        \"GFMs\": \"Patch-based continuous projection\"\n",
        "    },\n",
        "    \"Positional Info\": {\n",
        "        \"LLMs\": \"1D sequence positions\",\n",
        "        \"GFMs\": \"2D spatial + temporal positions\"\n",
        "    },\n",
        "    \"Training Objective\": {\n",
        "        \"LLMs\": \"Next token prediction or masked language modeling\",\n",
        "        \"GFMs\": \"Masked patch reconstruction or contrastive learning\"\n",
        "    },\n",
        "    \"Evaluation\": {\n",
        "        \"LLMs\": \"Perplexity and downstream language tasks\",\n",
        "        \"GFMs\": \"Reconstruction quality and spatial generalization\"\n",
        "    },\n",
        "    \"Deployment\": {\n",
        "        \"LLMs\": \"Text generation with streaming and caching\",\n",
        "        \"GFMs\": \"Spatial inference with tiling and batch processing\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Key Architectural Differences:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for aspect, comparison in key_differences.items():\n",
        "    print(f\"\\n{aspect}:\")\n",
        "    print(f\"  LLMs: {comparison['LLMs']}\")\n",
        "    print(f\"  GFMs: {comparison['GFMs']}\")"
      ],
      "id": "c393d1ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Reading and References\n",
        "\n",
        "### Essential Papers"
      ],
      "id": "01b7fb9e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "references = {\n",
        "    \"Foundation Papers\": [\n",
        "        \"Attention Is All You Need (Vaswani et al., 2017)\",\n",
        "        \"BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)\",\n",
        "        \"Language Models are Few-Shot Learners (Brown et al., 2020)\"\n",
        "    ],\n",
        "    \"Vision Transformers\": [\n",
        "        \"An Image is Worth 16x16 Words (Dosovitskiy et al., 2020)\",\n",
        "        \"Masked Autoencoders Are Scalable Vision Learners (He et al., 2021)\",\n",
        "        \"Scaling Vision Transformers (Zhai et al., 2021)\"\n",
        "    ],\n",
        "    \"Geospatial Foundation Models\": [\n",
        "        \"Prithvi: A Foundation Model for Earth Observation (IBM/NASA, 2023)\",\n",
        "        \"SatMAE: Masked Autoencoders for Satellite Imagery (Cong et al., 2022)\",\n",
        "        \"Clay: A Foundation Model for Earth Observation (Made with Clay, 2024)\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"Essential References:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for category, papers in references.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    for paper in papers:\n",
        "        print(f\"  â€¢ {paper}\")"
      ],
      "id": "74bf648e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Key concepts for foundation model architectures:\n",
        "- **Historical Evolution**: From symbolic AI to transformer-based foundation models\n",
        "- **Architecture Comparison**: LLMs use discrete tokenization, GFMs use continuous patch embeddings\n",
        "- **Development Pipeline**: 9-step process with domain-specific adaptations\n",
        "- **Scaling Trends**: LLMs scale in parameters/context, GFMs scale in spectral/spatial/temporal dimensions\n",
        "- **Training Objectives**: Next-token prediction vs. masked patch reconstruction\n",
        "- **Deployment Considerations**: Text streaming vs. spatial tiling and batch inference\n",
        "- **Course Integration**: Weekly progression from data processing to deployment"
      ],
      "id": "f08c041b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "geoai",
      "language": "python",
      "display_name": "GeoAI",
      "path": "/Users/kellycaylor/Library/Jupyter/kernels/geoai"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}