{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Model Evaluation & Validation\"\n",
        "subtitle: \"Comprehensive evaluation strategies for geospatial models\"\n",
        "jupyter: geoai\n",
        "format: html\n",
        "---\n",
        "\n",
        "\n",
        "## Introduction to Model Evaluation\n",
        "\n",
        "Model evaluation in geospatial AI requires specialized metrics and validation strategies that account for spatial dependencies, temporal variations, and domain-specific requirements. This cheatsheet covers comprehensive evaluation approaches.\n"
      ],
      "id": "611a59b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ],
      "id": "7c8b0bd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics for Different Task Types\n",
        "\n",
        "### Classification Metrics"
      ],
      "id": "28568a56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def comprehensive_classification_evaluation():\n",
        "    \"\"\"Demonstrate comprehensive classification evaluation metrics\"\"\"\n",
        "    \n",
        "    # Simulate classification results for land cover classification\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    num_samples = 1000\n",
        "    num_classes = 6\n",
        "    class_names = ['Water', 'Forest', 'Urban', 'Agriculture', 'Grassland', 'Bareland']\n",
        "    \n",
        "    # Generate realistic predictions (imbalanced classes)\n",
        "    class_probs = [0.1, 0.3, 0.2, 0.25, 0.1, 0.05]  # Different class frequencies\n",
        "    y_true = np.random.choice(num_classes, size=num_samples, p=class_probs)\n",
        "    \n",
        "    # Generate predictions with class-dependent accuracy\n",
        "    y_pred = y_true.copy()\n",
        "    class_accuracies = [0.95, 0.88, 0.82, 0.85, 0.78, 0.70]  # Different per-class accuracies\n",
        "    \n",
        "    for i in range(num_classes):\n",
        "        class_mask = y_true == i\n",
        "        num_class_samples = class_mask.sum()\n",
        "        num_errors = int(num_class_samples * (1 - class_accuracies[i]))\n",
        "        \n",
        "        if num_errors > 0:\n",
        "            error_indices = np.random.choice(np.where(class_mask)[0], num_errors, replace=False)\n",
        "            # Create confusion: assign to other classes\n",
        "            other_classes = [j for j in range(num_classes) if j != i]\n",
        "            y_pred[error_indices] = np.random.choice(other_classes, num_errors)\n",
        "    \n",
        "    # Generate prediction probabilities\n",
        "    y_probs = np.zeros((num_samples, num_classes))\n",
        "    for i in range(num_samples):\n",
        "        # Create realistic probability distributions\n",
        "        true_class = y_true[i]\n",
        "        pred_class = y_pred[i]\n",
        "        \n",
        "        # Base probabilities\n",
        "        base_probs = np.random.dirichlet([0.5] * num_classes)\n",
        "        \n",
        "        # Boost true class probability\n",
        "        base_probs[true_class] += 0.5\n",
        "        \n",
        "        # If prediction is correct, boost predicted class\n",
        "        if true_class == pred_class:\n",
        "            base_probs[pred_class] += 0.3\n",
        "        \n",
        "        # Normalize\n",
        "        y_probs[i] = base_probs / base_probs.sum()\n",
        "    \n",
        "    # Calculate comprehensive metrics\n",
        "    from sklearn.metrics import (accuracy_score, precision_recall_fscore_support, \n",
        "                                classification_report, confusion_matrix,\n",
        "                                roc_auc_score, average_precision_score)\n",
        "    \n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Per-class accuracy\n",
        "    per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "    \n",
        "    # Multiclass AUC (one-vs-rest)\n",
        "    auc_scores = []\n",
        "    for i in range(num_classes):\n",
        "        y_true_binary = (y_true == i).astype(int)\n",
        "        auc = roc_auc_score(y_true_binary, y_probs[:, i])\n",
        "        auc_scores.append(auc)\n",
        "    \n",
        "    print(\"Classification Evaluation Results:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Overall Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"Macro F1-Score: {macro_f1:.3f}\")\n",
        "    print(f\"Weighted F1-Score: {weighted_f1:.3f}\")\n",
        "    \n",
        "    print(\"\\nPer-Class Metrics:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'Class':<12} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Accuracy':<10} {'AUC':<10} {'Support':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name:<12} {precision[i]:<10.3f} {recall[i]:<10.3f} {f1[i]:<10.3f} {per_class_accuracy[i]:<10.3f} {auc_scores[i]:<10.3f} {support[i]:<10.0f}\")\n",
        "    \n",
        "    # Visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names, ax=axes[0,0])\n",
        "    axes[0,0].set_title('Confusion Matrix')\n",
        "    axes[0,0].set_xlabel('Predicted')\n",
        "    axes[0,0].set_ylabel('True')\n",
        "    \n",
        "    # Per-class metrics\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'Accuracy': per_class_accuracy\n",
        "    }, index=class_names)\n",
        "    \n",
        "    metrics_df.plot(kind='bar', ax=axes[0,1], alpha=0.8)\n",
        "    axes[0,1].set_title('Per-Class Performance Metrics')\n",
        "    axes[0,1].set_ylabel('Score')\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "    axes[0,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    \n",
        "    # Class distribution\n",
        "    unique, counts = np.unique(y_true, return_counts=True)\n",
        "    axes[1,0].bar(class_names, counts, color='skyblue', alpha=0.7)\n",
        "    axes[1,0].set_title('Class Distribution (Ground Truth)')\n",
        "    axes[1,0].set_ylabel('Number of Samples')\n",
        "    axes[1,0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # AUC scores\n",
        "    axes[1,1].bar(class_names, auc_scores, color='lightcoral', alpha=0.7)\n",
        "    axes[1,1].set_title('AUC Scores per Class')\n",
        "    axes[1,1].set_ylabel('AUC Score')\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)\n",
        "    axes[1,1].set_ylim(0, 1)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'auc_scores': auc_scores,\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred,\n",
        "        'y_probs': y_probs\n",
        "    }\n",
        "\n",
        "classification_results = comprehensive_classification_evaluation()"
      ],
      "id": "d6a7754c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regression Metrics"
      ],
      "id": "5508edac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def comprehensive_regression_evaluation():\n",
        "    \"\"\"Demonstrate comprehensive regression evaluation metrics\"\"\"\n",
        "    \n",
        "    # Simulate regression results for vegetation index prediction\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    num_samples = 800\n",
        "    \n",
        "    # Generate realistic NDVI values (target)\n",
        "    # Simulate seasonal pattern with spatial variation\n",
        "    time_component = np.sin(np.linspace(0, 4*np.pi, num_samples)) * 0.3\n",
        "    spatial_component = np.random.normal(0, 0.2, num_samples)\n",
        "    noise = np.random.normal(0, 0.1, num_samples)\n",
        "    \n",
        "    y_true = 0.5 + time_component + spatial_component + noise\n",
        "    y_true = np.clip(y_true, -1, 1)  # NDVI range\n",
        "    \n",
        "    # Generate predictions with realistic errors\n",
        "    # Add heteroscedastic noise (error depends on true value)\n",
        "    prediction_noise = np.random.normal(0, 0.05 + 0.1 * np.abs(y_true))\n",
        "    bias = -0.02  # Slight systematic bias\n",
        "    \n",
        "    y_pred = y_true + prediction_noise + bias\n",
        "    y_pred = np.clip(y_pred, -1, 1)\n",
        "    \n",
        "    # Calculate comprehensive regression metrics\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    # Additional metrics\n",
        "    def mean_absolute_percentage_error(y_true, y_pred):\n",
        "        \"\"\"Calculate MAPE, handling near-zero values\"\"\"\n",
        "        mask = np.abs(y_true) > 1e-6\n",
        "        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    \n",
        "    def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
        "        \"\"\"Calculate symmetric MAPE\"\"\"\n",
        "        return np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100\n",
        "    \n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    smape = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n",
        "    \n",
        "    # Residual analysis\n",
        "    residuals = y_true - y_pred\n",
        "    \n",
        "    # Statistical tests\n",
        "    # Shapiro-Wilk test for normality of residuals\n",
        "    shapiro_stat, shapiro_p = stats.shapiro(residuals[:100])  # Sample for computational efficiency\n",
        "    \n",
        "    # Durbin-Watson test for autocorrelation (simplified)\n",
        "    def durbin_watson(residuals):\n",
        "        \"\"\"Calculate Durbin-Watson statistic\"\"\"\n",
        "        diff = np.diff(residuals)\n",
        "        return np.sum(diff**2) / np.sum(residuals**2)\n",
        "    \n",
        "    dw_stat = durbin_watson(residuals)\n",
        "    \n",
        "    # Quantile-based metrics\n",
        "    q25_error = np.percentile(np.abs(residuals), 25)\n",
        "    median_error = np.median(np.abs(residuals))\n",
        "    q75_error = np.percentile(np.abs(residuals), 75)\n",
        "    q95_error = np.percentile(np.abs(residuals), 95)\n",
        "    \n",
        "    print(\"Regression Evaluation Results:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
        "    print(f\"Symmetric MAPE: {smape:.2f}%\")\n",
        "    \n",
        "    print(\"\\nResidual Analysis:\")\n",
        "    print(f\"Mean Residual (Bias): {residuals.mean():.4f}\")\n",
        "    print(f\"Std of Residuals: {residuals.std():.4f}\")\n",
        "    print(f\"Residual Skewness: {stats.skew(residuals):.4f}\")\n",
        "    print(f\"Residual Kurtosis: {stats.kurtosis(residuals):.4f}\")\n",
        "    print(f\"Shapiro-Wilk p-value: {shapiro_p:.4f}\")\n",
        "    print(f\"Durbin-Watson statistic: {dw_stat:.4f}\")\n",
        "    \n",
        "    print(\"\\nQuantile-based Error Analysis:\")\n",
        "    print(f\"25th percentile error: {q25_error:.4f}\")\n",
        "    print(f\"Median error: {median_error:.4f}\")\n",
        "    print(f\"75th percentile error: {q75_error:.4f}\")\n",
        "    print(f\"95th percentile error: {q95_error:.4f}\")\n",
        "    \n",
        "    # Visualizations\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # Scatter plot: True vs Predicted\n",
        "    axes[0,0].scatter(y_true, y_pred, alpha=0.6, s=20)\n",
        "    axes[0,0].plot([-1, 1], [-1, 1], 'r--', lw=2)  # Perfect prediction line\n",
        "    axes[0,0].set_xlabel('True Values')\n",
        "    axes[0,0].set_ylabel('Predicted Values')\n",
        "    axes[0,0].set_title(f'True vs Predicted (R² = {r2:.3f})')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Residual plot\n",
        "    axes[0,1].scatter(y_pred, residuals, alpha=0.6, s=20)\n",
        "    axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[0,1].set_xlabel('Predicted Values')\n",
        "    axes[0,1].set_ylabel('Residuals')\n",
        "    axes[0,1].set_title('Residual Plot')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Q-Q plot for residual normality\n",
        "    stats.probplot(residuals, dist=\"norm\", plot=axes[0,2])\n",
        "    axes[0,2].set_title('Q-Q Plot (Residual Normality)')\n",
        "    axes[0,2].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Residual histogram\n",
        "    axes[1,0].hist(residuals, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[1,0].axvline(residuals.mean(), color='red', linestyle='--', label=f'Mean = {residuals.mean():.3f}')\n",
        "    axes[1,0].set_xlabel('Residuals')\n",
        "    axes[1,0].set_ylabel('Frequency')\n",
        "    axes[1,0].set_title('Residual Distribution')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Error distribution by predicted value ranges\n",
        "    pred_ranges = np.linspace(y_pred.min(), y_pred.max(), 10)\n",
        "    range_errors = []\n",
        "    range_labels = []\n",
        "    \n",
        "    for i in range(len(pred_ranges)-1):\n",
        "        mask = (y_pred >= pred_ranges[i]) & (y_pred < pred_ranges[i+1])\n",
        "        if mask.sum() > 0:\n",
        "            range_errors.append(np.abs(residuals[mask]))\n",
        "            range_labels.append(f'{pred_ranges[i]:.2f}-{pred_ranges[i+1]:.2f}')\n",
        "    \n",
        "    axes[1,1].boxplot(range_errors, labels=range_labels)\n",
        "    axes[1,1].set_xlabel('Predicted Value Range')\n",
        "    axes[1,1].set_ylabel('Absolute Error')\n",
        "    axes[1,1].set_title('Error Distribution by Prediction Range')\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Time series of residuals (if applicable)\n",
        "    axes[1,2].plot(residuals, alpha=0.7)\n",
        "    axes[1,2].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[1,2].set_xlabel('Sample Index')\n",
        "    axes[1,2].set_ylabel('Residuals')\n",
        "    axes[1,2].set_title('Residuals over Time/Space')\n",
        "    axes[1,2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2,\n",
        "        'mape': mape, 'smape': smape,\n",
        "        'residuals': residuals,\n",
        "        'y_true': y_true, 'y_pred': y_pred\n",
        "    }\n",
        "\n",
        "regression_results = comprehensive_regression_evaluation()"
      ],
      "id": "bf949d86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Segmentation Metrics"
      ],
      "id": "15e27d5b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def comprehensive_segmentation_evaluation():\n",
        "    \"\"\"Demonstrate comprehensive segmentation evaluation metrics\"\"\"\n",
        "    \n",
        "    # Simulate segmentation results\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    height, width = 128, 128\n",
        "    num_classes = 4\n",
        "    class_names = ['Background', 'Water', 'Vegetation', 'Urban']\n",
        "    \n",
        "    # Generate realistic ground truth segmentation mask\n",
        "    y_true = np.zeros((height, width), dtype=int)\n",
        "    \n",
        "    # Create regions for different classes\n",
        "    # Water body (circular)\n",
        "    center_x, center_y = width//3, height//3\n",
        "    y_indices, x_indices = np.ogrid[:height, :width]\n",
        "    water_mask = (x_indices - center_x)**2 + (y_indices - center_y)**2 < (width//6)**2\n",
        "    y_true[water_mask] = 1\n",
        "    \n",
        "    # Vegetation (upper right)\n",
        "    y_true[20:60, 80:120] = 2\n",
        "    \n",
        "    # Urban (lower region)\n",
        "    y_true[80:120, 20:100] = 3\n",
        "    \n",
        "    # Generate predictions with realistic errors\n",
        "    y_pred = y_true.copy()\n",
        "    \n",
        "    # Add boundary errors\n",
        "    from scipy import ndimage\n",
        "    edges = ndimage.binary_dilation(ndimage.laplace(y_true) != 0)\n",
        "    \n",
        "    # Randomly flip some edge pixels\n",
        "    edge_indices = np.where(edges)\n",
        "    num_edge_errors = len(edge_indices[0]) // 4\n",
        "    error_indices = np.random.choice(len(edge_indices[0]), num_edge_errors, replace=False)\n",
        "    \n",
        "    for idx in error_indices:\n",
        "        y, x = edge_indices[0][idx], edge_indices[1][idx]\n",
        "        # Assign to random neighboring class\n",
        "        neighbors = [y_true[max(0,y-1):min(height,y+2), max(0,x-1):min(width,x+2)]]\n",
        "        unique_neighbors = np.unique(neighbors)\n",
        "        other_classes = [c for c in unique_neighbors if c != y_true[y, x]]\n",
        "        if other_classes:\n",
        "            y_pred[y, x] = np.random.choice(other_classes)\n",
        "    \n",
        "    # Add some random noise\n",
        "    num_random_errors = (height * width) // 50\n",
        "    error_y = np.random.randint(0, height, num_random_errors)\n",
        "    error_x = np.random.randint(0, width, num_random_errors)\n",
        "    for y, x in zip(error_y, error_x):\n",
        "        y_pred[y, x] = np.random.randint(0, num_classes)\n",
        "    \n",
        "    # Calculate segmentation metrics\n",
        "    def calculate_segmentation_metrics(y_true, y_pred, num_classes):\n",
        "        \"\"\"Calculate comprehensive segmentation metrics\"\"\"\n",
        "        \n",
        "        # Flatten arrays\n",
        "        y_true_flat = y_true.flatten()\n",
        "        y_pred_flat = y_pred.flatten()\n",
        "        \n",
        "        # Basic accuracy\n",
        "        accuracy = accuracy_score(y_true_flat, y_pred_flat)\n",
        "        \n",
        "        # Per-class metrics\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(\n",
        "            y_true_flat, y_pred_flat, average=None, zero_division=0\n",
        "        )\n",
        "        \n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_true_flat, y_pred_flat)\n",
        "        \n",
        "        # IoU (Intersection over Union) per class\n",
        "        iou_scores = []\n",
        "        dice_scores = []\n",
        "        \n",
        "        for i in range(num_classes):\n",
        "            # True positives, false positives, false negatives\n",
        "            tp = cm[i, i]\n",
        "            fp = cm[:, i].sum() - tp\n",
        "            fn = cm[i, :].sum() - tp\n",
        "            \n",
        "            # IoU\n",
        "            iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
        "            iou_scores.append(iou)\n",
        "            \n",
        "            # Dice coefficient\n",
        "            dice = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
        "            dice_scores.append(dice)\n",
        "        \n",
        "        # Mean IoU\n",
        "        mean_iou = np.mean(iou_scores)\n",
        "        \n",
        "        # Pixel accuracy (same as overall accuracy)\n",
        "        pixel_accuracy = accuracy\n",
        "        \n",
        "        # Mean accuracy (average of per-class accuracies)\n",
        "        class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
        "        mean_accuracy = np.mean(class_accuracies)\n",
        "        \n",
        "        # Frequency weighted IoU\n",
        "        class_frequencies = cm.sum(axis=1) / cm.sum()\n",
        "        freq_weighted_iou = np.sum(class_frequencies * iou_scores)\n",
        "        \n",
        "        return {\n",
        "            'pixel_accuracy': pixel_accuracy,\n",
        "            'mean_accuracy': mean_accuracy,\n",
        "            'mean_iou': mean_iou,\n",
        "            'freq_weighted_iou': freq_weighted_iou,\n",
        "            'iou_scores': iou_scores,\n",
        "            'dice_scores': dice_scores,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "    \n",
        "    metrics = calculate_segmentation_metrics(y_true, y_pred, num_classes)\n",
        "    \n",
        "    print(\"Segmentation Evaluation Results:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Pixel Accuracy: {metrics['pixel_accuracy']:.4f}\")\n",
        "    print(f\"Mean Accuracy: {metrics['mean_accuracy']:.4f}\")\n",
        "    print(f\"Mean IoU: {metrics['mean_iou']:.4f}\")\n",
        "    print(f\"Frequency Weighted IoU: {metrics['freq_weighted_iou']:.4f}\")\n",
        "    \n",
        "    print(\"\\nPer-Class Metrics:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Class':<12} {'IoU':<8} {'Dice':<8} {'Precision':<10} {'Recall':<10} {'F1':<8}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name:<12} {metrics['iou_scores'][i]:<8.3f} {metrics['dice_scores'][i]:<8.3f} \"\n",
        "              f\"{metrics['precision'][i]:<10.3f} {metrics['recall'][i]:<10.3f} {metrics['f1'][i]:<8.3f}\")\n",
        "    \n",
        "    # Visualizations\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # Ground truth\n",
        "    im1 = axes[0,0].imshow(y_true, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
        "    axes[0,0].set_title('Ground Truth')\n",
        "    axes[0,0].axis('off')\n",
        "    \n",
        "    # Predictions\n",
        "    im2 = axes[0,1].imshow(y_pred, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
        "    axes[0,1].set_title('Predictions')\n",
        "    axes[0,1].axis('off')\n",
        "    \n",
        "    # Error map\n",
        "    error_map = (y_true != y_pred).astype(int)\n",
        "    axes[0,2].imshow(error_map, cmap='Reds')\n",
        "    axes[0,2].set_title(f'Error Map ({error_map.sum()} errors)')\n",
        "    axes[0,2].axis('off')\n",
        "    \n",
        "    # Confusion matrix\n",
        "    sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names, ax=axes[1,0])\n",
        "    axes[1,0].set_title('Confusion Matrix')\n",
        "    axes[1,0].set_xlabel('Predicted')\n",
        "    axes[1,0].set_ylabel('True')\n",
        "    \n",
        "    # Per-class IoU\n",
        "    axes[1,1].bar(class_names, metrics['iou_scores'], color='skyblue', alpha=0.7)\n",
        "    axes[1,1].set_title('IoU Scores per Class')\n",
        "    axes[1,1].set_ylabel('IoU Score')\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)\n",
        "    axes[1,1].set_ylim(0, 1)\n",
        "    \n",
        "    # Metrics comparison\n",
        "    metrics_comparison = pd.DataFrame({\n",
        "        'IoU': metrics['iou_scores'],\n",
        "        'Dice': metrics['dice_scores'],\n",
        "        'F1': metrics['f1']\n",
        "    }, index=class_names)\n",
        "    \n",
        "    metrics_comparison.plot(kind='bar', ax=axes[1,2], alpha=0.8)\n",
        "    axes[1,2].set_title('Metric Comparison per Class')\n",
        "    axes[1,2].set_ylabel('Score')\n",
        "    axes[1,2].tick_params(axis='x', rotation=45)\n",
        "    axes[1,2].legend()\n",
        "    axes[1,2].set_ylim(0, 1)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return metrics, y_true, y_pred\n",
        "\n",
        "segmentation_metrics, gt_mask, pred_mask = comprehensive_segmentation_evaluation()"
      ],
      "id": "dee0f8f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spatial Validation Strategies\n",
        "\n",
        "### Cross-Validation with Spatial Awareness"
      ],
      "id": "43d27a16"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demonstrate_spatial_cross_validation():\n",
        "    \"\"\"Demonstrate spatial cross-validation strategies\"\"\"\n",
        "    \n",
        "    # Simulate spatial data with coordinates\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Generate spatial grid\n",
        "    grid_size = 20\n",
        "    x_coords = np.repeat(np.arange(grid_size), grid_size)\n",
        "    y_coords = np.tile(np.arange(grid_size), grid_size)\n",
        "    \n",
        "    n_samples = len(x_coords)\n",
        "    \n",
        "    # Generate spatially correlated features and target\n",
        "    # Create spatial autocorrelation structure\n",
        "    def spatial_correlation(x1, y1, x2, y2, correlation_range=5):\n",
        "        \"\"\"Calculate spatial correlation based on distance\"\"\"\n",
        "        distance = np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
        "        return np.exp(-distance / correlation_range)\n",
        "    \n",
        "    # Generate correlated target variable\n",
        "    y = np.zeros(n_samples)\n",
        "    base_trend = 0.5 * (x_coords / grid_size) + 0.3 * (y_coords / grid_size)\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # Add spatially correlated noise\n",
        "        spatial_component = 0\n",
        "        for j in range(min(50, n_samples)):  # Limit for computational efficiency\n",
        "            if i != j:\n",
        "                weight = spatial_correlation(x_coords[i], y_coords[i], x_coords[j], y_coords[j])\n",
        "                spatial_component += weight * np.random.normal(0, 0.1)\n",
        "        \n",
        "        y[i] = base_trend[i] + spatial_component + np.random.normal(0, 0.2)\n",
        "    \n",
        "    # Generate features\n",
        "    X = np.column_stack([\n",
        "        x_coords / grid_size,  # Normalized x coordinate\n",
        "        y_coords / grid_size,  # Normalized y coordinate\n",
        "        np.random.normal(0, 1, n_samples),  # Random feature 1\n",
        "        np.random.normal(0, 1, n_samples),  # Random feature 2\n",
        "    ])\n",
        "    \n",
        "    # Different cross-validation strategies\n",
        "    def random_cv_split(X, y, n_folds=5):\n",
        "        \"\"\"Standard random cross-validation\"\"\"\n",
        "        from sklearn.model_selection import KFold\n",
        "        \n",
        "        kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "        return list(kfold.split(X))\n",
        "    \n",
        "    def spatial_block_cv_split(x_coords, y_coords, n_blocks=4):\n",
        "        \"\"\"Spatial block cross-validation\"\"\"\n",
        "        \n",
        "        # Divide space into blocks\n",
        "        x_blocks = np.linspace(x_coords.min(), x_coords.max(), int(np.sqrt(n_blocks))+1)\n",
        "        y_blocks = np.linspace(y_coords.min(), y_coords.max(), int(np.sqrt(n_blocks))+1)\n",
        "        \n",
        "        folds = []\n",
        "        for i in range(len(x_blocks)-1):\n",
        "            for j in range(len(y_blocks)-1):\n",
        "                # Test block\n",
        "                test_mask = ((x_coords >= x_blocks[i]) & (x_coords < x_blocks[i+1]) &\n",
        "                           (y_coords >= y_blocks[j]) & (y_coords < y_blocks[j+1]))\n",
        "                \n",
        "                # Training set is everything else\n",
        "                train_mask = ~test_mask\n",
        "                \n",
        "                if test_mask.sum() > 0 and train_mask.sum() > 0:\n",
        "                    train_indices = np.where(train_mask)[0]\n",
        "                    test_indices = np.where(test_mask)[0]\n",
        "                    folds.append((train_indices, test_indices))\n",
        "        \n",
        "        return folds\n",
        "    \n",
        "    def spatial_buffer_cv_split(x_coords, y_coords, n_folds=5, buffer_distance=2):\n",
        "        \"\"\"Spatial cross-validation with buffer zones\"\"\"\n",
        "        from sklearn.model_selection import KFold\n",
        "        \n",
        "        # First, get random splits\n",
        "        kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "        random_splits = list(kfold.split(range(len(x_coords))))\n",
        "        \n",
        "        buffered_splits = []\n",
        "        for train_idx, test_idx in random_splits:\n",
        "            # Remove training samples too close to test samples\n",
        "            filtered_train_idx = []\n",
        "            \n",
        "            for train_i in train_idx:\n",
        "                min_dist_to_test = float('inf')\n",
        "                for test_i in test_idx:\n",
        "                    dist = np.sqrt((x_coords[train_i] - x_coords[test_i])**2 + \n",
        "                                 (y_coords[train_i] - y_coords[test_i])**2)\n",
        "                    min_dist_to_test = min(min_dist_to_test, dist)\n",
        "                \n",
        "                if min_dist_to_test >= buffer_distance:\n",
        "                    filtered_train_idx.append(train_i)\n",
        "            \n",
        "            if len(filtered_train_idx) > 0:\n",
        "                buffered_splits.append((np.array(filtered_train_idx), test_idx))\n",
        "        \n",
        "        return buffered_splits\n",
        "    \n",
        "    # Evaluate different CV strategies\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.metrics import mean_squared_error, r2_score\n",
        "    \n",
        "    def evaluate_cv_strategy(X, y, cv_splits, strategy_name):\n",
        "        \"\"\"Evaluate a cross-validation strategy\"\"\"\n",
        "        \n",
        "        r2_scores = []\n",
        "        mse_scores = []\n",
        "        \n",
        "        for fold, (train_idx, test_idx) in enumerate(cv_splits):\n",
        "            # Train model\n",
        "            model = LinearRegression()\n",
        "            model.fit(X[train_idx], y[train_idx])\n",
        "            \n",
        "            # Predict on test set\n",
        "            y_pred = model.predict(X[test_idx])\n",
        "            \n",
        "            # Calculate metrics\n",
        "            r2 = r2_score(y[test_idx], y_pred)\n",
        "            mse = mean_squared_error(y[test_idx], y_pred)\n",
        "            \n",
        "            r2_scores.append(r2)\n",
        "            mse_scores.append(mse)\n",
        "        \n",
        "        return {\n",
        "            'strategy': strategy_name,\n",
        "            'r2_mean': np.mean(r2_scores),\n",
        "            'r2_std': np.std(r2_scores),\n",
        "            'mse_mean': np.mean(mse_scores),\n",
        "            'mse_std': np.std(mse_scores),\n",
        "            'n_folds': len(cv_splits)\n",
        "        }\n",
        "    \n",
        "    # Apply different CV strategies\n",
        "    random_splits = random_cv_split(X, y, n_folds=5)\n",
        "    block_splits = spatial_block_cv_split(x_coords, y_coords, n_blocks=16)\n",
        "    buffer_splits = spatial_buffer_cv_split(x_coords, y_coords, n_folds=5, buffer_distance=2)\n",
        "    \n",
        "    # Evaluate strategies\n",
        "    results = []\n",
        "    results.append(evaluate_cv_strategy(X, y, random_splits, 'Random CV'))\n",
        "    results.append(evaluate_cv_strategy(X, y, block_splits, 'Spatial Block CV'))\n",
        "    results.append(evaluate_cv_strategy(X, y, buffer_splits, 'Spatial Buffer CV'))\n",
        "    \n",
        "    # Display results\n",
        "    print(\"Spatial Cross-Validation Comparison:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'Strategy':<20} {'R² Mean':<10} {'R² Std':<10} {'MSE Mean':<10} {'MSE Std':<10} {'Folds':<6}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for result in results:\n",
        "        print(f\"{result['strategy']:<20} {result['r2_mean']:<10.3f} {result['r2_std']:<10.3f} \"\n",
        "              f\"{result['mse_mean']:<10.3f} {result['mse_std']:<10.3f} {result['n_folds']:<6}\")\n",
        "    \n",
        "    # Visualizations\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    # Data distribution\n",
        "    scatter = axes[0,0].scatter(x_coords, y_coords, c=y, cmap='viridis', s=30)\n",
        "    axes[0,0].set_title('Spatial Distribution of Target Variable')\n",
        "    axes[0,0].set_xlabel('X Coordinate')\n",
        "    axes[0,0].set_ylabel('Y Coordinate')\n",
        "    plt.colorbar(scatter, ax=axes[0,0])\n",
        "    \n",
        "    # Random CV example (first fold)\n",
        "    train_idx, test_idx = random_splits[0]\n",
        "    axes[0,1].scatter(x_coords[train_idx], y_coords[train_idx], c='blue', s=20, alpha=0.6, label='Train')\n",
        "    axes[0,1].scatter(x_coords[test_idx], y_coords[test_idx], c='red', s=20, alpha=0.8, label='Test')\n",
        "    axes[0,1].set_title('Random CV (Fold 1)')\n",
        "    axes[0,1].set_xlabel('X Coordinate')\n",
        "    axes[0,1].set_ylabel('Y Coordinate')\n",
        "    axes[0,1].legend()\n",
        "    \n",
        "    # Block CV example (first fold)\n",
        "    if block_splits:\n",
        "        train_idx, test_idx = block_splits[0]\n",
        "        axes[0,2].scatter(x_coords[train_idx], y_coords[train_idx], c='blue', s=20, alpha=0.6, label='Train')\n",
        "        axes[0,2].scatter(x_coords[test_idx], y_coords[test_idx], c='red', s=20, alpha=0.8, label='Test')\n",
        "        axes[0,2].set_title('Spatial Block CV (Block 1)')\n",
        "        axes[0,2].set_xlabel('X Coordinate')\n",
        "        axes[0,2].set_ylabel('Y Coordinate')\n",
        "        axes[0,2].legend()\n",
        "    \n",
        "    # Buffer CV example (first fold)\n",
        "    if buffer_splits:\n",
        "        train_idx, test_idx = buffer_splits[0]\n",
        "        axes[1,0].scatter(x_coords[train_idx], y_coords[train_idx], c='blue', s=20, alpha=0.6, label='Train')\n",
        "        axes[1,0].scatter(x_coords[test_idx], y_coords[test_idx], c='red', s=20, alpha=0.8, label='Test')\n",
        "        axes[1,0].set_title('Spatial Buffer CV (Fold 1)')\n",
        "        axes[1,0].set_xlabel('X Coordinate')\n",
        "        axes[1,0].set_ylabel('Y Coordinate')\n",
        "        axes[1,0].legend()\n",
        "    \n",
        "    # Performance comparison\n",
        "    strategies = [r['strategy'] for r in results]\n",
        "    r2_means = [r['r2_mean'] for r in results]\n",
        "    r2_stds = [r['r2_std'] for r in results]\n",
        "    \n",
        "    bars = axes[1,1].bar(strategies, r2_means, yerr=r2_stds, capsize=5, alpha=0.7)\n",
        "    axes[1,1].set_title('Cross-Validation Performance Comparison')\n",
        "    axes[1,1].set_ylabel('R² Score')\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # MSE comparison\n",
        "    mse_means = [r['mse_mean'] for r in results]\n",
        "    mse_stds = [r['mse_std'] for r in results]\n",
        "    \n",
        "    bars = axes[1,2].bar(strategies, mse_means, yerr=mse_stds, capsize=5, alpha=0.7, color='lightcoral')\n",
        "    axes[1,2].set_title('MSE Comparison')\n",
        "    axes[1,2].set_ylabel('Mean Squared Error')\n",
        "    axes[1,2].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results, X, y, x_coords, y_coords\n",
        "\n",
        "spatial_cv_results, spatial_X, spatial_y, spatial_x, spatial_y = demonstrate_spatial_cross_validation()"
      ],
      "id": "9ef72eae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal Validation\n",
        "\n",
        "### Time Series Cross-Validation"
      ],
      "id": "ed8f8015"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demonstrate_temporal_validation():\n",
        "    \"\"\"Demonstrate temporal validation strategies for time series data\"\"\"\n",
        "    \n",
        "    # Generate temporal dataset\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Create time series with trend, seasonality, and noise\n",
        "    n_timesteps = 365 * 3  # 3 years of daily data\n",
        "    time_index = pd.date_range('2020-01-01', periods=n_timesteps, freq='D')\n",
        "    \n",
        "    # Generate synthetic time series\n",
        "    t = np.arange(n_timesteps)\n",
        "    \n",
        "    # Trend component\n",
        "    trend = 0.001 * t\n",
        "    \n",
        "    # Seasonal components\n",
        "    annual_cycle = 0.5 * np.sin(2 * np.pi * t / 365)\n",
        "    weekly_cycle = 0.1 * np.sin(2 * np.pi * t / 7)\n",
        "    \n",
        "    # Random noise\n",
        "    noise = np.random.normal(0, 0.2, n_timesteps)\n",
        "    \n",
        "    # Combine components\n",
        "    y = trend + annual_cycle + weekly_cycle + noise\n",
        "    \n",
        "    # Add some extreme events\n",
        "    extreme_events = np.random.choice(n_timesteps, 10, replace=False)\n",
        "    y[extreme_events] += np.random.normal(0, 2, 10)\n",
        "    \n",
        "    # Create features (lagged values, moving averages, etc.)\n",
        "    def create_temporal_features(y, lookback_window=30):\n",
        "        \"\"\"Create temporal features for time series prediction\"\"\"\n",
        "        \n",
        "        features = []\n",
        "        targets = []\n",
        "        \n",
        "        for i in range(lookback_window, len(y)):\n",
        "            # Lagged values\n",
        "            lag_features = y[i-lookback_window:i]\n",
        "            \n",
        "            # Statistical features\n",
        "            stat_features = [\n",
        "                np.mean(lag_features),\n",
        "                np.std(lag_features),\n",
        "                np.min(lag_features),\n",
        "                np.max(lag_features),\n",
        "                lag_features[-1],  # Most recent value\n",
        "                lag_features[-7]   # Value from a week ago\n",
        "            ]\n",
        "            \n",
        "            # Time features\n",
        "            day_of_year = (i % 365) / 365\n",
        "            day_of_week = (i % 7) / 7\n",
        "            \n",
        "            # Combine all features\n",
        "            all_features = list(lag_features) + stat_features + [day_of_year, day_of_week]\n",
        "            features.append(all_features)\n",
        "            targets.append(y[i])\n",
        "        \n",
        "        return np.array(features), np.array(targets)\n",
        "    \n",
        "    X, y_target = create_temporal_features(y, lookback_window=7)\n",
        "    \n",
        "    # Temporal validation strategies\n",
        "    def walk_forward_validation(X, y, n_splits=5, test_size=30):\n",
        "        \"\"\"Walk-forward (expanding window) validation\"\"\"\n",
        "        \n",
        "        n_samples = len(X)\n",
        "        min_train_size = n_samples // 2\n",
        "        \n",
        "        splits = []\n",
        "        for i in range(n_splits):\n",
        "            # Expanding training set\n",
        "            train_end = min_train_size + i * test_size\n",
        "            test_start = train_end\n",
        "            test_end = min(test_start + test_size, n_samples)\n",
        "            \n",
        "            if test_end > test_start:\n",
        "                train_idx = np.arange(0, train_end)\n",
        "                test_idx = np.arange(test_start, test_end)\n",
        "                splits.append((train_idx, test_idx))\n",
        "        \n",
        "        return splits\n",
        "    \n",
        "    def time_series_split_validation(X, y, n_splits=5):\n",
        "        \"\"\"Time series split validation (rolling window)\"\"\"\n",
        "        from sklearn.model_selection import TimeSeriesSplit\n",
        "        \n",
        "        tss = TimeSeriesSplit(n_splits=n_splits)\n",
        "        return list(tss.split(X))\n",
        "    \n",
        "    def seasonal_validation(X, y, season_length=365):\n",
        "        \"\"\"Seasonal validation - train on some seasons, test on others\"\"\"\n",
        "        \n",
        "        n_samples = len(X)\n",
        "        n_seasons = n_samples // season_length\n",
        "        \n",
        "        splits = []\n",
        "        for test_season in range(1, n_seasons):  # Skip first season for training\n",
        "            # Training: all seasons except test season\n",
        "            train_idx = []\n",
        "            for season in range(n_seasons):\n",
        "                if season != test_season:\n",
        "                    season_start = season * season_length\n",
        "                    season_end = min((season + 1) * season_length, n_samples)\n",
        "                    train_idx.extend(range(season_start, season_end))\n",
        "            \n",
        "            # Test: specific season\n",
        "            test_start = test_season * season_length\n",
        "            test_end = min((test_season + 1) * season_length, n_samples)\n",
        "            test_idx = list(range(test_start, test_end))\n",
        "            \n",
        "            if len(train_idx) > 0 and len(test_idx) > 0:\n",
        "                splits.append((np.array(train_idx), np.array(test_idx)))\n",
        "        \n",
        "        return splits\n",
        "    \n",
        "    # Evaluate different temporal validation strategies\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    \n",
        "    def evaluate_temporal_strategy(X, y, cv_splits, strategy_name):\n",
        "        \"\"\"Evaluate temporal validation strategy\"\"\"\n",
        "        \n",
        "        r2_scores = []\n",
        "        mae_scores = []\n",
        "        predictions_all = []\n",
        "        \n",
        "        for fold, (train_idx, test_idx) in enumerate(cv_splits):\n",
        "            # Train model\n",
        "            model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "            model.fit(X[train_idx], y[train_idx])\n",
        "            \n",
        "            # Predict\n",
        "            y_pred = model.predict(X[test_idx])\n",
        "            \n",
        "            # Metrics\n",
        "            r2 = r2_score(y[test_idx], y_pred)\n",
        "            mae = mean_absolute_error(y[test_idx], y_pred)\n",
        "            \n",
        "            r2_scores.append(r2)\n",
        "            mae_scores.append(mae)\n",
        "            predictions_all.append((test_idx, y[test_idx], y_pred))\n",
        "        \n",
        "        return {\n",
        "            'strategy': strategy_name,\n",
        "            'r2_mean': np.mean(r2_scores),\n",
        "            'r2_std': np.std(r2_scores),\n",
        "            'mae_mean': np.mean(mae_scores),\n",
        "            'mae_std': np.std(mae_scores),\n",
        "            'predictions': predictions_all,\n",
        "            'n_folds': len(cv_splits)\n",
        "        }\n",
        "    \n",
        "    # Apply different strategies\n",
        "    walk_forward_splits = walk_forward_validation(X, y_target, n_splits=5)\n",
        "    ts_splits = time_series_split_validation(X, y_target, n_splits=5)\n",
        "    seasonal_splits = seasonal_validation(X, y_target, season_length=365)\n",
        "    \n",
        "    # Evaluate strategies\n",
        "    temporal_results = []\n",
        "    temporal_results.append(evaluate_temporal_strategy(X, y_target, walk_forward_splits, 'Walk Forward'))\n",
        "    temporal_results.append(evaluate_temporal_strategy(X, y_target, ts_splits, 'Time Series Split'))\n",
        "    temporal_results.append(evaluate_temporal_strategy(X, y_target, seasonal_splits, 'Seasonal Validation'))\n",
        "    \n",
        "    # Display results\n",
        "    print(\"Temporal Validation Comparison:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'Strategy':<20} {'R² Mean':<10} {'R² Std':<10} {'MAE Mean':<10} {'MAE Std':<10} {'Folds':<6}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for result in temporal_results:\n",
        "        print(f\"{result['strategy']:<20} {result['r2_mean']:<10.3f} {result['r2_std']:<10.3f} \"\n",
        "              f\"{result['mae_mean']:<10.3f} {result['mae_std']:<10.3f} {result['n_folds']:<6}\")\n",
        "    \n",
        "    # Visualizations\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(18, 15))\n",
        "    \n",
        "    # Original time series\n",
        "    axes[0,0].plot(time_index[:len(y)], y, alpha=0.7)\n",
        "    axes[0,0].set_title('Original Time Series')\n",
        "    axes[0,0].set_xlabel('Date')\n",
        "    axes[0,0].set_ylabel('Value')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Performance comparison\n",
        "    strategies = [r['strategy'] for r in temporal_results]\n",
        "    r2_means = [r['r2_mean'] for r in temporal_results]\n",
        "    r2_stds = [r['r2_std'] for r in temporal_results]\n",
        "    \n",
        "    bars = axes[0,1].bar(strategies, r2_means, yerr=r2_stds, capsize=5, alpha=0.7)\n",
        "    axes[0,1].set_title('Temporal Validation Performance')\n",
        "    axes[0,1].set_ylabel('R² Score')\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Example predictions for each strategy\n",
        "    for idx, result in enumerate(temporal_results):\n",
        "        row = idx + 1\n",
        "        \n",
        "        # Get first fold predictions\n",
        "        test_idx, y_true_fold, y_pred_fold = result['predictions'][0]\n",
        "        \n",
        "        axes[row,0].plot(y_true_fold, alpha=0.7, label='True')\n",
        "        axes[row,0].plot(y_pred_fold, alpha=0.7, label='Predicted')\n",
        "        axes[row,0].set_title(f'{result[\"strategy\"]} - Fold 1 Predictions')\n",
        "        axes[row,0].set_ylabel('Value')\n",
        "        axes[row,0].legend()\n",
        "        axes[row,0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Residuals\n",
        "        residuals = y_true_fold - y_pred_fold\n",
        "        axes[row,1].plot(residuals, alpha=0.7)\n",
        "        axes[row,1].axhline(y=0, color='r', linestyle='--')\n",
        "        axes[row,1].set_title(f'{result[\"strategy\"]} - Residuals')\n",
        "        axes[row,1].set_ylabel('Residual')\n",
        "        axes[row,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return temporal_results, X, y_target, time_index\n",
        "\n",
        "temporal_results, temp_X, temp_y, temp_time = demonstrate_temporal_validation()"
      ],
      "id": "43090ae1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Uncertainty Quantification\n",
        "\n",
        "### Uncertainty Estimation Methods"
      ],
      "id": "e36a348b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demonstrate_uncertainty_quantification():\n",
        "    \"\"\"Demonstrate uncertainty quantification methods\"\"\"\n",
        "    \n",
        "    # Generate dataset with varying noise levels\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    n_samples = 300\n",
        "    X = np.linspace(0, 10, n_samples).reshape(-1, 1)\n",
        "    \n",
        "    # Create heteroscedastic data (varying uncertainty)\n",
        "    noise_levels = 0.1 + 0.3 * np.abs(np.sin(X.flatten()))\n",
        "    y = 2 * np.sin(X.flatten()) + 0.5 * X.flatten() + np.random.normal(0, noise_levels)\n",
        "    \n",
        "    # Split data\n",
        "    train_idx = np.random.choice(n_samples, int(0.7 * n_samples), replace=False)\n",
        "    test_idx = np.setdiff1d(np.arange(n_samples), train_idx)\n",
        "    \n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "    \n",
        "    # Method 1: Bootstrap Aggregation\n",
        "    def bootstrap_uncertainty(X_train, y_train, X_test, n_bootstrap=100):\n",
        "        \"\"\"Estimate uncertainty using bootstrap aggregation\"\"\"\n",
        "        \n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "        \n",
        "        predictions = []\n",
        "        \n",
        "        for i in range(n_bootstrap):\n",
        "            # Bootstrap sample\n",
        "            n_train = len(X_train)\n",
        "            bootstrap_idx = np.random.choice(n_train, n_train, replace=True)\n",
        "            X_boot = X_train[bootstrap_idx]\n",
        "            y_boot = y_train[bootstrap_idx]\n",
        "            \n",
        "            # Train model\n",
        "            model = RandomForestRegressor(n_estimators=20, random_state=i)\n",
        "            model.fit(X_boot, y_boot)\n",
        "            \n",
        "            # Predict\n",
        "            pred = model.predict(X_test)\n",
        "            predictions.append(pred)\n",
        "        \n",
        "        predictions = np.array(predictions)\n",
        "        \n",
        "        # Calculate statistics\n",
        "        mean_pred = np.mean(predictions, axis=0)\n",
        "        std_pred = np.std(predictions, axis=0)\n",
        "        \n",
        "        # Confidence intervals\n",
        "        ci_lower = np.percentile(predictions, 2.5, axis=0)\n",
        "        ci_upper = np.percentile(predictions, 97.5, axis=0)\n",
        "        \n",
        "        return mean_pred, std_pred, ci_lower, ci_upper\n",
        "    \n",
        "    # Method 2: Quantile Regression\n",
        "    def quantile_regression_uncertainty(X_train, y_train, X_test, quantiles=[0.025, 0.5, 0.975]):\n",
        "        \"\"\"Estimate uncertainty using quantile regression\"\"\"\n",
        "        \n",
        "        from sklearn.ensemble import GradientBoostingRegressor\n",
        "        \n",
        "        predictions = {}\n",
        "        \n",
        "        for q in quantiles:\n",
        "            model = GradientBoostingRegressor(loss='quantile', alpha=q, random_state=42)\n",
        "            model.fit(X_train, y_train)\n",
        "            predictions[q] = model.predict(X_test)\n",
        "        \n",
        "        return predictions\n",
        "    \n",
        "    # Method 3: Monte Carlo Dropout (simplified)\n",
        "    class MCDropoutModel(nn.Module):\n",
        "        \"\"\"Simple neural network with Monte Carlo Dropout\"\"\"\n",
        "        \n",
        "        def __init__(self, input_dim=1, hidden_dim=50, dropout_rate=0.5):\n",
        "            super().__init__()\n",
        "            self.layers = nn.Sequential(\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(), \n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(hidden_dim, 1)\n",
        "            )\n",
        "            self.dropout_rate = dropout_rate\n",
        "        \n",
        "        def forward(self, x):\n",
        "            return self.layers(x)\n",
        "        \n",
        "        def predict_with_uncertainty(self, x, n_samples=100):\n",
        "            \"\"\"Predict with MC Dropout uncertainty\"\"\"\n",
        "            \n",
        "            self.train()  # Keep in training mode for dropout\n",
        "            predictions = []\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                for _ in range(n_samples):\n",
        "                    pred = self(x)\n",
        "                    predictions.append(pred.numpy())\n",
        "            \n",
        "            predictions = np.array(predictions).squeeze()\n",
        "            \n",
        "            if predictions.ndim == 1:  # Single prediction\n",
        "                return predictions.mean(), predictions.std()\n",
        "            else:  # Multiple predictions\n",
        "                return predictions.mean(axis=0), predictions.std(axis=0)\n",
        "    \n",
        "    def mc_dropout_uncertainty(X_train, y_train, X_test):\n",
        "        \"\"\"Train MC Dropout model and get uncertainty estimates\"\"\"\n",
        "        \n",
        "        # Convert to tensors\n",
        "        X_train_tensor = torch.FloatTensor(X_train)\n",
        "        y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "        X_test_tensor = torch.FloatTensor(X_test)\n",
        "        \n",
        "        # Train model\n",
        "        model = MCDropoutModel()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        # Simple training loop\n",
        "        for epoch in range(200):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_train_tensor)\n",
        "            loss = criterion(outputs, y_train_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # Get predictions with uncertainty\n",
        "        mean_pred, std_pred = model.predict_with_uncertainty(X_test_tensor)\n",
        "        \n",
        "        return mean_pred, std_pred\n",
        "    \n",
        "    # Apply different uncertainty methods\n",
        "    print(\"Uncertainty Quantification Methods:\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Bootstrap\n",
        "    print(\"Running Bootstrap Aggregation...\")\n",
        "    boot_mean, boot_std, boot_lower, boot_upper = bootstrap_uncertainty(X_train, y_train, X_test)\n",
        "    \n",
        "    # Quantile regression\n",
        "    print(\"Running Quantile Regression...\")\n",
        "    quantile_preds = quantile_regression_uncertainty(X_train, y_train, X_test)\n",
        "    \n",
        "    # MC Dropout\n",
        "    print(\"Running MC Dropout...\")\n",
        "    mc_mean, mc_std = mc_dropout_uncertainty(X_train, y_train, X_test)\n",
        "    \n",
        "    # Calculate uncertainty metrics\n",
        "    def evaluate_uncertainty(y_true, mean_pred, std_pred=None, ci_lower=None, ci_upper=None):\n",
        "        \"\"\"Evaluate uncertainty estimation quality\"\"\"\n",
        "        \n",
        "        # Prediction accuracy\n",
        "        mae = mean_absolute_error(y_true, mean_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, mean_pred))\n",
        "        r2 = r2_score(y_true, mean_pred)\n",
        "        \n",
        "        metrics = {'mae': mae, 'rmse': rmse, 'r2': r2}\n",
        "        \n",
        "        if std_pred is not None:\n",
        "            # Uncertainty calibration\n",
        "            residuals = np.abs(y_true - mean_pred)\n",
        "            \n",
        "            # Correlation between predicted uncertainty and actual errors\n",
        "            uncertainty_correlation = np.corrcoef(std_pred, residuals)[0, 1]\n",
        "            metrics['uncertainty_correlation'] = uncertainty_correlation\n",
        "        \n",
        "        if ci_lower is not None and ci_upper is not None:\n",
        "            # Coverage probability (should be ~95% for 95% CI)\n",
        "            in_interval = (y_true >= ci_lower) & (y_true <= ci_upper)\n",
        "            coverage = np.mean(in_interval)\n",
        "            metrics['coverage'] = coverage\n",
        "            \n",
        "            # Interval width\n",
        "            interval_width = np.mean(ci_upper - ci_lower)\n",
        "            metrics['mean_interval_width'] = interval_width\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    # Evaluate methods\n",
        "    boot_metrics = evaluate_uncertainty(y_test, boot_mean, boot_std, boot_lower, boot_upper)\n",
        "    quantile_metrics = evaluate_uncertainty(y_test, quantile_preds[0.5], \n",
        "                                          ci_lower=quantile_preds[0.025], \n",
        "                                          ci_upper=quantile_preds[0.975])\n",
        "    mc_metrics = evaluate_uncertainty(y_test, mc_mean, mc_std)\n",
        "    \n",
        "    print(\"\\nUncertainty Evaluation Results:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Method':<20} {'MAE':<8} {'RMSE':<8} {'R²':<8} {'Coverage':<10} {'Uncert.Corr':<12}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    methods_data = [\n",
        "        ('Bootstrap', boot_metrics),\n",
        "        ('Quantile Reg.', quantile_metrics),\n",
        "        ('MC Dropout', mc_metrics)\n",
        "    ]\n",
        "    \n",
        "    for method_name, metrics in methods_data:\n",
        "        coverage = metrics.get('coverage', 0)\n",
        "        uncert_corr = metrics.get('uncertainty_correlation', 0)\n",
        "        print(f\"{method_name:<20} {metrics['mae']:<8.3f} {metrics['rmse']:<8.3f} {metrics['r2']:<8.3f} \"\n",
        "              f\"{coverage:<10.3f} {uncert_corr:<12.3f}\")\n",
        "    \n",
        "    # Visualizations\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Sort test data for plotting\n",
        "    sort_idx = np.argsort(X_test.flatten())\n",
        "    X_test_sorted = X_test[sort_idx]\n",
        "    y_test_sorted = y_test[sort_idx]\n",
        "    \n",
        "    # Bootstrap results\n",
        "    axes[0,0].scatter(X_test, y_test, alpha=0.6, s=20, label='True')\n",
        "    axes[0,0].plot(X_test_sorted, boot_mean[sort_idx], 'r-', label='Prediction')\n",
        "    axes[0,0].fill_between(X_test_sorted.flatten(), \n",
        "                          boot_lower[sort_idx], boot_upper[sort_idx], \n",
        "                          alpha=0.3, color='red', label='95% CI')\n",
        "    axes[0,0].set_title('Bootstrap Aggregation')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Quantile regression\n",
        "    axes[0,1].scatter(X_test, y_test, alpha=0.6, s=20, label='True')\n",
        "    axes[0,1].plot(X_test_sorted, quantile_preds[0.5][sort_idx], 'g-', label='Median')\n",
        "    axes[0,1].fill_between(X_test_sorted.flatten(),\n",
        "                          quantile_preds[0.025][sort_idx], quantile_preds[0.975][sort_idx],\n",
        "                          alpha=0.3, color='green', label='95% PI')\n",
        "    axes[0,1].set_title('Quantile Regression')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # MC Dropout\n",
        "    axes[1,0].scatter(X_test, y_test, alpha=0.6, s=20, label='True')\n",
        "    axes[1,0].plot(X_test_sorted, mc_mean[sort_idx], 'b-', label='Mean')\n",
        "    \n",
        "    # Calculate confidence intervals for MC Dropout\n",
        "    mc_lower = mc_mean - 1.96 * mc_std\n",
        "    mc_upper = mc_mean + 1.96 * mc_std\n",
        "    \n",
        "    axes[1,0].fill_between(X_test_sorted.flatten(),\n",
        "                          mc_lower[sort_idx], mc_upper[sort_idx],\n",
        "                          alpha=0.3, color='blue', label='95% CI')\n",
        "    axes[1,0].set_title('MC Dropout')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Uncertainty comparison\n",
        "    residuals_boot = np.abs(y_test - boot_mean)\n",
        "    residuals_mc = np.abs(y_test - mc_mean)\n",
        "    \n",
        "    axes[1,1].scatter(boot_std, residuals_boot, alpha=0.6, label='Bootstrap', s=30)\n",
        "    axes[1,1].scatter(mc_std, residuals_mc, alpha=0.6, label='MC Dropout', s=30)\n",
        "    axes[1,1].set_xlabel('Predicted Uncertainty')\n",
        "    axes[1,1].set_ylabel('Absolute Error')\n",
        "    axes[1,1].set_title('Uncertainty vs Actual Error')\n",
        "    axes[1,1].legend()\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return methods_data\n",
        "\n",
        "uncertainty_methods = demonstrate_uncertainty_quantification()"
      ],
      "id": "9a9cd452",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Key concepts for model evaluation and validation:\n",
        "- **Classification Metrics**: Accuracy, precision, recall, F1, AUC, confusion matrices\n",
        "- **Regression Metrics**: MAE, MSE, RMSE, R², residual analysis\n",
        "- **Segmentation Metrics**: IoU, Dice coefficient, pixel accuracy, mean accuracy\n",
        "- **Spatial Validation**: Block CV, buffer zones, spatial independence\n",
        "- **Temporal Validation**: Walk-forward, time series splits, seasonal validation\n",
        "- **Uncertainty Quantification**: Bootstrap, quantile regression, Monte Carlo dropout\n",
        "- **Domain-Specific Considerations**: Spatial autocorrelation, temporal dependencies\n",
        "- **Comprehensive Evaluation**: Multiple metrics, visualization, statistical testing"
      ],
      "id": "48452f78"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "geoai",
      "language": "python",
      "display_name": "GeoAI",
      "path": "/Users/kellycaylor/Library/Jupyter/kernels/geoai"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}