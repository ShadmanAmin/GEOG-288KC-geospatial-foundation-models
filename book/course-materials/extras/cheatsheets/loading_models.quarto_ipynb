{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Loading Pre-trained Models\"\n",
        "subtitle: \"Working with HuggingFace model hub\"\n",
        "jupyter: geoai\n",
        "format: html\n",
        "---\n",
        "\n",
        "\n",
        "## Introduction to Pre-trained Models\n",
        "\n",
        "Pre-trained models are essential for geospatial AI, providing foundation models trained on large satellite imagery datasets.\n"
      ],
      "id": "8bcc5e4c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig, AutoImageProcessor\n",
        "from huggingface_hub import hf_hub_download\n",
        "import timm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers available: {'✓' if 'transformers' in globals() else '✗'}\")\n",
        "print(f\"Timm available: {'✓' if 'timm' in globals() else '✗'}\")"
      ],
      "id": "e7a6467b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HuggingFace Model Hub\n",
        "\n",
        "### Loading vision transformers"
      ],
      "id": "f5299685"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example model configurations (adjust based on actual available models)\n",
        "GEOSPATIAL_MODELS = {\n",
        "    'prithvi': {\n",
        "        'model_id': 'ibm-nasa-geospatial/Prithvi-100M',\n",
        "        'description': 'NASA-IBM Prithvi foundation model for Earth observations'\n",
        "    },\n",
        "    'satlas': {\n",
        "        'model_id': 'allenai/satlas-pretrain',  \n",
        "        'description': 'Satlas foundation model for satellite imagery'\n",
        "    },\n",
        "    'clay': {\n",
        "        'model_id': 'made-with-clay/Clay',\n",
        "        'description': 'Clay foundation model for Earth observation'\n",
        "    }\n",
        "}\n",
        "\n",
        "def load_geospatial_model(model_name='prithvi', device='cpu'):\n",
        "    \"\"\"Load a geospatial foundation model\"\"\"\n",
        "    \n",
        "    if model_name not in GEOSPATIAL_MODELS:\n",
        "        print(f\"Model {model_name} not found. Available: {list(GEOSPATIAL_MODELS.keys())}\")\n",
        "        return None\n",
        "    \n",
        "    model_info = GEOSPATIAL_MODELS[model_name]\n",
        "    model_id = model_info['model_id']\n",
        "    \n",
        "    try:\n",
        "        # Load configuration\n",
        "        config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
        "        \n",
        "        # Load model\n",
        "        model = AutoModel.from_pretrained(\n",
        "            model_id,\n",
        "            config=config,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16 if device != 'cpu' else torch.float32\n",
        "        )\n",
        "        \n",
        "        # Load image processor if available\n",
        "        try:\n",
        "            processor = AutoImageProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "        except:\n",
        "            processor = None\n",
        "            print(f\"No image processor found for {model_name}\")\n",
        "        \n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        print(f\"Loaded {model_info['description']}\")\n",
        "        return model, processor, config\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {model_name}: {e}\")\n",
        "        print(\"Creating mock model for demonstration...\")\n",
        "        \n",
        "        # Create a mock transformer model for demonstration\n",
        "        class MockGeospatialModel(nn.Module):\n",
        "            def __init__(self, num_bands=6, embed_dim=768, num_patches=196):\n",
        "                super().__init__()\n",
        "                self.patch_embed = nn.Conv2d(num_bands, embed_dim, 16, 16)\n",
        "                self.transformer = nn.TransformerEncoder(\n",
        "                    nn.TransformerEncoderLayer(embed_dim, 12), \n",
        "                    num_layers=12\n",
        "                )\n",
        "                self.norm = nn.LayerNorm(embed_dim)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                # Patch embedding\n",
        "                x = self.patch_embed(x)  # [B, embed_dim, H/16, W/16]\n",
        "                B, C, H, W = x.shape\n",
        "                x = x.flatten(2).transpose(1, 2)  # [B, num_patches, embed_dim]\n",
        "                \n",
        "                # Add positional embeddings (simplified)\n",
        "                x = self.transformer(x)\n",
        "                x = self.norm(x)\n",
        "                \n",
        "                return {'last_hidden_state': x}\n",
        "        \n",
        "        model = MockGeospatialModel()\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        return model, None, {'description': f'Mock {model_name} model'}\n",
        "\n",
        "# Load a geospatial model\n",
        "model, processor, config = load_geospatial_model('prithvi', device='cpu')\n",
        "\n",
        "if model is not None:\n",
        "    print(f\"Model loaded successfully\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "id": "9889a8ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model configuration and metadata"
      ],
      "id": "3170324a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def inspect_model_config(model, config):\n",
        "    \"\"\"Inspect model configuration and capabilities\"\"\"\n",
        "    \n",
        "    print(\"=== Model Configuration ===\")\n",
        "    \n",
        "    if hasattr(config, '__dict__'):\n",
        "        for key, value in config.__dict__.items():\n",
        "            if not key.startswith('_'):\n",
        "                print(f\"{key}: {value}\")\n",
        "    \n",
        "    print(\"\\n=== Model Architecture ===\")\n",
        "    \n",
        "    # Count parameters by layer type\n",
        "    layer_params = {}\n",
        "    for name, module in model.named_modules():\n",
        "        layer_type = type(module).__name__\n",
        "        if layer_type not in layer_params:\n",
        "            layer_params[layer_type] = 0\n",
        "        layer_params[layer_type] += sum(p.numel() for p in module.parameters())\n",
        "    \n",
        "    for layer_type, params in sorted(layer_params.items(), key=lambda x: x[1], reverse=True):\n",
        "        if params > 0:\n",
        "            print(f\"{layer_type}: {params:,} parameters\")\n",
        "\n",
        "if model is not None and config is not None:\n",
        "    inspect_model_config(model, config)"
      ],
      "id": "85657993",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom model loading utilities"
      ],
      "id": "8c56cb47"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class GeospatialModelLoader:\n",
        "    \"\"\"Utility class for loading geospatial models\"\"\"\n",
        "    \n",
        "    def __init__(self, cache_dir='./model_cache'):\n",
        "        self.cache_dir = cache_dir\n",
        "        self.loaded_models = {}\n",
        "    \n",
        "    def download_model_file(self, model_id, filename, subfolder=None):\n",
        "        \"\"\"Download specific model file\"\"\"\n",
        "        try:\n",
        "            file_path = hf_hub_download(\n",
        "                repo_id=model_id,\n",
        "                filename=filename,\n",
        "                subfolder=subfolder,\n",
        "                cache_dir=self.cache_dir\n",
        "            )\n",
        "            print(f\"Downloaded {filename} to {file_path}\")\n",
        "            return file_path\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {filename}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def load_with_custom_weights(self, model_id, local_weights_path=None):\n",
        "        \"\"\"Load model with custom weights\"\"\"\n",
        "        \n",
        "        # Create base model architecture\n",
        "        try:\n",
        "            config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
        "            model = AutoModel.from_pretrained(\n",
        "                model_id, \n",
        "                config=config, \n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            \n",
        "            # Load custom weights if provided\n",
        "            if local_weights_path and os.path.exists(local_weights_path):\n",
        "                print(f\"Loading custom weights from {local_weights_path}\")\n",
        "                state_dict = torch.load(local_weights_path, map_location='cpu')\n",
        "                model.load_state_dict(state_dict, strict=False)\n",
        "            \n",
        "            return model, config\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return None, None\n",
        "    \n",
        "    def cache_model(self, model_name, model, config):\n",
        "        \"\"\"Cache loaded model in memory\"\"\"\n",
        "        self.loaded_models[model_name] = {\n",
        "            'model': model,\n",
        "            'config': config,\n",
        "            'timestamp': torch.tensor(0)  # Placeholder timestamp\n",
        "        }\n",
        "    \n",
        "    def get_cached_model(self, model_name):\n",
        "        \"\"\"Retrieve cached model\"\"\"\n",
        "        return self.loaded_models.get(model_name, None)\n",
        "\n",
        "# Initialize model loader\n",
        "loader = GeospatialModelLoader()\n",
        "\n",
        "# Example usage\n",
        "# custom_model, custom_config = loader.load_with_custom_weights(\n",
        "#     'ibm-nasa-geospatial/Prithvi-100M',\n",
        "#     local_weights_path='path/to/custom_weights.pth'\n",
        "# )\n",
        "\n",
        "print(\"Model loader utilities ready\")"
      ],
      "id": "75666e62",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TIMM (Torch Image Models) Integration\n",
        "\n",
        "### Loading vision models from TIMM"
      ],
      "id": "fafdd4cf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_timm_model(model_name, pretrained=True, num_classes=1000):\n",
        "    \"\"\"Load vision model from TIMM\"\"\"\n",
        "    \n",
        "    try:\n",
        "        # List some common geospatial-friendly architectures\n",
        "        if model_name == 'demo':\n",
        "            model_name = 'resnet50'  # Use ResNet50 for demo\n",
        "        \n",
        "        model = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=num_classes,\n",
        "            global_pool='avg'  # Global average pooling\n",
        "        )\n",
        "        \n",
        "        # Get model information\n",
        "        model_info = {\n",
        "            'name': model_name,\n",
        "            'num_params': sum(p.numel() for p in model.parameters()),\n",
        "            'input_size': model.default_cfg.get('input_size', (3, 224, 224)),\n",
        "            'mean': model.default_cfg.get('mean', (0.485, 0.456, 0.406)),\n",
        "            'std': model.default_cfg.get('std', (0.229, 0.224, 0.225))\n",
        "        }\n",
        "        \n",
        "        return model, model_info\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading TIMM model {model_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Load a TIMM model\n",
        "model_timm, model_info = load_timm_model('demo', pretrained=True, num_classes=10)\n",
        "\n",
        "if model_timm is not None:\n",
        "    print(f\"TIMM Model: {model_info['name']}\")\n",
        "    print(f\"Parameters: {model_info['num_params']:,}\")\n",
        "    print(f\"Input size: {model_info['input_size']}\")\n",
        "    print(f\"Normalization - Mean: {model_info['mean']}\")\n",
        "    print(f\"Normalization - Std: {model_info['std']}\")"
      ],
      "id": "2b0948f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adapting models for multi-spectral data"
      ],
      "id": "8859f11d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MultispectralAdapter:\n",
        "    \"\"\"Adapter for converting RGB models to multi-spectral\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def adapt_input_layer(model, num_input_channels=6, method='repeat'):\n",
        "        \"\"\"Adapt first layer for multi-spectral input\"\"\"\n",
        "        \n",
        "        # Find first convolutional layer\n",
        "        first_conv = None\n",
        "        first_conv_name = None\n",
        "        \n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                first_conv = module\n",
        "                first_conv_name = name\n",
        "                break\n",
        "        \n",
        "        if first_conv is None:\n",
        "            print(\"No convolutional layer found\")\n",
        "            return model\n",
        "        \n",
        "        old_weight = first_conv.weight.data\n",
        "        old_in_channels = old_weight.shape[1]\n",
        "        \n",
        "        print(f\"Adapting {first_conv_name}: {old_in_channels} -> {num_input_channels} channels\")\n",
        "        \n",
        "        if method == 'repeat':\n",
        "            # Repeat RGB weights for additional channels\n",
        "            if num_input_channels > old_in_channels:\n",
        "                # Calculate how to distribute new channels\n",
        "                repeats = num_input_channels // old_in_channels\n",
        "                remainder = num_input_channels % old_in_channels\n",
        "                \n",
        "                new_weight = old_weight.repeat(1, repeats, 1, 1)\n",
        "                if remainder > 0:\n",
        "                    extra_weight = old_weight[:, :remainder, :, :]\n",
        "                    new_weight = torch.cat([new_weight, extra_weight], dim=1)\n",
        "                \n",
        "                # Scale weights to maintain similar activation magnitudes\n",
        "                new_weight = new_weight / (num_input_channels / old_in_channels)\n",
        "                \n",
        "            else:\n",
        "                # Use subset of weights\n",
        "                new_weight = old_weight[:, :num_input_channels, :, :]\n",
        "        \n",
        "        elif method == 'zeros':\n",
        "            # Initialize new channels with zeros\n",
        "            new_weight = torch.zeros(\n",
        "                old_weight.shape[0], num_input_channels,\n",
        "                old_weight.shape[2], old_weight.shape[3]\n",
        "            )\n",
        "            # Copy RGB weights to first channels\n",
        "            copy_channels = min(old_in_channels, num_input_channels)\n",
        "            new_weight[:, :copy_channels, :, :] = old_weight[:, :copy_channels, :, :]\n",
        "        \n",
        "        elif method == 'random':\n",
        "            # Random initialization for new channels\n",
        "            new_weight = torch.randn(\n",
        "                old_weight.shape[0], num_input_channels,\n",
        "                old_weight.shape[2], old_weight.shape[3]\n",
        "            ) * 0.01\n",
        "            # Copy RGB weights\n",
        "            copy_channels = min(old_in_channels, num_input_channels)\n",
        "            new_weight[:, :copy_channels, :, :] = old_weight[:, :copy_channels, :, :]\n",
        "        \n",
        "        # Create new layer\n",
        "        new_conv = nn.Conv2d(\n",
        "            num_input_channels,\n",
        "            first_conv.out_channels,\n",
        "            first_conv.kernel_size,\n",
        "            first_conv.stride,\n",
        "            first_conv.padding,\n",
        "            first_conv.dilation,\n",
        "            first_conv.groups,\n",
        "            first_conv.bias is not None\n",
        "        )\n",
        "        \n",
        "        # Set weights and bias\n",
        "        new_conv.weight.data = new_weight\n",
        "        if first_conv.bias is not None:\n",
        "            new_conv.bias.data = first_conv.bias.data\n",
        "        \n",
        "        # Replace layer in model\n",
        "        parent_module = model\n",
        "        attr_chain = first_conv_name.split('.')\n",
        "        \n",
        "        for attr_name in attr_chain[:-1]:\n",
        "            parent_module = getattr(parent_module, attr_name)\n",
        "        \n",
        "        setattr(parent_module, attr_chain[-1], new_conv)\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    @staticmethod\n",
        "    def freeze_backbone(model, freeze_layers=['conv1', 'layer1', 'layer2']):\n",
        "        \"\"\"Freeze specified layers of the model\"\"\"\n",
        "        \n",
        "        frozen_params = 0\n",
        "        total_params = 0\n",
        "        \n",
        "        for name, param in model.named_parameters():\n",
        "            total_params += param.numel()\n",
        "            \n",
        "            should_freeze = any(layer in name for layer in freeze_layers)\n",
        "            if should_freeze:\n",
        "                param.requires_grad = False\n",
        "                frozen_params += param.numel()\n",
        "        \n",
        "        print(f\"Frozen {frozen_params:,} / {total_params:,} parameters\")\n",
        "        print(f\"Frozen layers: {freeze_layers}\")\n",
        "        \n",
        "        return model\n",
        "\n",
        "# Example: Adapt ResNet for 6-channel input (RGB + NIR + SWIR1 + SWIR2)\n",
        "if model_timm is not None:\n",
        "    print(\"Original model input shape:\", model_timm.conv1.weight.shape)\n",
        "    \n",
        "    # Adapt for multispectral input\n",
        "    adapter = MultispectralAdapter()\n",
        "    model_adapted = adapter.adapt_input_layer(model_timm, num_input_channels=6, method='repeat')\n",
        "    \n",
        "    print(\"Adapted model input shape:\", model_adapted.conv1.weight.shape)\n",
        "    \n",
        "    # Freeze early layers\n",
        "    model_adapted = adapter.freeze_backbone(model_adapted, ['conv1', 'layer1'])"
      ],
      "id": "5292f527",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Input Preprocessing\n",
        "\n",
        "### Creating preprocessing pipelines"
      ],
      "id": "fd8d723c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SatellitePreprocessor:\n",
        "    \"\"\"Preprocessing pipeline for satellite imagery\"\"\"\n",
        "    \n",
        "    def __init__(self, \n",
        "                 input_size=(224, 224),\n",
        "                 mean=[0.485, 0.456, 0.406, 0.5, 0.5, 0.5],  # RGB + additional bands\n",
        "                 std=[0.229, 0.224, 0.225, 0.2, 0.2, 0.2],\n",
        "                 clip_values=None):\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.mean = torch.tensor(mean)\n",
        "        self.std = torch.tensor(std)\n",
        "        self.clip_values = clip_values\n",
        "    \n",
        "    def __call__(self, image):\n",
        "        \"\"\"Apply preprocessing to image tensor\"\"\"\n",
        "        \n",
        "        # Ensure tensor format [C, H, W]\n",
        "        if image.ndim == 4:\n",
        "            image = image.squeeze(0)\n",
        "        \n",
        "        # Resize if needed\n",
        "        if image.shape[-2:] != self.input_size:\n",
        "            image = torch.nn.functional.interpolate(\n",
        "                image.unsqueeze(0), \n",
        "                size=self.input_size, \n",
        "                mode='bilinear', \n",
        "                align_corners=False\n",
        "            ).squeeze(0)\n",
        "        \n",
        "        # Clip values if specified\n",
        "        if self.clip_values:\n",
        "            image = torch.clamp(image, self.clip_values[0], self.clip_values[1])\n",
        "        \n",
        "        # Normalize\n",
        "        if len(self.mean) >= image.shape[0]:\n",
        "            mean = self.mean[:image.shape[0]]\n",
        "            std = self.std[:image.shape[0]]\n",
        "            \n",
        "            # Reshape for broadcasting\n",
        "            mean = mean.view(-1, 1, 1)\n",
        "            std = std.view(-1, 1, 1)\n",
        "            \n",
        "            image = (image - mean) / std\n",
        "        \n",
        "        return image\n",
        "    \n",
        "    def batch_preprocess(self, batch):\n",
        "        \"\"\"Preprocess a batch of images\"\"\"\n",
        "        if isinstance(batch, list):\n",
        "            return torch.stack([self(img) for img in batch])\n",
        "        elif batch.ndim == 4:\n",
        "            return torch.stack([self(batch[i]) for i in range(batch.shape[0])])\n",
        "        else:\n",
        "            return self(batch)\n",
        "\n",
        "# Create preprocessor\n",
        "preprocessor = SatellitePreprocessor(\n",
        "    input_size=(224, 224),\n",
        "    mean=[0.485, 0.456, 0.406, 0.4, 0.3, 0.2],  # Adjusted for 6 bands\n",
        "    std=[0.229, 0.224, 0.225, 0.15, 0.12, 0.10],\n",
        "    clip_values=[0, 1]\n",
        ")\n",
        "\n",
        "# Test preprocessing\n",
        "sample_image = torch.rand(6, 256, 256)  # 6-band image\n",
        "processed_image = preprocessor(sample_image)\n",
        "\n",
        "print(f\"Original shape: {sample_image.shape}\")\n",
        "print(f\"Processed shape: {processed_image.shape}\")\n",
        "print(f\"Processed range: [{processed_image.min():.3f}, {processed_image.max():.3f}]\")"
      ],
      "id": "725538eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Band selection and configuration"
      ],
      "id": "01a82d60"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class BandSelector:\n",
        "    \"\"\"Utility for selecting and configuring satellite bands\"\"\"\n",
        "    \n",
        "    BAND_CONFIGS = {\n",
        "        'landsat8': {\n",
        "            'bands': ['coastal', 'blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'pan', 'cirrus', 'thermal1', 'thermal2'],\n",
        "            'rgb': [3, 2, 1],  # Red, Green, Blue (1-indexed)\n",
        "            'false_color': [4, 3, 2],  # NIR, Red, Green\n",
        "            'swir': [6, 5, 4]  # SWIR2, SWIR1, NIR\n",
        "        },\n",
        "        'sentinel2': {\n",
        "            'bands': ['coastal', 'blue', 'green', 'red', 'red_edge1', 'red_edge2', 'red_edge3', 'nir', 'red_edge4', 'water_vapor', 'swir1', 'swir2'],\n",
        "            'rgb': [3, 2, 1],\n",
        "            'false_color': [7, 3, 2],\n",
        "            'agriculture': [7, 5, 2]\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    def __init__(self, satellite='landsat8'):\n",
        "        self.satellite = satellite\n",
        "        self.config = self.BAND_CONFIGS.get(satellite, {})\n",
        "    \n",
        "    def select_bands(self, image, band_combination='rgb'):\n",
        "        \"\"\"Select specific band combination\"\"\"\n",
        "        \n",
        "        if band_combination not in self.config:\n",
        "            print(f\"Unknown combination: {band_combination}\")\n",
        "            print(f\"Available: {list(self.config.keys())}\")\n",
        "            return image\n",
        "        \n",
        "        band_indices = self.config[band_combination]\n",
        "        # Convert to 0-indexed\n",
        "        selected_indices = [i - 1 for i in band_indices]\n",
        "        \n",
        "        # Select bands\n",
        "        if image.ndim == 3:  # [C, H, W]\n",
        "            selected_image = image[selected_indices]\n",
        "        elif image.ndim == 4:  # [B, C, H, W]\n",
        "            selected_image = image[:, selected_indices]\n",
        "        else:\n",
        "            print(f\"Unsupported image shape: {image.shape}\")\n",
        "            return image\n",
        "        \n",
        "        return selected_image\n",
        "    \n",
        "    def get_band_names(self, band_combination='rgb'):\n",
        "        \"\"\"Get names of selected bands\"\"\"\n",
        "        if band_combination in self.config:\n",
        "            indices = self.config[band_combination]\n",
        "            return [self.config['bands'][i-1] for i in indices]\n",
        "        return []\n",
        "\n",
        "# Example usage\n",
        "selector = BandSelector('landsat8')\n",
        "\n",
        "# Simulate 11-band Landsat image\n",
        "landsat_image = torch.rand(11, 256, 256)\n",
        "\n",
        "# Select RGB combination\n",
        "rgb_image = selector.select_bands(landsat_image, 'rgb')\n",
        "false_color_image = selector.select_bands(landsat_image, 'false_color')\n",
        "\n",
        "print(f\"Original image: {landsat_image.shape}\")\n",
        "print(f\"RGB selection: {rgb_image.shape}\")\n",
        "print(f\"RGB bands: {selector.get_band_names('rgb')}\")\n",
        "print(f\"False color bands: {selector.get_band_names('false_color')}\")"
      ],
      "id": "46a2ff4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Inference Setup\n",
        "\n",
        "### Inference wrapper class"
      ],
      "id": "f1d52eef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class GeospatialInference:\n",
        "    \"\"\"Wrapper for geospatial model inference\"\"\"\n",
        "    \n",
        "    def __init__(self, model, preprocessor=None, device='cpu'):\n",
        "        self.model = model\n",
        "        self.preprocessor = preprocessor\n",
        "        self.device = device\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def predict(self, images, return_features=False):\n",
        "        \"\"\"Run inference on images\"\"\"\n",
        "        \n",
        "        # Handle single image or batch\n",
        "        if isinstance(images, torch.Tensor):\n",
        "            if images.ndim == 3:  # Single image [C, H, W]\n",
        "                images = images.unsqueeze(0)  # Add batch dim\n",
        "            batch = images\n",
        "        elif isinstance(images, list):\n",
        "            batch = torch.stack(images)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported input type: {type(images)}\")\n",
        "        \n",
        "        # Preprocess if preprocessor available\n",
        "        if self.preprocessor:\n",
        "            batch = self.preprocessor.batch_preprocess(batch)\n",
        "        \n",
        "        # Move to device\n",
        "        batch = batch.to(self.device)\n",
        "        \n",
        "        # Forward pass\n",
        "        if hasattr(self.model, 'forward_features'):\n",
        "            # TIMM-style model\n",
        "            features = self.model.forward_features(batch)\n",
        "            logits = self.model.forward_head(features)\n",
        "            \n",
        "            if return_features:\n",
        "                return logits, features\n",
        "            return logits\n",
        "            \n",
        "        else:\n",
        "            # Standard forward pass\n",
        "            outputs = self.model(batch)\n",
        "            \n",
        "            # Handle different output formats\n",
        "            if isinstance(outputs, dict):\n",
        "                if 'last_hidden_state' in outputs:\n",
        "                    features = outputs['last_hidden_state']\n",
        "                    # Global average pooling for classification\n",
        "                    if len(features.shape) == 3:  # [B, seq_len, dim]\n",
        "                        pooled = features.mean(dim=1)\n",
        "                    else:\n",
        "                        pooled = features\n",
        "                    \n",
        "                    if return_features:\n",
        "                        return pooled, features\n",
        "                    return pooled\n",
        "                else:\n",
        "                    return outputs\n",
        "            else:\n",
        "                return outputs\n",
        "    \n",
        "    def extract_features(self, images, layer_name=None):\n",
        "        \"\"\"Extract features from specific layer\"\"\"\n",
        "        \n",
        "        features = {}\n",
        "        \n",
        "        def hook(name):\n",
        "            def fn(module, input, output):\n",
        "                features[name] = output.detach()\n",
        "            return fn\n",
        "        \n",
        "        # Register hooks\n",
        "        hooks = []\n",
        "        for name, module in self.model.named_modules():\n",
        "            if layer_name is None or layer_name in name:\n",
        "                hooks.append(module.register_forward_hook(hook(name)))\n",
        "        \n",
        "        # Forward pass\n",
        "        _ = self.predict(images)\n",
        "        \n",
        "        # Remove hooks\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "        \n",
        "        return features\n",
        "    \n",
        "    def get_attention_maps(self, images, layer_idx=-1):\n",
        "        \"\"\"Extract attention maps (for transformer models)\"\"\"\n",
        "        \n",
        "        # This is a simplified example - actual implementation\n",
        "        # depends on specific model architecture\n",
        "        \n",
        "        attention_maps = {}\n",
        "        \n",
        "        def attention_hook(name):\n",
        "            def fn(module, input, output):\n",
        "                # Assuming output contains attention weights\n",
        "                if hasattr(output, 'attentions'):\n",
        "                    attention_maps[name] = output.attentions\n",
        "                elif len(output) > 1 and hasattr(output[1], 'shape'):\n",
        "                    attention_maps[name] = output[1]  # Attention weights\n",
        "            return fn\n",
        "        \n",
        "        # Look for attention modules\n",
        "        hooks = []\n",
        "        for name, module in self.model.named_modules():\n",
        "            if 'attention' in name.lower() or 'attn' in name.lower():\n",
        "                hooks.append(module.register_forward_hook(attention_hook(name)))\n",
        "        \n",
        "        # Forward pass\n",
        "        _ = self.predict(images)\n",
        "        \n",
        "        # Remove hooks\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "        \n",
        "        return attention_maps if attention_maps else None\n",
        "\n",
        "# Create inference wrapper\n",
        "if model_adapted is not None and preprocessor is not None:\n",
        "    inference = GeospatialInference(model_adapted, preprocessor, device='cpu')\n",
        "    \n",
        "    # Test inference\n",
        "    test_image = torch.rand(6, 256, 256)  # 6-band image\n",
        "    predictions = inference.predict(test_image)\n",
        "    \n",
        "    print(f\"Input shape: {test_image.shape}\")\n",
        "    print(f\"Prediction shape: {predictions.shape}\")\n",
        "    print(\"Inference wrapper ready\")\n",
        "else:\n",
        "    print(\"Model or preprocessor not available for inference demo\")"
      ],
      "id": "d68b67b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch inference utilities"
      ],
      "id": "c47497d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class BatchInference:\n",
        "    \"\"\"Utilities for efficient batch inference\"\"\"\n",
        "    \n",
        "    def __init__(self, model, batch_size=32, device='cpu'):\n",
        "        self.model = model\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def predict_dataset(self, dataloader, progress=True):\n",
        "        \"\"\"Run inference on entire dataset\"\"\"\n",
        "        \n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        \n",
        "        total_batches = len(dataloader) if hasattr(dataloader, '__len__') else None\n",
        "        \n",
        "        for batch_idx, batch in enumerate(dataloader):\n",
        "            if progress and total_batches:\n",
        "                print(f\"Processing batch {batch_idx + 1}/{total_batches}\", end='\\r')\n",
        "            \n",
        "            # Handle different batch formats\n",
        "            if isinstance(batch, dict):\n",
        "                images = batch['image'].to(self.device)\n",
        "                if 'label' in batch:\n",
        "                    labels = batch['label']\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "            else:\n",
        "                images = batch[0].to(self.device) if isinstance(batch, (list, tuple)) else batch.to(self.device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = self.model(images)\n",
        "            \n",
        "            # Handle different output formats\n",
        "            if isinstance(outputs, dict):\n",
        "                predictions = outputs.get('logits', outputs.get('last_hidden_state', outputs))\n",
        "            else:\n",
        "                predictions = outputs\n",
        "            \n",
        "            all_predictions.append(predictions.cpu())\n",
        "        \n",
        "        # Concatenate all predictions\n",
        "        final_predictions = torch.cat(all_predictions, dim=0)\n",
        "        \n",
        "        result = {'predictions': final_predictions}\n",
        "        if all_labels:\n",
        "            result['labels'] = np.array(all_labels)\n",
        "        \n",
        "        if progress:\n",
        "            print(f\"\\nCompleted inference on {len(final_predictions)} samples\")\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def predict_large_image(self, large_image, window_size=224, stride=112, aggregation='mean'):\n",
        "        \"\"\"Run inference on large image using sliding windows\"\"\"\n",
        "        \n",
        "        if large_image.ndim == 3:  # [C, H, W]\n",
        "            c, h, w = large_image.shape\n",
        "        else:\n",
        "            raise ValueError(\"Expected 3D image [C, H, W]\")\n",
        "        \n",
        "        # Calculate number of windows\n",
        "        h_windows = (h - window_size) // stride + 1\n",
        "        w_windows = (w - window_size) // stride + 1\n",
        "        \n",
        "        predictions = []\n",
        "        window_positions = []\n",
        "        \n",
        "        for i in range(h_windows):\n",
        "            for j in range(w_windows):\n",
        "                # Extract window\n",
        "                start_h = i * stride\n",
        "                end_h = start_h + window_size\n",
        "                start_w = j * stride  \n",
        "                end_w = start_w + window_size\n",
        "                \n",
        "                window = large_image[:, start_h:end_h, start_w:end_w]\n",
        "                \n",
        "                # Predict on window\n",
        "                window_batch = window.unsqueeze(0).to(self.device)\n",
        "                output = self.model(window_batch)\n",
        "                \n",
        "                predictions.append(output.cpu())\n",
        "                window_positions.append((start_h, end_h, start_w, end_w))\n",
        "        \n",
        "        return {\n",
        "            'predictions': predictions,\n",
        "            'positions': window_positions,\n",
        "            'grid_shape': (h_windows, w_windows)\n",
        "        }\n",
        "\n",
        "# Example usage with mock data\n",
        "if model_adapted is not None:\n",
        "    batch_inference = BatchInference(model_adapted, batch_size=16, device='cpu')\n",
        "    \n",
        "    # Simulate large image inference\n",
        "    large_test_image = torch.rand(6, 512, 512)\n",
        "    results = batch_inference.predict_large_image(\n",
        "        large_test_image, \n",
        "        window_size=224, \n",
        "        stride=112\n",
        "    )\n",
        "    \n",
        "    print(f\"Large image shape: {large_test_image.shape}\")\n",
        "    print(f\"Number of windows: {len(results['predictions'])}\")\n",
        "    print(f\"Grid shape: {results['grid_shape']}\")"
      ],
      "id": "d15e7aad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Key concepts for loading pre-trained models:\n",
        "- **HuggingFace Hub**: Access to geospatial foundation models\n",
        "- **TIMM Integration**: Computer vision models for satellite imagery\n",
        "- **Multi-spectral adaptation**: Adapting RGB models for satellite bands\n",
        "- **Preprocessing pipelines**: Normalization and band selection\n",
        "- **Inference wrappers**: Efficient model prediction\n",
        "- **Batch processing**: Large-scale inference utilities\n",
        "- **Feature extraction**: Accessing intermediate model representations"
      ],
      "id": "882a801a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "geoai",
      "language": "python",
      "display_name": "GeoAI",
      "path": "/Users/kellycaylor/Library/Jupyter/kernels/geoai"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}