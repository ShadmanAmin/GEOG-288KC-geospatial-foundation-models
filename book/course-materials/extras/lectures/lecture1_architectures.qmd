---
title: "Lecture 1: Architectures, Histories, and Development Pipelines"
subtitle: "LLMs vs. Geospatial Foundation Models (GFMs)"
jupyter: geoai
format:
  revealjs:
    theme: ../../meds-website-styles.scss
    slide-number: true
    toc: false
    incremental: true
    code-overflow: wrap
execute:
  echo: true
  warning: false
  freeze: auto
---

## Welcome

- Course focus: Building Geospatial Foundation Models (GFMs)
- Today: compare LLM and GFM development pipelines; key architecture ideas; brief history

## Agenda

- AI/ML → Transformers: a 10,000‑ft history
- LLM development pipeline (9 steps)
- GFM development pipeline (9 steps, geospatialized)
- Key differences and implications

## From AI to Transformers (Very Brief History)

- 1950s–1990s: Symbolic AI, early neural nets
- 2012: Deep learning (ImageNet)
- 2017: Transformers ("Attention Is All You Need")
- 2018–2020: BERT/GPT families
- 2021–2024: Scaling laws; instruction tuning; multimodality; diffusion

## Transformer Essentials

- Tokenization → Embeddings → Positional encodings → Attention → MLP → Stacking → Pretraining → Finetuning
- Key superpower: learn long‑range dependencies via attention

## 9‑Step Development Pipeline (LLMs)

1. Data preparation & sampling (text corpora, dedup, mixing)
2. Tokenization (BPE, vocab; special tokens)
3. Architecture (GPT/BERT; depth/width; context length)
4. Pretraining objective (next‑token, masked LM)
5. Training loop (optimizers, LR schedule, mixed precision)
6. Evaluation (perplexity, downstream probes)
7. Load/use pretrained weights
8. Finetuning (task‑specific; PEFT)
9. Implementation & deployment (APIs, inference optimizations)

## 9‑Step Development Pipeline (GFMs)

1. Data preparation & sampling (multi‑spectral, georegistration, tiling, cloud handling)
2. Tokenization (patch‑based; continuous to embeddings; 2D/temporal positions)
3. Architecture (ViT encoders; spatial/temporal attention; memory constraints)
4. Pretraining objective (masked patch reconstruction; contrastive for multimodal)
5. Training loop (cloud masks, mixed precision, gradient strategies)
6. Evaluation (reconstruction, linear probing, spatial/temporal generalization)
7. Load/use pretrained weights (Prithvi, SatMAE; adapters)
8. Finetuning (task heads; PEFT; few‑shot for limited labels)
9. Implementation & deployment (tiling inference; APIs; geospatial UIs)

## Step‑by‑Step: LLM vs. GFM Differences

### 1) Data Preparation
- LLMs: text scraping, deduplication, quality filters
- GFMs: sensor calibration, atmospheric correction, cloud masks, georegistration, temporal compositing

### 2) Tokenization
- LLMs: discrete vocab; BPE; special tokens
- GFMs: no discrete vocab; image patches → linear projection; 2D + time encodings

### 3) Architecture
- LLMs: decoder‑only (GPT) or encoder (BERT)
- GFMs: ViT encoders; spatial/temporal attention; multispectral embeddings

### 4) Pretraining Objective
- LLMs: next‑token (AR) or masked LM (denoise text)
- GFMs: masked patch reconstruction; contrastive with text or modalities (optional)

### 5) Training Loop
- LLMs: LR schedules, batch packing, AMP
- GFMs: all of the above + cloud/validity masks; patch sampling from rasters

### 6) Evaluation
- LLMs: perplexity; downstream tasks (classification, QA)
- GFMs: reconstruction metrics; linear probes; spatial/temporal OOD generalization

### 7) Pretrained Weights
- LLMs: model hubs; tokenizer alignment
- GFMs: Prithvi/SatMAE weights; band order & resolution alignment

### 8) Finetuning
- LLMs: instruction tuning; PEFT
- GFMs: task heads (segmentation, regression); adapters/LoRA; few labels

### 9) Deployment
- LLMs: token streaming; KV cache; latency budgets
- GFMs: tiling/overlap; geospatial projections; retrieval by time/area; batch inference

## Scaling and Evolution (LLMs)

- Parameters: 100M → 10B → 70B+
- Context windows: 512 → 128k+
- Training data: curated web, books, code; cleaner data ≈ better sample efficiency

## Scaling and Evolution (GFMs)

- Parameters: 50M → 500M (varies); encoder focus
- Inputs: 6+ bands; 2D + time; multi‑sensor fusion (optical/SAR)
- Data volume constrained by storage and tiling/IO throughput

## Example: Tokenization Contrast

LLM (discrete): tokens → embedding lookup

```python
# | echo: true
vocab_size, embed_dim = 50_000, 768
import numpy as np
try:
    import torch
    tok_ids = torch.tensor([1, 2, 3, 4])
    emb = torch.nn.Embedding(vocab_size, embed_dim)(tok_ids)
    print(emb.shape)
except Exception as e:
    print("LLM embedding example skipped:", e)
```

GFM (continuous patches): patch → linear projection

```python
# | echo: true
import numpy as np
patch_dim, embed_dim = 16*16*6, 768
rng = np.random.default_rng(42)
patches = rng.normal(size=(4, patch_dim))
try:
    import torch
    proj = torch.nn.Linear(patch_dim, embed_dim)
    out = proj(torch.from_numpy(patches).float())
    print(out.shape)
except Exception as e:
    print("GFM projection example skipped:", e)
```

## Putting It Together (Course Mapping)

- Weeks 1–3: Data → Attention → Architecture
- Weeks 4–7: Pretraining → Training loop → Evaluation → Pretrained integration
- Weeks 8–10: Finetuning → Deployment → Synthesis

## References & Further Reading

- Transformers: Vaswani et al. (2017)
- Vision Transformers: Dosovitskiy et al. (2020)
- Masked Autoencoders: He et al. (2021)
- Geospatial FMs: Prithvi, SatMAE (model docs & papers)

## Hands‑On Next

- See Week 1 Interactive Session: Geospatial Data Foundations
- Start building the data pipeline with deterministic outputs


Neural networks expect inputs in a very specific, structured format. The first step of any deep learning workflow is to translate our data into a “language” that neural networks understand. This translation process produces embeddings, which are consistent numerical representations that encode our data in a form that makes them ready for model processing.

An embedding is simply a standardized vector representation of raw input data. As a toy example, `[0, 0, 1, 0]` can be thought of as an embedding of the number `2` using a most significant bit (MSB) binary positional encoding scheme. In text processing, embeddings are necessary because tokens like `forest` and `water` are categorical symbols; they must be mapped to consistent numerical values before a neural network can operate on them.

Unlike text, geospatial data values are usually already coded in a numerical scheme. Generally, each pixel contains measured values such as surface reflectance at specific wavelengths, or derived quantities such as NDVI, or categories such as land cover. 

If we don’t need to “make the data numeric”,  why do we still need embeddings for geospatial data? The answer is that **embeddings make data both interpretable and meaningful**. In a language model, every occurrence of the token `cat` maps to the same embedding (say `1356`), because `cat` usually carries the same semantic meaning, even in different contexts. In contrast, the same pixel value—for example, `0.15`—can mean something entirely different depending on whether it represents surface albedo, slope, or NDVI. Without additional semantic context, the model has no way to disambiguate these cases.

| Data Type            | Semantic Consistency 🧠 | Analytical Tractability 📊 | Primary Goal of Embeddings 🎯 |
|----------------------|------------------------|----------------------------|--------------------------------|
| **Text**             | ✅ Words/tokens have consistent meaning across a diverse contexts | ❌ Raw text is not numeric; cannot be directly processed by neural networks | **Enable computation** — convert categorical symbols into a consistent numerical space |
| **Numerical / Geospatial** | ❌ Numeric values can represent different quantities across datasets | ✅ Already stored as numbers; can be directly input to computations | **Add context** — encode what, where, and when into numerical features |

### Geospatial Semantics

While text embeddings typically just map words to vectors, geospatial embeddings must carry richer meaning. A pixel or patch value isn’t the whole story; its meaning depends on extra context. Therefore, the goal of embedding in geospatial models is not merely to turn categories into numbers, it’s to transform raw measurements into a representation that is both meaningful and efficient for learning. This might involve:

- Combining multiple spectral bands into a single vector
- Normalizing across different sensors to remove acquisition biases
- Compressing high-dimensional spectral signatures into a lower-dimensional space that preserves the most relevant patterns for the task at hand. 
- Incorporating spatial and temporal context, enabling the model to understand not just what is in a pixel, but where and when it is.


#### Semantic Dimensions for Geospatial Embeddings

| 📐 Semantic Dimension | 🧩 What It Is | 🚀 Why It Matters for GFMs |
|---|---|---|
| 🗂 **Source meaning** | The same number can represent different physical measurements depending on where the data came from and how it was processed | Without knowing the source, two identical values might mean very different things |
| 🌍 **Location reference** | Information that describes where the measurement is on Earth and how the image is laid out | Changes in location reference affect how measurements line up and can change their meaning |
| 🔎 **Detail and scale** | How much area each pixel represents, and whether it shows a single thing or a mix of things | Larger pixels mix more features, which changes how the value should be interpreted |
| 📏 **Measurement units and type** | Values can be in different units (meters, degrees, percentages) or represent categories instead of continuous numbers | Mixing units or types can confuse the model without clear handling |
| 📅 **When and how it was collected** | The date, the tool or satellite used, the viewing angle, and environmental conditions | Even the same number can mean different things at different times or with different tools |
| 🧮 **Calculated or classified values** | Numbers created from other measurements, like vegetation indexes or land cover labels | These values already include assumptions that influence how they should be used |
| 📦 **Extra context** | Additional information like location, time, or sensor type added alongside the pixel values | Helps the model understand what, where, and when each measurement is from |

:::{.callout-important}
## Embedding for learning
Geospatial embeddings must combine the measured values with extra information about **what was measured, where, when, and how**, so the model learns from the *full meaning* of each observation.
:::

