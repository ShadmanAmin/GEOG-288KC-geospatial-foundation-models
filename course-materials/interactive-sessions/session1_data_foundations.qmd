---
title: "Week 1 Interactive Session: Geospatial Data Foundations"
subtitle: "From pixels to tokens: Building robust data pipelines for foundation models"
editor_options: 
  chunk_output_type: console
jupyter: geoai
format: 
    html:
        toc: true
        toc-depth: 3
        code-fold: show
---

## Introduction

This session covers the fundamental data preparation steps for building geospatial foundation models. Just as text requires tokenization and careful data loading for LLMs, geospatial data requires specialized preprocessing, robust handling of missing data (clouds!), and spatial-temporal sequence creation for foundation models like Prithvi.

We'll build complete data pipelines that can handle real-world satellite imagery for foundation model training, including the 6-band HLS data used by NASA-IBM's Prithvi foundation model.

### Learning Objectives
- Understand geospatial data as foundation model input
- Implement robust data preprocessing pipelines  
- Handle missing data (clouds, gaps) effectively
- Create temporal sequences from satellite imagery
- Build efficient PyTorch data loaders for training
- Compare text vs. image tokenization approaches

### Session Overview
We'll cover:
1. **Understanding Geospatial Embeddings** (vs. text embeddings)
2. **Tokenizing Satellite Imagery** (vs. text tokenization)  
3. **Creating Spatial-Temporal Sequences** (vs. text sequences)
4. **Building Data Loaders** (adapted for geospatial data)
5. **Preprocessing Pipelines** (radiometric calibration, cloud masking)

---

## 1. Understanding Geospatial Embeddings

Text models work with discrete tokens that map to embedding vectors. But geospatial data is fundamentally different - we're working with continuous multi-spectral measurements that represent physical properties of Earth's surface.

### Text vs. Geospatial Embeddings

Let's first understand how text embeddings work, then see how we adapt this for satellite imagery:

```{python}
# | echo: true
import torch
import numpy as np
import matplotlib.pyplot as plt

# Text approach
print("=== TEXT EMBEDDINGS ===")
vocab_size = 50257  # GPT-2 vocabulary
embed_dim = 768     # GPT-2 embedding dimension

# Discrete token IDs -> embedding lookup
text_vocab = {"the": 1, "cat": 2, "sat": 3, "on": 4, "mat": 5}
token_ids = torch.tensor([1, 2, 3, 4, 1, 5])  # "the cat sat on the mat"

text_embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)
text_embeddings = text_embedding_layer(token_ids)

print(f"Text tokens shape: {token_ids.shape}")
print(f"Text embeddings shape: {text_embeddings.shape}")
print(f"Process: Discrete tokens ‚Üí Lookup table ‚Üí Embeddings")

print("\n=== GEOSPATIAL EMBEDDINGS ===")
# Geospatial approach - continuous patch vectors
patch_size = 16
num_bands = 6  # HLS bands: Blue, Green, Red, NIR, SWIR1, SWIR2
input_dim = patch_size * patch_size * num_bands  # 16 * 16 * 6 = 1536

# Continuous patch values -> learned projection
np.random.seed(42)
patch_data = torch.randn(4, input_dim)  # 4 patches from satellite image

geo_projection_layer = torch.nn.Linear(input_dim, embed_dim)
geo_embeddings = geo_projection_layer(patch_data)

print(f"Patch data shape: {patch_data.shape}")
print(f"Geo embeddings shape: {geo_embeddings.shape}")
print(f"Process: Continuous patches ‚Üí Linear projection ‚Üí Embeddings")

print(f"\nKey difference:")
print(f"‚Ä¢ Text: {vocab_size:,} discrete symbols in vocabulary")
print(f"‚Ä¢ Geospatial: Infinite continuous values (no traditional vocabulary)")
```

:::{.callout-important}
## Fundamental Difference

**Text models**: Use discrete symbol lookup tables  
**Geospatial models**: Use continuous vector projections

This is why we can't simply apply text tokenization to images!
:::

---

## 2. Downloading and Loading Geospatial Data

Let's start by downloading sample satellite imagery:

```{python}
# | echo: true
import urllib.request
import os
from pathlib import Path
import rasterio
import numpy as np

# Use the course data directory
data_dir = Path("../../data")
data_dir.mkdir(exist_ok=True)

# Download sample satellite image (similar to Raschka's text download)
url = "https://raw.githubusercontent.com/kellycaylor/geoAI/main/data/landcover_sample.tif"
sample_file = data_dir / "landcover_sample.tif"

if not sample_file.exists():
    print("Downloading sample satellite image...")
    urllib.request.urlretrieve(url, sample_file)
    print(f"Downloaded to: {sample_file}")
else:
    print(f"Sample satellite image already exists at: {sample_file}")

print(f"File size: {sample_file.stat().st_size / 1024:.1f} KB")

# Load and examine the data
try:
    with rasterio.open(sample_file) as src:
        print(f"Image dimensions: {src.width} x {src.height}")
        print(f"Number of bands: {src.count}")
        print(f"Data type: {src.dtypes[0]}")
        print(f"Coordinate system: {src.crs}")
        print(f"Bounds: {src.bounds}")
        
        # Read first 3 bands for RGB visualization
        rgb_data = src.read([1, 2, 3])  # Assuming RGB bands
        
except Exception as e:
    print(f"Could not read with rasterio: {e}")
    rgb_data = None
```

Let's also create synthetic HLS data to match what Prithvi uses:

```{python}
# | echo: true
def create_synthetic_hls_data(height=224, width=224):
    """
    Create synthetic 6-band HLS-like data for demonstration
    Bands: Blue, Green, Red, NIR, SWIR1, SWIR2
    """
    np.random.seed(42)  # For reproducible results
    
    # Create realistic spectral signatures for different land cover types
    x = np.linspace(0, 4*np.pi, width)
    y = np.linspace(0, 4*np.pi, height)
    X, Y = np.meshgrid(x, y)
    
    # Base elevation-like pattern
    elevation = np.sin(X/3) * np.cos(Y/3) + 0.5 * np.sin(X) * np.sin(Y)
    
    # Simulate different spectral bands with realistic values
    bands = {}
    
    # Visible bands (vegetation appears darker)
    bands['blue'] = np.clip(0.1 + 0.05 * elevation + 0.02 * np.random.randn(height, width), 0, 1)
    bands['green'] = np.clip(0.15 + 0.08 * elevation + 0.02 * np.random.randn(height, width), 0, 1)
    bands['red'] = np.clip(0.12 + 0.06 * elevation + 0.02 * np.random.randn(height, width), 0, 1)
    
    # NIR band (vegetation appears bright)
    vegetation_mask = elevation > 0.2
    bands['nir'] = np.clip(0.4 * vegetation_mask + 0.1 * (~vegetation_mask) + 
                   0.05 * elevation + 0.03 * np.random.randn(height, width), 0, 1)
    
    # SWIR bands (sensitive to moisture and soil)
    bands['swir1'] = np.clip(0.2 + 0.1 * elevation + 0.03 * np.random.randn(height, width), 0, 1)
    bands['swir2'] = np.clip(0.15 + 0.08 * elevation + 0.03 * np.random.randn(height, width), 0, 1)
    
    # Stack bands in HLS order
    hls_data = np.stack([
        bands['blue'], bands['green'], bands['red'],
        bands['nir'], bands['swir1'], bands['swir2']
    ])
    
    return hls_data, bands

# Create our dataset
hls_image, band_dict = create_synthetic_hls_data()
print(f"Synthetic HLS data shape: {hls_image.shape}")  # (6, 224, 224)
print(f"Data type: {hls_image.dtype}")
print(f"Value range: [{hls_image.min():.3f}, {hls_image.max():.3f}]")

# Visualize the multi-spectral data
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

band_names = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']
for i, band_name in enumerate(band_names):
    im = axes[i].imshow(band_dict[band_name], cmap='viridis', vmin=0, vmax=1)
    axes[i].set_title(f'{band_name.upper()} Band')
    axes[i].axis('off')
    plt.colorbar(im, ax=axes[i], fraction=0.046)

plt.tight_layout()
plt.show()

# Create RGB composite for reference
rgb_composite = np.stack([band_dict['red'], band_dict['green'], band_dict['blue']], axis=2)
rgb_composite = np.clip(rgb_composite * 2.5, 0, 1)  # Enhance contrast

plt.figure(figsize=(8, 8))
plt.imshow(rgb_composite)
plt.title('RGB Composite (Red-Green-Blue)')
plt.axis('off')
plt.show()
```

---

## 3. Tokenizing Satellite Imagery

Just as there are different approaches to text tokenization (character, word, subword), we need specialized approaches for satellite imagery. The key challenge: **images have no natural token boundaries** like spaces and punctuation in text.

### Text Tokenization Review

Let's quickly review how text tokenization works:

```{python}
# | echo: true
import re

# Text tokenization example
text = "Hello, world. This, is a test."

# Simple regex tokenization
text_tokens = re.split(r'([,.]|\s)', text)
text_tokens = [item for item in text_tokens if item.strip()]
print(f"Text tokens: {text_tokens}")
print(f"Number of tokens: {len(text_tokens)}")

# Text has natural boundaries: spaces, punctuation
# Each token has semantic meaning: "Hello", "world", "test"
```

### Geospatial Tokenization Challenge

```{python}
# | echo: true
# Unlike text, images have no obvious boundaries
print("GEOSPATIAL TOKENIZATION CHALLENGE:")
print("üå≥ Where does the 'tree' token begin and end?")
print("üçÉ Should each leaf be a separate token?") 
print("üè† Is a house one token or multiple (roof, walls, door)?")
print("‚òÅÔ∏è How do we handle missing data (clouds)?")
print("üìê How do we preserve spatial relationships?")
```

### Patch-Based Tokenization (Vision Transformer Style)

Following the approach used by Prithvi and other vision foundation models:

```{python}
# | echo: true
class PatchTokenizer:
    """
    Patch-based tokenizer for satellite imagery
    Similar to Vision Transformer approach
    """
    
    def __init__(self, patch_size=16, flatten_patches=True):
        self.patch_size = patch_size
        self.flatten_patches = flatten_patches
        
    def extract_patches(self, image):
        """
        Extract non-overlapping patches from multi-band image
        
        Args:
            image: Multi-band image of shape (bands, height, width)
            
        Returns:
            patches: Array of shape (num_patches, bands * patch_size * patch_size)
        """
        bands, height, width = image.shape
        
        # Calculate number of patches
        patches_h = height // self.patch_size
        patches_w = width // self.patch_size
        
        # Crop image to fit exact number of patches
        cropped_h = patches_h * self.patch_size
        cropped_w = patches_w * self.patch_size
        cropped_image = image[:, :cropped_h, :cropped_w]
        
        # Reshape into patches
        patches = cropped_image.reshape(
            bands, patches_h, self.patch_size, patches_w, self.patch_size
        )
        # Rearrange dimensions: (patches_h, patches_w, bands, patch_size, patch_size)
        patches = patches.transpose(1, 3, 0, 2, 4)
        # Flatten spatial patch dimensions: (num_patches, bands, patch_size, patch_size)
        patches = patches.reshape(-1, bands, self.patch_size, self.patch_size)
        
        if self.flatten_patches:
            # Flatten each patch: (num_patches, bands * patch_size * patch_size)
            patches = patches.reshape(patches.shape[0], -1)
            
        return patches
    
    def get_patch_positions(self, image_shape):
        """Get 2D positions of patches for positional encoding"""
        _, height, width = image_shape
        patches_h = height // self.patch_size
        patches_w = width // self.patch_size
        
        positions = []
        for i in range(patches_h):
            for j in range(patches_w):
                positions.append([i, j])
        
        return np.array(positions)

# Test patch tokenization with different patch sizes
patch_sizes = [8, 16, 32]

print("PATCH TOKENIZATION COMPARISON:")
print("=" * 50)

for patch_size in patch_sizes:
    tokenizer = PatchTokenizer(patch_size=patch_size)
    patches = tokenizer.extract_patches(hls_image)
    positions = tokenizer.get_patch_positions(hls_image.shape)
    
    print(f"\nPatch size {patch_size}x{patch_size}:")
    print(f"  Number of patches: {patches.shape[0]}")
    print(f"  Patch dimension: {patches.shape[1]}")
    print(f"  Token compression ratio: {hls_image.size / patches.size:.1f}x")
    print(f"  Spatial positions shape: {positions.shape}")
```

### Visualizing Patch Extraction

```{python}
# | echo: true
def visualize_patch_extraction(image, patch_size=16):
    """Visualize how image is divided into patches"""
    # Use RGB bands for visualization
    rgb_image = np.stack([image[2], image[1], image[0]], axis=2)  # Red, Green, Blue
    rgb_image = np.clip(rgb_image * 2.5, 0, 1)  # Enhance for visibility
    
    height, width = rgb_image.shape[:2]
    patches_h = height // patch_size
    patches_w = width // patch_size
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    
    # Original image
    axes[0].imshow(rgb_image)
    axes[0].set_title('Original Image')
    axes[0].axis('off')
    
    # Image with patch boundaries
    axes[1].imshow(rgb_image)
    
    # Draw patch boundaries
    for i in range(0, patches_h + 1):
        y = i * patch_size
        axes[1].axhline(y=y, color='red', linewidth=1, alpha=0.7)
    
    for j in range(0, patches_w + 1):
        x = j * patch_size
        axes[1].axvline(x=x, color='red', linewidth=1, alpha=0.7)
    
    axes[1].set_title(f'Patches ({patch_size}x{patch_size}): {patches_h}√ó{patches_w} = {patches_h*patches_w} patches')
    axes[1].axis('off')
    
    plt.tight_layout()
    plt.show()

# Visualize patch extraction for our HLS data
visualize_patch_extraction(hls_image, patch_size=16)
```

---

## 4. Building PyTorch Data Loaders for Geospatial Data

Now we build PyTorch data loaders for satellite imagery. Instead of text sequences, we create spatial-temporal sequences of satellite patches.

### GeospatialDataset for Patch Sequences

```{python}
# | echo: true
import torch
from torch.utils.data import Dataset, DataLoader

class GeospatialDatasetV1(Dataset):
    """
    Geospatial dataset for foundation model training
    Creates sequences of image patches with spatial and temporal relationships
    """
    
    def __init__(self, images, patch_size=16, max_length=256, stride=None, 
                 include_positions=True, mask_ratio=0.0):
        """
        Args:
            images: List of multi-band images or single image
            patch_size: Size of patches to extract
            max_length: Maximum sequence length (number of patches)
            stride: Stride for overlapping sequences (default: max_length for non-overlapping)
            include_positions: Whether to include spatial position information
            mask_ratio: Ratio of patches to mask (for MAE pretraining)
        """
        self.patch_size = patch_size
        self.max_length = max_length
        self.stride = stride or max_length
        self.include_positions = include_positions
        self.mask_ratio = mask_ratio
        
        self.input_patches = []
        self.target_patches = []
        self.positions = []
        self.masks = []
        
        # Initialize tokenizer
        self.tokenizer = PatchTokenizer(patch_size=patch_size)
        
        # Process images into patch sequences
        if not isinstance(images, list):
            images = [images]
            
        for image in images:
            self._process_image(image)
    
    def _process_image(self, image):
        """Process a single image into patch sequences"""
        # Extract patches
        patches = self.tokenizer.extract_patches(image)
        positions = self.tokenizer.get_patch_positions(image.shape)
        
        # Create sequences using sliding window approach
        for i in range(0, len(patches) - self.max_length + 1, self.stride):
            # Input sequence
            input_sequence = patches[i:i + self.max_length]
            
            # Target sequence (for next-patch prediction or reconstruction)
            if self.mask_ratio > 0:
                # For masked autoencoder: target is same as input, mask indicates what to predict
                target_sequence = input_sequence.copy()
                mask = self._create_random_mask(self.max_length)
            else:
                # For autoregressive: target is shifted by one patch
                if i + self.max_length < len(patches):
                    target_sequence = patches[i + 1:i + self.max_length + 1]
                    mask = np.ones(self.max_length, dtype=bool)  # No masking
                else:
                    continue
            
            # Position sequence
            pos_sequence = positions[i:i + self.max_length] if self.include_positions else None
            
            self.input_patches.append(torch.from_numpy(input_sequence).float())
            self.target_patches.append(torch.from_numpy(target_sequence).float())
            if pos_sequence is not None:
                self.positions.append(torch.from_numpy(pos_sequence).long())
            self.masks.append(torch.from_numpy(mask).bool())
    
    def _create_random_mask(self, sequence_length):
        """Create random mask for masked autoencoder training"""
        num_masked = int(sequence_length * self.mask_ratio)
        mask = np.ones(sequence_length, dtype=bool)
        
        if num_masked > 0:
            mask_indices = np.random.choice(sequence_length, num_masked, replace=False)
            mask[mask_indices] = False
        
        return mask
    
    def __len__(self):
        return len(self.input_patches)
    
    def __getitem__(self, idx):
        item = {
            'input_patches': self.input_patches[idx],
            'target_patches': self.target_patches[idx], 
            'mask': self.masks[idx]
        }
        
        if self.include_positions:
            item['positions'] = self.positions[idx]
            
        return item

# Test the geospatial dataset
print("CREATING GEOSPATIAL DATASET:")
print("=" * 40)

# Create dataset with different configurations
dataset_configs = [
    {"name": "Autoregressive", "mask_ratio": 0.0, "max_length": 64},
    {"name": "Masked Autoencoder", "mask_ratio": 0.75, "max_length": 64},
]

for config in dataset_configs:
    dataset = GeospatialDatasetV1(
        hls_image, 
        patch_size=16, 
        max_length=config["max_length"],
        mask_ratio=config["mask_ratio"]
    )
    
    print(f"\n{config['name']} Dataset:")
    print(f"  Number of sequences: {len(dataset)}")
    print(f"  Patch size: 16x16")
    print(f"  Sequence length: {config['max_length']} patches")
    print(f"  Mask ratio: {config['mask_ratio']:.0%}")
    
    # Check first item
    item = dataset[0]
    print(f"  Input shape: {item['input_patches'].shape}")
    print(f"  Target shape: {item['target_patches'].shape}")
    print(f"  Mask shape: {item['mask'].shape}")
    if 'positions' in item:
        print(f"  Positions shape: {item['positions'].shape}")
```

### Creating Data Loaders

```{python}
# | echo: true
def create_geospatial_dataloader(images, batch_size=4, patch_size=16, max_length=64, 
                               mask_ratio=0.75, shuffle=True, num_workers=0):
    """
    Create geospatial data loader for foundation model training
    """
    
    # Create dataset
    dataset = GeospatialDatasetV1(
        images=images,
        patch_size=patch_size,
        max_length=max_length,
        mask_ratio=mask_ratio
    )
    
    # Create dataloader  
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=shuffle, 
        num_workers=num_workers,
        drop_last=True  # For consistent batch sizes
    )
    
    return dataloader

# Create data loader for our HLS data
batch_size = 2
max_length = 32  # Shorter sequences for demonstration
dataloader = create_geospatial_dataloader(
    hls_image,
    batch_size=batch_size,
    max_length=max_length,
    mask_ratio=0.75  # Prithvi-style masking
)

print(f"GEOSPATIAL DATA LOADER:")
print(f"Batch size: {batch_size}")
print(f"Max sequence length: {max_length}")
print(f"Total batches: {len(dataloader)}")

# Test batch loading
for batch_idx, batch in enumerate(dataloader):
    print(f"\nBatch {batch_idx + 1}:")
    print(f"  Input patches: {batch['input_patches'].shape}")
    print(f"  Target patches: {batch['target_patches'].shape}")
    print(f"  Masks: {batch['mask'].shape}")
    print(f"  Positions: {batch['positions'].shape}")
    
    # Show masking statistics
    mask_ratio = (~batch['mask']).float().mean().item()
    print(f"  Actual mask ratio: {mask_ratio:.1%}")
    
    if batch_idx >= 1:  # Only show first 2 batches
        break
```

---

## 5. Handling Missing Data: The Cloud Problem

Unlike text data (where missing words are rare), satellite imagery has **massive missing data problems**:

:::{.callout-warning}
## Scale of Missing Data in Satellite Imagery

- **40-60%** of optical satellite images contain clouds
- **Seasonal variation**: Up to 80% cloud cover in tropical regions during rainy season  
- **Geographic bias**: Persistent cloud cover creates data deserts

This is like having 40-60% of words in text be `<UNK>` tokens!
:::

### Cloud-Aware Data Loading

```{python}
# | echo: true
def create_cloudy_satellite_image(clean_image):
    """Simulate realistic cloud contamination"""
    cloudy_image = clean_image.copy()
    
    # Create realistic cloud patterns
    height, width = clean_image.shape[1], clean_image.shape[2]
    np.random.seed(123)
    
    # Large cloud system
    center_y, center_x = height//3, 2*width//3
    y, x = np.ogrid[:height, :width]
    cloud1 = ((x - center_x)**2 + (y - center_y)**2) < (height/6)**2
    
    # Smaller cloud patches
    cloud2 = ((x - width//4)**2 + (y - 3*height//4)**2) < (height/10)**2
    
    # Combine clouds
    cloud_mask = np.logical_or(cloud1, cloud2)
    
    # Apply clouds (bright white values)
    cloudy_image[:, cloud_mask] = 0.9  # Clouds are bright in all bands
    
    return cloudy_image, cloud_mask

# Create cloudy version of our data
cloudy_hls, cloud_mask = create_cloudy_satellite_image(hls_image)

# Visualize the cloud problem
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Clean image
rgb_clean = np.stack([hls_image[2], hls_image[1], hls_image[0]], axis=2)
rgb_clean = np.clip(rgb_clean * 2.5, 0, 1)
axes[0].imshow(rgb_clean)
axes[0].set_title('Clean Image\n(What we want)')
axes[0].axis('off')

# Cloudy image  
rgb_cloudy = np.stack([cloudy_hls[2], cloudy_hls[1], cloudy_hls[0]], axis=2)
rgb_cloudy = np.clip(rgb_cloudy * 2.5, 0, 1)
axes[1].imshow(rgb_cloudy)
axes[1].set_title('Reality: Cloudy Image\n(What we actually get)')
axes[1].axis('off')

# Cloud mask
axes[2].imshow(cloud_mask, cmap='RdBu_r')
axes[2].set_title(f'Cloud Mask\nCoverage: {cloud_mask.mean():.1%}')
axes[2].axis('off')

plt.tight_layout()
plt.show()
```

### Cloud-Aware Dataset

```{python}
# | echo: true
class CloudAwareGeospatialDataset(GeospatialDatasetV1):
    """
    Extended dataset that handles cloud contamination
    """
    
    def __init__(self, images, cloud_masks=None, cloud_threshold=0.5, **kwargs):
        """
        Args:
            cloud_masks: Optional cloud masks for each image
            cloud_threshold: Threshold for considering a patch contaminated
        """
        self.cloud_masks = cloud_masks
        self.cloud_threshold = cloud_threshold
        
        super().__init__(images, **kwargs)
    
    def _process_image(self, image):
        """Override to handle cloud contamination"""
        # Extract patches normally
        patches = self.tokenizer.extract_patches(image)
        positions = self.tokenizer.get_patch_positions(image.shape)
        
        # Handle cloud masks if provided
        if self.cloud_masks is not None:
            # Process cloud mask to patch level
            cloud_mask = self.cloud_masks[0] if len(self.cloud_masks) == 1 else self.cloud_masks[0]
            
            # Expand cloud mask to match image bands
            cloud_mask_expanded = np.expand_dims(cloud_mask, axis=0)
            cloud_patches = self.tokenizer.extract_patches(cloud_mask_expanded)
            
            # Calculate contamination ratio for each patch
            contamination_ratios = cloud_patches.mean(axis=1)
            is_contaminated = contamination_ratios > self.cloud_threshold
        else:
            is_contaminated = np.zeros(len(patches), dtype=bool)
        
        # Create sequences, tracking cloud contamination
        for i in range(0, len(patches) - self.max_length + 1, self.stride):
            input_sequence = patches[i:i + self.max_length]
            contamination_sequence = is_contaminated[i:i + self.max_length]
            
            # Skip sequences that are too contaminated
            if contamination_sequence.mean() > 0.8:  # Skip if >80% contaminated
                continue
                
            # Create mask that combines random masking + cloud masking
            if self.mask_ratio > 0:
                target_sequence = input_sequence.copy()
                random_mask = self._create_random_mask(self.max_length)
                # Don't attend to cloud-contaminated patches
                combined_mask = random_mask & (~contamination_sequence)
            else:
                if i + self.max_length < len(patches):
                    target_sequence = patches[i + 1:i + self.max_length + 1]
                    combined_mask = ~contamination_sequence
                else:
                    continue
            
            pos_sequence = positions[i:i + self.max_length] if self.include_positions else None
            
            self.input_patches.append(torch.from_numpy(input_sequence).float())
            self.target_patches.append(torch.from_numpy(target_sequence).float())
            if pos_sequence is not None:
                self.positions.append(torch.from_numpy(pos_sequence).long())
            self.masks.append(torch.from_numpy(combined_mask).bool())

# Test cloud-aware dataset
cloud_aware_dataset = CloudAwareGeospatialDataset(
    [cloudy_hls],
    cloud_masks=[cloud_mask],
    patch_size=16,
    max_length=32,
    mask_ratio=0.75
)

print(f"CLOUD-AWARE DATASET:")
print(f"Original HLS sequences: {len(GeospatialDatasetV1([hls_image], patch_size=16, max_length=32, mask_ratio=0.75))}")
print(f"Cloud-aware sequences: {len(cloud_aware_dataset)}")
print(f"Reduction due to cloud filtering: {(1 - len(cloud_aware_dataset)/len(GeospatialDatasetV1([hls_image], patch_size=16, max_length=32, mask_ratio=0.75)))*100:.1f}%")
```

---

## 6. Temporal Sequences: Time Series as "Documents"

We create temporal "documents" from time series of satellite imagery, similar to how text documents are constructed from sentences:

```{python}
# | echo: true
def create_temporal_sequence(location_images, dates, max_sequence_length=12):
    """
    Create temporal sequences from satellite imagery time series
    Similar to how text documents are constructed from sentences
    """
    
    # Each location becomes a "document"
    # Each date becomes a "sentence" of patches
    # Each patch becomes a "token"
    
    temporal_tokens = []
    temporal_positions = []
    
    tokenizer = PatchTokenizer(patch_size=16)
    
    for img_idx, (image, date) in enumerate(zip(location_images, dates)):
        # Extract patches from this date
        patches = tokenizer.extract_patches(image)
        
        # Add temporal encoding to each patch
        temporal_encoding = np.full(len(patches), img_idx)  # Time step
        
        temporal_tokens.append(patches)
        temporal_positions.append(temporal_encoding)
    
    # Concatenate temporal sequence (like concatenating sentences)
    full_sequence = np.concatenate(temporal_tokens, axis=0)
    temporal_positions = np.concatenate(temporal_positions)
    
    # Truncate if too long (like text sequence length limits)
    if len(full_sequence) > max_sequence_length:
        full_sequence = full_sequence[:max_sequence_length]
        temporal_positions = temporal_positions[:max_sequence_length]
    
    return full_sequence, temporal_positions

# Simulate a time series of the same location
time_series = []
dates = ['2023-01', '2023-04', '2023-07', '2023-10']  # Seasonal progression

for season in range(4):
    # Create seasonal variations of the same scene
    seasonal_image = hls_image.copy()
    
    # Simulate seasonal changes
    if season == 1:  # Spring: more green
        seasonal_image[1, :, :] += 0.2  # Green band
    elif season == 2:  # Summer: brighter, more green
        seasonal_image *= 1.1
        seasonal_image[1, :, :] += 0.3  # Much more green
    elif season == 3:  # Fall: more red/brown
        seasonal_image[2, :, :] += 0.2  # Red band
        seasonal_image[1, :, :] -= 0.1  # Less green
    
    seasonal_image = np.clip(seasonal_image, 0, 1)
    time_series.append(seasonal_image)

# Create temporal sequence
temporal_sequence, time_positions = create_temporal_sequence(time_series, dates)

print(f"TEMPORAL SEQUENCE:")
print(f"Temporal sequence length: {len(temporal_sequence)} tokens")
print(f"Time positions: {np.unique(time_positions, return_counts=True)}")
print(f"Tokens per time step: {np.bincount(time_positions)}")
```

---

## Summary: Building Data Foundations for GFMs

We've successfully built a comprehensive data preparation framework for geospatial foundation models:

### Key Components Developed

| Data Challenge | Our Geospatial Solution |
|----------------|-------------------------|
| **Tokenization** | Patch-based spatial extraction with multi-spectral support |
| **Vocabulary** | Continuous projection (no discrete vocabulary needed) |
| **Sequences** | Spatial patches and temporal series from satellite imagery |
| **Missing Data** | Cloud-aware contamination handling (40-60% data loss) |
| **Positional Encoding** | 2D spatial + temporal position encoding |
| **Data Loading** | Sliding window over image patches with masking support |

### What We Built

1. **`PatchTokenizer`**: Converts satellite imagery to spatial tokens
2. **`GeospatialDatasetV1`**: PyTorch dataset for image patch sequences
3. **`CloudAwareGeospatialDataset`**: Handles missing data from clouds
4. **Temporal sequences**: Time series as multi-image "documents"
5. **Complete data loaders**: Ready for foundation model training

### Week 1 Deliverable: Complete Geospatial Data Pipeline

```{python}
# | echo: true
print("üéØ WEEK 1 DELIVERABLE COMPLETE:")
print("=" * 50)
print("‚úÖ Data downloading and loading")
print("‚úÖ Multi-spectral tokenization") 
print("‚úÖ Patch-based sequence creation")
print("‚úÖ Cloud-aware missing data handling")
print("‚úÖ Temporal sequence construction")
print("‚úÖ PyTorch data loaders ready for training")
print("\nüöÄ Ready for Week 2: Spatial-Temporal Attention Mechanisms!")
```

### Next Steps

This data pipeline foundation enables:
- **Week 2**: Building custom attention mechanisms for spatial-temporal relationships
- **Week 3**: Assembling complete Vision Transformer architecture
- **Week 4**: Implementing masked autoencoder pretraining
- **Weeks 5-10**: Training, evaluation, and deployment

We've built a robust geospatial data preparation system that handles the unique challenges of satellite imagery while incorporating proven concepts from foundation model data loading approaches.
