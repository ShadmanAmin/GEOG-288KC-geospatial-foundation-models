<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Foundation Model Architectures</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">GEOG 288KC</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">🏠 home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../Syllabus.html"> 
<span class="menu-text">📋 syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-weekly-sessions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">💻 weekly sessions</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-weekly-sessions">    
        <li>
    <a class="dropdown-item" href="../../../course-materials/c00-introduction-to-deeplearning-architecture.html">
 <span class="dropdown-text">Session 0 - 🪜 Introduction to Deep Learning Architecture</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/c01-geospatial-data-foundations.html">
 <span class="dropdown-text">Session 1 - 📊 Geospatial Data Foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/c02-spatial-temporal-attention-mechanisms.html">
 <span class="dropdown-text">Session 2 - 🧠 Spatial-Temporal Attention Mechanisms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/c03-complete-gfm-architecture.html">
 <span class="dropdown-text">Session 3 - 🏗️ Complete GFM Architecture</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/c04-pretraining-implementation.html">
 <span class="dropdown-text">Session 4 - 🔥 Pretraining Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/c05-training-loop-optimization.html">
 <span class="dropdown-text">Session 5 - ⚙️ Training Loop Optimization</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/c06-model-evaluation-analysis.html">
 <span class="dropdown-text">Session 6 - 📈 Model Evaluation &amp; Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/c07-integration-with-existing-models.html">
 <span class="dropdown-text">Session 7 - 🤝 Integration with Existing Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/c08-task-specific-finetuning.html">
 <span class="dropdown-text">Session 8 - 🔧 Task-Specific Fine-tuning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/c09-model-implementation-deployment.html">
 <span class="dropdown-text">Session 9 - ☁️ Model Implementation &amp; Deployment</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/c10-project-presentations-synthesis.html">
 <span class="dropdown-text">Session 10 - 🎯 Project Presentations &amp; Synthesis</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-cheatsheets" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">👀 Cheatsheets</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-cheatsheets">    
        <li class="dropdown-header">🏗️ Foundation Models &amp; AI</li>
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/foundation_model_architectures.html">
 <span class="dropdown-text">Foundation Model Architectures</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/gfm_architecture.html">
 <span class="dropdown-text">GFM Architecture Cheatsheet</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/finetuning_basics.html">
 <span class="dropdown-text">Fine-tuning Basics</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/multimodal_learning.html">
 <span class="dropdown-text">Multi-modal Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/model_evaluation_validation.html">
 <span class="dropdown-text">Model Evaluation &amp; Validation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/loading_models.qmd">
 <span class="dropdown-text">Loading Pre-trained Models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/model_inference.html">
 <span class="dropdown-text">Model Inference &amp; Feature Extraction</span></a>
  </li>  
        <li class="dropdown-header">🗺️ Geospatial Data &amp; Remote Sensing</li>
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/geospatial_data_remote_sensing.html">
 <span class="dropdown-text">Geospatial Data &amp; Remote Sensing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/earth_engine_basics.qmd">
 <span class="dropdown-text">Earth Engine Basics</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/stac_apis.html">
 <span class="dropdown-text">STAC APIs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/rasterio_basics.html">
 <span class="dropdown-text">Rasterio Basics</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/xarray_basics.html">
 <span class="dropdown-text">Xarray for Multi-dimensional Data</span></a>
  </li>  
        <li class="dropdown-header">🔥 PyTorch &amp; Deep Learning</li>
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/pytorch_tensors.html">
 <span class="dropdown-text">PyTorch Tensors</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/torchgeo_basics.html">
 <span class="dropdown-text">TorchGeo Datasets &amp; Transforms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/dataloader_satellite.html">
 <span class="dropdown-text">Data Loading for Satellite Imagery</span></a>
  </li>  
        <li class="dropdown-header">📊 Visualization &amp; Analysis</li>
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/plotting_satellite.html">
 <span class="dropdown-text">Plotting Satellite Data</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/folium_basics.html">
 <span class="dropdown-text">Interactive Maps with Folium</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/matplotlib_geospatial.html">
 <span class="dropdown-text">Geospatial Plotting with Matplotlib</span></a>
  </li>  
        <li class="dropdown-header">☁️ Deployment &amp; Scaling</li>
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/cheatsheets/cloud_scalable_computing.html">
 <span class="dropdown-text">Cloud &amp; Scalable Computing</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-extras" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">📖 extras</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-extras">    
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/resources/course_resources.html">
 <span class="dropdown-text">📚 Reference Materials</span></a>
  </li>  
        <li class="dropdown-header">🎯 Practical Examples</li>
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/examples/normalization_comparison.html">
 <span class="dropdown-text">Normalization Comparison</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/examples/resnet.html">
 <span class="dropdown-text">ResNet Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/examples/text_encoder.html">
 <span class="dropdown-text">Text Encoder</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/examples/tiling-and-patches.html">
 <span class="dropdown-text">Tiling and Patches</span></a>
  </li>  
        <li class="dropdown-header">📁 Project Templates</li>
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/projects/project-proposal-template.html">
 <span class="dropdown-text">Project Proposal Template</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../course-materials/extras/projects/mvp-template.html">
 <span class="dropdown-text">MVP Presentation Template</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kellycaylor/geoAI" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Foundation Model Architectures</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">LLMs vs.&nbsp;Geospatial Foundation Models (GFMs)</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-foundation-model-architectures" id="toc-introduction-to-foundation-model-architectures" class="nav-link active" data-scroll-target="#introduction-to-foundation-model-architectures">Introduction to Foundation Model Architectures</a></li>
  <li><a href="#evolution-from-ai-to-transformers" id="toc-evolution-from-ai-to-transformers" class="nav-link" data-scroll-target="#evolution-from-ai-to-transformers">Evolution from AI to Transformers</a>
  <ul class="collapse">
  <li><a href="#key-historical-milestones" id="toc-key-historical-milestones" class="nav-link" data-scroll-target="#key-historical-milestones">Key Historical Milestones</a></li>
  <li><a href="#transformer-architecture-essentials" id="toc-transformer-architecture-essentials" class="nav-link" data-scroll-target="#transformer-architecture-essentials">Transformer Architecture Essentials</a></li>
  </ul></li>
  <li><a href="#step-development-pipeline-comparison" id="toc-step-development-pipeline-comparison" class="nav-link" data-scroll-target="#step-development-pipeline-comparison">9-Step Development Pipeline Comparison</a>
  <ul class="collapse">
  <li><a href="#llm-development-pipeline" id="toc-llm-development-pipeline" class="nav-link" data-scroll-target="#llm-development-pipeline">LLM Development Pipeline</a></li>
  <li><a href="#gfm-development-pipeline" id="toc-gfm-development-pipeline" class="nav-link" data-scroll-target="#gfm-development-pipeline">GFM Development Pipeline</a></li>
  </ul></li>
  <li><a href="#step-by-step-detailed-comparison" id="toc-step-by-step-detailed-comparison" class="nav-link" data-scroll-target="#step-by-step-detailed-comparison">Step-by-Step Detailed Comparison</a>
  <ul class="collapse">
  <li><a href="#data-preparation-differences" id="toc-data-preparation-differences" class="nav-link" data-scroll-target="#data-preparation-differences">1. Data Preparation Differences</a></li>
  <li><a href="#tokenization-approaches" id="toc-tokenization-approaches" class="nav-link" data-scroll-target="#tokenization-approaches">2. Tokenization Approaches</a></li>
  <li><a href="#architecture-comparison" id="toc-architecture-comparison" class="nav-link" data-scroll-target="#architecture-comparison">3. Architecture Comparison</a></li>
  <li><a href="#pretraining-objectives" id="toc-pretraining-objectives" class="nav-link" data-scroll-target="#pretraining-objectives">4. Pretraining Objectives</a></li>
  </ul></li>
  <li><a href="#scaling-and-evolution" id="toc-scaling-and-evolution" class="nav-link" data-scroll-target="#scaling-and-evolution">Scaling and Evolution</a>
  <ul class="collapse">
  <li><a href="#parameter-scaling-comparison" id="toc-parameter-scaling-comparison" class="nav-link" data-scroll-target="#parameter-scaling-comparison">Parameter Scaling Comparison</a></li>
  <li><a href="#data-requirements-and-constraints" id="toc-data-requirements-and-constraints" class="nav-link" data-scroll-target="#data-requirements-and-constraints">Data Requirements and Constraints</a></li>
  </ul></li>
  <li><a href="#implementation-examples" id="toc-implementation-examples" class="nav-link" data-scroll-target="#implementation-examples">Implementation Examples</a>
  <ul class="collapse">
  <li><a href="#embedding-creation" id="toc-embedding-creation" class="nav-link" data-scroll-target="#embedding-creation">Embedding Creation</a></li>
  <li><a href="#positional-encoding-comparison" id="toc-positional-encoding-comparison" class="nav-link" data-scroll-target="#positional-encoding-comparison">Positional Encoding Comparison</a></li>
  </ul></li>
  <li><a href="#course-mapping-and-applications" id="toc-course-mapping-and-applications" class="nav-link" data-scroll-target="#course-mapping-and-applications">Course Mapping and Applications</a>
  <ul class="collapse">
  <li><a href="#weekly-course-structure" id="toc-weekly-course-structure" class="nav-link" data-scroll-target="#weekly-course-structure">Weekly Course Structure</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  </ul></li>
  <li><a href="#further-reading-and-references" id="toc-further-reading-and-references" class="nav-link" data-scroll-target="#further-reading-and-references">Further Reading and References</a>
  <ul class="collapse">
  <li><a href="#essential-papers" id="toc-essential-papers" class="nav-link" data-scroll-target="#essential-papers">Essential Papers</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="introduction-to-foundation-model-architectures" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-foundation-model-architectures">Introduction to Foundation Model Architectures</h2>
<p>Foundation models are large-scale models trained on diverse data that can be adapted to various downstream tasks. This cheatsheet compares Language Model (LLM) and Geospatial Foundation Model (GFM) development pipelines.</p>
<div id="d22bd685" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoConfig</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CUDA available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PyTorch version: 2.7.1
CUDA available: False</code></pre>
</div>
</div>
</section>
<section id="evolution-from-ai-to-transformers" class="level2">
<h2 class="anchored" data-anchor-id="evolution-from-ai-to-transformers">Evolution from AI to Transformers</h2>
<section id="key-historical-milestones" class="level3">
<h3 class="anchored" data-anchor-id="key-historical-milestones">Key Historical Milestones</h3>
<div id="1df3af0d" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Timeline of key developments</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>timeline <span class="op">=</span> {</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"1950s-1990s"</span>: <span class="st">"Symbolic AI, early neural networks"</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2012"</span>: <span class="st">"Deep learning breakthrough (ImageNet)"</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2017"</span>: <span class="st">"Transformers ('Attention Is All You Need')"</span>,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2018-2020"</span>: <span class="st">"BERT/GPT families emerge"</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2021-2024"</span>: <span class="st">"Scaling laws, instruction tuning, multimodality"</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AI/ML → Transformers Timeline:"</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> year, development <span class="kw">in</span> timeline.items():</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>development<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>AI/ML → Transformers Timeline:
1950s-1990s: Symbolic AI, early neural networks
2012: Deep learning breakthrough (ImageNet)
2017: Transformers ('Attention Is All You Need')
2018-2020: BERT/GPT families emerge
2021-2024: Scaling laws, instruction tuning, multimodality</code></pre>
</div>
</div>
</section>
<section id="transformer-architecture-essentials" class="level3">
<h3 class="anchored" data-anchor-id="transformer-architecture-essentials">Transformer Architecture Essentials</h3>
<div id="da947e72" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleTransformerBlock(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simplified transformer block to illustrate key components"""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">8</span>, mlp_ratio<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multi-head attention</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> nn.MultiheadAttention(embed_dim, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feed-forward network</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> nn.Sequential(</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            nn.Linear(embed_dim, <span class="bu">int</span>(embed_dim <span class="op">*</span> mlp_ratio)),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="bu">int</span>(embed_dim <span class="op">*</span> mlp_ratio), embed_dim)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer normalization</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Self-attention with residual connection</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        attn_out, _ <span class="op">=</span> <span class="va">self</span>.attention(x, x, x)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> attn_out)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># MLP with residual connection</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        mlp_out <span class="op">=</span> <span class="va">self</span>.mlp(x)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> mlp_out)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Example transformer block</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>transformer_block <span class="op">=</span> SimpleTransformerBlock(embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">100</span>, <span class="dv">768</span>)  <span class="co"># [batch_size, seq_len, embed_dim]</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> transformer_block(sample_input)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>sample_input<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> transformer_block.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([2, 100, 768])
Output shape: torch.Size([2, 100, 768])
Parameters: 7,087,872</code></pre>
</div>
</div>
</section>
</section>
<section id="step-development-pipeline-comparison" class="level2">
<h2 class="anchored" data-anchor-id="step-development-pipeline-comparison">9-Step Development Pipeline Comparison</h2>
<section id="llm-development-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="llm-development-pipeline">LLM Development Pipeline</h3>
<div id="c84cb54b" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>llm_pipeline <span class="op">=</span> {</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"1. Data Preparation"</span>: <span class="st">"Text corpora, deduplication, quality filtering, mixing ratios"</span>,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2. Tokenization"</span>: <span class="st">"BPE, vocabulary construction, special tokens"</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"3. Architecture"</span>: <span class="st">"GPT/BERT variants, depth/width scaling, context length"</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"4. Pretraining Objective"</span>: <span class="st">"Next-token prediction, masked language modeling"</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"5. Training Loop"</span>: <span class="st">"Optimizers, LR schedules, mixed precision, gradient clipping"</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"6. Evaluation"</span>: <span class="st">"Perplexity, downstream task probing, benchmarks"</span>,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"7. Pretrained Weights"</span>: <span class="st">"Model hubs, tokenizer alignment, loading utilities"</span>,</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"8. Finetuning"</span>: <span class="st">"Task-specific heads, PEFT methods, instruction tuning"</span>,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"9. Deployment"</span>: <span class="st">"API serving, KV caching, inference optimization"</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LLM Development Pipeline:"</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, description <span class="kw">in</span> llm_pipeline.items():</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>description<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LLM Development Pipeline:
1. Data Preparation: Text corpora, deduplication, quality filtering, mixing ratios
2. Tokenization: BPE, vocabulary construction, special tokens
3. Architecture: GPT/BERT variants, depth/width scaling, context length
4. Pretraining Objective: Next-token prediction, masked language modeling
5. Training Loop: Optimizers, LR schedules, mixed precision, gradient clipping
6. Evaluation: Perplexity, downstream task probing, benchmarks
7. Pretrained Weights: Model hubs, tokenizer alignment, loading utilities
8. Finetuning: Task-specific heads, PEFT methods, instruction tuning
9. Deployment: API serving, KV caching, inference optimization</code></pre>
</div>
</div>
</section>
<section id="gfm-development-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="gfm-development-pipeline">GFM Development Pipeline</h3>
<div id="f2ee47c6" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>gfm_pipeline <span class="op">=</span> {</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"1. Data Preparation"</span>: <span class="st">"Multi-spectral data, georegistration, tiling, cloud masking"</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2. Tokenization"</span>: <span class="st">"Patch-based, continuous embeddings, 2D/temporal positions"</span>,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"3. Architecture"</span>: <span class="st">"ViT encoders, spatial/temporal attention, memory constraints"</span>,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"4. Pretraining Objective"</span>: <span class="st">"Masked patch reconstruction, contrastive learning"</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"5. Training Loop"</span>: <span class="st">"Cloud masks, mixed precision, gradient accumulation"</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"6. Evaluation"</span>: <span class="st">"Reconstruction metrics, linear probing, generalization"</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"7. Pretrained Weights"</span>: <span class="st">"Prithvi, SatMAE, adapter loading, band alignment"</span>,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"8. Finetuning"</span>: <span class="st">"Task heads, PEFT, few-shot learning for limited labels"</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"9. Deployment"</span>: <span class="st">"Tiling inference, geospatial APIs, batch processing"</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GFM Development Pipeline:"</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, description <span class="kw">in</span> gfm_pipeline.items():</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>description<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
GFM Development Pipeline:
1. Data Preparation: Multi-spectral data, georegistration, tiling, cloud masking
2. Tokenization: Patch-based, continuous embeddings, 2D/temporal positions
3. Architecture: ViT encoders, spatial/temporal attention, memory constraints
4. Pretraining Objective: Masked patch reconstruction, contrastive learning
5. Training Loop: Cloud masks, mixed precision, gradient accumulation
6. Evaluation: Reconstruction metrics, linear probing, generalization
7. Pretrained Weights: Prithvi, SatMAE, adapter loading, band alignment
8. Finetuning: Task heads, PEFT, few-shot learning for limited labels
9. Deployment: Tiling inference, geospatial APIs, batch processing</code></pre>
</div>
</div>
</section>
</section>
<section id="step-by-step-detailed-comparison" class="level2">
<h2 class="anchored" data-anchor-id="step-by-step-detailed-comparison">Step-by-Step Detailed Comparison</h2>
<section id="data-preparation-differences" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation-differences">1. Data Preparation Differences</h3>
<div id="cb33f1b1" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demonstrate_data_preparation():</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Show key differences in data preparation"""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LLM data preparation simulation</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"LLM Data Preparation:"</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Text scraping from web, books, code"</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Deduplication algorithms"</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Quality filtering (language detection, toxicity)"</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Data mixing ratios optimization"</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate text preprocessing</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    sample_texts <span class="op">=</span> [</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"The quick brown fox jumps over the lazy dog."</span>,</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Machine learning is transforming many industries."</span>,</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Climate change requires urgent global action."</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Basic tokenization simulation</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text <span class="kw">in</span> sample_texts:</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        vocab.update(text.lower().replace(<span class="st">'.'</span>, <span class="st">''</span>).split())</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Sample vocabulary size: </span><span class="sc">{</span><span class="bu">len</span>(vocab)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Sample tokens: </span><span class="sc">{</span><span class="bu">list</span>(vocab)[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># GFM data preparation simulation</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"GFM Data Preparation:"</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Sensor calibration and atmospheric correction"</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Georegistration and projection alignment"</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Cloud masking and quality assessment"</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Temporal compositing and gap filling"</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate satellite data preprocessing</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">42</span>)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate multi-spectral satellite patch</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>    patch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>    num_bands <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>    satellite_patch <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">4096</span>, (num_bands, patch_size, patch_size))</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate cloud mask</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>    cloud_mask <span class="op">=</span> np.random.random((patch_size, patch_size)) <span class="op">&gt;</span> <span class="fl">0.8</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply atmospheric correction (simplified)</span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>    corrected_patch <span class="op">=</span> satellite_patch.astype(np.float32) <span class="op">/</span> <span class="fl">4095.0</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>    corrected_patch[:, cloud_mask] <span class="op">=</span> np.nan  <span class="co"># Mask cloudy pixels across all bands</span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Satellite patch shape: </span><span class="sc">{</span>satellite_patch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Cloud coverage: </span><span class="sc">{</span>cloud_mask<span class="sc">.</span><span class="bu">sum</span>() <span class="op">/</span> cloud_mask<span class="sc">.</span>size <span class="op">*</span> <span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%"</span>)</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Valid pixels: </span><span class="sc">{</span>(<span class="op">~</span>np.isnan(corrected_patch[<span class="dv">0</span>]))<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>demonstrate_data_preparation()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LLM Data Preparation:
- Text scraping from web, books, code
- Deduplication algorithms
- Quality filtering (language detection, toxicity)
- Data mixing ratios optimization

Sample vocabulary size: 20
Sample tokens: ['change', 'global', 'lazy', 'learning', 'the', 'industries', 'many', 'dog', 'quick', 'transforming']

==================================================
GFM Data Preparation:
- Sensor calibration and atmospheric correction
- Georegistration and projection alignment
- Cloud masking and quality assessment
- Temporal compositing and gap filling

Satellite patch shape: (6, 64, 64)
Cloud coverage: 20.3%
Valid pixels: 3,265</code></pre>
</div>
</div>
</section>
<section id="tokenization-approaches" class="level3">
<h3 class="anchored" data-anchor-id="tokenization-approaches">2. Tokenization Approaches</h3>
<div id="883469f5" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_tokenization():</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare LLM vs GFM tokenization approaches"""</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"LLM Tokenization (Discrete):"</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate BPE tokenization</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    vocab_size, embed_dim <span class="op">=</span> <span class="dv">50000</span>, <span class="dv">768</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample token sequence</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    token_ids <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">234</span>, <span class="dv">5678</span>, <span class="dv">2</span>])  <span class="co"># [CLS, word1, word2, word3, SEP]</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Embedding lookup</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    embedding_layer <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    token_embeddings <span class="op">=</span> embedding_layer(token_ids)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token IDs: </span><span class="sc">{</span>token_ids<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token embeddings shape: </span><span class="sc">{</span>token_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Vocabulary size: </span><span class="sc">{</span>vocab_size<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">30</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"GFM Tokenization (Continuous Patches):"</span>)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate patch-based tokenization</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    num_bands <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    embed_dim <span class="op">=</span> <span class="dv">768</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create sample patches</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    num_patches <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    patch_dim <span class="op">=</span> patch_size <span class="op">*</span> patch_size <span class="op">*</span> num_bands</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    patches <span class="op">=</span> torch.randn(num_patches, patch_dim)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Linear projection (continuous "tokenization")</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>    patch_projection <span class="op">=</span> nn.Linear(patch_dim, embed_dim)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    patch_embeddings <span class="op">=</span> patch_projection(patches)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Patch dimensions: </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">x</span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">x</span><span class="sc">{</span>num_bands<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>patch_dim<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Patch embeddings shape: </span><span class="sc">{</span>patch_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"No discrete vocabulary - continuous projection"</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> token_embeddings, patch_embeddings</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>token_emb, patch_emb <span class="op">=</span> compare_tokenization()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LLM Tokenization (Discrete):
Token IDs: [1, 15, 234, 5678, 2]
Token embeddings shape: torch.Size([5, 768])
Vocabulary size: 50,000

------------------------------
GFM Tokenization (Continuous Patches):
Patch dimensions: 16x16x6 = 1536
Patch embeddings shape: torch.Size([4, 768])
No discrete vocabulary - continuous projection</code></pre>
</div>
</div>
</section>
<section id="architecture-comparison" class="level3">
<h3 class="anchored" data-anchor-id="architecture-comparison">3. Architecture Comparison</h3>
<div id="cde01ca9" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LLMArchitecture(nn.Module):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simplified LLM architecture (GPT-style)"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size<span class="op">=</span><span class="dv">50000</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_layers<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_encoding <span class="op">=</span> nn.Embedding(<span class="dv">2048</span>, embed_dim)  <span class="co"># Max sequence length</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>            SimpleTransformerBlock(embed_dim, num_heads) </span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_final <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_head <span class="op">=</span> nn.Linear(embed_dim, vocab_size)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids):</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> input_ids.shape[<span class="dv">1</span>]</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> torch.arange(seq_len, device<span class="op">=</span>input_ids.device)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Token + positional embeddings</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(input_ids) <span class="op">+</span> <span class="va">self</span>.positional_encoding(positions)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transformer layers</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_final(x)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.output_head(x)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GFMArchitecture(nn.Module):</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simplified GFM architecture (ViT-style)"""</span></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, patch_size<span class="op">=</span><span class="dv">16</span>, num_bands<span class="op">=</span><span class="dv">6</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_layers<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_bands <span class="op">=</span> num_bands</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>        patch_dim <span class="op">=</span> patch_size <span class="op">*</span> patch_size <span class="op">*</span> num_bands</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embedding <span class="op">=</span> nn.Linear(patch_dim, embed_dim)</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2D positional embedding</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed_h <span class="op">=</span> nn.Embedding(<span class="dv">100</span>, embed_dim <span class="op">//</span> <span class="dv">2</span>)  <span class="co"># Height positions</span></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed_w <span class="op">=</span> nn.Embedding(<span class="dv">100</span>, embed_dim <span class="op">//</span> <span class="dv">2</span>)  <span class="co"># Width positions</span></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>            SimpleTransformerBlock(embed_dim, num_heads) </span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_final <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, patches, patch_positions):</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>        batch_size, num_patches, patch_dim <span class="op">=</span> patches.shape</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch embeddings</span></span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embedding(patches)</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2D positional embeddings</span></span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>        pos_h, pos_w <span class="op">=</span> patch_positions[:, :, <span class="dv">0</span>], patch_positions[:, :, <span class="dv">1</span>]</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>        pos_emb <span class="op">=</span> torch.cat([</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pos_embed_h(pos_h),</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pos_embed_w(pos_w)</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>        ], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> pos_emb</span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transformer layers</span></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_final(x)</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare architectures</span></span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a>llm_model <span class="op">=</span> LLMArchitecture(vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">384</span>, num_layers<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a>gfm_model <span class="op">=</span> GFMArchitecture(patch_size<span class="op">=</span><span class="dv">16</span>, num_bands<span class="op">=</span><span class="dv">6</span>, embed_dim<span class="op">=</span><span class="dv">384</span>, num_layers<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a>llm_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> llm_model.parameters())</span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a>gfm_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> gfm_model.parameters())</span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Architecture Comparison:"</span>)</span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LLM parameters: </span><span class="sc">{</span>llm_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GFM parameters: </span><span class="sc">{</span>gfm_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Test forward passes</span></span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a>sample_tokens <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10000</span>, (<span class="dv">2</span>, <span class="dv">50</span>))  <span class="co"># [batch_size, seq_len]</span></span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a>sample_patches <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">16</span>, <span class="dv">16</span><span class="op">*</span><span class="dv">16</span><span class="op">*</span><span class="dv">6</span>)  <span class="co"># [batch_size, num_patches, patch_dim]</span></span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a>sample_positions <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">2</span>, <span class="dv">16</span>, <span class="dv">2</span>))  <span class="co"># [batch_size, num_patches, 2]</span></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a>llm_output <span class="op">=</span> llm_model(sample_tokens)</span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a>gfm_output <span class="op">=</span> gfm_model(sample_patches, sample_positions)</span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">LLM output shape: </span><span class="sc">{</span>llm_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GFM output shape: </span><span class="sc">{</span>gfm_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Architecture Comparison:
LLM parameters: 19,123,984
GFM parameters: 11,276,160

LLM output shape: torch.Size([2, 50, 10000])
GFM output shape: torch.Size([2, 16, 384])</code></pre>
</div>
</div>
</section>
<section id="pretraining-objectives" class="level3">
<h3 class="anchored" data-anchor-id="pretraining-objectives">4. Pretraining Objectives</h3>
<div id="9fd8a7e9" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demonstrate_pretraining_objectives():</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare pretraining objectives for LLMs vs GFMs"""</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"LLM Pretraining Objectives:"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"1. Next-Token Prediction (GPT-style)"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"2. Masked Language Modeling (BERT-style)"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate next-token prediction</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    sequence <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]])</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> torch.tensor([[<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]])  <span class="co"># Shifted by one</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mock logits</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    vocab_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">5</span>, vocab_size)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cross-entropy loss</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    ce_loss <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    next_token_loss <span class="op">=</span> ce_loss(logits.view(<span class="op">-</span><span class="dv">1</span>, vocab_size), targets.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Next-token prediction loss: </span><span class="sc">{</span>next_token_loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">30</span>)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"GFM Pretraining Objectives:"</span>)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"1. Masked Patch Reconstruction (MAE-style)"</span>)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"2. Contrastive Learning (optional)"</span>)</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate masked patch reconstruction</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    batch_size, num_patches, patch_dim <span class="op">=</span> <span class="dv">2</span>, <span class="dv">64</span>, <span class="dv">768</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    original_patches <span class="op">=</span> torch.randn(batch_size, num_patches, patch_dim)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random masking</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    mask_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    num_masked <span class="op">=</span> <span class="bu">int</span>(num_patches <span class="op">*</span> mask_ratio)</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create random mask</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.zeros(batch_size, num_patches, dtype<span class="op">=</span>torch.<span class="bu">bool</span>)</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>        masked_indices <span class="op">=</span> torch.randperm(num_patches)[:num_masked]</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>        mask[i, masked_indices] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reconstruction loss (simplified)</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>    reconstructed_patches <span class="op">=</span> torch.randn_like(original_patches)</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>    reconstruction_loss <span class="op">=</span> nn.MSELoss()(</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        reconstructed_patches[mask], </span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>        original_patches[mask]</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Mask ratio: </span><span class="sc">{</span>mask_ratio<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Masked patches per sample: </span><span class="sc">{</span>num_masked<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Reconstruction loss: </span><span class="sc">{</span>reconstruction_loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>demonstrate_pretraining_objectives()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LLM Pretraining Objectives:
1. Next-Token Prediction (GPT-style)
2. Masked Language Modeling (BERT-style)
Next-token prediction loss: 7.3600

------------------------------
GFM Pretraining Objectives:
1. Masked Patch Reconstruction (MAE-style)
2. Contrastive Learning (optional)
Mask ratio: 75.0%
Masked patches per sample: 48
Reconstruction loss: 2.0067</code></pre>
</div>
</div>
</section>
</section>
<section id="scaling-and-evolution" class="level2">
<h2 class="anchored" data-anchor-id="scaling-and-evolution">Scaling and Evolution</h2>
<section id="parameter-scaling-comparison" class="level3">
<h3 class="anchored" data-anchor-id="parameter-scaling-comparison">Parameter Scaling Comparison</h3>
<div id="10a36461" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_scaling_trends():</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare scaling trends between LLMs and GFMs"""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LLM scaling milestones</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    llm_milestones <span class="op">=</span> {</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">'GPT-1'</span>: <span class="fl">117e6</span>,</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">'BERT-Base'</span>: <span class="fl">110e6</span>,</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">'GPT-2'</span>: <span class="fl">1.5e9</span>,</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">'GPT-3'</span>: <span class="fl">175e9</span>,</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">'PaLM'</span>: <span class="fl">540e9</span>,</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'GPT-4'</span>: <span class="fl">1000e9</span>  <span class="co"># Estimated</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># GFM scaling examples</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    gfm_milestones <span class="op">=</span> {</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'SatMAE-Base'</span>: <span class="fl">86e6</span>,</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Prithvi-100M'</span>: <span class="fl">100e6</span>,</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Clay-v0.1'</span>: <span class="fl">139e6</span>,</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">'SatLas-Base'</span>: <span class="fl">300e6</span>,</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Scale-MAE'</span>: <span class="fl">600e6</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualization</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>    fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LLM scaling</span></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> <span class="bu">list</span>(llm_milestones.keys())</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> [llm_milestones[m]<span class="op">/</span><span class="fl">1e9</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>    ax1.bar(models, params, color<span class="op">=</span><span class="st">'skyblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>    ax1.set_yscale(<span class="st">'log'</span>)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">'Parameters (Billions)'</span>)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="st">'LLM Parameter Scaling'</span>)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>    ax1.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># GFM scaling</span></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> <span class="bu">list</span>(gfm_milestones.keys())</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> [gfm_milestones[m]<span class="op">/</span><span class="fl">1e6</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>    ax2.bar(models, params, color<span class="op">=</span><span class="st">'lightcoral'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">'Parameters (Millions)'</span>)</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>    ax2.set_title(<span class="st">'GFM Parameter Scaling'</span>)</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>    ax2.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Context/Input scaling</span></span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Context/Input Scaling:"</span>)</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">LLMs:"</span>)</span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Context length: 512 → 2K → 8K → 128K+ tokens"</span>)</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Training data: Web text, books, code (curated)"</span>)</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Focus: Language understanding and generation"</span>)</span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GFMs:"</span>)</span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Input bands: 3 (RGB) → 6+ (multispectral) → hyperspectral"</span>)</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Spatial resolution: Various (10m to 0.3m)"</span>)</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Temporal dimension: Single → time series → multi-temporal"</span>)</span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Focus: Earth observation and environmental monitoring"</span>)</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>compare_scaling_trends()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="foundation_model_architectures_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Context/Input Scaling:

LLMs:
- Context length: 512 → 2K → 8K → 128K+ tokens
- Training data: Web text, books, code (curated)
- Focus: Language understanding and generation

GFMs:
- Input bands: 3 (RGB) → 6+ (multispectral) → hyperspectral
- Spatial resolution: Various (10m to 0.3m)
- Temporal dimension: Single → time series → multi-temporal
- Focus: Earth observation and environmental monitoring</code></pre>
</div>
</div>
</section>
<section id="data-requirements-and-constraints" class="level3">
<h3 class="anchored" data-anchor-id="data-requirements-and-constraints">Data Requirements and Constraints</h3>
<div id="e3d44914" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_data_requirements():</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare data requirements and constraints"""</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    comparison <span class="op">=</span> {</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Data Volume"</span>: {</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">"LLMs"</span>: <span class="st">"Terabytes of text (web crawls, books, code)"</span>,</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"GFMs"</span>: <span class="st">"Petabytes of satellite imagery (constrained by storage/IO)"</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Data Quality"</span>: {</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"LLMs"</span>: <span class="st">"Deduplication, toxicity filtering, language detection"</span>,</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>            <span class="st">"GFMs"</span>: <span class="st">"Cloud masking, atmospheric correction, sensor calibration"</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Preprocessing"</span>: {</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">"LLMs"</span>: <span class="st">"Tokenization, sequence packing, attention masks"</span>,</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>            <span class="st">"GFMs"</span>: <span class="st">"Patch extraction, normalization, spatial alignment"</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Storage Format"</span>: {</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>            <span class="st">"LLMs"</span>: <span class="st">"Compressed text files, tokenized sequences"</span>,</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"GFMs"</span>: <span class="st">"Cloud-optimized formats (COG, Zarr), tiled storage"</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Access Patterns"</span>: {</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            <span class="st">"LLMs"</span>: <span class="st">"Sequential text processing, random sampling"</span>,</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>            <span class="st">"GFMs"</span>: <span class="st">"Spatial/temporal queries, patch sampling, tiling"</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Data Requirements Comparison:"</span>)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> aspect, details <span class="kw">in</span> comparison.items():</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>aspect<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  LLMs: </span><span class="sc">{</span>details[<span class="st">'LLMs'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  GFMs: </span><span class="sc">{</span>details[<span class="st">'GFMs'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>compare_data_requirements()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Data Requirements Comparison:
==================================================

Data Volume:
  LLMs: Terabytes of text (web crawls, books, code)
  GFMs: Petabytes of satellite imagery (constrained by storage/IO)

Data Quality:
  LLMs: Deduplication, toxicity filtering, language detection
  GFMs: Cloud masking, atmospheric correction, sensor calibration

Preprocessing:
  LLMs: Tokenization, sequence packing, attention masks
  GFMs: Patch extraction, normalization, spatial alignment

Storage Format:
  LLMs: Compressed text files, tokenized sequences
  GFMs: Cloud-optimized formats (COG, Zarr), tiled storage

Access Patterns:
  LLMs: Sequential text processing, random sampling
  GFMs: Spatial/temporal queries, patch sampling, tiling</code></pre>
</div>
</div>
</section>
</section>
<section id="implementation-examples" class="level2">
<h2 class="anchored" data-anchor-id="implementation-examples">Implementation Examples</h2>
<section id="embedding-creation" class="level3">
<h3 class="anchored" data-anchor-id="embedding-creation">Embedding Creation</h3>
<div id="2dac1ab5" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demonstrate_embeddings():</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Show embedding creation for both domains"""</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Text Embeddings (LLM):"</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate text tokenization and embedding</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="st">"The forest shows signs of deforestation."</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> text.lower().replace(<span class="st">'.'</span>, <span class="st">''</span>).split()</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create simple vocabulary</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> {word: i <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">set</span>(tokens))}</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    vocab[<span class="st">'&lt;PAD&gt;'</span>] <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    vocab[<span class="st">'&lt;UNK&gt;'</span>] <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert tokens to IDs</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    token_ids <span class="op">=</span> [vocab.get(token, vocab[<span class="st">'&lt;UNK&gt;'</span>]) <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    token_tensor <span class="op">=</span> torch.tensor(token_ids).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Embedding layer</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    embed_layer <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(vocab), <span class="dv">256</span>)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    text_embeddings <span class="op">=</span> embed_layer(token_tensor)</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token IDs: </span><span class="sc">{</span>token_ids<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Embeddings shape: </span><span class="sc">{</span>text_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">40</span>)</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Patch Embeddings (GFM):"</span>)</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate satellite patch processing</span></span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>    patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>    num_bands <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create sample satellite patch</span></span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>    satellite_patch <span class="op">=</span> torch.randn(<span class="dv">1</span>, num_bands, patch_size, patch_size)</span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape to patch format</span></span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>    patch_flat <span class="op">=</span> satellite_patch.view(<span class="dv">1</span>, num_bands <span class="op">*</span> patch_size <span class="op">*</span> patch_size)</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Linear projection to embedding space</span></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>    patch_projection <span class="op">=</span> nn.Linear(num_bands <span class="op">*</span> patch_size <span class="op">*</span> patch_size, <span class="dv">256</span>)</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>    patch_embeddings <span class="op">=</span> patch_projection(patch_flat)</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Original patch shape: </span><span class="sc">{</span>satellite_patch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Flattened patch shape: </span><span class="sc">{</span>patch_flat<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Patch embeddings shape: </span><span class="sc">{</span>patch_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text_embeddings, patch_embeddings</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>text_emb, patch_emb <span class="op">=</span> demonstrate_embeddings()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Text Embeddings (LLM):
Original text: The forest shows signs of deforestation.
Tokens: ['the', 'forest', 'shows', 'signs', 'of', 'deforestation']
Token IDs: [5, 0, 3, 2, 4, 1]
Embeddings shape: torch.Size([1, 6, 256])

----------------------------------------
Patch Embeddings (GFM):
Original patch shape: torch.Size([1, 6, 16, 16])
Flattened patch shape: torch.Size([1, 1536])
Patch embeddings shape: torch.Size([1, 256])</code></pre>
</div>
</div>
</section>
<section id="positional-encoding-comparison" class="level3">
<h3 class="anchored" data-anchor-id="positional-encoding-comparison">Positional Encoding Comparison</h3>
<div id="6dd40823" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_positional_encodings():</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare positional encoding strategies"""</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"1D Positional Encoding (LLM):"</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sinusoidal_positional_encoding(seq_len, embed_dim):</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Create sinusoidal positional encodings"""</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(seq_len, embed_dim)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, seq_len).unsqueeze(<span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, embed_dim, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> </span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>                           <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> embed_dim))</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pe</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    seq_len, embed_dim <span class="op">=</span> <span class="dv">100</span>, <span class="dv">256</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    pos_encoding_1d <span class="op">=</span> sinusoidal_positional_encoding(seq_len, embed_dim)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"1D positional encoding shape: </span><span class="sc">{</span>pos_encoding_1d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize positional encoding</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>    plt.imshow(pos_encoding_1d[:<span class="dv">50</span>, :<span class="dv">50</span>].T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'1D Positional Encoding (LLM)'</span>)</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Sequence Position'</span>)</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Embedding Dimension'</span>)</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">2D Positional Encoding (GFM):"</span>)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_2d_positional_encoding(height, width, embed_dim):</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Create 2D positional encodings for spatial data"""</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(height, width, embed_dim)</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create position grids</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>        y_pos <span class="op">=</span> torch.arange(height).unsqueeze(<span class="dv">1</span>).repeat(<span class="dv">1</span>, width).<span class="bu">float</span>()</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>        x_pos <span class="op">=</span> torch.arange(width).unsqueeze(<span class="dv">0</span>).repeat(height, <span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simplified 2D positional encoding</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use sinusoidal encoding for y positions in first half of embedding</span></span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embed_dim <span class="op">//</span> <span class="dv">2</span>):</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>                pe[:, :, i] <span class="op">=</span> torch.sin(y_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (i <span class="op">/</span> embed_dim)))</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>                pe[:, :, i] <span class="op">=</span> torch.cos(y_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (i <span class="op">/</span> embed_dim)))</span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use sinusoidal encoding for x positions in second half of embedding</span></span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embed_dim <span class="op">//</span> <span class="dv">2</span>, embed_dim):</span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a>            j <span class="op">=</span> i <span class="op">-</span> embed_dim <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> j <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>                pe[:, :, i] <span class="op">=</span> torch.sin(x_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (j <span class="op">/</span> embed_dim)))</span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>                pe[:, :, i] <span class="op">=</span> torch.cos(x_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (j <span class="op">/</span> embed_dim)))</span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pe</span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a>    height, width <span class="op">=</span> <span class="dv">8</span>, <span class="dv">8</span></span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a>    pos_encoding_2d <span class="op">=</span> create_2d_positional_encoding(height, width, embed_dim)</span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"2D positional encoding shape: </span><span class="sc">{</span>pos_encoding_2d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize 2D positional encoding</span></span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Show first 64 dimensions as an 8x8 grid</span></span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>    pos_2d_viz <span class="op">=</span> pos_encoding_2d[:, :, :<span class="dv">64</span>].mean(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>    plt.imshow(pos_2d_viz, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'equal'</span>)</span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'2D Positional Encoding (GFM)'</span>)</span>
<span id="cb25-72"><a href="#cb25-72" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Width'</span>)</span>
<span id="cb25-73"><a href="#cb25-73" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Height'</span>)</span>
<span id="cb25-74"><a href="#cb25-74" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb25-75"><a href="#cb25-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-76"><a href="#cb25-76" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb25-77"><a href="#cb25-77" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb25-78"><a href="#cb25-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-79"><a href="#cb25-79" aria-hidden="true" tabindex="-1"></a>compare_positional_encodings()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1D Positional Encoding (LLM):
1D positional encoding shape: torch.Size([100, 256])

2D Positional Encoding (GFM):
2D positional encoding shape: torch.Size([8, 8, 256])</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="foundation_model_architectures_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="course-mapping-and-applications" class="level2">
<h2 class="anchored" data-anchor-id="course-mapping-and-applications">Course Mapping and Applications</h2>
<section id="weekly-course-structure" class="level3">
<h3 class="anchored" data-anchor-id="weekly-course-structure">Weekly Course Structure</h3>
<div id="7f543cb1" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>course_mapping <span class="op">=</span> {</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Weeks 1-3"</span>: {</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Focus"</span>: <span class="st">"Data → Attention → Architecture"</span>,</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLM Topics"</span>: [<span class="st">"Text preprocessing"</span>, <span class="st">"Tokenization"</span>, <span class="st">"Transformer blocks"</span>],</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFM Topics"</span>: [<span class="st">"Satellite data"</span>, <span class="st">"Patch embedding"</span>, <span class="st">"Spatial attention"</span>]</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Weeks 4-7"</span>: {</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Focus"</span>: <span class="st">"Pretraining → Training → Evaluation → Integration"</span>,</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLM Topics"</span>: [<span class="st">"Language modeling"</span>, <span class="st">"Training loops"</span>, <span class="st">"Perplexity"</span>],</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFM Topics"</span>: [<span class="st">"Masked reconstruction"</span>, <span class="st">"Cloud handling"</span>, <span class="st">"Linear probing"</span>]</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Weeks 8-10"</span>: {</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Focus"</span>: <span class="st">"Finetuning → Deployment → Synthesis"</span>,</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLM Topics"</span>: [<span class="st">"Instruction tuning"</span>, <span class="st">"PEFT"</span>, <span class="st">"API deployment"</span>],</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFM Topics"</span>: [<span class="st">"Task heads"</span>, <span class="st">"Few-shot learning"</span>, <span class="st">"Geospatial inference"</span>]</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Course Structure Mapping:"</span>)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> period, details <span class="kw">in</span> course_mapping.items():</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>period<span class="sc">}</span><span class="ss"> - </span><span class="sc">{</span>details[<span class="st">'Focus'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  LLM Focus: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(details[<span class="st">'LLM Topics'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  GFM Focus: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(details[<span class="st">'GFM Topics'</span>])<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Course Structure Mapping:
==================================================

Weeks 1-3 - Data → Attention → Architecture
  LLM Focus: Text preprocessing, Tokenization, Transformer blocks
  GFM Focus: Satellite data, Patch embedding, Spatial attention

Weeks 4-7 - Pretraining → Training → Evaluation → Integration
  LLM Focus: Language modeling, Training loops, Perplexity
  GFM Focus: Masked reconstruction, Cloud handling, Linear probing

Weeks 8-10 - Finetuning → Deployment → Synthesis
  LLM Focus: Instruction tuning, PEFT, API deployment
  GFM Focus: Task heads, Few-shot learning, Geospatial inference</code></pre>
</div>
</div>
</section>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h3>
<div id="057ab909" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>key_differences <span class="op">=</span> {</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Data Nature"</span>: {</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"Discrete text tokens with semantic consistency"</span>,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"Continuous pixel values requiring contextual interpretation"</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Tokenization"</span>: {</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"Vocabulary-based discrete mapping"</span>,</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"Patch-based continuous projection"</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Positional Info"</span>: {</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"1D sequence positions"</span>,</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"2D spatial + temporal positions"</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Training Objective"</span>: {</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"Next token prediction or masked language modeling"</span>,</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"Masked patch reconstruction or contrastive learning"</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Evaluation"</span>: {</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"Perplexity and downstream language tasks"</span>,</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"Reconstruction quality and spatial generalization"</span></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deployment"</span>: {</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"Text generation with streaming and caching"</span>,</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"Spatial inference with tiling and batch processing"</span></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Key Architectural Differences:"</span>)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> aspect, comparison <span class="kw">in</span> key_differences.items():</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>aspect<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  LLMs: </span><span class="sc">{</span>comparison[<span class="st">'LLMs'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  GFMs: </span><span class="sc">{</span>comparison[<span class="st">'GFMs'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Key Architectural Differences:
============================================================

Data Nature:
  LLMs: Discrete text tokens with semantic consistency
  GFMs: Continuous pixel values requiring contextual interpretation

Tokenization:
  LLMs: Vocabulary-based discrete mapping
  GFMs: Patch-based continuous projection

Positional Info:
  LLMs: 1D sequence positions
  GFMs: 2D spatial + temporal positions

Training Objective:
  LLMs: Next token prediction or masked language modeling
  GFMs: Masked patch reconstruction or contrastive learning

Evaluation:
  LLMs: Perplexity and downstream language tasks
  GFMs: Reconstruction quality and spatial generalization

Deployment:
  LLMs: Text generation with streaming and caching
  GFMs: Spatial inference with tiling and batch processing</code></pre>
</div>
</div>
</section>
</section>
<section id="further-reading-and-references" class="level2">
<h2 class="anchored" data-anchor-id="further-reading-and-references">Further Reading and References</h2>
<section id="essential-papers" class="level3">
<h3 class="anchored" data-anchor-id="essential-papers">Essential Papers</h3>
<div id="9d8934c8" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>references <span class="op">=</span> {</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Foundation Papers"</span>: [</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Attention Is All You Need (Vaswani et al., 2017)"</span>,</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)"</span>,</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Language Models are Few-Shot Learners (Brown et al., 2020)"</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Vision Transformers"</span>: [</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"An Image is Worth 16x16 Words (Dosovitskiy et al., 2020)"</span>,</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Masked Autoencoders Are Scalable Vision Learners (He et al., 2021)"</span>,</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Scaling Vision Transformers (Zhai et al., 2021)"</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Geospatial Foundation Models"</span>: [</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Prithvi: A Foundation Model for Earth Observation (IBM/NASA, 2023)"</span>,</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">"SatMAE: Masked Autoencoders for Satellite Imagery (Cong et al., 2022)"</span>,</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Clay: A Foundation Model for Earth Observation (Made with Clay, 2024)"</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Essential References:"</span>)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">40</span>)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> category, papers <span class="kw">in</span> references.items():</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>category<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> paper <span class="kw">in</span> papers:</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  • </span><span class="sc">{</span>paper<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Essential References:
========================================

Foundation Papers:
  • Attention Is All You Need (Vaswani et al., 2017)
  • BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)
  • Language Models are Few-Shot Learners (Brown et al., 2020)

Vision Transformers:
  • An Image is Worth 16x16 Words (Dosovitskiy et al., 2020)
  • Masked Autoencoders Are Scalable Vision Learners (He et al., 2021)
  • Scaling Vision Transformers (Zhai et al., 2021)

Geospatial Foundation Models:
  • Prithvi: A Foundation Model for Earth Observation (IBM/NASA, 2023)
  • SatMAE: Masked Autoencoders for Satellite Imagery (Cong et al., 2022)
  • Clay: A Foundation Model for Earth Observation (Made with Clay, 2024)</code></pre>
</div>
</div>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Key concepts for foundation model architectures: - <strong>Historical Evolution</strong>: From symbolic AI to transformer-based foundation models - <strong>Architecture Comparison</strong>: LLMs use discrete tokenization, GFMs use continuous patch embeddings - <strong>Development Pipeline</strong>: 9-step process with domain-specific adaptations - <strong>Scaling Trends</strong>: LLMs scale in parameters/context, GFMs scale in spectral/spatial/temporal dimensions - <strong>Training Objectives</strong>: Next-token prediction vs.&nbsp;masked patch reconstruction - <strong>Deployment Considerations</strong>: Text streaming vs.&nbsp;spatial tiling and batch inference - <strong>Course Integration</strong>: Weekly progression from data processing to deployment</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb33" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Foundation Model Architectures"</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "LLMs vs. Geospatial Foundation Models (GFMs)"</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> geoai</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction to Foundation Model Architectures</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>Foundation models are large-scale models trained on diverse data that can be adapted to various downstream tasks. This cheatsheet compares Language Model (LLM) and Geospatial Foundation Model (GFM) development pipelines.</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoConfig</span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CUDA available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## Evolution from AI to Transformers</span></span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Historical Milestones</span></span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Timeline of key developments</span></span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>timeline <span class="op">=</span> {</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>    <span class="st">"1950s-1990s"</span>: <span class="st">"Symbolic AI, early neural networks"</span>,</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2012"</span>: <span class="st">"Deep learning breakthrough (ImageNet)"</span>,</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2017"</span>: <span class="st">"Transformers ('Attention Is All You Need')"</span>,</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2018-2020"</span>: <span class="st">"BERT/GPT families emerge"</span>,</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2021-2024"</span>: <span class="st">"Scaling laws, instruction tuning, multimodality"</span></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"AI/ML → Transformers Timeline:"</span>)</span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> year, development <span class="kw">in</span> timeline.items():</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>year<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>development<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-45"><a href="#cb33-45" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-46"><a href="#cb33-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-47"><a href="#cb33-47" aria-hidden="true" tabindex="-1"></a><span class="fu">### Transformer Architecture Essentials</span></span>
<span id="cb33-50"><a href="#cb33-50" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-51"><a href="#cb33-51" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleTransformerBlock(nn.Module):</span>
<span id="cb33-52"><a href="#cb33-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simplified transformer block to illustrate key components"""</span></span>
<span id="cb33-53"><a href="#cb33-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-54"><a href="#cb33-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">8</span>, mlp_ratio<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb33-55"><a href="#cb33-55" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-56"><a href="#cb33-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb33-57"><a href="#cb33-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-58"><a href="#cb33-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multi-head attention</span></span>
<span id="cb33-59"><a href="#cb33-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> nn.MultiheadAttention(embed_dim, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-60"><a href="#cb33-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-61"><a href="#cb33-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feed-forward network</span></span>
<span id="cb33-62"><a href="#cb33-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> nn.Sequential(</span>
<span id="cb33-63"><a href="#cb33-63" aria-hidden="true" tabindex="-1"></a>            nn.Linear(embed_dim, <span class="bu">int</span>(embed_dim <span class="op">*</span> mlp_ratio)),</span>
<span id="cb33-64"><a href="#cb33-64" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb33-65"><a href="#cb33-65" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="bu">int</span>(embed_dim <span class="op">*</span> mlp_ratio), embed_dim)</span>
<span id="cb33-66"><a href="#cb33-66" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb33-67"><a href="#cb33-67" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-68"><a href="#cb33-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Layer normalization</span></span>
<span id="cb33-69"><a href="#cb33-69" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb33-70"><a href="#cb33-70" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb33-71"><a href="#cb33-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-72"><a href="#cb33-72" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb33-73"><a href="#cb33-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Self-attention with residual connection</span></span>
<span id="cb33-74"><a href="#cb33-74" aria-hidden="true" tabindex="-1"></a>        attn_out, _ <span class="op">=</span> <span class="va">self</span>.attention(x, x, x)</span>
<span id="cb33-75"><a href="#cb33-75" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> attn_out)</span>
<span id="cb33-76"><a href="#cb33-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-77"><a href="#cb33-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># MLP with residual connection</span></span>
<span id="cb33-78"><a href="#cb33-78" aria-hidden="true" tabindex="-1"></a>        mlp_out <span class="op">=</span> <span class="va">self</span>.mlp(x)</span>
<span id="cb33-79"><a href="#cb33-79" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> mlp_out)</span>
<span id="cb33-80"><a href="#cb33-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-81"><a href="#cb33-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb33-82"><a href="#cb33-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-83"><a href="#cb33-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Example transformer block</span></span>
<span id="cb33-84"><a href="#cb33-84" aria-hidden="true" tabindex="-1"></a>transformer_block <span class="op">=</span> SimpleTransformerBlock(embed_dim<span class="op">=</span><span class="dv">768</span>, num_heads<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb33-85"><a href="#cb33-85" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">100</span>, <span class="dv">768</span>)  <span class="co"># [batch_size, seq_len, embed_dim]</span></span>
<span id="cb33-86"><a href="#cb33-86" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> transformer_block(sample_input)</span>
<span id="cb33-87"><a href="#cb33-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-88"><a href="#cb33-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>sample_input<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-89"><a href="#cb33-89" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-90"><a href="#cb33-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> transformer_block.parameters())<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb33-91"><a href="#cb33-91" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-92"><a href="#cb33-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-93"><a href="#cb33-93" aria-hidden="true" tabindex="-1"></a><span class="fu">## 9-Step Development Pipeline Comparison</span></span>
<span id="cb33-94"><a href="#cb33-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-95"><a href="#cb33-95" aria-hidden="true" tabindex="-1"></a><span class="fu">### LLM Development Pipeline</span></span>
<span id="cb33-98"><a href="#cb33-98" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-99"><a href="#cb33-99" aria-hidden="true" tabindex="-1"></a>llm_pipeline <span class="op">=</span> {</span>
<span id="cb33-100"><a href="#cb33-100" aria-hidden="true" tabindex="-1"></a>    <span class="st">"1. Data Preparation"</span>: <span class="st">"Text corpora, deduplication, quality filtering, mixing ratios"</span>,</span>
<span id="cb33-101"><a href="#cb33-101" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2. Tokenization"</span>: <span class="st">"BPE, vocabulary construction, special tokens"</span>,</span>
<span id="cb33-102"><a href="#cb33-102" aria-hidden="true" tabindex="-1"></a>    <span class="st">"3. Architecture"</span>: <span class="st">"GPT/BERT variants, depth/width scaling, context length"</span>,</span>
<span id="cb33-103"><a href="#cb33-103" aria-hidden="true" tabindex="-1"></a>    <span class="st">"4. Pretraining Objective"</span>: <span class="st">"Next-token prediction, masked language modeling"</span>,</span>
<span id="cb33-104"><a href="#cb33-104" aria-hidden="true" tabindex="-1"></a>    <span class="st">"5. Training Loop"</span>: <span class="st">"Optimizers, LR schedules, mixed precision, gradient clipping"</span>,</span>
<span id="cb33-105"><a href="#cb33-105" aria-hidden="true" tabindex="-1"></a>    <span class="st">"6. Evaluation"</span>: <span class="st">"Perplexity, downstream task probing, benchmarks"</span>,</span>
<span id="cb33-106"><a href="#cb33-106" aria-hidden="true" tabindex="-1"></a>    <span class="st">"7. Pretrained Weights"</span>: <span class="st">"Model hubs, tokenizer alignment, loading utilities"</span>,</span>
<span id="cb33-107"><a href="#cb33-107" aria-hidden="true" tabindex="-1"></a>    <span class="st">"8. Finetuning"</span>: <span class="st">"Task-specific heads, PEFT methods, instruction tuning"</span>,</span>
<span id="cb33-108"><a href="#cb33-108" aria-hidden="true" tabindex="-1"></a>    <span class="st">"9. Deployment"</span>: <span class="st">"API serving, KV caching, inference optimization"</span></span>
<span id="cb33-109"><a href="#cb33-109" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-110"><a href="#cb33-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-111"><a href="#cb33-111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"LLM Development Pipeline:"</span>)</span>
<span id="cb33-112"><a href="#cb33-112" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, description <span class="kw">in</span> llm_pipeline.items():</span>
<span id="cb33-113"><a href="#cb33-113" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>description<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-114"><a href="#cb33-114" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-115"><a href="#cb33-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-116"><a href="#cb33-116" aria-hidden="true" tabindex="-1"></a><span class="fu">### GFM Development Pipeline</span></span>
<span id="cb33-119"><a href="#cb33-119" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-120"><a href="#cb33-120" aria-hidden="true" tabindex="-1"></a>gfm_pipeline <span class="op">=</span> {</span>
<span id="cb33-121"><a href="#cb33-121" aria-hidden="true" tabindex="-1"></a>    <span class="st">"1. Data Preparation"</span>: <span class="st">"Multi-spectral data, georegistration, tiling, cloud masking"</span>,</span>
<span id="cb33-122"><a href="#cb33-122" aria-hidden="true" tabindex="-1"></a>    <span class="st">"2. Tokenization"</span>: <span class="st">"Patch-based, continuous embeddings, 2D/temporal positions"</span>,</span>
<span id="cb33-123"><a href="#cb33-123" aria-hidden="true" tabindex="-1"></a>    <span class="st">"3. Architecture"</span>: <span class="st">"ViT encoders, spatial/temporal attention, memory constraints"</span>,</span>
<span id="cb33-124"><a href="#cb33-124" aria-hidden="true" tabindex="-1"></a>    <span class="st">"4. Pretraining Objective"</span>: <span class="st">"Masked patch reconstruction, contrastive learning"</span>,</span>
<span id="cb33-125"><a href="#cb33-125" aria-hidden="true" tabindex="-1"></a>    <span class="st">"5. Training Loop"</span>: <span class="st">"Cloud masks, mixed precision, gradient accumulation"</span>,</span>
<span id="cb33-126"><a href="#cb33-126" aria-hidden="true" tabindex="-1"></a>    <span class="st">"6. Evaluation"</span>: <span class="st">"Reconstruction metrics, linear probing, generalization"</span>,</span>
<span id="cb33-127"><a href="#cb33-127" aria-hidden="true" tabindex="-1"></a>    <span class="st">"7. Pretrained Weights"</span>: <span class="st">"Prithvi, SatMAE, adapter loading, band alignment"</span>,</span>
<span id="cb33-128"><a href="#cb33-128" aria-hidden="true" tabindex="-1"></a>    <span class="st">"8. Finetuning"</span>: <span class="st">"Task heads, PEFT, few-shot learning for limited labels"</span>,</span>
<span id="cb33-129"><a href="#cb33-129" aria-hidden="true" tabindex="-1"></a>    <span class="st">"9. Deployment"</span>: <span class="st">"Tiling inference, geospatial APIs, batch processing"</span></span>
<span id="cb33-130"><a href="#cb33-130" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-131"><a href="#cb33-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-132"><a href="#cb33-132" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GFM Development Pipeline:"</span>)</span>
<span id="cb33-133"><a href="#cb33-133" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step, description <span class="kw">in</span> gfm_pipeline.items():</span>
<span id="cb33-134"><a href="#cb33-134" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>description<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-135"><a href="#cb33-135" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-136"><a href="#cb33-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-137"><a href="#cb33-137" aria-hidden="true" tabindex="-1"></a><span class="fu">## Step-by-Step Detailed Comparison</span></span>
<span id="cb33-138"><a href="#cb33-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-139"><a href="#cb33-139" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. Data Preparation Differences</span></span>
<span id="cb33-142"><a href="#cb33-142" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-143"><a href="#cb33-143" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demonstrate_data_preparation():</span>
<span id="cb33-144"><a href="#cb33-144" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Show key differences in data preparation"""</span></span>
<span id="cb33-145"><a href="#cb33-145" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-146"><a href="#cb33-146" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LLM data preparation simulation</span></span>
<span id="cb33-147"><a href="#cb33-147" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"LLM Data Preparation:"</span>)</span>
<span id="cb33-148"><a href="#cb33-148" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Text scraping from web, books, code"</span>)</span>
<span id="cb33-149"><a href="#cb33-149" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Deduplication algorithms"</span>)</span>
<span id="cb33-150"><a href="#cb33-150" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Quality filtering (language detection, toxicity)"</span>)</span>
<span id="cb33-151"><a href="#cb33-151" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Data mixing ratios optimization"</span>)</span>
<span id="cb33-152"><a href="#cb33-152" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-153"><a href="#cb33-153" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate text preprocessing</span></span>
<span id="cb33-154"><a href="#cb33-154" aria-hidden="true" tabindex="-1"></a>    sample_texts <span class="op">=</span> [</span>
<span id="cb33-155"><a href="#cb33-155" aria-hidden="true" tabindex="-1"></a>        <span class="st">"The quick brown fox jumps over the lazy dog."</span>,</span>
<span id="cb33-156"><a href="#cb33-156" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Machine learning is transforming many industries."</span>,</span>
<span id="cb33-157"><a href="#cb33-157" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Climate change requires urgent global action."</span></span>
<span id="cb33-158"><a href="#cb33-158" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb33-159"><a href="#cb33-159" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-160"><a href="#cb33-160" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Basic tokenization simulation</span></span>
<span id="cb33-161"><a href="#cb33-161" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb33-162"><a href="#cb33-162" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> text <span class="kw">in</span> sample_texts:</span>
<span id="cb33-163"><a href="#cb33-163" aria-hidden="true" tabindex="-1"></a>        vocab.update(text.lower().replace(<span class="st">'.'</span>, <span class="st">''</span>).split())</span>
<span id="cb33-164"><a href="#cb33-164" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-165"><a href="#cb33-165" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Sample vocabulary size: </span><span class="sc">{</span><span class="bu">len</span>(vocab)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-166"><a href="#cb33-166" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Sample tokens: </span><span class="sc">{</span><span class="bu">list</span>(vocab)[:<span class="dv">10</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-167"><a href="#cb33-167" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-168"><a href="#cb33-168" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb33-169"><a href="#cb33-169" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-170"><a href="#cb33-170" aria-hidden="true" tabindex="-1"></a>    <span class="co"># GFM data preparation simulation</span></span>
<span id="cb33-171"><a href="#cb33-171" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"GFM Data Preparation:"</span>)</span>
<span id="cb33-172"><a href="#cb33-172" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Sensor calibration and atmospheric correction"</span>)</span>
<span id="cb33-173"><a href="#cb33-173" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Georegistration and projection alignment"</span>)</span>
<span id="cb33-174"><a href="#cb33-174" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Cloud masking and quality assessment"</span>)</span>
<span id="cb33-175"><a href="#cb33-175" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Temporal compositing and gap filling"</span>)</span>
<span id="cb33-176"><a href="#cb33-176" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-177"><a href="#cb33-177" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate satellite data preprocessing</span></span>
<span id="cb33-178"><a href="#cb33-178" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">42</span>)</span>
<span id="cb33-179"><a href="#cb33-179" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-180"><a href="#cb33-180" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate multi-spectral satellite patch</span></span>
<span id="cb33-181"><a href="#cb33-181" aria-hidden="true" tabindex="-1"></a>    patch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb33-182"><a href="#cb33-182" aria-hidden="true" tabindex="-1"></a>    num_bands <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb33-183"><a href="#cb33-183" aria-hidden="true" tabindex="-1"></a>    satellite_patch <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">4096</span>, (num_bands, patch_size, patch_size))</span>
<span id="cb33-184"><a href="#cb33-184" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-185"><a href="#cb33-185" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate cloud mask</span></span>
<span id="cb33-186"><a href="#cb33-186" aria-hidden="true" tabindex="-1"></a>    cloud_mask <span class="op">=</span> np.random.random((patch_size, patch_size)) <span class="op">&gt;</span> <span class="fl">0.8</span></span>
<span id="cb33-187"><a href="#cb33-187" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-188"><a href="#cb33-188" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply atmospheric correction (simplified)</span></span>
<span id="cb33-189"><a href="#cb33-189" aria-hidden="true" tabindex="-1"></a>    corrected_patch <span class="op">=</span> satellite_patch.astype(np.float32) <span class="op">/</span> <span class="fl">4095.0</span></span>
<span id="cb33-190"><a href="#cb33-190" aria-hidden="true" tabindex="-1"></a>    corrected_patch[:, cloud_mask] <span class="op">=</span> np.nan  <span class="co"># Mask cloudy pixels across all bands</span></span>
<span id="cb33-191"><a href="#cb33-191" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-192"><a href="#cb33-192" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Satellite patch shape: </span><span class="sc">{</span>satellite_patch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-193"><a href="#cb33-193" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Cloud coverage: </span><span class="sc">{</span>cloud_mask<span class="sc">.</span><span class="bu">sum</span>() <span class="op">/</span> cloud_mask<span class="sc">.</span>size <span class="op">*</span> <span class="dv">100</span><span class="sc">:.1f}</span><span class="ss">%"</span>)</span>
<span id="cb33-194"><a href="#cb33-194" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Valid pixels: </span><span class="sc">{</span>(<span class="op">~</span>np.isnan(corrected_patch[<span class="dv">0</span>]))<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb33-195"><a href="#cb33-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-196"><a href="#cb33-196" aria-hidden="true" tabindex="-1"></a>demonstrate_data_preparation()</span>
<span id="cb33-197"><a href="#cb33-197" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-198"><a href="#cb33-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-199"><a href="#cb33-199" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. Tokenization Approaches</span></span>
<span id="cb33-202"><a href="#cb33-202" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-203"><a href="#cb33-203" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_tokenization():</span>
<span id="cb33-204"><a href="#cb33-204" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare LLM vs GFM tokenization approaches"""</span></span>
<span id="cb33-205"><a href="#cb33-205" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-206"><a href="#cb33-206" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"LLM Tokenization (Discrete):"</span>)</span>
<span id="cb33-207"><a href="#cb33-207" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate BPE tokenization</span></span>
<span id="cb33-208"><a href="#cb33-208" aria-hidden="true" tabindex="-1"></a>    vocab_size, embed_dim <span class="op">=</span> <span class="dv">50000</span>, <span class="dv">768</span></span>
<span id="cb33-209"><a href="#cb33-209" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-210"><a href="#cb33-210" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample token sequence</span></span>
<span id="cb33-211"><a href="#cb33-211" aria-hidden="true" tabindex="-1"></a>    token_ids <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">234</span>, <span class="dv">5678</span>, <span class="dv">2</span>])  <span class="co"># [CLS, word1, word2, word3, SEP]</span></span>
<span id="cb33-212"><a href="#cb33-212" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-213"><a href="#cb33-213" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Embedding lookup</span></span>
<span id="cb33-214"><a href="#cb33-214" aria-hidden="true" tabindex="-1"></a>    embedding_layer <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb33-215"><a href="#cb33-215" aria-hidden="true" tabindex="-1"></a>    token_embeddings <span class="op">=</span> embedding_layer(token_ids)</span>
<span id="cb33-216"><a href="#cb33-216" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-217"><a href="#cb33-217" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token IDs: </span><span class="sc">{</span>token_ids<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-218"><a href="#cb33-218" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token embeddings shape: </span><span class="sc">{</span>token_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-219"><a href="#cb33-219" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Vocabulary size: </span><span class="sc">{</span>vocab_size<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb33-220"><a href="#cb33-220" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-221"><a href="#cb33-221" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">30</span>)</span>
<span id="cb33-222"><a href="#cb33-222" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-223"><a href="#cb33-223" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"GFM Tokenization (Continuous Patches):"</span>)</span>
<span id="cb33-224"><a href="#cb33-224" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate patch-based tokenization</span></span>
<span id="cb33-225"><a href="#cb33-225" aria-hidden="true" tabindex="-1"></a>    patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb33-226"><a href="#cb33-226" aria-hidden="true" tabindex="-1"></a>    num_bands <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb33-227"><a href="#cb33-227" aria-hidden="true" tabindex="-1"></a>    embed_dim <span class="op">=</span> <span class="dv">768</span></span>
<span id="cb33-228"><a href="#cb33-228" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-229"><a href="#cb33-229" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create sample patches</span></span>
<span id="cb33-230"><a href="#cb33-230" aria-hidden="true" tabindex="-1"></a>    num_patches <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb33-231"><a href="#cb33-231" aria-hidden="true" tabindex="-1"></a>    patch_dim <span class="op">=</span> patch_size <span class="op">*</span> patch_size <span class="op">*</span> num_bands</span>
<span id="cb33-232"><a href="#cb33-232" aria-hidden="true" tabindex="-1"></a>    patches <span class="op">=</span> torch.randn(num_patches, patch_dim)</span>
<span id="cb33-233"><a href="#cb33-233" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-234"><a href="#cb33-234" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Linear projection (continuous "tokenization")</span></span>
<span id="cb33-235"><a href="#cb33-235" aria-hidden="true" tabindex="-1"></a>    patch_projection <span class="op">=</span> nn.Linear(patch_dim, embed_dim)</span>
<span id="cb33-236"><a href="#cb33-236" aria-hidden="true" tabindex="-1"></a>    patch_embeddings <span class="op">=</span> patch_projection(patches)</span>
<span id="cb33-237"><a href="#cb33-237" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-238"><a href="#cb33-238" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Patch dimensions: </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">x</span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss">x</span><span class="sc">{</span>num_bands<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>patch_dim<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-239"><a href="#cb33-239" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Patch embeddings shape: </span><span class="sc">{</span>patch_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-240"><a href="#cb33-240" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"No discrete vocabulary - continuous projection"</span>)</span>
<span id="cb33-241"><a href="#cb33-241" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-242"><a href="#cb33-242" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> token_embeddings, patch_embeddings</span>
<span id="cb33-243"><a href="#cb33-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-244"><a href="#cb33-244" aria-hidden="true" tabindex="-1"></a>token_emb, patch_emb <span class="op">=</span> compare_tokenization()</span>
<span id="cb33-245"><a href="#cb33-245" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-246"><a href="#cb33-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-247"><a href="#cb33-247" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. Architecture Comparison</span></span>
<span id="cb33-250"><a href="#cb33-250" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-251"><a href="#cb33-251" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LLMArchitecture(nn.Module):</span>
<span id="cb33-252"><a href="#cb33-252" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simplified LLM architecture (GPT-style)"""</span></span>
<span id="cb33-253"><a href="#cb33-253" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-254"><a href="#cb33-254" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size<span class="op">=</span><span class="dv">50000</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_layers<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb33-255"><a href="#cb33-255" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-256"><a href="#cb33-256" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(vocab_size, embed_dim)</span>
<span id="cb33-257"><a href="#cb33-257" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.positional_encoding <span class="op">=</span> nn.Embedding(<span class="dv">2048</span>, embed_dim)  <span class="co"># Max sequence length</span></span>
<span id="cb33-258"><a href="#cb33-258" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-259"><a href="#cb33-259" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb33-260"><a href="#cb33-260" aria-hidden="true" tabindex="-1"></a>            SimpleTransformerBlock(embed_dim, num_heads) </span>
<span id="cb33-261"><a href="#cb33-261" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb33-262"><a href="#cb33-262" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb33-263"><a href="#cb33-263" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-264"><a href="#cb33-264" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_final <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb33-265"><a href="#cb33-265" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_head <span class="op">=</span> nn.Linear(embed_dim, vocab_size)</span>
<span id="cb33-266"><a href="#cb33-266" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-267"><a href="#cb33-267" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, input_ids):</span>
<span id="cb33-268"><a href="#cb33-268" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> input_ids.shape[<span class="dv">1</span>]</span>
<span id="cb33-269"><a href="#cb33-269" aria-hidden="true" tabindex="-1"></a>        positions <span class="op">=</span> torch.arange(seq_len, device<span class="op">=</span>input_ids.device)</span>
<span id="cb33-270"><a href="#cb33-270" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-271"><a href="#cb33-271" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Token + positional embeddings</span></span>
<span id="cb33-272"><a href="#cb33-272" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embedding(input_ids) <span class="op">+</span> <span class="va">self</span>.positional_encoding(positions)</span>
<span id="cb33-273"><a href="#cb33-273" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-274"><a href="#cb33-274" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transformer layers</span></span>
<span id="cb33-275"><a href="#cb33-275" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb33-276"><a href="#cb33-276" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb33-277"><a href="#cb33-277" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-278"><a href="#cb33-278" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_final(x)</span>
<span id="cb33-279"><a href="#cb33-279" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.output_head(x)</span>
<span id="cb33-280"><a href="#cb33-280" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-281"><a href="#cb33-281" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb33-282"><a href="#cb33-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-283"><a href="#cb33-283" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GFMArchitecture(nn.Module):</span>
<span id="cb33-284"><a href="#cb33-284" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simplified GFM architecture (ViT-style)"""</span></span>
<span id="cb33-285"><a href="#cb33-285" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-286"><a href="#cb33-286" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, patch_size<span class="op">=</span><span class="dv">16</span>, num_bands<span class="op">=</span><span class="dv">6</span>, embed_dim<span class="op">=</span><span class="dv">768</span>, num_layers<span class="op">=</span><span class="dv">12</span>, num_heads<span class="op">=</span><span class="dv">12</span>):</span>
<span id="cb33-287"><a href="#cb33-287" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb33-288"><a href="#cb33-288" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb33-289"><a href="#cb33-289" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_bands <span class="op">=</span> num_bands</span>
<span id="cb33-290"><a href="#cb33-290" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-291"><a href="#cb33-291" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch embedding</span></span>
<span id="cb33-292"><a href="#cb33-292" aria-hidden="true" tabindex="-1"></a>        patch_dim <span class="op">=</span> patch_size <span class="op">*</span> patch_size <span class="op">*</span> num_bands</span>
<span id="cb33-293"><a href="#cb33-293" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.patch_embedding <span class="op">=</span> nn.Linear(patch_dim, embed_dim)</span>
<span id="cb33-294"><a href="#cb33-294" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-295"><a href="#cb33-295" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2D positional embedding</span></span>
<span id="cb33-296"><a href="#cb33-296" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed_h <span class="op">=</span> nn.Embedding(<span class="dv">100</span>, embed_dim <span class="op">//</span> <span class="dv">2</span>)  <span class="co"># Height positions</span></span>
<span id="cb33-297"><a href="#cb33-297" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embed_w <span class="op">=</span> nn.Embedding(<span class="dv">100</span>, embed_dim <span class="op">//</span> <span class="dv">2</span>)  <span class="co"># Width positions</span></span>
<span id="cb33-298"><a href="#cb33-298" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-299"><a href="#cb33-299" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb33-300"><a href="#cb33-300" aria-hidden="true" tabindex="-1"></a>            SimpleTransformerBlock(embed_dim, num_heads) </span>
<span id="cb33-301"><a href="#cb33-301" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb33-302"><a href="#cb33-302" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb33-303"><a href="#cb33-303" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-304"><a href="#cb33-304" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_final <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb33-305"><a href="#cb33-305" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-306"><a href="#cb33-306" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, patches, patch_positions):</span>
<span id="cb33-307"><a href="#cb33-307" aria-hidden="true" tabindex="-1"></a>        batch_size, num_patches, patch_dim <span class="op">=</span> patches.shape</span>
<span id="cb33-308"><a href="#cb33-308" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-309"><a href="#cb33-309" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Patch embeddings</span></span>
<span id="cb33-310"><a href="#cb33-310" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.patch_embedding(patches)</span>
<span id="cb33-311"><a href="#cb33-311" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-312"><a href="#cb33-312" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2D positional embeddings</span></span>
<span id="cb33-313"><a href="#cb33-313" aria-hidden="true" tabindex="-1"></a>        pos_h, pos_w <span class="op">=</span> patch_positions[:, :, <span class="dv">0</span>], patch_positions[:, :, <span class="dv">1</span>]</span>
<span id="cb33-314"><a href="#cb33-314" aria-hidden="true" tabindex="-1"></a>        pos_emb <span class="op">=</span> torch.cat([</span>
<span id="cb33-315"><a href="#cb33-315" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pos_embed_h(pos_h),</span>
<span id="cb33-316"><a href="#cb33-316" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.pos_embed_w(pos_w)</span>
<span id="cb33-317"><a href="#cb33-317" aria-hidden="true" tabindex="-1"></a>        ], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb33-318"><a href="#cb33-318" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-319"><a href="#cb33-319" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> pos_emb</span>
<span id="cb33-320"><a href="#cb33-320" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-321"><a href="#cb33-321" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transformer layers</span></span>
<span id="cb33-322"><a href="#cb33-322" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb33-323"><a href="#cb33-323" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb33-324"><a href="#cb33-324" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-325"><a href="#cb33-325" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_final(x)</span>
<span id="cb33-326"><a href="#cb33-326" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb33-327"><a href="#cb33-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-328"><a href="#cb33-328" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare architectures</span></span>
<span id="cb33-329"><a href="#cb33-329" aria-hidden="true" tabindex="-1"></a>llm_model <span class="op">=</span> LLMArchitecture(vocab_size<span class="op">=</span><span class="dv">10000</span>, embed_dim<span class="op">=</span><span class="dv">384</span>, num_layers<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb33-330"><a href="#cb33-330" aria-hidden="true" tabindex="-1"></a>gfm_model <span class="op">=</span> GFMArchitecture(patch_size<span class="op">=</span><span class="dv">16</span>, num_bands<span class="op">=</span><span class="dv">6</span>, embed_dim<span class="op">=</span><span class="dv">384</span>, num_layers<span class="op">=</span><span class="dv">6</span>, num_heads<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb33-331"><a href="#cb33-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-332"><a href="#cb33-332" aria-hidden="true" tabindex="-1"></a>llm_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> llm_model.parameters())</span>
<span id="cb33-333"><a href="#cb33-333" aria-hidden="true" tabindex="-1"></a>gfm_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> gfm_model.parameters())</span>
<span id="cb33-334"><a href="#cb33-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-335"><a href="#cb33-335" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Architecture Comparison:"</span>)</span>
<span id="cb33-336"><a href="#cb33-336" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LLM parameters: </span><span class="sc">{</span>llm_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb33-337"><a href="#cb33-337" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GFM parameters: </span><span class="sc">{</span>gfm_params<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb33-338"><a href="#cb33-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-339"><a href="#cb33-339" aria-hidden="true" tabindex="-1"></a><span class="co"># Test forward passes</span></span>
<span id="cb33-340"><a href="#cb33-340" aria-hidden="true" tabindex="-1"></a>sample_tokens <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10000</span>, (<span class="dv">2</span>, <span class="dv">50</span>))  <span class="co"># [batch_size, seq_len]</span></span>
<span id="cb33-341"><a href="#cb33-341" aria-hidden="true" tabindex="-1"></a>sample_patches <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">16</span>, <span class="dv">16</span><span class="op">*</span><span class="dv">16</span><span class="op">*</span><span class="dv">6</span>)  <span class="co"># [batch_size, num_patches, patch_dim]</span></span>
<span id="cb33-342"><a href="#cb33-342" aria-hidden="true" tabindex="-1"></a>sample_positions <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">10</span>, (<span class="dv">2</span>, <span class="dv">16</span>, <span class="dv">2</span>))  <span class="co"># [batch_size, num_patches, 2]</span></span>
<span id="cb33-343"><a href="#cb33-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-344"><a href="#cb33-344" aria-hidden="true" tabindex="-1"></a>llm_output <span class="op">=</span> llm_model(sample_tokens)</span>
<span id="cb33-345"><a href="#cb33-345" aria-hidden="true" tabindex="-1"></a>gfm_output <span class="op">=</span> gfm_model(sample_patches, sample_positions)</span>
<span id="cb33-346"><a href="#cb33-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-347"><a href="#cb33-347" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">LLM output shape: </span><span class="sc">{</span>llm_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-348"><a href="#cb33-348" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"GFM output shape: </span><span class="sc">{</span>gfm_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-349"><a href="#cb33-349" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-350"><a href="#cb33-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-351"><a href="#cb33-351" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4. Pretraining Objectives</span></span>
<span id="cb33-354"><a href="#cb33-354" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-355"><a href="#cb33-355" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demonstrate_pretraining_objectives():</span>
<span id="cb33-356"><a href="#cb33-356" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare pretraining objectives for LLMs vs GFMs"""</span></span>
<span id="cb33-357"><a href="#cb33-357" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-358"><a href="#cb33-358" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"LLM Pretraining Objectives:"</span>)</span>
<span id="cb33-359"><a href="#cb33-359" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"1. Next-Token Prediction (GPT-style)"</span>)</span>
<span id="cb33-360"><a href="#cb33-360" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"2. Masked Language Modeling (BERT-style)"</span>)</span>
<span id="cb33-361"><a href="#cb33-361" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-362"><a href="#cb33-362" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate next-token prediction</span></span>
<span id="cb33-363"><a href="#cb33-363" aria-hidden="true" tabindex="-1"></a>    sequence <span class="op">=</span> torch.tensor([[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]])</span>
<span id="cb33-364"><a href="#cb33-364" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> torch.tensor([[<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]])  <span class="co"># Shifted by one</span></span>
<span id="cb33-365"><a href="#cb33-365" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-366"><a href="#cb33-366" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mock logits</span></span>
<span id="cb33-367"><a href="#cb33-367" aria-hidden="true" tabindex="-1"></a>    vocab_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb33-368"><a href="#cb33-368" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">5</span>, vocab_size)</span>
<span id="cb33-369"><a href="#cb33-369" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-370"><a href="#cb33-370" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cross-entropy loss</span></span>
<span id="cb33-371"><a href="#cb33-371" aria-hidden="true" tabindex="-1"></a>    ce_loss <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb33-372"><a href="#cb33-372" aria-hidden="true" tabindex="-1"></a>    next_token_loss <span class="op">=</span> ce_loss(logits.view(<span class="op">-</span><span class="dv">1</span>, vocab_size), targets.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb33-373"><a href="#cb33-373" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-374"><a href="#cb33-374" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Next-token prediction loss: </span><span class="sc">{</span>next_token_loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-375"><a href="#cb33-375" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-376"><a href="#cb33-376" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">30</span>)</span>
<span id="cb33-377"><a href="#cb33-377" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-378"><a href="#cb33-378" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"GFM Pretraining Objectives:"</span>)</span>
<span id="cb33-379"><a href="#cb33-379" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"1. Masked Patch Reconstruction (MAE-style)"</span>)</span>
<span id="cb33-380"><a href="#cb33-380" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"2. Contrastive Learning (optional)"</span>)</span>
<span id="cb33-381"><a href="#cb33-381" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-382"><a href="#cb33-382" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate masked patch reconstruction</span></span>
<span id="cb33-383"><a href="#cb33-383" aria-hidden="true" tabindex="-1"></a>    batch_size, num_patches, patch_dim <span class="op">=</span> <span class="dv">2</span>, <span class="dv">64</span>, <span class="dv">768</span></span>
<span id="cb33-384"><a href="#cb33-384" aria-hidden="true" tabindex="-1"></a>    original_patches <span class="op">=</span> torch.randn(batch_size, num_patches, patch_dim)</span>
<span id="cb33-385"><a href="#cb33-385" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-386"><a href="#cb33-386" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Random masking</span></span>
<span id="cb33-387"><a href="#cb33-387" aria-hidden="true" tabindex="-1"></a>    mask_ratio <span class="op">=</span> <span class="fl">0.75</span></span>
<span id="cb33-388"><a href="#cb33-388" aria-hidden="true" tabindex="-1"></a>    num_masked <span class="op">=</span> <span class="bu">int</span>(num_patches <span class="op">*</span> mask_ratio)</span>
<span id="cb33-389"><a href="#cb33-389" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-390"><a href="#cb33-390" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create random mask</span></span>
<span id="cb33-391"><a href="#cb33-391" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.zeros(batch_size, num_patches, dtype<span class="op">=</span>torch.<span class="bu">bool</span>)</span>
<span id="cb33-392"><a href="#cb33-392" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb33-393"><a href="#cb33-393" aria-hidden="true" tabindex="-1"></a>        masked_indices <span class="op">=</span> torch.randperm(num_patches)[:num_masked]</span>
<span id="cb33-394"><a href="#cb33-394" aria-hidden="true" tabindex="-1"></a>        mask[i, masked_indices] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb33-395"><a href="#cb33-395" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-396"><a href="#cb33-396" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reconstruction loss (simplified)</span></span>
<span id="cb33-397"><a href="#cb33-397" aria-hidden="true" tabindex="-1"></a>    reconstructed_patches <span class="op">=</span> torch.randn_like(original_patches)</span>
<span id="cb33-398"><a href="#cb33-398" aria-hidden="true" tabindex="-1"></a>    reconstruction_loss <span class="op">=</span> nn.MSELoss()(</span>
<span id="cb33-399"><a href="#cb33-399" aria-hidden="true" tabindex="-1"></a>        reconstructed_patches[mask], </span>
<span id="cb33-400"><a href="#cb33-400" aria-hidden="true" tabindex="-1"></a>        original_patches[mask]</span>
<span id="cb33-401"><a href="#cb33-401" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb33-402"><a href="#cb33-402" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-403"><a href="#cb33-403" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Mask ratio: </span><span class="sc">{</span>mask_ratio<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb33-404"><a href="#cb33-404" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Masked patches per sample: </span><span class="sc">{</span>num_masked<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-405"><a href="#cb33-405" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Reconstruction loss: </span><span class="sc">{</span>reconstruction_loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-406"><a href="#cb33-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-407"><a href="#cb33-407" aria-hidden="true" tabindex="-1"></a>demonstrate_pretraining_objectives()</span>
<span id="cb33-408"><a href="#cb33-408" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-409"><a href="#cb33-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-410"><a href="#cb33-410" aria-hidden="true" tabindex="-1"></a><span class="fu">## Scaling and Evolution</span></span>
<span id="cb33-411"><a href="#cb33-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-412"><a href="#cb33-412" aria-hidden="true" tabindex="-1"></a><span class="fu">### Parameter Scaling Comparison</span></span>
<span id="cb33-415"><a href="#cb33-415" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-416"><a href="#cb33-416" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_scaling_trends():</span>
<span id="cb33-417"><a href="#cb33-417" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare scaling trends between LLMs and GFMs"""</span></span>
<span id="cb33-418"><a href="#cb33-418" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-419"><a href="#cb33-419" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LLM scaling milestones</span></span>
<span id="cb33-420"><a href="#cb33-420" aria-hidden="true" tabindex="-1"></a>    llm_milestones <span class="op">=</span> {</span>
<span id="cb33-421"><a href="#cb33-421" aria-hidden="true" tabindex="-1"></a>        <span class="st">'GPT-1'</span>: <span class="fl">117e6</span>,</span>
<span id="cb33-422"><a href="#cb33-422" aria-hidden="true" tabindex="-1"></a>        <span class="st">'BERT-Base'</span>: <span class="fl">110e6</span>,</span>
<span id="cb33-423"><a href="#cb33-423" aria-hidden="true" tabindex="-1"></a>        <span class="st">'GPT-2'</span>: <span class="fl">1.5e9</span>,</span>
<span id="cb33-424"><a href="#cb33-424" aria-hidden="true" tabindex="-1"></a>        <span class="st">'GPT-3'</span>: <span class="fl">175e9</span>,</span>
<span id="cb33-425"><a href="#cb33-425" aria-hidden="true" tabindex="-1"></a>        <span class="st">'PaLM'</span>: <span class="fl">540e9</span>,</span>
<span id="cb33-426"><a href="#cb33-426" aria-hidden="true" tabindex="-1"></a>        <span class="st">'GPT-4'</span>: <span class="fl">1000e9</span>  <span class="co"># Estimated</span></span>
<span id="cb33-427"><a href="#cb33-427" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-428"><a href="#cb33-428" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-429"><a href="#cb33-429" aria-hidden="true" tabindex="-1"></a>    <span class="co"># GFM scaling examples</span></span>
<span id="cb33-430"><a href="#cb33-430" aria-hidden="true" tabindex="-1"></a>    gfm_milestones <span class="op">=</span> {</span>
<span id="cb33-431"><a href="#cb33-431" aria-hidden="true" tabindex="-1"></a>        <span class="st">'SatMAE-Base'</span>: <span class="fl">86e6</span>,</span>
<span id="cb33-432"><a href="#cb33-432" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Prithvi-100M'</span>: <span class="fl">100e6</span>,</span>
<span id="cb33-433"><a href="#cb33-433" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Clay-v0.1'</span>: <span class="fl">139e6</span>,</span>
<span id="cb33-434"><a href="#cb33-434" aria-hidden="true" tabindex="-1"></a>        <span class="st">'SatLas-Base'</span>: <span class="fl">300e6</span>,</span>
<span id="cb33-435"><a href="#cb33-435" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Scale-MAE'</span>: <span class="fl">600e6</span></span>
<span id="cb33-436"><a href="#cb33-436" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-437"><a href="#cb33-437" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-438"><a href="#cb33-438" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualization</span></span>
<span id="cb33-439"><a href="#cb33-439" aria-hidden="true" tabindex="-1"></a>    fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb33-440"><a href="#cb33-440" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-441"><a href="#cb33-441" aria-hidden="true" tabindex="-1"></a>    <span class="co"># LLM scaling</span></span>
<span id="cb33-442"><a href="#cb33-442" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> <span class="bu">list</span>(llm_milestones.keys())</span>
<span id="cb33-443"><a href="#cb33-443" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> [llm_milestones[m]<span class="op">/</span><span class="fl">1e9</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb33-444"><a href="#cb33-444" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-445"><a href="#cb33-445" aria-hidden="true" tabindex="-1"></a>    ax1.bar(models, params, color<span class="op">=</span><span class="st">'skyblue'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb33-446"><a href="#cb33-446" aria-hidden="true" tabindex="-1"></a>    ax1.set_yscale(<span class="st">'log'</span>)</span>
<span id="cb33-447"><a href="#cb33-447" aria-hidden="true" tabindex="-1"></a>    ax1.set_ylabel(<span class="st">'Parameters (Billions)'</span>)</span>
<span id="cb33-448"><a href="#cb33-448" aria-hidden="true" tabindex="-1"></a>    ax1.set_title(<span class="st">'LLM Parameter Scaling'</span>)</span>
<span id="cb33-449"><a href="#cb33-449" aria-hidden="true" tabindex="-1"></a>    ax1.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb33-450"><a href="#cb33-450" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-451"><a href="#cb33-451" aria-hidden="true" tabindex="-1"></a>    <span class="co"># GFM scaling</span></span>
<span id="cb33-452"><a href="#cb33-452" aria-hidden="true" tabindex="-1"></a>    models <span class="op">=</span> <span class="bu">list</span>(gfm_milestones.keys())</span>
<span id="cb33-453"><a href="#cb33-453" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> [gfm_milestones[m]<span class="op">/</span><span class="fl">1e6</span> <span class="cf">for</span> m <span class="kw">in</span> models]</span>
<span id="cb33-454"><a href="#cb33-454" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-455"><a href="#cb33-455" aria-hidden="true" tabindex="-1"></a>    ax2.bar(models, params, color<span class="op">=</span><span class="st">'lightcoral'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb33-456"><a href="#cb33-456" aria-hidden="true" tabindex="-1"></a>    ax2.set_ylabel(<span class="st">'Parameters (Millions)'</span>)</span>
<span id="cb33-457"><a href="#cb33-457" aria-hidden="true" tabindex="-1"></a>    ax2.set_title(<span class="st">'GFM Parameter Scaling'</span>)</span>
<span id="cb33-458"><a href="#cb33-458" aria-hidden="true" tabindex="-1"></a>    ax2.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb33-459"><a href="#cb33-459" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-460"><a href="#cb33-460" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb33-461"><a href="#cb33-461" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb33-462"><a href="#cb33-462" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-463"><a href="#cb33-463" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Context/Input scaling</span></span>
<span id="cb33-464"><a href="#cb33-464" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Context/Input Scaling:"</span>)</span>
<span id="cb33-465"><a href="#cb33-465" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">LLMs:"</span>)</span>
<span id="cb33-466"><a href="#cb33-466" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Context length: 512 → 2K → 8K → 128K+ tokens"</span>)</span>
<span id="cb33-467"><a href="#cb33-467" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Training data: Web text, books, code (curated)"</span>)</span>
<span id="cb33-468"><a href="#cb33-468" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Focus: Language understanding and generation"</span>)</span>
<span id="cb33-469"><a href="#cb33-469" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-470"><a href="#cb33-470" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GFMs:"</span>)</span>
<span id="cb33-471"><a href="#cb33-471" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Input bands: 3 (RGB) → 6+ (multispectral) → hyperspectral"</span>)</span>
<span id="cb33-472"><a href="#cb33-472" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Spatial resolution: Various (10m to 0.3m)"</span>)</span>
<span id="cb33-473"><a href="#cb33-473" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Temporal dimension: Single → time series → multi-temporal"</span>)</span>
<span id="cb33-474"><a href="#cb33-474" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"- Focus: Earth observation and environmental monitoring"</span>)</span>
<span id="cb33-475"><a href="#cb33-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-476"><a href="#cb33-476" aria-hidden="true" tabindex="-1"></a>compare_scaling_trends()</span>
<span id="cb33-477"><a href="#cb33-477" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-478"><a href="#cb33-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-479"><a href="#cb33-479" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data Requirements and Constraints</span></span>
<span id="cb33-482"><a href="#cb33-482" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-483"><a href="#cb33-483" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_data_requirements():</span>
<span id="cb33-484"><a href="#cb33-484" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare data requirements and constraints"""</span></span>
<span id="cb33-485"><a href="#cb33-485" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-486"><a href="#cb33-486" aria-hidden="true" tabindex="-1"></a>    comparison <span class="op">=</span> {</span>
<span id="cb33-487"><a href="#cb33-487" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Data Volume"</span>: {</span>
<span id="cb33-488"><a href="#cb33-488" aria-hidden="true" tabindex="-1"></a>            <span class="st">"LLMs"</span>: <span class="st">"Terabytes of text (web crawls, books, code)"</span>,</span>
<span id="cb33-489"><a href="#cb33-489" aria-hidden="true" tabindex="-1"></a>            <span class="st">"GFMs"</span>: <span class="st">"Petabytes of satellite imagery (constrained by storage/IO)"</span></span>
<span id="cb33-490"><a href="#cb33-490" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb33-491"><a href="#cb33-491" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Data Quality"</span>: {</span>
<span id="cb33-492"><a href="#cb33-492" aria-hidden="true" tabindex="-1"></a>            <span class="st">"LLMs"</span>: <span class="st">"Deduplication, toxicity filtering, language detection"</span>,</span>
<span id="cb33-493"><a href="#cb33-493" aria-hidden="true" tabindex="-1"></a>            <span class="st">"GFMs"</span>: <span class="st">"Cloud masking, atmospheric correction, sensor calibration"</span></span>
<span id="cb33-494"><a href="#cb33-494" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb33-495"><a href="#cb33-495" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Preprocessing"</span>: {</span>
<span id="cb33-496"><a href="#cb33-496" aria-hidden="true" tabindex="-1"></a>            <span class="st">"LLMs"</span>: <span class="st">"Tokenization, sequence packing, attention masks"</span>,</span>
<span id="cb33-497"><a href="#cb33-497" aria-hidden="true" tabindex="-1"></a>            <span class="st">"GFMs"</span>: <span class="st">"Patch extraction, normalization, spatial alignment"</span></span>
<span id="cb33-498"><a href="#cb33-498" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb33-499"><a href="#cb33-499" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Storage Format"</span>: {</span>
<span id="cb33-500"><a href="#cb33-500" aria-hidden="true" tabindex="-1"></a>            <span class="st">"LLMs"</span>: <span class="st">"Compressed text files, tokenized sequences"</span>,</span>
<span id="cb33-501"><a href="#cb33-501" aria-hidden="true" tabindex="-1"></a>            <span class="st">"GFMs"</span>: <span class="st">"Cloud-optimized formats (COG, Zarr), tiled storage"</span></span>
<span id="cb33-502"><a href="#cb33-502" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb33-503"><a href="#cb33-503" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Access Patterns"</span>: {</span>
<span id="cb33-504"><a href="#cb33-504" aria-hidden="true" tabindex="-1"></a>            <span class="st">"LLMs"</span>: <span class="st">"Sequential text processing, random sampling"</span>,</span>
<span id="cb33-505"><a href="#cb33-505" aria-hidden="true" tabindex="-1"></a>            <span class="st">"GFMs"</span>: <span class="st">"Spatial/temporal queries, patch sampling, tiling"</span></span>
<span id="cb33-506"><a href="#cb33-506" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb33-507"><a href="#cb33-507" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-508"><a href="#cb33-508" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-509"><a href="#cb33-509" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Data Requirements Comparison:"</span>)</span>
<span id="cb33-510"><a href="#cb33-510" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb33-511"><a href="#cb33-511" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-512"><a href="#cb33-512" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> aspect, details <span class="kw">in</span> comparison.items():</span>
<span id="cb33-513"><a href="#cb33-513" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>aspect<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb33-514"><a href="#cb33-514" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  LLMs: </span><span class="sc">{</span>details[<span class="st">'LLMs'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-515"><a href="#cb33-515" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  GFMs: </span><span class="sc">{</span>details[<span class="st">'GFMs'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-516"><a href="#cb33-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-517"><a href="#cb33-517" aria-hidden="true" tabindex="-1"></a>compare_data_requirements()</span>
<span id="cb33-518"><a href="#cb33-518" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-519"><a href="#cb33-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-520"><a href="#cb33-520" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implementation Examples</span></span>
<span id="cb33-521"><a href="#cb33-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-522"><a href="#cb33-522" aria-hidden="true" tabindex="-1"></a><span class="fu">### Embedding Creation</span></span>
<span id="cb33-525"><a href="#cb33-525" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-526"><a href="#cb33-526" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demonstrate_embeddings():</span>
<span id="cb33-527"><a href="#cb33-527" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Show embedding creation for both domains"""</span></span>
<span id="cb33-528"><a href="#cb33-528" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-529"><a href="#cb33-529" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Text Embeddings (LLM):"</span>)</span>
<span id="cb33-530"><a href="#cb33-530" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate text tokenization and embedding</span></span>
<span id="cb33-531"><a href="#cb33-531" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> <span class="st">"The forest shows signs of deforestation."</span></span>
<span id="cb33-532"><a href="#cb33-532" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> text.lower().replace(<span class="st">'.'</span>, <span class="st">''</span>).split()</span>
<span id="cb33-533"><a href="#cb33-533" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-534"><a href="#cb33-534" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create simple vocabulary</span></span>
<span id="cb33-535"><a href="#cb33-535" aria-hidden="true" tabindex="-1"></a>    vocab <span class="op">=</span> {word: i <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">set</span>(tokens))}</span>
<span id="cb33-536"><a href="#cb33-536" aria-hidden="true" tabindex="-1"></a>    vocab[<span class="st">'&lt;PAD&gt;'</span>] <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb33-537"><a href="#cb33-537" aria-hidden="true" tabindex="-1"></a>    vocab[<span class="st">'&lt;UNK&gt;'</span>] <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb33-538"><a href="#cb33-538" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-539"><a href="#cb33-539" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert tokens to IDs</span></span>
<span id="cb33-540"><a href="#cb33-540" aria-hidden="true" tabindex="-1"></a>    token_ids <span class="op">=</span> [vocab.get(token, vocab[<span class="st">'&lt;UNK&gt;'</span>]) <span class="cf">for</span> token <span class="kw">in</span> tokens]</span>
<span id="cb33-541"><a href="#cb33-541" aria-hidden="true" tabindex="-1"></a>    token_tensor <span class="op">=</span> torch.tensor(token_ids).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb33-542"><a href="#cb33-542" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-543"><a href="#cb33-543" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Embedding layer</span></span>
<span id="cb33-544"><a href="#cb33-544" aria-hidden="true" tabindex="-1"></a>    embed_layer <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(vocab), <span class="dv">256</span>)</span>
<span id="cb33-545"><a href="#cb33-545" aria-hidden="true" tabindex="-1"></a>    text_embeddings <span class="op">=</span> embed_layer(token_tensor)</span>
<span id="cb33-546"><a href="#cb33-546" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-547"><a href="#cb33-547" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Original text: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-548"><a href="#cb33-548" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Tokens: </span><span class="sc">{</span>tokens<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-549"><a href="#cb33-549" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Token IDs: </span><span class="sc">{</span>token_ids<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-550"><a href="#cb33-550" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Embeddings shape: </span><span class="sc">{</span>text_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-551"><a href="#cb33-551" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-552"><a href="#cb33-552" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="op">+</span> <span class="st">"-"</span><span class="op">*</span><span class="dv">40</span>)</span>
<span id="cb33-553"><a href="#cb33-553" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-554"><a href="#cb33-554" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Patch Embeddings (GFM):"</span>)</span>
<span id="cb33-555"><a href="#cb33-555" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate satellite patch processing</span></span>
<span id="cb33-556"><a href="#cb33-556" aria-hidden="true" tabindex="-1"></a>    patch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb33-557"><a href="#cb33-557" aria-hidden="true" tabindex="-1"></a>    num_bands <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb33-558"><a href="#cb33-558" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-559"><a href="#cb33-559" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create sample satellite patch</span></span>
<span id="cb33-560"><a href="#cb33-560" aria-hidden="true" tabindex="-1"></a>    satellite_patch <span class="op">=</span> torch.randn(<span class="dv">1</span>, num_bands, patch_size, patch_size)</span>
<span id="cb33-561"><a href="#cb33-561" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-562"><a href="#cb33-562" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape to patch format</span></span>
<span id="cb33-563"><a href="#cb33-563" aria-hidden="true" tabindex="-1"></a>    patch_flat <span class="op">=</span> satellite_patch.view(<span class="dv">1</span>, num_bands <span class="op">*</span> patch_size <span class="op">*</span> patch_size)</span>
<span id="cb33-564"><a href="#cb33-564" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-565"><a href="#cb33-565" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Linear projection to embedding space</span></span>
<span id="cb33-566"><a href="#cb33-566" aria-hidden="true" tabindex="-1"></a>    patch_projection <span class="op">=</span> nn.Linear(num_bands <span class="op">*</span> patch_size <span class="op">*</span> patch_size, <span class="dv">256</span>)</span>
<span id="cb33-567"><a href="#cb33-567" aria-hidden="true" tabindex="-1"></a>    patch_embeddings <span class="op">=</span> patch_projection(patch_flat)</span>
<span id="cb33-568"><a href="#cb33-568" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-569"><a href="#cb33-569" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Original patch shape: </span><span class="sc">{</span>satellite_patch<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-570"><a href="#cb33-570" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Flattened patch shape: </span><span class="sc">{</span>patch_flat<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-571"><a href="#cb33-571" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Patch embeddings shape: </span><span class="sc">{</span>patch_embeddings<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-572"><a href="#cb33-572" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-573"><a href="#cb33-573" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text_embeddings, patch_embeddings</span>
<span id="cb33-574"><a href="#cb33-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-575"><a href="#cb33-575" aria-hidden="true" tabindex="-1"></a>text_emb, patch_emb <span class="op">=</span> demonstrate_embeddings()</span>
<span id="cb33-576"><a href="#cb33-576" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-577"><a href="#cb33-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-578"><a href="#cb33-578" aria-hidden="true" tabindex="-1"></a><span class="fu">### Positional Encoding Comparison</span></span>
<span id="cb33-581"><a href="#cb33-581" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-582"><a href="#cb33-582" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_positional_encodings():</span>
<span id="cb33-583"><a href="#cb33-583" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare positional encoding strategies"""</span></span>
<span id="cb33-584"><a href="#cb33-584" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-585"><a href="#cb33-585" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"1D Positional Encoding (LLM):"</span>)</span>
<span id="cb33-586"><a href="#cb33-586" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-587"><a href="#cb33-587" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sinusoidal_positional_encoding(seq_len, embed_dim):</span>
<span id="cb33-588"><a href="#cb33-588" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Create sinusoidal positional encodings"""</span></span>
<span id="cb33-589"><a href="#cb33-589" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(seq_len, embed_dim)</span>
<span id="cb33-590"><a href="#cb33-590" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> torch.arange(<span class="dv">0</span>, seq_len).unsqueeze(<span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb33-591"><a href="#cb33-591" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-592"><a href="#cb33-592" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, embed_dim, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> </span>
<span id="cb33-593"><a href="#cb33-593" aria-hidden="true" tabindex="-1"></a>                           <span class="op">-</span>(np.log(<span class="fl">10000.0</span>) <span class="op">/</span> embed_dim))</span>
<span id="cb33-594"><a href="#cb33-594" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-595"><a href="#cb33-595" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)</span>
<span id="cb33-596"><a href="#cb33-596" aria-hidden="true" tabindex="-1"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)</span>
<span id="cb33-597"><a href="#cb33-597" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-598"><a href="#cb33-598" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pe</span>
<span id="cb33-599"><a href="#cb33-599" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-600"><a href="#cb33-600" aria-hidden="true" tabindex="-1"></a>    seq_len, embed_dim <span class="op">=</span> <span class="dv">100</span>, <span class="dv">256</span></span>
<span id="cb33-601"><a href="#cb33-601" aria-hidden="true" tabindex="-1"></a>    pos_encoding_1d <span class="op">=</span> sinusoidal_positional_encoding(seq_len, embed_dim)</span>
<span id="cb33-602"><a href="#cb33-602" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-603"><a href="#cb33-603" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"1D positional encoding shape: </span><span class="sc">{</span>pos_encoding_1d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-604"><a href="#cb33-604" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-605"><a href="#cb33-605" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize positional encoding</span></span>
<span id="cb33-606"><a href="#cb33-606" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb33-607"><a href="#cb33-607" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb33-608"><a href="#cb33-608" aria-hidden="true" tabindex="-1"></a>    plt.imshow(pos_encoding_1d[:<span class="dv">50</span>, :<span class="dv">50</span>].T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb33-609"><a href="#cb33-609" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'1D Positional Encoding (LLM)'</span>)</span>
<span id="cb33-610"><a href="#cb33-610" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Sequence Position'</span>)</span>
<span id="cb33-611"><a href="#cb33-611" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Embedding Dimension'</span>)</span>
<span id="cb33-612"><a href="#cb33-612" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb33-613"><a href="#cb33-613" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-614"><a href="#cb33-614" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">2D Positional Encoding (GFM):"</span>)</span>
<span id="cb33-615"><a href="#cb33-615" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-616"><a href="#cb33-616" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> create_2d_positional_encoding(height, width, embed_dim):</span>
<span id="cb33-617"><a href="#cb33-617" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Create 2D positional encodings for spatial data"""</span></span>
<span id="cb33-618"><a href="#cb33-618" aria-hidden="true" tabindex="-1"></a>        pe <span class="op">=</span> torch.zeros(height, width, embed_dim)</span>
<span id="cb33-619"><a href="#cb33-619" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-620"><a href="#cb33-620" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create position grids</span></span>
<span id="cb33-621"><a href="#cb33-621" aria-hidden="true" tabindex="-1"></a>        y_pos <span class="op">=</span> torch.arange(height).unsqueeze(<span class="dv">1</span>).repeat(<span class="dv">1</span>, width).<span class="bu">float</span>()</span>
<span id="cb33-622"><a href="#cb33-622" aria-hidden="true" tabindex="-1"></a>        x_pos <span class="op">=</span> torch.arange(width).unsqueeze(<span class="dv">0</span>).repeat(height, <span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb33-623"><a href="#cb33-623" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-624"><a href="#cb33-624" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simplified 2D positional encoding</span></span>
<span id="cb33-625"><a href="#cb33-625" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use sinusoidal encoding for y positions in first half of embedding</span></span>
<span id="cb33-626"><a href="#cb33-626" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embed_dim <span class="op">//</span> <span class="dv">2</span>):</span>
<span id="cb33-627"><a href="#cb33-627" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb33-628"><a href="#cb33-628" aria-hidden="true" tabindex="-1"></a>                pe[:, :, i] <span class="op">=</span> torch.sin(y_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (i <span class="op">/</span> embed_dim)))</span>
<span id="cb33-629"><a href="#cb33-629" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb33-630"><a href="#cb33-630" aria-hidden="true" tabindex="-1"></a>                pe[:, :, i] <span class="op">=</span> torch.cos(y_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (i <span class="op">/</span> embed_dim)))</span>
<span id="cb33-631"><a href="#cb33-631" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-632"><a href="#cb33-632" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use sinusoidal encoding for x positions in second half of embedding</span></span>
<span id="cb33-633"><a href="#cb33-633" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embed_dim <span class="op">//</span> <span class="dv">2</span>, embed_dim):</span>
<span id="cb33-634"><a href="#cb33-634" aria-hidden="true" tabindex="-1"></a>            j <span class="op">=</span> i <span class="op">-</span> embed_dim <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb33-635"><a href="#cb33-635" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> j <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb33-636"><a href="#cb33-636" aria-hidden="true" tabindex="-1"></a>                pe[:, :, i] <span class="op">=</span> torch.sin(x_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (j <span class="op">/</span> embed_dim)))</span>
<span id="cb33-637"><a href="#cb33-637" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb33-638"><a href="#cb33-638" aria-hidden="true" tabindex="-1"></a>                pe[:, :, i] <span class="op">=</span> torch.cos(x_pos <span class="op">/</span> (<span class="dv">10000</span> <span class="op">**</span> (j <span class="op">/</span> embed_dim)))</span>
<span id="cb33-639"><a href="#cb33-639" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-640"><a href="#cb33-640" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pe</span>
<span id="cb33-641"><a href="#cb33-641" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-642"><a href="#cb33-642" aria-hidden="true" tabindex="-1"></a>    height, width <span class="op">=</span> <span class="dv">8</span>, <span class="dv">8</span></span>
<span id="cb33-643"><a href="#cb33-643" aria-hidden="true" tabindex="-1"></a>    pos_encoding_2d <span class="op">=</span> create_2d_positional_encoding(height, width, embed_dim)</span>
<span id="cb33-644"><a href="#cb33-644" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-645"><a href="#cb33-645" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"2D positional encoding shape: </span><span class="sc">{</span>pos_encoding_2d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-646"><a href="#cb33-646" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-647"><a href="#cb33-647" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize 2D positional encoding</span></span>
<span id="cb33-648"><a href="#cb33-648" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb33-649"><a href="#cb33-649" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Show first 64 dimensions as an 8x8 grid</span></span>
<span id="cb33-650"><a href="#cb33-650" aria-hidden="true" tabindex="-1"></a>    pos_2d_viz <span class="op">=</span> pos_encoding_2d[:, :, :<span class="dv">64</span>].mean(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb33-651"><a href="#cb33-651" aria-hidden="true" tabindex="-1"></a>    plt.imshow(pos_2d_viz, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'equal'</span>)</span>
<span id="cb33-652"><a href="#cb33-652" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'2D Positional Encoding (GFM)'</span>)</span>
<span id="cb33-653"><a href="#cb33-653" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Width'</span>)</span>
<span id="cb33-654"><a href="#cb33-654" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Height'</span>)</span>
<span id="cb33-655"><a href="#cb33-655" aria-hidden="true" tabindex="-1"></a>    plt.colorbar()</span>
<span id="cb33-656"><a href="#cb33-656" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-657"><a href="#cb33-657" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb33-658"><a href="#cb33-658" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb33-659"><a href="#cb33-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-660"><a href="#cb33-660" aria-hidden="true" tabindex="-1"></a>compare_positional_encodings()</span>
<span id="cb33-661"><a href="#cb33-661" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-662"><a href="#cb33-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-663"><a href="#cb33-663" aria-hidden="true" tabindex="-1"></a><span class="fu">## Course Mapping and Applications</span></span>
<span id="cb33-664"><a href="#cb33-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-665"><a href="#cb33-665" aria-hidden="true" tabindex="-1"></a><span class="fu">### Weekly Course Structure</span></span>
<span id="cb33-668"><a href="#cb33-668" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-669"><a href="#cb33-669" aria-hidden="true" tabindex="-1"></a>course_mapping <span class="op">=</span> {</span>
<span id="cb33-670"><a href="#cb33-670" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Weeks 1-3"</span>: {</span>
<span id="cb33-671"><a href="#cb33-671" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Focus"</span>: <span class="st">"Data → Attention → Architecture"</span>,</span>
<span id="cb33-672"><a href="#cb33-672" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLM Topics"</span>: [<span class="st">"Text preprocessing"</span>, <span class="st">"Tokenization"</span>, <span class="st">"Transformer blocks"</span>],</span>
<span id="cb33-673"><a href="#cb33-673" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFM Topics"</span>: [<span class="st">"Satellite data"</span>, <span class="st">"Patch embedding"</span>, <span class="st">"Spatial attention"</span>]</span>
<span id="cb33-674"><a href="#cb33-674" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb33-675"><a href="#cb33-675" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Weeks 4-7"</span>: {</span>
<span id="cb33-676"><a href="#cb33-676" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Focus"</span>: <span class="st">"Pretraining → Training → Evaluation → Integration"</span>,</span>
<span id="cb33-677"><a href="#cb33-677" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLM Topics"</span>: [<span class="st">"Language modeling"</span>, <span class="st">"Training loops"</span>, <span class="st">"Perplexity"</span>],</span>
<span id="cb33-678"><a href="#cb33-678" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFM Topics"</span>: [<span class="st">"Masked reconstruction"</span>, <span class="st">"Cloud handling"</span>, <span class="st">"Linear probing"</span>]</span>
<span id="cb33-679"><a href="#cb33-679" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb33-680"><a href="#cb33-680" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Weeks 8-10"</span>: {</span>
<span id="cb33-681"><a href="#cb33-681" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Focus"</span>: <span class="st">"Finetuning → Deployment → Synthesis"</span>,</span>
<span id="cb33-682"><a href="#cb33-682" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLM Topics"</span>: [<span class="st">"Instruction tuning"</span>, <span class="st">"PEFT"</span>, <span class="st">"API deployment"</span>],</span>
<span id="cb33-683"><a href="#cb33-683" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFM Topics"</span>: [<span class="st">"Task heads"</span>, <span class="st">"Few-shot learning"</span>, <span class="st">"Geospatial inference"</span>]</span>
<span id="cb33-684"><a href="#cb33-684" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-685"><a href="#cb33-685" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-686"><a href="#cb33-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-687"><a href="#cb33-687" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Course Structure Mapping:"</span>)</span>
<span id="cb33-688"><a href="#cb33-688" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">50</span>)</span>
<span id="cb33-689"><a href="#cb33-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-690"><a href="#cb33-690" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> period, details <span class="kw">in</span> course_mapping.items():</span>
<span id="cb33-691"><a href="#cb33-691" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>period<span class="sc">}</span><span class="ss"> - </span><span class="sc">{</span>details[<span class="st">'Focus'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-692"><a href="#cb33-692" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  LLM Focus: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(details[<span class="st">'LLM Topics'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-693"><a href="#cb33-693" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  GFM Focus: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(details[<span class="st">'GFM Topics'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-694"><a href="#cb33-694" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-695"><a href="#cb33-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-696"><a href="#cb33-696" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Takeaways</span></span>
<span id="cb33-699"><a href="#cb33-699" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-700"><a href="#cb33-700" aria-hidden="true" tabindex="-1"></a>key_differences <span class="op">=</span> {</span>
<span id="cb33-701"><a href="#cb33-701" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Data Nature"</span>: {</span>
<span id="cb33-702"><a href="#cb33-702" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"Discrete text tokens with semantic consistency"</span>,</span>
<span id="cb33-703"><a href="#cb33-703" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"Continuous pixel values requiring contextual interpretation"</span></span>
<span id="cb33-704"><a href="#cb33-704" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb33-705"><a href="#cb33-705" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Tokenization"</span>: {</span>
<span id="cb33-706"><a href="#cb33-706" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"Vocabulary-based discrete mapping"</span>,</span>
<span id="cb33-707"><a href="#cb33-707" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"Patch-based continuous projection"</span></span>
<span id="cb33-708"><a href="#cb33-708" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb33-709"><a href="#cb33-709" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Positional Info"</span>: {</span>
<span id="cb33-710"><a href="#cb33-710" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"1D sequence positions"</span>,</span>
<span id="cb33-711"><a href="#cb33-711" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"2D spatial + temporal positions"</span></span>
<span id="cb33-712"><a href="#cb33-712" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb33-713"><a href="#cb33-713" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Training Objective"</span>: {</span>
<span id="cb33-714"><a href="#cb33-714" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"Next token prediction or masked language modeling"</span>,</span>
<span id="cb33-715"><a href="#cb33-715" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"Masked patch reconstruction or contrastive learning"</span></span>
<span id="cb33-716"><a href="#cb33-716" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb33-717"><a href="#cb33-717" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Evaluation"</span>: {</span>
<span id="cb33-718"><a href="#cb33-718" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"Perplexity and downstream language tasks"</span>,</span>
<span id="cb33-719"><a href="#cb33-719" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"Reconstruction quality and spatial generalization"</span></span>
<span id="cb33-720"><a href="#cb33-720" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb33-721"><a href="#cb33-721" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deployment"</span>: {</span>
<span id="cb33-722"><a href="#cb33-722" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLMs"</span>: <span class="st">"Text generation with streaming and caching"</span>,</span>
<span id="cb33-723"><a href="#cb33-723" aria-hidden="true" tabindex="-1"></a>        <span class="st">"GFMs"</span>: <span class="st">"Spatial inference with tiling and batch processing"</span></span>
<span id="cb33-724"><a href="#cb33-724" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb33-725"><a href="#cb33-725" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-726"><a href="#cb33-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-727"><a href="#cb33-727" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Key Architectural Differences:"</span>)</span>
<span id="cb33-728"><a href="#cb33-728" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb33-729"><a href="#cb33-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-730"><a href="#cb33-730" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> aspect, comparison <span class="kw">in</span> key_differences.items():</span>
<span id="cb33-731"><a href="#cb33-731" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>aspect<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb33-732"><a href="#cb33-732" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  LLMs: </span><span class="sc">{</span>comparison[<span class="st">'LLMs'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-733"><a href="#cb33-733" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  GFMs: </span><span class="sc">{</span>comparison[<span class="st">'GFMs'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-734"><a href="#cb33-734" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-735"><a href="#cb33-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-736"><a href="#cb33-736" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Reading and References</span></span>
<span id="cb33-737"><a href="#cb33-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-738"><a href="#cb33-738" aria-hidden="true" tabindex="-1"></a><span class="fu">### Essential Papers</span></span>
<span id="cb33-741"><a href="#cb33-741" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb33-742"><a href="#cb33-742" aria-hidden="true" tabindex="-1"></a>references <span class="op">=</span> {</span>
<span id="cb33-743"><a href="#cb33-743" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Foundation Papers"</span>: [</span>
<span id="cb33-744"><a href="#cb33-744" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Attention Is All You Need (Vaswani et al., 2017)"</span>,</span>
<span id="cb33-745"><a href="#cb33-745" aria-hidden="true" tabindex="-1"></a>        <span class="st">"BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)"</span>,</span>
<span id="cb33-746"><a href="#cb33-746" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Language Models are Few-Shot Learners (Brown et al., 2020)"</span></span>
<span id="cb33-747"><a href="#cb33-747" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb33-748"><a href="#cb33-748" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Vision Transformers"</span>: [</span>
<span id="cb33-749"><a href="#cb33-749" aria-hidden="true" tabindex="-1"></a>        <span class="st">"An Image is Worth 16x16 Words (Dosovitskiy et al., 2020)"</span>,</span>
<span id="cb33-750"><a href="#cb33-750" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Masked Autoencoders Are Scalable Vision Learners (He et al., 2021)"</span>,</span>
<span id="cb33-751"><a href="#cb33-751" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Scaling Vision Transformers (Zhai et al., 2021)"</span></span>
<span id="cb33-752"><a href="#cb33-752" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb33-753"><a href="#cb33-753" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Geospatial Foundation Models"</span>: [</span>
<span id="cb33-754"><a href="#cb33-754" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Prithvi: A Foundation Model for Earth Observation (IBM/NASA, 2023)"</span>,</span>
<span id="cb33-755"><a href="#cb33-755" aria-hidden="true" tabindex="-1"></a>        <span class="st">"SatMAE: Masked Autoencoders for Satellite Imagery (Cong et al., 2022)"</span>,</span>
<span id="cb33-756"><a href="#cb33-756" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Clay: A Foundation Model for Earth Observation (Made with Clay, 2024)"</span></span>
<span id="cb33-757"><a href="#cb33-757" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb33-758"><a href="#cb33-758" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb33-759"><a href="#cb33-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-760"><a href="#cb33-760" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Essential References:"</span>)</span>
<span id="cb33-761"><a href="#cb33-761" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">40</span>)</span>
<span id="cb33-762"><a href="#cb33-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-763"><a href="#cb33-763" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> category, papers <span class="kw">in</span> references.items():</span>
<span id="cb33-764"><a href="#cb33-764" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>category<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb33-765"><a href="#cb33-765" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> paper <span class="kw">in</span> papers:</span>
<span id="cb33-766"><a href="#cb33-766" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  • </span><span class="sc">{</span>paper<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb33-767"><a href="#cb33-767" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb33-768"><a href="#cb33-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-769"><a href="#cb33-769" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb33-770"><a href="#cb33-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-771"><a href="#cb33-771" aria-hidden="true" tabindex="-1"></a>Key concepts for foundation model architectures:</span>
<span id="cb33-772"><a href="#cb33-772" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Historical Evolution**: From symbolic AI to transformer-based foundation models</span>
<span id="cb33-773"><a href="#cb33-773" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Architecture Comparison**: LLMs use discrete tokenization, GFMs use continuous patch embeddings</span>
<span id="cb33-774"><a href="#cb33-774" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Development Pipeline**: 9-step process with domain-specific adaptations</span>
<span id="cb33-775"><a href="#cb33-775" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scaling Trends**: LLMs scale in parameters/context, GFMs scale in spectral/spatial/temporal dimensions</span>
<span id="cb33-776"><a href="#cb33-776" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Training Objectives**: Next-token prediction vs. masked patch reconstruction</span>
<span id="cb33-777"><a href="#cb33-777" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deployment Considerations**: Text streaming vs. spatial tiling and batch inference</span>
<span id="cb33-778"><a href="#cb33-778" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Course Integration**: Weekly progression from data processing to deployment</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="course-materials/images/geog-logo.png" class="img-fluid figure-img" width="250"></p>
<figcaption>The Department of Geography logo</figcaption>
</figure>
</div>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This website is built with <a href="https://github.com/environmental-data-science/geog-288kc-geospatial-foundation-models"><i class="fa-brands fa-github" title="the github octocat logo" aria-label="github"></i></a> and <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>