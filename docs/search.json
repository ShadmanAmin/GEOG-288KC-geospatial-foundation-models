[
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Building Geospatial Foundation Models",
    "section": "Course Description",
    "text": "Course Description\nThis project-driven seminar teaches students to build geospatial foundation models (GFMs) from scratch. Students implement every layer of the pipeline‚Äîfrom data pipelines and tokenization through attention mechanisms, full architectures, pretraining, evaluation, and deployment‚Äîculminating in a working end-to-end GFM tailored to a chosen geospatial application.\nBy the end of the course, students will be able to:\n\nDesign and implement geospatial data pipelines for multi-spectral, spatial, and temporal data\nBuild attention mechanisms and assemble transformer-based architectures for geospatial inputs\nPretrain using masked autoencoding and evaluate learned representations\nFine-tune models for specific Earth observation tasks\nDeploy models via APIs and interactive interfaces with honest performance analysis"
  },
  {
    "objectID": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "href": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "title": "Building Geospatial Foundation Models",
    "section": "Getting Started with the UCSB AI Sandbox",
    "text": "Getting Started with the UCSB AI Sandbox\nHere are detailed instructions for setting up your environment on the UCSB AI Sandbox, including foundation model installation and GPU optimization."
  },
  {
    "objectID": "index.html#course-structure-3-stages-10-weeks",
    "href": "index.html#course-structure-3-stages-10-weeks",
    "title": "Building Geospatial Foundation Models",
    "section": "Course Structure: 3 Stages, 10 Weeks",
    "text": "Course Structure: 3 Stages, 10 Weeks\n\n\n\n\n\nflowchart TD\n    subgraph Stage1 [\"üèóÔ∏è Stage 1: Build GFM Architecture\"]\n        direction LR\n        W1[\"üìä&lt;br/&gt;Week 1&lt;br/&gt;Data Foundations&lt;br/&gt;Pipelines & Tokenization\"] --&gt; W2[\"üß†&lt;br/&gt;Week 2&lt;br/&gt;Attention Mechanisms&lt;br/&gt;Spatial-Temporal Focus\"]\n        W2 --&gt; W3[\"üèõÔ∏è&lt;br/&gt;Week 3&lt;br/&gt;Complete Architecture&lt;br/&gt;Vision Transformer\"]\n    end\n    \n    subgraph Stage2 [\"üöÄ Stage 2: Train Foundation Model\"]\n        direction LR\n        W4[\"üé≠&lt;br/&gt;Week 4&lt;br/&gt;Pretraining&lt;br/&gt;Masked Autoencoder\"] --&gt; W5[\"‚ö°&lt;br/&gt;Week 5&lt;br/&gt;Training Optimization&lt;br/&gt;Stability & Efficiency\"]\n        W5 --&gt; W6[\"üìà&lt;br/&gt;Week 6&lt;br/&gt;Evaluation & Analysis&lt;br/&gt;Embeddings & Probing\"]\n        W6 --&gt; W7[\"üîó&lt;br/&gt;Week 7&lt;br/&gt;Model Integration&lt;br/&gt;Prithvi, SatMAE\"]\n    end\n    \n    subgraph Stage3 [\"üéØ Stage 3: Apply & Deploy\"]\n        direction LR\n        W8[\"üéØ&lt;br/&gt;Week 8&lt;br/&gt;Fine-tuning&lt;br/&gt;Task-Specific Training\"] --&gt; W9[\"üöÄ&lt;br/&gt;Week 9&lt;br/&gt;Deployment&lt;br/&gt;APIs & Interfaces\"]\n        W9 --&gt; W10[\"üé§&lt;br/&gt;Week 10&lt;br/&gt;Presentations&lt;br/&gt;Project Synthesis\"]\n    end\n    \n    Stage1 --&gt; Stage2\n    Stage2 --&gt; Stage3\n    \n    style Stage1 fill:#e3f2fd\n    style Stage2 fill:#fff3e0  \n    style Stage3 fill:#e8f5e8\n    style W1 fill:#bbdefb\n    style W4 fill:#ffe0b2\n    style W8 fill:#c8e6c8\n\n\n\n\n\n\n\nüèóÔ∏è Stage 1: Build GFM Architecture (Weeks 1-3)\n\nWeek 1: Geospatial Data Foundations (data pipelines, tokenization, loaders)\nWeek 2: Spatial-Temporal Attention Mechanisms (from-scratch implementation)\nWeek 3: Complete GFM Architecture (Vision Transformer for geospatial)\n\n\n\nüöÄ Stage 2: Train a Foundation Model (Weeks 4-7)\n\nWeek 4: Pretraining Implementation (masked autoencoder)\nWeek 5: Training Loop Optimization (stability, efficiency, mixed precision)\nWeek 6: Model Evaluation & Analysis (embeddings, probing, reconstructions)\nWeek 7: Integration with Existing Models (Prithvi, SatMAE)\n\n\n\nüéØ Stage 3: Apply & Deploy (Weeks 8-10)\n\nWeek 8: Task-Specific Fine-tuning (efficient strategies, few-shot)\nWeek 9: Model Implementation & Deployment (APIs, UI, benchmarking)\nWeek 10: Project Presentations & Synthesis\n\nQuick links:\n\nWeekly materials: see navbar ‚Üí üóìÔ∏è weekly materials\nInteractive sessions: see navbar ‚Üí üíª interactive sessions"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Building Geospatial Foundation Models",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\nKelly Caylor\nEmail: caylor@ucsb.edu\nLearn more: Bren profile\n\n\n\n\nTA\n\n\n\n\n\n\n\nAnna Boser\nEmail: anaboser@ucsb.edu\nLearn more: Bren profile"
  },
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#geospatial-data-fundamentals",
    "href": "cheatsheets.html#geospatial-data-fundamentals",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#pytorch-for-geospatial-ai",
    "href": "cheatsheets.html#pytorch-for-geospatial-ai",
    "title": "Cheatsheets",
    "section": "üî• PyTorch for Geospatial AI",
    "text": "üî• PyTorch for Geospatial AI\n\nPyTorch Tensors & GPU Operations\nTorchGeo Datasets & Transforms\nData Loading for Satellite Imagery"
  },
  {
    "objectID": "cheatsheets.html#foundation-models-huggingface",
    "href": "cheatsheets.html#foundation-models-huggingface",
    "title": "Cheatsheets",
    "section": "ü§ó Foundation Models & HuggingFace",
    "text": "ü§ó Foundation Models & HuggingFace\n\nLoading Pre-trained Models\nModel Inference & Feature Extraction\nFine-tuning Strategies"
  },
  {
    "objectID": "cheatsheets.html#visualization-analysis",
    "href": "cheatsheets.html#visualization-analysis",
    "title": "Cheatsheets",
    "section": "üìä Visualization & Analysis",
    "text": "üìä Visualization & Analysis\n\nPlotting Satellite Imagery\nInteractive Maps with Folium\nGeospatial Plotting with Matplotlib"
  },
  {
    "objectID": "course-materials/04-pretraining-implementation.html",
    "href": "course-materials/04-pretraining-implementation.html",
    "title": "Week 4 Interactive Session: Pretraining Implementation",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n4\nStage 2: Train Foundation Model\nMAE Pretraining\nmodels/gfm_mae.py; modules/losses/mae_loss.py\ntorch masking, numpy\nMasking + reconstruction; loss decreases on toy batch\n\n\n\n\n\n\nImplement MAE wrapper with masking strategy\nBuild masked reconstruction loss; verify gradients\nOverfit a toy batch to confirm learning\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/04-pretraining-implementation.html#course-roadmap-mapping",
    "href": "course-materials/04-pretraining-implementation.html#course-roadmap-mapping",
    "title": "Week 4 Interactive Session: Pretraining Implementation",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n4\nStage 2: Train Foundation Model\nMAE Pretraining\nmodels/gfm_mae.py; modules/losses/mae_loss.py\ntorch masking, numpy\nMasking + reconstruction; loss decreases on toy batch\n\n\n\n\n\n\nImplement MAE wrapper with masking strategy\nBuild masked reconstruction loss; verify gradients\nOverfit a toy batch to confirm learning\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/06-model-evaluation-analysis.html",
    "href": "course-materials/06-model-evaluation-analysis.html",
    "title": "Week 6 Interactive Session: Model Evaluation & Analysis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n6\nStage 2: Train Foundation Model\nEvaluation & Analysis\nevaluation/visualization.py; (optional) evaluation/metrics.py\nmatplotlib; torchmetrics optional\nRecon visuals; track validation loss/PSNR\n\n\n\n\n\n\nImplement reconstruction visualization utilities\nAdd a simple metric (e.g., PSNR) and validation loop hooks\nInterpret embeddings or reconstructions qualitatively\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/06-model-evaluation-analysis.html#course-roadmap-mapping",
    "href": "course-materials/06-model-evaluation-analysis.html#course-roadmap-mapping",
    "title": "Week 6 Interactive Session: Model Evaluation & Analysis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n6\nStage 2: Train Foundation Model\nEvaluation & Analysis\nevaluation/visualization.py; (optional) evaluation/metrics.py\nmatplotlib; torchmetrics optional\nRecon visuals; track validation loss/PSNR\n\n\n\n\n\n\nImplement reconstruction visualization utilities\nAdd a simple metric (e.g., PSNR) and validation loop hooks\nInterpret embeddings or reconstructions qualitatively\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/08-task-specific-finetuning.html",
    "href": "course-materials/08-task-specific-finetuning.html",
    "title": "Week 8 Interactive Session: Task-Specific Fine-tuning",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n8\nStage 3: Apply & Deploy\nTask Fine-tuning\ntasks/{classification.py|segmentation.py} (light heads)\ntorch.nn.CrossEntropyLoss; timm optional\nHead swap on frozen encoder; small dataset demo\n\n\n\n\n\n\nImplement a simple classifier/segmentation head\nFine-tune with frozen encoder; evaluate on a tiny dataset\nDiscuss efficient strategies (LoRA/prompting as concepts)\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/08-task-specific-finetuning.html#course-roadmap-mapping",
    "href": "course-materials/08-task-specific-finetuning.html#course-roadmap-mapping",
    "title": "Week 8 Interactive Session: Task-Specific Fine-tuning",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n8\nStage 3: Apply & Deploy\nTask Fine-tuning\ntasks/{classification.py|segmentation.py} (light heads)\ntorch.nn.CrossEntropyLoss; timm optional\nHead swap on frozen encoder; small dataset demo\n\n\n\n\n\n\nImplement a simple classifier/segmentation head\nFine-tune with frozen encoder; evaluate on a tiny dataset\nDiscuss efficient strategies (LoRA/prompting as concepts)\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/00-introduction-to-deeplearning-architecture.html",
    "href": "course-materials/00-introduction-to-deeplearning-architecture.html",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/00-introduction-to-deeplearning-architecture.html#week-0-overview",
    "href": "course-materials/00-introduction-to-deeplearning-architecture.html#week-0-overview",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/00-introduction-to-deeplearning-architecture.html#build-your-own-gfm-architecture-cheatsheet",
    "href": "course-materials/00-introduction-to-deeplearning-architecture.html#build-your-own-gfm-architecture-cheatsheet",
    "title": "Week 0: Getting Started",
    "section": "Build-your-own GFM: Architecture Cheatsheet",
    "text": "Build-your-own GFM: Architecture Cheatsheet\nThis is a quick-reference for how our Geospatial Foundation Model (GFM) is organized, what each part does, and where to look during the course. We implement simple, readable components first, then show how to swap in optimized library counterparts as needed.\n\nRoadmap at a glance\n\ngraph TD\n  A[Week 1: Data Foundations] --&gt; B[Week 2: Attention & Blocks]\n  B --&gt; C[Week 3: Full Encoder]\n  C --&gt; D[Week 4: MAE Pretraining]\n  D --&gt; E[Week 5: Training Loop]\n  E --&gt; F[Week 6: Eval & Viz]\n  F --&gt; G[Week 7+: Interop & Scale]\n\n\nMinimal structure you‚Äôll use\ngeogfm/\n  core/\n    config.py                # Minimal typed configs for model, data, training\n  data/\n    loaders.py               # build_dataloader(...)\n    datasets/\n      stac_dataset.py        # Simple STAC-backed dataset\n    transforms/\n      normalization.py       # Per-channel normalization\n      patchify.py            # Extract fixed-size patches\n  modules/\n    attention/\n      multihead_attention.py     # Standard MHA (from scratch)\n    embeddings/\n      patch_embedding.py         # Conv patch embedding\n      positional_encoding.py     # Simple positional encoding\n    blocks/\n      transformer_block.py       # PreNorm block (MHA + MLP)\n    heads/\n      reconstruction_head.py     # Lightweight decoder/readout\n    losses/\n      mae_loss.py                # Masked reconstruction loss\n  models/\n    gfm_vit.py                   # GeoViT-style encoder\n    gfm_mae.py                   # MAE wrapper (masking + encoder + head)\n  training/\n    optimizer.py                 # AdamW builder\n    loop.py                      # fit/train_step/eval_step with basic checkpointing\n  evaluation/\n    visualization.py             # Visualize inputs vs reconstructions\n\n# Outside the package (repo root)\nconfigs/   # Small YAML/JSON run configs\ntests/     # Unit tests\ndata/      # Datasets, splits, stats, build scripts\n\n\nWhat each part does (one-liners)\n\ncore/config.py: Typed configs for model/data/training; keeps parameters organized.\ndata/datasets/stac_dataset.py: Reads imagery + metadata (e.g., STAC), returns tensors.\ndata/transforms/normalization.py: Normalizes channels using precomputed stats.\ndata/transforms/patchify.py: Turns large images into uniform patches for ViT.\ndata/loaders.py: Builds PyTorch DataLoaders for train/val.\nmodules/embeddings/patch_embedding.py: Projects image patches into token vectors.\nmodules/embeddings/positional_encoding.py: Adds position info to tokens.\nmodules/attention/multihead_attention.py: Lets tokens attend to each other.\nmodules/blocks/transformer_block.py: Core transformer layer (attention + MLP).\nmodules/heads/reconstruction_head.py: Reconstructs pixels from encoded tokens.\nmodules/losses/mae_loss.py: Computes masked reconstruction loss for MAE.\nmodels/gfm_vit.py: Assembles the encoder backbone from blocks.\nmodels/gfm_mae.py: Wraps encoder with masking + reconstruction for pretraining.\ntraining/optimizer.py: Creates AdamW with common defaults.\ntraining/loop.py: Runs epochs, backprop, validation, and simple checkpoints.\nevaluation/visualization.py: Plots sample inputs and reconstructions.\n\n\n\nFrom-scratch vs library-backed\n\nUse PyTorch for Dataset/DataLoader, AdamW, schedulers, AMP, and checkpointing.\nBuild core blocks from scratch first: PatchEmbedding, MHA, TransformerBlock, MAE loss/head.\nLater, swap in optimized options: torch.nn.MultiheadAttention, timm ViT blocks, FlashAttention, TorchGeo datasets, torchmetrics.\n\n\n\nQuick start (conceptual)\nfrom geogfm.core.config import ModelConfig, DataConfig, TrainConfig\nfrom geogfm.models.gfm_vit import GeoViTBackbone\nfrom geogfm.models.gfm_mae import MaskedAutoencoder\nfrom geogfm.data.loaders import build_dataloader\nfrom geogfm.training.loop import fit\n\nmodel_cfg = ModelConfig(architecture=\"gfm_vit\", embed_dim=768, depth=12, image_size=224)\ndata_cfg = DataConfig(dataset=\"stac\", patch_size=16, num_workers=8)\ntrain_cfg = TrainConfig(epochs=1, batch_size=8, optimizer={\"name\": \"adamw\", \"lr\": 2e-4})\n\nencoder = GeoViTBackbone(model_cfg)\nmodel = MaskedAutoencoder(model_cfg, encoder)\ntrain_dl, val_dl = build_dataloader(data_cfg)\nfit(model, (train_dl, val_dl), train_cfg)\nWhat to notice: - The encoder and MAE wrapper are separate, so we can reuse the encoder for other tasks later. - Data transforms (normalize/patchify) are decoupled from the model and driven by config.\n\n\nMVP vs later phases\n\nMVP (Weeks 1‚Äì6): files shown above; single-node training; basic logging and visualization.\nLater (Weeks 7‚Äì10): interop with existing models (e.g., Prithvi), task heads (classification/segmentation), inference tiling, Hub integration.\n\n\n\nComprehensive course roadmap (stages, files, libraries)\nThis table maps each week to the broader stages (see the course index), the key geogfm files you‚Äôll touch, and the primary deep learning tools you‚Äôll rely on.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n1\nStage 1: Build GFM Architecture\nData Foundations\ncore/config.py; data/datasets/stac_dataset.py; data/transforms/{normalization.py, patchify.py}; data/loaders.py\ntorch.utils.data.Dataset/DataLoader, rasterio, numpy\nConfig-driven dataloaders that yield normalized patches\n\n\n2\nStage 1\nAttention & Blocks\nmodules/embeddings/{patch_embedding.py, positional_encoding.py}; modules/attention/multihead_attention.py; modules/blocks/transformer_block.py\ntorch.nn (compare with torch.nn.MultiheadAttention)\nBlocks run forward with stable shapes; unit tests green\n\n\n3\nStage 1\nComplete Architecture\nmodels/gfm_vit.py; modules/heads/reconstruction_head.py\ntorch.nn (timm as reference)\nEncoder assembled; end-to-end forward on dummy input\n\n\n4\nStage 2: Train Foundation Model\nMAE Pretraining\nmodels/gfm_mae.py; modules/losses/mae_loss.py\ntorch masking utilities, numpy\nMasking + reconstruction; loss decreases on toy batch\n\n\n5\nStage 2\nTraining Optimization\ntraining/optimizer.py; training/loop.py\ntorch.optim.AdamW; schedulers, AMP optional\nSingle-epoch run; basic checkpoint save/restore\n\n\n6\nStage 2\nEvaluation & Analysis\nevaluation/visualization.py; (optional) evaluation/metrics.py\nmatplotlib; torchmetrics optional\nRecon visuals; track validation loss/PSNR\n\n\n7\nStage 2\nIntegration w/ Pretrained\n(light) core/registry.py; interoperability/huggingface.py stubs\nhuggingface_hub, transformers optional\nShow mapping to Prithvi structure; plan switch\n\n\n8\nStage 3: Apply & Deploy\nTask Fine-tuning\ntasks/{classification.py|segmentation.py} (light heads)\ntorch.nn.CrossEntropyLoss; timm optional\nHead swap on frozen encoder; small dataset demo\n\n\n9\nStage 3\nDeployment & Inference\ninference/{tiling.py, sliding_window.py}\nnumpy, rasterio windows\nSliding-window inference on a small scene\n\n\n10\nStage 3\nPresentations & Synthesis\nProject deliverables (no new geogfm code required)\n‚Äî\nPresent MVP builds, analysis, transition plan"
  },
  {
    "objectID": "course-materials/07-integration-with-existing-models.html",
    "href": "course-materials/07-integration-with-existing-models.html",
    "title": "Week 7 Interactive Session: Integration with Existing Models",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n7\nStage 2: Train Foundation Model\nIntegration w/ Pretrained\n(light) core/registry.py; interoperability/huggingface.py stubs\nhuggingface_hub, transformers optional\nShow mapping to Prithvi structure; plan switch\n\n\n\n\n\n\nCreate a light registry for model variants\nAdd basic HF Hub glue to load model configs/weights\nCompare MVP structure to Prithvi and define switch points\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/07-integration-with-existing-models.html#course-roadmap-mapping",
    "href": "course-materials/07-integration-with-existing-models.html#course-roadmap-mapping",
    "title": "Week 7 Interactive Session: Integration with Existing Models",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n7\nStage 2: Train Foundation Model\nIntegration w/ Pretrained\n(light) core/registry.py; interoperability/huggingface.py stubs\nhuggingface_hub, transformers optional\nShow mapping to Prithvi structure; plan switch\n\n\n\n\n\n\nCreate a light registry for model variants\nAdd basic HF Hub glue to load model configs/weights\nCompare MVP structure to Prithvi and define switch points\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/10-project-presentations-synthesis.html",
    "href": "course-materials/10-project-presentations-synthesis.html",
    "title": "Week 10 Interactive Session: Project Presentations & Synthesis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n10\nStage 3: Apply & Deploy\nPresentations & Synthesis\nProject deliverables (no new geogfm code required)\n‚Äî\nPresent MVP builds, analysis, transition plan\n\n\n\n\n\n\nSynthesize architecture, training, and evaluation learnings\nPresent MVP results and insights\nOutline next steps with Prithvi/scale-up\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/10-project-presentations-synthesis.html#course-roadmap-mapping",
    "href": "course-materials/10-project-presentations-synthesis.html#course-roadmap-mapping",
    "title": "Week 10 Interactive Session: Project Presentations & Synthesis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n10\nStage 3: Apply & Deploy\nPresentations & Synthesis\nProject deliverables (no new geogfm code required)\n‚Äî\nPresent MVP builds, analysis, transition plan\n\n\n\n\n\n\nSynthesize architecture, training, and evaluation learnings\nPresent MVP results and insights\nOutline next steps with Prithvi/scale-up\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/02-spatial-temporal-attention-mechanisms.html",
    "href": "course-materials/02-spatial-temporal-attention-mechanisms.html",
    "title": "Week 2 Interactive Session: Spatial-Temporal Attention Mechanisms",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n2\nStage 1: Build GFM Architecture\nAttention & Blocks\nmodules/embeddings/{patch_embedding.py, positional_encoding.py}; modules/attention/multihead_attention.py; modules/blocks/transformer_block.py\ntorch.nn (compare with torch.nn.MultiheadAttention)\nBlocks run forward with stable shapes; unit tests green\n\n\n\n\n\n\nImplement patch/positional embeddings and MHA from scratch\nAssemble a PreNorm transformer block (MHA + MLP)\nValidate tensor shapes and simple layer tests\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/02-spatial-temporal-attention-mechanisms.html#course-roadmap-mapping",
    "href": "course-materials/02-spatial-temporal-attention-mechanisms.html#course-roadmap-mapping",
    "title": "Week 2 Interactive Session: Spatial-Temporal Attention Mechanisms",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n2\nStage 1: Build GFM Architecture\nAttention & Blocks\nmodules/embeddings/{patch_embedding.py, positional_encoding.py}; modules/attention/multihead_attention.py; modules/blocks/transformer_block.py\ntorch.nn (compare with torch.nn.MultiheadAttention)\nBlocks run forward with stable shapes; unit tests green\n\n\n\n\n\n\nImplement patch/positional embeddings and MHA from scratch\nAssemble a PreNorm transformer block (MHA + MLP)\nValidate tensor shapes and simple layer tests\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/03-complete-gfm-architecture.html",
    "href": "course-materials/03-complete-gfm-architecture.html",
    "title": "Week 3 Interactive Session: Complete GFM Architecture",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n3\nStage 1: Build GFM Architecture\nComplete Architecture\nmodels/gfm_vit.py; modules/heads/reconstruction_head.py\ntorch.nn (timm as reference)\nEncoder assembled; end-to-end forward on dummy input\n\n\n\n\n\n\nWire blocks into a GeoViT-style encoder\nAdd a simple reconstruction head\nRun end-to-end forward pass on dummy input\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/03-complete-gfm-architecture.html#course-roadmap-mapping",
    "href": "course-materials/03-complete-gfm-architecture.html#course-roadmap-mapping",
    "title": "Week 3 Interactive Session: Complete GFM Architecture",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n3\nStage 1: Build GFM Architecture\nComplete Architecture\nmodels/gfm_vit.py; modules/heads/reconstruction_head.py\ntorch.nn (timm as reference)\nEncoder assembled; end-to-end forward on dummy input\n\n\n\n\n\n\nWire blocks into a GeoViT-style encoder\nAdd a simple reconstruction head\nRun end-to-end forward pass on dummy input\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/09-model-implementation-deployment.html",
    "href": "course-materials/09-model-implementation-deployment.html",
    "title": "Week 9 Interactive Session: Model Implementation & Deployment",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n9\nStage 3: Apply & Deploy\nDeployment & Inference\ninference/{tiling.py, sliding_window.py}\nnumpy, rasterio windows\nSliding-window inference on a small scene\n\n\n\n\n\n\nImplement sliding-window/tiling inference utilities\nStitch predictions; sanity-check outputs\nOutline API/UI deployment considerations\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/09-model-implementation-deployment.html#course-roadmap-mapping",
    "href": "course-materials/09-model-implementation-deployment.html#course-roadmap-mapping",
    "title": "Week 9 Interactive Session: Model Implementation & Deployment",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n9\nStage 3: Apply & Deploy\nDeployment & Inference\ninference/{tiling.py, sliding_window.py}\nnumpy, rasterio windows\nSliding-window inference on a small scene\n\n\n\n\n\n\nImplement sliding-window/tiling inference utilities\nStitch predictions; sanity-check outputs\nOutline API/UI deployment considerations\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/05-training-loop-optimization.html",
    "href": "course-materials/05-training-loop-optimization.html",
    "title": "Week 5 Interactive Session: Training Loop Optimization",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n5\nStage 2: Train Foundation Model\nTraining Optimization\ntraining/optimizer.py; training/loop.py\ntorch.optim.AdamW; schedulers, AMP optional\nSingle-epoch run; basic checkpoint save/restore\n\n\n\n\n\n\nBuild fit, train_step, eval_step with logging\nConfigure AdamW; optionally add LR scheduler/AMP\nSave and restore a basic checkpoint\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/05-training-loop-optimization.html#course-roadmap-mapping",
    "href": "course-materials/05-training-loop-optimization.html#course-roadmap-mapping",
    "title": "Week 5 Interactive Session: Training Loop Optimization",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n5\nStage 2: Train Foundation Model\nTraining Optimization\ntraining/optimizer.py; training/loop.py\ntorch.optim.AdamW; schedulers, AMP optional\nSingle-epoch run; basic checkpoint save/restore\n\n\n\n\n\n\nBuild fit, train_step, eval_step with logging\nConfigure AdamW; optionally add LR scheduler/AMP\nSave and restore a basic checkpoint\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html",
    "href": "course-materials/01-geospatial-data-foundations.html",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "",
    "text": "Today we‚Äôre building a complete pipeline that transforms raw satellite imagery into model-ready embeddings. This mirrors how Large Language Models process text: raw text ‚Üí tokens ‚Üí embeddings. Our geospatial version: raw GeoTIFF ‚Üí patches ‚Üí embeddings."
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#introduction",
    "href": "course-materials/01-geospatial-data-foundations.html#introduction",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "",
    "text": "Today we‚Äôre building a complete pipeline that transforms raw satellite imagery into model-ready embeddings. This mirrors how Large Language Models process text: raw text ‚Üí tokens ‚Üí embeddings. Our geospatial version: raw GeoTIFF ‚Üí patches ‚Üí embeddings."
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#course-roadmap-mapping",
    "href": "course-materials/01-geospatial-data-foundations.html#course-roadmap-mapping",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Course Roadmap Mapping",
    "text": "Course Roadmap Mapping\nThis week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n1\nStage 1: Build GFM Architecture\nData Foundations\ncore/config.py; data/datasets/stac_dataset.py; data/transforms/{normalization.py, patchify.py}; data/loaders.py\ntorch.utils.data.Dataset/DataLoader, rasterio, numpy\nConfig-driven dataloaders that yield normalized patches\n\n\n\n\nWeekly goals\n\nImplement a minimal dataset, transforms, and dataloaders\nNormalize channels; extract patches deterministically\nVerify shapes/CRS/stats prints; run a tiny DataLoader"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#learning-objectives",
    "href": "course-materials/01-geospatial-data-foundations.html#learning-objectives",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy building this pipeline, you will: - Implement GeoTIFF loading and preprocessing functions - Create patch extraction with spatial metadata - Build tensor normalization and encoding functions\n- Construct a PyTorch DataLoader for model training - Connect to a simple embedding layer to verify end-to-end functionality"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#session-roadmap",
    "href": "course-materials/01-geospatial-data-foundations.html#session-roadmap",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Session Roadmap",
    "text": "Session Roadmap\n\n\n\n\n\nflowchart TD\n    A[\"Setup & GeoTIFF Loading\"] --&gt; B[\"Geo Preprocessing Functions\"]\n    B --&gt; C[\"Patch Extraction with Metadata\"] \n    C --&gt; D[\"Tensor Operations & Normalization\"]\n    D --&gt; E[\"DataLoader Construction\"]\n    E --&gt; F[\"Embedding Layer Integration\"]\n    F --&gt; G[\"End-to-End Pipeline Test\"]"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#setting-up",
    "href": "course-materials/01-geospatial-data-foundations.html#setting-up",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Setting Up",
    "text": "Setting Up\nLet‚Äôs establish our development environment and define the core constants we‚Äôll use throughout.\n\nImports and Configuration\n\n\nCode\nimport os\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport rasterio as rio\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Dict, Any\n\n# Set seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Pipeline constants\nPATCH_SIZE = 64\nSTRIDE = 32  # 50% overlap\nBATCH_SIZE = 8\nEMBEDDING_DIM = 256\n\nprint(f\"‚úì Environment setup complete\")\nprint(f\"‚úì Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\nprint(f\"‚úì Stride: {STRIDE} (overlap: {(PATCH_SIZE-STRIDE)/PATCH_SIZE*100:.0f}%)\")\n\n\n‚úì Environment setup complete\n‚úì Patch size: 64x64\n‚úì Stride: 32 (overlap: 50%)\n\n\n\n\nData Preparation\n\n\nCode\n# Set up data paths - use book/data for course sample data\nif \"__file__\" in globals():\n    # From course-materials folder, go up 2 levels to book folder, then to data\n    DATA_DIR = Path(__file__).parent.parent / \"data\"\nelse:\n    # Fallback for interactive environments - look for book folder\n    current = Path.cwd()\n    while current.name not in [\"book\", \"geoAI\"] and current.parent != current:\n        current = current.parent\n    if current.name == \"book\":\n        DATA_DIR = current / \"data\"\n    elif current.name == \"geoAI\":\n        DATA_DIR = current / \"book\" / \"data\"\n    else:\n        DATA_DIR = Path(\"data\")\n\nDATA_DIR.mkdir(exist_ok=True)\nSAMPLE_PATH = DATA_DIR / \"landcover_sample.tif\"\n\n# Verify data file exists\nif not SAMPLE_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {SAMPLE_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(f\"‚úì Data ready: {SAMPLE_PATH.name}\")\nprint(f\"‚úì File size: {SAMPLE_PATH.stat().st_size / 1024:.1f} KB\")\nprint(f\"‚úì Full path: {SAMPLE_PATH}\")\n\n\n‚úì Data ready: landcover_sample.tif\n‚úì File size: 12.6 KB\n‚úì Full path: /Users/kellycaylor/dev/geoAI/book/data/landcover_sample.tif"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#step-1-geotiff-loading-and-inspection",
    "href": "course-materials/01-geospatial-data-foundations.html#step-1-geotiff-loading-and-inspection",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 1: GeoTIFF Loading and Inspection",
    "text": "Step 1: GeoTIFF Loading and Inspection\nGoal: Build a function that loads and extracts essential information from any GeoTIFF.\n\nüõ†Ô∏è Build It: GeoTIFF Loader Function\nYour task: Complete this function to load a GeoTIFF and return both the data and metadata.\n\n\nCode\ndef load_geotiff(file_path: Path) -&gt; Tuple[np.ndarray, Dict[str, Any]]:\n    \"\"\"\n    Load a GeoTIFF and extract data + metadata.\n    \n    Returns:\n        data: (bands, height, width) array\n        metadata: dict with CRS, transform, resolution, etc.\n    \"\"\"\n    with rio.open(file_path) as src:\n        # TODO: Load the data array\n        data = src.read()  # YOUR CODE: Load raster data\n        \n        # TODO: Extract metadata\n        metadata = {\n            'crs': src.crs,  # YOUR CODE: Get coordinate reference system\n            'transform': src.transform,  # YOUR CODE: Get geospatial transform\n            'shape': data.shape,  # YOUR CODE: Get array dimensions\n            'dtype': data.dtype,  # YOUR CODE: Get data type\n            'resolution': src.res,  # YOUR CODE: Get pixel resolution\n            'bounds': src.bounds,  # YOUR CODE: Get spatial bounds\n        }\n    \n    return data, metadata\n\n# Test your function\ndata, metadata = load_geotiff(SAMPLE_PATH)\nprint(f\"‚úì Loaded shape: {data.shape}\")\nprint(f\"‚úì Data type: {metadata['dtype']}\")\nprint(f\"‚úì Resolution: {metadata['resolution']}\")\nprint(f\"‚úì CRS: {metadata['crs']}\")\n\n\n‚úì Loaded shape: (3, 64, 64)\n‚úì Data type: uint8\n‚úì Resolution: (0.25, 0.25)\n‚úì CRS: PROJCS[\"Projection: Transverse Mercator; Datum: WGS84; Ellipsoid: WGS84\",GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",19],PARAMETER[\"scale_factor\",0.9993],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",-5300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n\n\nERROR 1: PROJ: proj_identify: /Users/kellycaylor/mambaforge/share/proj/proj.db contains DATABASE.LAYOUT.VERSION.MINOR = 2 whereas a number &gt;= 5 is expected. It comes from another PROJ installation.\n\n\n\n\nüîç Verify It: Inspect Your Data\n\n\nCode\n# Examine the data you loaded\nbands, height, width = data.shape\nprint(f\"Image dimensions: {height}√ó{width} pixels\")\nprint(f\"Number of bands: {bands}\")\nprint(f\"Value ranges per band:\")\nfor i, band in enumerate(data):\n    print(f\"  Band {i+1}: {band.min():.0f} to {band.max():.0f}\")\n\n# Quick visualization\nfig, axes = plt.subplots(1, bands, figsize=(12, 4))\nif bands == 1:\n    axes = [axes]\n\nfor i, band in enumerate(data):\n    axes[i].imshow(band, cmap='viridis')\n    axes[i].set_title(f'Band {i+1}')\n    axes[i].axis('off')\n\nplt.suptitle('Raw Satellite Bands')\nplt.tight_layout()\nplt.show()\n\n\nImage dimensions: 64√ó64 pixels\nNumber of bands: 3\nValue ranges per band:\n  Band 1: 0 to 254\n  Band 2: 0 to 254\n  Band 3: 0 to 254"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#step-2-geo-preprocessing-functions",
    "href": "course-materials/01-geospatial-data-foundations.html#step-2-geo-preprocessing-functions",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 2: Geo Preprocessing Functions",
    "text": "Step 2: Geo Preprocessing Functions\nGoal: Build preprocessing functions that operate on the full image before patch extraction.\n\nüõ†Ô∏è Build It: Normalization Functions\nWe‚Äôll create two normalization functions that can work with either local statistics (calculated from the input data) or global statistics (pre-computed from a training dataset). Global statistics ensure consistent normalization across different image tiles and are crucial for foundation model training.\nWhy use global statistics? When training on multiple images, each tile might have different value ranges. Using global statistics ensures that the same pixel value represents the same relative intensity across all training data.\n\nMin-Max Normalization Function\n\n\nCode\ndef minmax_normalize(data: np.ndarray, \n                    global_min: np.ndarray = None, \n                    global_max: np.ndarray = None) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Min-max normalize spectral bands to [0,1] range.\n    \n    Args:\n        data: (bands, height, width) array\n        global_min: Optional (bands,) array of global minimums per band\n        global_max: Optional (bands,) array of global maximums per band\n    \n    Returns:\n        normalized: (bands, height, width) array with values in [0,1]\n        stats: Dictionary containing the min/max values used\n    \"\"\"\n    bands, height, width = data.shape\n    normalized = np.zeros_like(data, dtype=np.float32)\n    \n    # Use global stats if provided, otherwise calculate from data\n    if global_min is None or global_max is None:\n        # Calculate per-band statistics from this data\n        mins = np.array([data[i].min() for i in range(bands)])\n        maxs = np.array([data[i].max() for i in range(bands)])\n        stats_source = \"local (calculated from input)\"\n    else:\n        # Use provided global statistics\n        mins = global_min\n        maxs = global_max\n        stats_source = \"global (provided)\"\n    \n    # Apply normalization per band\n    for i in range(bands):\n        band_range = maxs[i] - mins[i]\n        if band_range &gt; 0:  # Avoid division by zero\n            normalized[i] = (data[i] - mins[i]) / band_range\n        else:\n            normalized[i] = 0  # Handle constant bands\n    \n    # Package statistics for inspection\n    stats = {\n        'source': stats_source,\n        'mins': mins,\n        'maxs': maxs,\n        'output_range': (normalized.min(), normalized.max())\n    }\n    \n    return normalized, stats\n\n\n\n\nZ-Score Normalization Function\n\n\nCode\ndef zscore_normalize(data: np.ndarray,\n                    global_mean: np.ndarray = None,\n                    global_std: np.ndarray = None) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Z-score normalize spectral bands to mean=0, std=1.\n    \n    Args:\n        data: (bands, height, width) array\n        global_mean: Optional (bands,) array of global means per band\n        global_std: Optional (bands,) array of global standard deviations per band\n    \n    Returns:\n        normalized: (bands, height, width) standardized array\n        stats: Dictionary containing the mean/std values used\n    \"\"\"\n    bands, height, width = data.shape\n    normalized = np.zeros_like(data, dtype=np.float32)\n    \n    # Use global stats if provided, otherwise calculate from data\n    if global_mean is None or global_std is None:\n        # Calculate per-band statistics from this data\n        means = np.array([data[i].mean() for i in range(bands)])\n        stds = np.array([data[i].std() for i in range(bands)])\n        stats_source = \"local (calculated from input)\"\n    else:\n        # Use provided global statistics\n        means = global_mean\n        stds = global_std\n        stats_source = \"global (provided)\"\n    \n    # Apply normalization per band\n    for i in range(bands):\n        if stds[i] &gt; 0:  # Avoid division by zero\n            normalized[i] = (data[i] - means[i]) / stds[i]\n        else:\n            normalized[i] = 0  # Handle constant bands\n    \n    # Package statistics for inspection\n    stats = {\n        'source': stats_source,\n        'means': means,\n        'stds': stds,\n        'output_mean': normalized.mean(),\n        'output_std': normalized.std()\n    }\n    \n    return normalized, stats\n\nprint(\"‚úì Normalization functions created\")\nprint(\"  - minmax_normalize: scales to [0,1] range\")\nprint(\"  - zscore_normalize: standardizes to mean=0, std=1\")\n\n\n‚úì Normalization functions created\n  - minmax_normalize: scales to [0,1] range\n  - zscore_normalize: standardizes to mean=0, std=1\n\n\n\n\nTest Both Functions with Local Statistics\n\n\nCode\n# Test min-max normalization with local statistics\nminmax_data, minmax_stats = minmax_normalize(data)\nprint(\"üìä Min-Max Normalization (local stats):\")\nprint(f\"  Source: {minmax_stats['source']}\")\nprint(f\"  Original range: {data.min():.0f} to {data.max():.0f}\")\nprint(f\"  Normalized range: {minmax_stats['output_range'][0]:.3f} to {minmax_stats['output_range'][1]:.3f}\")\nprint(f\"  Per-band mins: {minmax_stats['mins']}\")\nprint(f\"  Per-band maxs: {minmax_stats['maxs']}\")\n\nprint()\n\n# Test z-score normalization with local statistics  \nzscore_data, zscore_stats = zscore_normalize(data)\nprint(\"üìä Z-Score Normalization (local stats):\")\nprint(f\"  Source: {zscore_stats['source']}\")\nprint(f\"  Output mean: {zscore_stats['output_mean']:.6f}\")\nprint(f\"  Output std: {zscore_stats['output_std']:.6f}\")\nprint(f\"  Per-band means: {zscore_stats['means']}\")\nprint(f\"  Per-band stds: {zscore_stats['stds']}\")\n\n\nüìä Min-Max Normalization (local stats):\n  Source: local (calculated from input)\n  Original range: 0 to 254\n  Normalized range: 0.000 to 1.000\n  Per-band mins: [0 0 0]\n  Per-band maxs: [254 254 254]\n\nüìä Z-Score Normalization (local stats):\n  Source: local (calculated from input)\n  Output mean: 0.000000\n  Output std: 1.000000\n  Per-band means: [126.14306641 126.14306641 126.14306641]\n  Per-band stds: [73.10237725 73.10237725 73.10237725]\n\n\n\n\nTest with Global Statistics\n\n\nCode\n# Simulate global statistics from a larger dataset\n# In practice, these would be pre-computed from your entire training corpus\nglobal_mins = np.array([100, 150, 200])  # Example global minimums per band\nglobal_maxs = np.array([1500, 2000, 2500])  # Example global maximums per band\nglobal_means = np.array([800, 1200, 1600])  # Example global means per band\nglobal_stds = np.array([300, 400, 500])  # Example global standard deviations per band\n\nprint(\"üåç Testing with Global Statistics:\")\nprint(f\"  Global mins: {global_mins}\")\nprint(f\"  Global maxs: {global_maxs}\")\nprint(f\"  Global means: {global_means}\")\nprint(f\"  Global stds: {global_stds}\")\n\nprint()\n\n# Test with global statistics\nminmax_global, minmax_global_stats = minmax_normalize(data, global_mins, global_maxs)\nzscore_global, zscore_global_stats = zscore_normalize(data, global_means, global_stds)\n\nprint(\"üìä Min-Max with Global Stats:\")\nprint(f\"  Source: {minmax_global_stats['source']}\")\nprint(f\"  Output range: {minmax_global_stats['output_range'][0]:.3f} to {minmax_global_stats['output_range'][1]:.3f}\")\n\nprint()\n\nprint(\"üìä Z-Score with Global Stats:\")\nprint(f\"  Source: {zscore_global_stats['source']}\")\nprint(f\"  Output mean: {zscore_global_stats['output_mean']:.3f}\")\nprint(f\"  Output std: {zscore_global_stats['output_std']:.3f}\")\n\n\nüåç Testing with Global Statistics:\n  Global mins: [100 150 200]\n  Global maxs: [1500 2000 2500]\n  Global means: [ 800 1200 1600]\n  Global stds: [300 400 500]\n\nüìä Min-Max with Global Stats:\n  Source: global (provided)\n  Output range: 0.000 to 0.182\n\nüìä Z-Score with Global Stats:\n  Source: global (provided)\n  Output mean: 168.496\n  Output std: 36.333\n\n\nWhat to notice: When using global statistics, the output ranges and distributions differ from local normalization. This is expected and ensures consistency across different image tiles in your dataset.\n\n\n\nüõ†Ô∏è Build It: Spatial Cropping Function\n\n\nCode\ndef crop_to_patches(data: np.ndarray, patch_size: int, stride: int) -&gt; np.ndarray:\n    \"\"\"\n    Crop image to dimensions that allow complete patch extraction.\n    \n    Args:\n        data: (bands, height, width) array\n        patch_size: size of patches to extract\n        stride: step size between patches\n        \n    Returns:\n        cropped: (bands, new_height, new_width) array\n    \"\"\"\n    bands, height, width = data.shape\n    \n    # TODO: Calculate how many complete patches fit\n    patches_h = (height - patch_size) // stride + 1\n    patches_w = (width - patch_size) // stride + 1\n    \n    # TODO: Calculate the required dimensions\n    new_height = (patches_h - 1) * stride + patch_size\n    new_width = (patches_w - 1) * stride + patch_size\n    \n    # TODO: Crop the data\n    cropped = data[:, :new_height, :new_width]\n    \n    print(f\"‚úì Cropped from {height}√ó{width} to {new_height}√ó{new_width}\")\n    print(f\"‚úì Will generate {patches_h}√ó{patches_w} = {patches_h*patches_w} patches\")\n    \n    return cropped\n\n# Test your cropping function\ncropped_data = crop_to_patches(data, 8, STRIDE)\n\n\n‚úì Cropped from 64√ó64 to 40√ó40\n‚úì Will generate 2√ó2 = 4 patches"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#step-3-patch-extraction-with-metadata",
    "href": "course-materials/01-geospatial-data-foundations.html#step-3-patch-extraction-with-metadata",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 3: Patch Extraction with Metadata",
    "text": "Step 3: Patch Extraction with Metadata\nGoal: Extract patches while preserving spatial context information.\n\nüõ†Ô∏è Build It: Patch Extraction Function\n\n\nCode\ndef extract_patches_with_metadata(\n    data: np.ndarray, \n    transform,\n    patch_size: int, \n    stride: int\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Extract patches with their spatial coordinates.\n    \n    Args:\n        data: (bands, height, width) normalized array\n        transform: rasterio transform object\n        patch_size: size of patches\n        stride: step between patches\n        \n    Returns:\n        patches: (n_patches, bands, patch_size, patch_size) array\n        coordinates: (n_patches, 4) array of [min_x, min_y, max_x, max_y]\n    \"\"\"\n    bands, height, width = data.shape\n    patches = []\n    coordinates = []\n    \n    # TODO: Iterate through patch positions\n    for row in range(0, height - patch_size + 1, stride):\n        for col in range(0, width - patch_size + 1, stride):\n            # TODO: Extract patch from all bands\n            patch = data[:, row:row+patch_size, col:col+patch_size]\n            patches.append(patch)\n            \n            # TODO: Calculate real-world coordinates using transform\n            min_x, max_y = transform * (col, row)  # Top-left\n            max_x, min_y = transform * (col + patch_size, row + patch_size)  # Bottom-right\n            coordinates.append([min_x, min_y, max_x, max_y])\n    \n    patches = np.array(patches)\n    coordinates = np.array(coordinates)\n    \n    print(f\"‚úì Extracted {len(patches)} patches\")\n    print(f\"‚úì Patch shape: {patches.shape}\")\n    print(f\"‚úì Coordinate shape: {coordinates.shape}\")\n    \n    return patches, coordinates\n\n# Test your patch extraction\npatches, coords = extract_patches_with_metadata(\n    data, metadata['transform'], 8, 4\n)\n\n# Visualize a few patches\nfig, axes = plt.subplots(2, 4, figsize=(12, 6))\nfor i in range(8):\n    row, col = i // 4, i % 4\n    # Show first band of each patch\n    axes[row, col].imshow(patches[i, 0], cmap='viridis')\n    axes[row, col].set_title(f'Patch {i}')\n    axes[row, col].axis('off')\n\nplt.suptitle('Sample Extracted Patches (Band 1)')\nplt.tight_layout()\nplt.show()\n\n\n‚úì Extracted 225 patches\n‚úì Patch shape: (225, 3, 8, 8)\n‚úì Coordinate shape: (225, 4)"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#step-4-tensor-operations-metadata-encoding",
    "href": "course-materials/01-geospatial-data-foundations.html#step-4-tensor-operations-metadata-encoding",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 4: Tensor Operations & Metadata Encoding",
    "text": "Step 4: Tensor Operations & Metadata Encoding\nGoal: Convert numpy arrays to PyTorch tensors and encode metadata.\n\nüõ†Ô∏è Build It: Metadata Encoder\n\n\nCode\ndef encode_metadata(coordinates: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Encode spatial metadata as features.\n    \n    Args:\n        coordinates: (n_patches, 4) array of [min_x, min_y, max_x, max_y]\n        \n    Returns:\n        encoded: (n_patches, n_features) array\n    \"\"\"\n    # TODO: Calculate spatial features\n    center_x = (coordinates[:, 0] + coordinates[:, 2]) / 2\n    center_y = (coordinates[:, 1] + coordinates[:, 3]) / 2\n    width = coordinates[:, 2] - coordinates[:, 0]\n    height = coordinates[:, 3] - coordinates[:, 1]\n    area = width * height\n    \n    # TODO: Normalize spatial features (handle zero std to avoid divide by zero)\n    def safe_normalize(values):\n        \"\"\"Normalize values, handling zero standard deviation.\"\"\"\n        mean_val = values.mean()\n        std_val = values.std()\n        if std_val &gt; 0:\n            return (values - mean_val) / std_val\n        else:\n            return np.zeros_like(values)  # All values are the same\n    \n    features = np.column_stack([\n        safe_normalize(center_x),     # Normalized center X\n        safe_normalize(center_y),     # Normalized center Y  \n        safe_normalize(area),         # Normalized area (handles constant area)\n        width / height,               # Aspect ratio\n    ])\n    \n    print(f\"‚úì Encoded metadata shape: {features.shape}\")\n    print(f\"‚úì Feature statistics:\")\n    feature_names = ['center_x', 'center_y', 'area', 'aspect_ratio']\n    for i, name in enumerate(feature_names):\n        print(f\"  {name}: mean={features[:, i].mean():.3f}, std={features[:, i].std():.3f}\")\n    \n    return features.astype(np.float32)\n\n# Test metadata encoding\nencoded_metadata = encode_metadata(coords)\n\n\n‚úì Encoded metadata shape: (225, 4)\n‚úì Feature statistics:\n  center_x: mean=0.000, std=1.000\n  center_y: mean=0.000, std=1.000\n  area: mean=0.000, std=0.000\n  aspect_ratio: mean=1.000, std=0.000\n\n\n\n\nüõ†Ô∏è Build It: Tensor Conversion\n\n\nCode\ndef create_tensors(patches: np.ndarray, metadata: np.ndarray) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Convert numpy arrays to PyTorch tensors.\n    \n    Args:\n        patches: (n_patches, bands, height, width) array\n        metadata: (n_patches, n_features) array\n        \n    Returns:\n        patch_tensors: (n_patches, bands, height, width) tensor\n        metadata_tensors: (n_patches, n_features) tensor\n    \"\"\"\n    # TODO: Convert to tensors with appropriate dtypes\n    patch_tensors = torch.from_numpy(patches).float()\n    metadata_tensors = torch.from_numpy(metadata).float()\n    \n    print(f\"‚úì Patch tensors: {patch_tensors.shape}, dtype: {patch_tensors.dtype}\")\n    print(f\"‚úì Metadata tensors: {metadata_tensors.shape}, dtype: {metadata_tensors.dtype}\")\n    \n    return patch_tensors, metadata_tensors\n\n# Create tensors\npatch_tensors, metadata_tensors = create_tensors(patches, encoded_metadata)\n\n\n‚úì Patch tensors: torch.Size([225, 3, 8, 8]), dtype: torch.float32\n‚úì Metadata tensors: torch.Size([225, 4]), dtype: torch.float32"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#step-5-dataloader-construction",
    "href": "course-materials/01-geospatial-data-foundations.html#step-5-dataloader-construction",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 5: DataLoader Construction",
    "text": "Step 5: DataLoader Construction\nGoal: Build a PyTorch Dataset and DataLoader for training.\n\nüõ†Ô∏è Build It: Custom Dataset Class\n\n\nCode\nclass GeospatialDataset(Dataset):\n    \"\"\"Dataset for geospatial patches with metadata.\"\"\"\n    \n    def __init__(self, patch_tensors: torch.Tensor, metadata_tensors: torch.Tensor):\n        \"\"\"\n        Args:\n            patch_tensors: (n_patches, bands, height, width)\n            metadata_tensors: (n_patches, n_features)\n        \"\"\"\n        self.patches = patch_tensors\n        self.metadata = metadata_tensors\n        \n        # TODO: Create dummy labels for demonstration (in real use, load from file)\n        self.labels = torch.randint(0, 5, (len(patch_tensors),))  # 5 land cover classes\n        \n    def __len__(self) -&gt; int:\n        \"\"\"Return number of patches.\"\"\"\n        return len(self.patches)\n    \n    def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Get a single item.\n        \n        Returns:\n            patch: (bands, height, width) tensor\n            metadata: (n_features,) tensor  \n            label: scalar tensor\n        \"\"\"\n        return self.patches[idx], self.metadata[idx], self.labels[idx]\n\n# Test your dataset\ndataset = GeospatialDataset(patch_tensors, metadata_tensors)\nprint(f\"‚úì Dataset length: {len(dataset)}\")\n\n# Test getting an item\nsample_patch, sample_metadata, sample_label = dataset[0]\nprint(f\"‚úì Sample patch shape: {sample_patch.shape}\")\nprint(f\"‚úì Sample metadata shape: {sample_metadata.shape}\")\nprint(f\"‚úì Sample label: {sample_label.item()}\")\n\n\n‚úì Dataset length: 225\n‚úì Sample patch shape: torch.Size([3, 8, 8])\n‚úì Sample metadata shape: torch.Size([4])\n‚úì Sample label: 2\n\n\n\n\nüõ†Ô∏è Build It: DataLoader\n\n\nCode\n# TODO: Create DataLoader with appropriate batch size and shuffling\ndataloader = DataLoader(\n    dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True,\n    num_workers=0,  # Set to 0 for compatibility\n    pin_memory=True if torch.cuda.is_available() else False\n)\n\nprint(f\"‚úì DataLoader created with batch size {BATCH_SIZE}\")\nprint(f\"‚úì Number of batches: {len(dataloader)}\")\n\n# Test the DataLoader\nfor batch_idx, (patches, metadata, labels) in enumerate(dataloader):\n    print(f\"‚úì Batch {batch_idx}:\")\n    print(f\"  Patches: {patches.shape}\")\n    print(f\"  Metadata: {metadata.shape}\")\n    print(f\"  Labels: {labels.shape}\")\n    if batch_idx == 1:  # Show first two batches\n        break\n\n\n‚úì DataLoader created with batch size 8\n‚úì Number of batches: 29\n‚úì Batch 0:\n  Patches: torch.Size([8, 3, 8, 8])\n  Metadata: torch.Size([8, 4])\n  Labels: torch.Size([8])\n‚úì Batch 1:\n  Patches: torch.Size([8, 3, 8, 8])\n  Metadata: torch.Size([8, 4])\n  Labels: torch.Size([8])"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#step-6-embedding-layer-integration",
    "href": "course-materials/01-geospatial-data-foundations.html#step-6-embedding-layer-integration",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 6: Embedding Layer Integration",
    "text": "Step 6: Embedding Layer Integration\nGoal: Connect to a simple embedding layer to verify end-to-end functionality.\n\nüõ†Ô∏è Build It: Simple GFM Embedding Layer\n\n\nCode\nclass SimpleGFMEmbedding(nn.Module):\n    \"\"\"Simple embedding layer for geospatial patches.\"\"\"\n    \n    def __init__(self, input_channels: int, metadata_features: int, embed_dim: int, patch_size: int = 64):\n        super().__init__()\n        \n        # TODO: Build adaptive patch encoder based on patch size\n        if patch_size &gt;= 32:\n            # Larger patches: multi-layer CNN\n            kernel1 = min(8, patch_size // 4)\n            kernel2 = min(4, patch_size // 8)\n            self.patch_encoder = nn.Sequential(\n                nn.Conv2d(input_channels, 32, kernel_size=kernel1, stride=kernel1//2),\n                nn.ReLU(),\n                nn.Conv2d(32, 64, kernel_size=kernel2, stride=kernel2//2), \n                nn.ReLU(),\n                nn.AdaptiveAvgPool2d(1),\n                nn.Flatten(),\n            )\n        else:\n            # Smaller patches: simpler encoder\n            kernel = min(4, patch_size // 2)\n            self.patch_encoder = nn.Sequential(\n                nn.Conv2d(input_channels, 64, kernel_size=kernel, stride=1),\n                nn.ReLU(),\n                nn.AdaptiveAvgPool2d(1),\n                nn.Flatten(),\n            )\n        \n        # TODO: Build metadata encoder\n        self.metadata_encoder = nn.Sequential(\n            nn.Linear(metadata_features, 32),\n            nn.ReLU(),\n            nn.Linear(32, 32),\n        )\n        \n        # TODO: Build fusion layer\n        # Calculate patch encoder output size\n        with torch.no_grad():\n            dummy_patch = torch.randn(1, input_channels, patch_size, patch_size)\n            patch_feat_size = self.patch_encoder(dummy_patch).shape[1]\n        \n        self.fusion = nn.Sequential(\n            nn.Linear(patch_feat_size + 32, embed_dim),\n            nn.ReLU(),\n            nn.Linear(embed_dim, embed_dim),\n        )\n        \n    def forward(self, patches: torch.Tensor, metadata: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            patches: (batch, channels, height, width)\n            metadata: (batch, n_features)\n            \n        Returns:\n            embeddings: (batch, embed_dim)\n        \"\"\"\n        # TODO: Encode patches and metadata\n        patch_features = self.patch_encoder(patches)\n        metadata_features = self.metadata_encoder(metadata)\n        \n        # TODO: Fuse features\n        combined = torch.cat([patch_features, metadata_features], dim=1)\n        embeddings = self.fusion(combined)\n        \n        return embeddings\n\n# Create and test the model\nmodel = SimpleGFMEmbedding(\n    input_channels=bands, \n    metadata_features=encoded_metadata.shape[1], \n    embed_dim=EMBEDDING_DIM,\n    patch_size=PATCH_SIZE\n)\n\nprint(f\"‚úì Model created\")\nprint(f\"‚úì Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n\n‚úì Model created\n‚úì Model parameters: 130,848"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#step-7-end-to-end-pipeline-test",
    "href": "course-materials/01-geospatial-data-foundations.html#step-7-end-to-end-pipeline-test",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 7: End-to-End Pipeline Test",
    "text": "Step 7: End-to-End Pipeline Test\nGoal: Run the complete pipeline and verify everything works together.\n\nüõ†Ô∏è Build It: Complete Pipeline Function\n\n\nCode\ndef geotiff_to_embeddings_pipeline(\n    file_path: Path,\n    patch_size: int = 8,\n    stride: int = 4,\n    batch_size: int = 8,\n    embed_dim: int = 256\n) -&gt; torch.Tensor:\n    \"\"\"\n    Complete pipeline from GeoTIFF to embeddings.\n    \n    Args:\n        file_path: Path to GeoTIFF file\n        patch_size: Size of patches to extract\n        stride: Step between patches  \n        batch_size: Batch size for processing\n        embed_dim: Embedding dimension\n        \n    Returns:\n        all_embeddings: (n_patches, embed_dim) tensor\n    \"\"\"\n    print(\"üöÄ Starting GeoTIFF ‚Üí Embeddings Pipeline\")\n    \n    # Step 1: Load data\n    print(\"üìÅ Loading GeoTIFF...\")\n    data, metadata = load_geotiff(file_path)\n    \n    # Step 2: Preprocess\n    print(\"üîß Preprocessing...\")\n    norm_data, norm_stats = minmax_normalize(data)\n    cropped_data = crop_to_patches(norm_data, patch_size, stride)\n    \n    # Step 3: Extract patches\n    print(\"‚úÇÔ∏è Extracting patches...\")\n    patches, coords = extract_patches_with_metadata(cropped_data, metadata['transform'], patch_size, stride)\n    \n    # Step 4: Encode metadata\n    print(\"üìä Encoding metadata...\")\n    encoded_meta = encode_metadata(coords)\n    \n    # Step 5: Create tensors\n    print(\"üî¢ Creating tensors...\")\n    patch_tensors, meta_tensors = create_tensors(patches, encoded_meta)\n    \n    # Step 6: Create dataset and dataloader\n    print(\"üì¶ Creating DataLoader...\")\n    dataset = GeospatialDataset(patch_tensors, meta_tensors)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    # Step 7: Create model and generate embeddings\n    print(\"üß† Generating embeddings...\")\n    model = SimpleGFMEmbedding(\n        input_channels=data.shape[0],\n        metadata_features=encoded_meta.shape[1], \n        embed_dim=embed_dim,\n        patch_size=patch_size\n    )\n    model.eval()\n    \n    all_embeddings = []\n    with torch.no_grad():\n        for patches_batch, meta_batch, _ in dataloader:\n            embeddings = model(patches_batch, meta_batch)\n            all_embeddings.append(embeddings)\n    \n    all_embeddings = torch.cat(all_embeddings, dim=0)\n    print(f\"‚úÖ Pipeline complete! Generated {len(all_embeddings)} embeddings\")\n    \n    return all_embeddings\n\n# Run the complete pipeline\nembeddings = geotiff_to_embeddings_pipeline(SAMPLE_PATH)\nprint(f\"\\nüéâ Final Result:\")\nprint(f\"‚úì Embeddings shape: {embeddings.shape}\")\nprint(f\"‚úì Embedding statistics:\")\nprint(f\"  Mean: {embeddings.mean().item():.4f}\")\nprint(f\"  Std: {embeddings.std().item():.4f}\")\nprint(f\"  Min: {embeddings.min().item():.4f}\")\nprint(f\"  Max: {embeddings.max().item():.4f}\")\n\n\nüöÄ Starting GeoTIFF ‚Üí Embeddings Pipeline\nüìÅ Loading GeoTIFF...\nüîß Preprocessing...\n‚úì Cropped from 64√ó64 to 64√ó64\n‚úì Will generate 15√ó15 = 225 patches\n‚úÇÔ∏è Extracting patches...\n‚úì Extracted 225 patches\n‚úì Patch shape: (225, 3, 8, 8)\n‚úì Coordinate shape: (225, 4)\nüìä Encoding metadata...\n‚úì Encoded metadata shape: (225, 4)\n‚úì Feature statistics:\n  center_x: mean=0.000, std=1.000\n  center_y: mean=0.000, std=1.000\n  area: mean=0.000, std=0.000\n  aspect_ratio: mean=1.000, std=0.000\nüî¢ Creating tensors...\n‚úì Patch tensors: torch.Size([225, 3, 8, 8]), dtype: torch.float32\n‚úì Metadata tensors: torch.Size([225, 4]), dtype: torch.float32\nüì¶ Creating DataLoader...\nüß† Generating embeddings...\n‚úÖ Pipeline complete! Generated 225 embeddings\n\nüéâ Final Result:\n‚úì Embeddings shape: torch.Size([225, 256])\n‚úì Embedding statistics:\n  Mean: 0.0023\n  Std: 0.0716\n  Min: -0.2246\n  Max: 0.2414\n\n\n\n\nüîç Verify It: Pipeline Output Analysis\n\n\nCode\n# Visualize embedding similarities\nprint(\"üîç Analyzing embedding relationships...\")\n\n# Check if we have enough embeddings for analysis\nif len(embeddings) &lt; 10:\n    print(f\"‚ö†Ô∏è Only {len(embeddings)} embeddings available, using all of them\")\n    sample_size = len(embeddings)\nelse:\n    print(f\"‚úì Using first 10 of {len(embeddings)} embeddings for similarity analysis\")\n    sample_size = 10\n\nif sample_size &gt; 1:\n    # Compute pairwise cosine similarities\n    from torch.nn.functional import cosine_similarity\n    \n    sample_embeddings = embeddings[:sample_size]\n    similarity_matrix = torch.zeros(sample_size, sample_size)\n    \n    for i in range(sample_size):\n        for j in range(sample_size):\n            if i == j:\n                similarity_matrix[i, j] = 1.0  # Perfect self-similarity\n            else:\n                sim = cosine_similarity(sample_embeddings[i:i+1], sample_embeddings[j:j+1], dim=1)\n                similarity_matrix[i, j] = sim.item()\n    \n    # Plot similarity matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(similarity_matrix.numpy(), cmap='viridis', vmin=-1, vmax=1)\n    plt.colorbar(label='Cosine Similarity')\n    plt.title(f'Embedding Similarity Matrix (First {sample_size} Patches)')\n    plt.xlabel('Patch Index')\n    plt.ylabel('Patch Index')\n    plt.show()\n    \n    print(f\"‚úì Average similarity: {similarity_matrix.mean().item():.4f}\")\n    print(f\"‚úì Similarity range: {similarity_matrix.min().item():.4f} to {similarity_matrix.max().item():.4f}\")\nelse:\n    print(\"‚ö†Ô∏è Not enough embeddings for similarity analysis\")\n\n\nüîç Analyzing embedding relationships...\n‚úì Using first 10 of 225 embeddings for similarity analysis\n\n\n\n\n\n\n\n\n\n‚úì Average similarity: 0.9832\n‚úì Similarity range: 0.9332 to 1.0000"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#conclusion",
    "href": "course-materials/01-geospatial-data-foundations.html#conclusion",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Conclusion",
    "text": "Conclusion\nüéâ Congratulations! You‚Äôve successfully built a complete pipeline that transforms raw satellite imagery into model-ready embeddings.\n\nWhat You Built:\n\nGeoTIFF Loader: Extracts both pixel data and spatial metadata\nPreprocessing Functions: Normalization and spatial cropping\n\nPatch Extractor: Creates patches while preserving spatial context\nMetadata Encoder: Transforms coordinates into learned features\nPyTorch Integration: Dataset, DataLoader, and model components\nEmbedding Generator: Simple CNN that produces vector representations\n\n\n\nKey Insights:\n\nSpatial Context Matters: Each patch carries location information\nPreprocessing is Critical: Normalization ensures stable training\n\nModular Design: Each step can be optimized independently\nEnd-to-End Testing: Verify the complete pipeline works\n\n\n\nWhat‚Äôs Next:\nIn the following sessions, you‚Äôll enhance each component: - Week 2: Advanced attention mechanisms for spatial relationships - Week 3: Complete GFM architecture with transformer blocks - Week 4: Pretraining strategies and masked autoencoding\nThe pipeline you built today forms the foundation for everything that follows! üöÄ"
  },
  {
    "objectID": "course-materials/01-geospatial-data-foundations.html#resources",
    "href": "course-materials/01-geospatial-data-foundations.html#resources",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Resources",
    "text": "Resources\n\nPyTorch DataLoader Documentation\nRasterio User Guide\nGeospatial Foundation Model Examples"
  },
  {
    "objectID": "course-materials/extras/projects/project-application-template.html",
    "href": "course-materials/extras/projects/project-application-template.html",
    "title": "Project Application Template",
    "section": "",
    "text": "Student Name: [Your Name]\nEmail: [Your Email]\nDate: [Submission Date]\n\n\nPlease describe your past experience with:\nRemote Sensing and Geospatial Data (2-3 sentences): - [Describe your experience with satellite imagery, GIS, remote sensing tools, etc.]\nMachine Learning and Deep Learning (2-3 sentences): - [Describe your experience with Python, PyTorch, model training, etc.]\nSpecific Tools and Platforms (1-2 sentences): - [Mention experience with Earth Engine, QGIS, ArcGIS, cloud platforms, etc.]\n\n\n\nApplication Domain (2-3 sentences): - [Describe the specific application area you‚Äôre interested in exploring] - [Examples: crop monitoring, deforestation detection, disaster response, urban planning, etc.]\nResearch Questions or Goals (2-3 sentences): - [What specific questions do you hope to answer or problems to solve?] - [What would successful outcomes look like for your project?]\n\n\n\nAvailable Data (1-2 sentences): - [Do you have access to specific datasets, ground truth data, or study areas?] - [If not, describe what types of data you would need to acquire]\nComputational Resources (1 sentence): - [Describe your current access to computational resources, cloud credits, etc.]\n\n\n\nInitial Ideas (2-3 sentences): - [Any preliminary ideas about which foundation models or techniques to explore?] - [This can be very preliminary - you‚Äôll refine throughout the course]\n\n\n\nWhat you hope to learn (2-3 sentences): - [What specific skills or knowledge do you want to gain from this course?] - [How does this fit into your broader academic or career goals?]"
  },
  {
    "objectID": "course-materials/extras/projects/project-application-template.html#project-application-due-week-0",
    "href": "course-materials/extras/projects/project-application-template.html#project-application-due-week-0",
    "title": "Project Application Template",
    "section": "",
    "text": "Student Name: [Your Name]\nEmail: [Your Email]\nDate: [Submission Date]\n\n\nPlease describe your past experience with:\nRemote Sensing and Geospatial Data (2-3 sentences): - [Describe your experience with satellite imagery, GIS, remote sensing tools, etc.]\nMachine Learning and Deep Learning (2-3 sentences): - [Describe your experience with Python, PyTorch, model training, etc.]\nSpecific Tools and Platforms (1-2 sentences): - [Mention experience with Earth Engine, QGIS, ArcGIS, cloud platforms, etc.]\n\n\n\nApplication Domain (2-3 sentences): - [Describe the specific application area you‚Äôre interested in exploring] - [Examples: crop monitoring, deforestation detection, disaster response, urban planning, etc.]\nResearch Questions or Goals (2-3 sentences): - [What specific questions do you hope to answer or problems to solve?] - [What would successful outcomes look like for your project?]\n\n\n\nAvailable Data (1-2 sentences): - [Do you have access to specific datasets, ground truth data, or study areas?] - [If not, describe what types of data you would need to acquire]\nComputational Resources (1 sentence): - [Describe your current access to computational resources, cloud credits, etc.]\n\n\n\nInitial Ideas (2-3 sentences): - [Any preliminary ideas about which foundation models or techniques to explore?] - [This can be very preliminary - you‚Äôll refine throughout the course]\n\n\n\nWhat you hope to learn (2-3 sentences): - [What specific skills or knowledge do you want to gain from this course?] - [How does this fit into your broader academic or career goals?]"
  },
  {
    "objectID": "course-materials/extras/projects/project-proposal-template.html",
    "href": "course-materials/extras/projects/project-proposal-template.html",
    "title": "Project Proposal Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Descriptive Project Title]\nDate: [Submission Date]\n\n\nProblem Description (3-4 sentences): - [Clearly describe the problem you‚Äôre addressing] - [Why is this problem important? What is the broader significance?] - [What are the current limitations or gaps in existing approaches?]\nResearch Objectives (2-3 bullet points): - [Specific, measurable goals for your project] - [What specific outcomes will you deliver?]\n\n\n\nPrimary Dataset(s): - Source: [e.g., Landsat, Sentinel, MODIS, custom dataset] - Spatial Coverage: [Geographic extent] - Temporal Coverage: [Time period] - Resolution: [Spatial and temporal resolution] - Size: [Approximate data volume]\nGround Truth/Labels (if applicable): - [Description of labeled data for training/validation] - [Source and quality of labels]\nData Availability Assessment: - [x] Data is readily accessible - [ ] Data requires special access/permissions - [ ] Data needs to be collected/generated - [ ] Alternative data sources identified\n\n\n\nFoundation Model Selection: - Primary Model: [e.g., Prithvi, SatMAE, custom architecture] - Justification: [Why is this model appropriate for your problem?] - Baseline Comparisons: [What will you compare against?]\nMethodology Overview (4-5 sentences): - [High-level description of your approach] - [Key technical components and workflow] - [How will you adapt/fine-tune the foundation model?]\nKey Technical Components: - [ ] Data preprocessing pipeline - [ ] Model fine-tuning/adaptation - [ ] Evaluation framework - [ ] Visualization/interpretation tools - [ ] Scalable inference pipeline\n\n\n\nWeek 4-5: [Specific goals and deliverables] Week 6: [Specific goals and deliverables]\nWeek 7 (MVP): [What will your MVP demonstration include?] Week 8-9: [Final implementation goals] Week 10: [Final presentation preparation]\n\n\n\nPerformance Metrics: - [How will you measure success? Specific metrics] - [Quantitative measures: accuracy, F1, IoU, etc.] - [Qualitative measures: interpretability, usability, etc.]\nValidation Approach: - [Cross-validation strategy] - [Train/validation/test splits] - [Spatial/temporal validation considerations]\n\n\n\nTechnical Risks and Mitigation: - Risk 1: [Description] ‚Üí Mitigation: [How you‚Äôll address it] - Risk 2: [Description] ‚Üí Mitigation: [How you‚Äôll address it]\nResource Requirements: - Computational: [GPU hours, memory requirements, storage] - Data: [Download time, storage space, preprocessing time] - Time: [Realistic assessment of scope given 10-week timeframe]\nBackup Plans: - [What will you do if primary approach doesn‚Äôt work?] - [How can you ensure you have a working system for final presentation?]\n\n\n\nPrimary Deliverables: - [What specific outputs will your project produce?] - [Code repository, trained models, analysis results, etc.]\nPotential Impact: - [How could this work be used in practice?] - [What are the broader implications of success?]\nFuture Work: - [How could this project be extended beyond the course?] - [What are the next logical steps?]\n\n\n\nKey Literature (3-5 papers): - [List relevant papers that inform your approach]\nSoftware/Tools: - [List key libraries, platforms, tools you‚Äôll use]\nAdditional Resources: - [Any other resources, datasets, collaborations, etc.]"
  },
  {
    "objectID": "course-materials/extras/projects/project-proposal-template.html#project-proposal-due-week-3",
    "href": "course-materials/extras/projects/project-proposal-template.html#project-proposal-due-week-3",
    "title": "Project Proposal Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Descriptive Project Title]\nDate: [Submission Date]\n\n\nProblem Description (3-4 sentences): - [Clearly describe the problem you‚Äôre addressing] - [Why is this problem important? What is the broader significance?] - [What are the current limitations or gaps in existing approaches?]\nResearch Objectives (2-3 bullet points): - [Specific, measurable goals for your project] - [What specific outcomes will you deliver?]\n\n\n\nPrimary Dataset(s): - Source: [e.g., Landsat, Sentinel, MODIS, custom dataset] - Spatial Coverage: [Geographic extent] - Temporal Coverage: [Time period] - Resolution: [Spatial and temporal resolution] - Size: [Approximate data volume]\nGround Truth/Labels (if applicable): - [Description of labeled data for training/validation] - [Source and quality of labels]\nData Availability Assessment: - [x] Data is readily accessible - [ ] Data requires special access/permissions - [ ] Data needs to be collected/generated - [ ] Alternative data sources identified\n\n\n\nFoundation Model Selection: - Primary Model: [e.g., Prithvi, SatMAE, custom architecture] - Justification: [Why is this model appropriate for your problem?] - Baseline Comparisons: [What will you compare against?]\nMethodology Overview (4-5 sentences): - [High-level description of your approach] - [Key technical components and workflow] - [How will you adapt/fine-tune the foundation model?]\nKey Technical Components: - [ ] Data preprocessing pipeline - [ ] Model fine-tuning/adaptation - [ ] Evaluation framework - [ ] Visualization/interpretation tools - [ ] Scalable inference pipeline\n\n\n\nWeek 4-5: [Specific goals and deliverables] Week 6: [Specific goals and deliverables]\nWeek 7 (MVP): [What will your MVP demonstration include?] Week 8-9: [Final implementation goals] Week 10: [Final presentation preparation]\n\n\n\nPerformance Metrics: - [How will you measure success? Specific metrics] - [Quantitative measures: accuracy, F1, IoU, etc.] - [Qualitative measures: interpretability, usability, etc.]\nValidation Approach: - [Cross-validation strategy] - [Train/validation/test splits] - [Spatial/temporal validation considerations]\n\n\n\nTechnical Risks and Mitigation: - Risk 1: [Description] ‚Üí Mitigation: [How you‚Äôll address it] - Risk 2: [Description] ‚Üí Mitigation: [How you‚Äôll address it]\nResource Requirements: - Computational: [GPU hours, memory requirements, storage] - Data: [Download time, storage space, preprocessing time] - Time: [Realistic assessment of scope given 10-week timeframe]\nBackup Plans: - [What will you do if primary approach doesn‚Äôt work?] - [How can you ensure you have a working system for final presentation?]\n\n\n\nPrimary Deliverables: - [What specific outputs will your project produce?] - [Code repository, trained models, analysis results, etc.]\nPotential Impact: - [How could this work be used in practice?] - [What are the broader implications of success?]\nFuture Work: - [How could this project be extended beyond the course?] - [What are the next logical steps?]\n\n\n\nKey Literature (3-5 papers): - [List relevant papers that inform your approach]\nSoftware/Tools: - [List key libraries, platforms, tools you‚Äôll use]\nAdditional Resources: - [Any other resources, datasets, collaborations, etc.]"
  },
  {
    "objectID": "course-materials/extras/projects/mvp-template.html",
    "href": "course-materials/extras/projects/mvp-template.html",
    "title": "Initial MVP Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Your Project Title]\nPresentation Date: [Date]\n\n\nProblem Recap (30 seconds): - [Brief reminder of the problem you‚Äôre solving]\nCurrent Status (1 minute): - [What have you accomplished so far?] - [What is working in your current implementation?]\n\n\n\nSystem Architecture:\n[Include a simple diagram or flowchart of your system]\nData Input ‚Üí Preprocessing ‚Üí Model ‚Üí Post-processing ‚Üí Output\nKey Components Implemented: - [x] Data loading and preprocessing pipeline - [x] Model setup and configuration\n- [x] Basic training/fine-tuning workflow - [ ] Advanced evaluation metrics - [ ] Visualization dashboard - [ ] Scalable deployment\nTechnology Stack: - Data Processing: [e.g., Earth Engine, rasterio, xarray] - Model Framework: [e.g., PyTorch, TorchGeo, HuggingFace] - Visualization: [e.g., Matplotlib, Folium, Plotly] - Infrastructure: [e.g., UCSB AI Sandbox, local development]\n\n\n\nDemo Script (prepare talking points for each step):\n\nData Loading Demo (1 minute):\n\n[Show loading and preprocessing of your specific dataset]\n[Highlight any data challenges you‚Äôve solved]\n\nModel Inference Demo (2 minutes):\n\n[Run your model on sample data]\n[Show inputs and outputs clearly]\n\nResults Visualization (1-2 minutes):\n\n[Display results in an intuitive format]\n[Compare with baselines or ground truth if available]\n\n\n\n\n\nQuantitative Results: - Dataset Size: [Number of samples, spatial/temporal coverage] - Model Performance: [Key metrics with numbers] - Processing Time: [Inference speed, training time]\nExample Results Table: | Metric | Baseline | Your Model | Notes | |‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî-| | Accuracy | XX% | YY% | [Context] | | F1-Score | XX | YY | [Context] | | Processing Speed | XX sec | YY sec | [Context] |\nQualitative Observations: - [What patterns or insights have you discovered?] - [What works well? What doesn‚Äôt work as expected?]\n\n\n\nTechnical Challenges Overcome: 1. Challenge: [Specific technical problem] Solution: [How you solved it]\n\nChallenge: [Another problem] Solution: [Your approach]\n\nCurrent Limitations: - [What are the current limitations of your system?] - [What assumptions are you making?]\n\n\n\nImmediate Goals (Week 8): - [ ] [Specific technical improvement] - [ ] [Additional evaluation or validation] - [ ] [User interface or API development]\nFinal Implementation Goals (Week 9-10): - [ ] [Advanced features or optimizations] - [ ] [Comprehensive evaluation and comparison] - [ ] [Documentation and reproducibility]\nRisks and Contingencies: - [What could go wrong? How will you adapt?]\n\n\n\nSpecific Areas Where You‚Äôd Like Input: 1. [Technical question about approach or implementation] 2. [Question about evaluation methodology]\n3. [Question about scope or next steps]\nOpen Discussion: - [Any other areas where peer feedback would be valuable]"
  },
  {
    "objectID": "course-materials/extras/projects/mvp-template.html#initial-mvp-presentation-due-week-7",
    "href": "course-materials/extras/projects/mvp-template.html#initial-mvp-presentation-due-week-7",
    "title": "Initial MVP Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Your Project Title]\nPresentation Date: [Date]\n\n\nProblem Recap (30 seconds): - [Brief reminder of the problem you‚Äôre solving]\nCurrent Status (1 minute): - [What have you accomplished so far?] - [What is working in your current implementation?]\n\n\n\nSystem Architecture:\n[Include a simple diagram or flowchart of your system]\nData Input ‚Üí Preprocessing ‚Üí Model ‚Üí Post-processing ‚Üí Output\nKey Components Implemented: - [x] Data loading and preprocessing pipeline - [x] Model setup and configuration\n- [x] Basic training/fine-tuning workflow - [ ] Advanced evaluation metrics - [ ] Visualization dashboard - [ ] Scalable deployment\nTechnology Stack: - Data Processing: [e.g., Earth Engine, rasterio, xarray] - Model Framework: [e.g., PyTorch, TorchGeo, HuggingFace] - Visualization: [e.g., Matplotlib, Folium, Plotly] - Infrastructure: [e.g., UCSB AI Sandbox, local development]\n\n\n\nDemo Script (prepare talking points for each step):\n\nData Loading Demo (1 minute):\n\n[Show loading and preprocessing of your specific dataset]\n[Highlight any data challenges you‚Äôve solved]\n\nModel Inference Demo (2 minutes):\n\n[Run your model on sample data]\n[Show inputs and outputs clearly]\n\nResults Visualization (1-2 minutes):\n\n[Display results in an intuitive format]\n[Compare with baselines or ground truth if available]\n\n\n\n\n\nQuantitative Results: - Dataset Size: [Number of samples, spatial/temporal coverage] - Model Performance: [Key metrics with numbers] - Processing Time: [Inference speed, training time]\nExample Results Table: | Metric | Baseline | Your Model | Notes | |‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî-| | Accuracy | XX% | YY% | [Context] | | F1-Score | XX | YY | [Context] | | Processing Speed | XX sec | YY sec | [Context] |\nQualitative Observations: - [What patterns or insights have you discovered?] - [What works well? What doesn‚Äôt work as expected?]\n\n\n\nTechnical Challenges Overcome: 1. Challenge: [Specific technical problem] Solution: [How you solved it]\n\nChallenge: [Another problem] Solution: [Your approach]\n\nCurrent Limitations: - [What are the current limitations of your system?] - [What assumptions are you making?]\n\n\n\nImmediate Goals (Week 8): - [ ] [Specific technical improvement] - [ ] [Additional evaluation or validation] - [ ] [User interface or API development]\nFinal Implementation Goals (Week 9-10): - [ ] [Advanced features or optimizations] - [ ] [Comprehensive evaluation and comparison] - [ ] [Documentation and reproducibility]\nRisks and Contingencies: - [What could go wrong? How will you adapt?]\n\n\n\nSpecific Areas Where You‚Äôd Like Input: 1. [Technical question about approach or implementation] 2. [Question about evaluation methodology]\n3. [Question about scope or next steps]\nOpen Discussion: - [Any other areas where peer feedback would be valuable]"
  },
  {
    "objectID": "course-materials/extras/projects/mvp-template.html#mvp-demonstration-checklist",
    "href": "course-materials/extras/projects/mvp-template.html#mvp-demonstration-checklist",
    "title": "Initial MVP Template",
    "section": "MVP Demonstration Checklist",
    "text": "MVP Demonstration Checklist\n\nBefore Your Presentation:\n\nTest all code and demos on presentation computer\nPrepare backup slides in case live demo fails\nTime your presentation (aim for 10-12 minutes + 3-5 for Q&A)\nHave clear talking points for each section\nSave sample outputs/screenshots as backup\n\n\n\nDuring Presentation:\n\nSpeak clearly and at appropriate pace\nShow your screen clearly for demos\nEngage with questions and feedback\nStay within time limits\nBe honest about current limitations\n\n\n\nAfter Presentation:\n\nTake notes on feedback received\nUpdate project plan based on suggestions\nSchedule follow-up consultations if needed"
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html",
    "href": "course-materials/extras/resources/course_resources.html",
    "title": "GeoAI Course Resources",
    "section": "",
    "text": "Handbook of Geospatial Artificial Intelligence (Gao, Hu, Li, 2023) ‚Äì Comprehensive reference with 30+ topics, code examples, datasets.\nDeep Learning for the Earth Sciences (Camps-Valls, Tuia, Zhu, 2021) ‚Äì Earth data meets deep learning; ideal for understanding EO models.\nGeoAI and Human Geography (Xiao Huang, 2025) ‚Äì Connects GeoAI with environmental and human systems.\n\n\n\n\n\nBoutayeb et al., 2024 ‚Äì A comprehensive survey of GeoAI methods and applications."
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#essential-background-and-foundational-resources",
    "href": "course-materials/extras/resources/course_resources.html#essential-background-and-foundational-resources",
    "title": "GeoAI Course Resources",
    "section": "",
    "text": "Handbook of Geospatial Artificial Intelligence (Gao, Hu, Li, 2023) ‚Äì Comprehensive reference with 30+ topics, code examples, datasets.\nDeep Learning for the Earth Sciences (Camps-Valls, Tuia, Zhu, 2021) ‚Äì Earth data meets deep learning; ideal for understanding EO models.\nGeoAI and Human Geography (Xiao Huang, 2025) ‚Äì Connects GeoAI with environmental and human systems.\n\n\n\n\n\nBoutayeb et al., 2024 ‚Äì A comprehensive survey of GeoAI methods and applications."
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#week-by-week-resources",
    "href": "course-materials/extras/resources/course_resources.html#week-by-week-resources",
    "title": "GeoAI Course Resources",
    "section": "Week-by-Week Resources",
    "text": "Week-by-Week Resources\n\nWeek 1: Introduction to Geospatial Foundation Models\n\nüåê Development Seed Blog (2024): Using Foundation Models for Earth Observation\nüöÄ NASA/IBM Release: Prithvi HLS Foundation Model\n‚òÅÔ∏è AWS ML Blog (2025): Deploying GeoFMs on AWS\n\n\n\nWeek 2: Working with Geospatial Data\n\nüìö Book: Learning Geospatial Analysis with Python (4th ed., 2023)\nüß∞ TorchGeo Docs: https://pytorch.org/geo\nüåç OpenGeoAI Toolkit: https://github.com/opengeos/geoai\n\n\n\nWeek 3: Loading & Using Foundation Models\n\nü§ó Hugging Face Prithvi Models: https://huggingface.co/ibm-nasa-geospatial\nüìì Demo Notebooks: Available on Hugging Face model cards.\nüß™ AWS GeoFM Deployment Examples: Code and pipelines via GitHub.\nüß† IEEE GRSS Blog: Self-Supervised Geospatial Backbones\n\n\n\nWeek 4: Multi-modal & Generative Models\n\nüõ∞Ô∏è Sensor Fusion via TorchGeo: Stack Sentinel-1/2 & others.\nüåê Presto, Clay, DOFA Models: Explained in DevSeed & AWS blogs.\nüß™ DiffusionSat Example: Medium Tutorial on Generating EO Images\n\n\n\nWeek 5: Model Adaptation & Evaluation\n\nüîå Adapters for GeoFM: Explained via Development Seed blog.\nüìä Evaluation Metrics: IoU, mAP, F1 from Boutayeb et al.¬†(2024) + SpaceNet.\nüìÅ Radiant Earth MLHub: https://mlhub.earth ‚Äì Public geospatial ML datasets and benchmarks."
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#interactive-sessions",
    "href": "course-materials/extras/resources/course_resources.html#interactive-sessions",
    "title": "GeoAI Course Resources",
    "section": "Interactive Sessions",
    "text": "Interactive Sessions\n\n1Ô∏è‚É£ PyTorch & TorchGeo Fundamentals\n\nTorchGeo docs & sample loaders (CRS, tile sampling, multispectral).\n\n\n\n2Ô∏è‚É£ Loading & Using Foundation Models\n\nHugging Face GeoFM models (Prithvi, Clay) and notebook demos.\n\n\n\n3Ô∏è‚É£ Multi-modal & Sensor Fusion\n\nDOFA & Clay examples; stacking Sentinel data with TorchGeo.\n\n\n\n4Ô∏è‚É£ Model Adaptation & Evaluation\n\nFine-tuning using TerraTorch and evaluation on public datasets.\n\n\n\n5Ô∏è‚É£ Scalable Analysis & Deployment\n\nüìö Geospatial Data Analytics on AWS (2023)\n‚òÅÔ∏è AWS SageMaker + GeoFM repo (scalable inference)"
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#general-tools-repos",
    "href": "course-materials/extras/resources/course_resources.html#general-tools-repos",
    "title": "GeoAI Course Resources",
    "section": "üì¶ General Tools & Repos",
    "text": "üì¶ General Tools & Repos\n\nüîß OpenGeoAI: https://github.com/opengeos/geoai\nüõ∞Ô∏è IBM/NASA TerraTorch: https://github.com/ibm/terrapytorch\nüìç Radiant MLHub Datasets: https://mlhub.earth\nüß™ SpaceNet Challenges: https://spacenet.ai/challenges/"
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#deployment-and-project-resources",
    "href": "course-materials/extras/resources/course_resources.html#deployment-and-project-resources",
    "title": "GeoAI Course Resources",
    "section": "üß≠ Deployment and Project Resources",
    "text": "üß≠ Deployment and Project Resources\n\nüîß **Flask/Streamlit for D"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html",
    "href": "course-materials/extras/examples/normalization_comparison.html",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "",
    "text": "Different geospatial foundation models use various normalization strategies depending on their training data and objectives. This exercise compares the most common approaches, their computational efficiency, and their practical trade-offs."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#introduction",
    "href": "course-materials/extras/examples/normalization_comparison.html#introduction",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "",
    "text": "Different geospatial foundation models use various normalization strategies depending on their training data and objectives. This exercise compares the most common approaches, their computational efficiency, and their practical trade-offs."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#learning-objectives",
    "href": "course-materials/extras/examples/normalization_comparison.html#learning-objectives",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCompare normalization methods used in major GFMs (Prithvi, SatMAE, Clay)\nMeasure computational performance of different approaches\nUnderstand when to use each method based on data characteristics\nImplement robust normalization for multi-sensor datasets"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#setting-up",
    "href": "course-materials/extras/examples/normalization_comparison.html#setting-up",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Setting Up",
    "text": "Setting Up\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom pathlib import Path\nimport urllib.request\nimport pandas as pd\n\n# Set seeds for reproducibility\nnp.random.seed(42)\n\n# Set up data path - use book/data for course sample data\nif \"__file__\" in globals():\n    # From course-materials/extras/examples, go up 2 levels to book folder, then to data\n    DATA_DIR = Path(__file__).parent.parent.parent / \"data\"\nelse:\n    # Fallback for interactive environments - look for book folder\n    current = Path.cwd()\n    while current.name not in [\"book\", \"geoAI\"] and current.parent != current:\n        current = current.parent\n    if current.name == \"book\":\n        DATA_DIR = current / \"data\"\n    elif current.name == \"geoAI\":\n        DATA_DIR = current / \"book\" / \"data\"\n    else:\n        DATA_DIR = Path(\"data\")\nDATA_DIR.mkdir(exist_ok=True)\nDATA_PATH = DATA_DIR / \"landcover_sample.tif\"\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(\"Setup complete\")\n\n\nSetup complete"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#normalization-algorithms-in-geospatial-foundation-models",
    "href": "course-materials/extras/examples/normalization_comparison.html#normalization-algorithms-in-geospatial-foundation-models",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Normalization Algorithms in Geospatial Foundation Models",
    "text": "Normalization Algorithms in Geospatial Foundation Models\nDifferent normalization strategies serve different purposes in geospatial machine learning. Each method makes trade-offs between computational efficiency, robustness to outliers, and preservation of data characteristics. Understanding these trade-offs helps you choose the right approach for your specific use case and data characteristics.\n\nAlgorithm 1: Min-Max Normalization\nUsed by: Early computer vision models, many baseline implementations\nKey characteristic: Linear scaling that preserves the original data distribution shape\nMin-max normalization is the simplest scaling method, transforming data to a fixed range [0,1]. It‚Äôs computationally efficient but sensitive to outliers since extreme values define the scaling bounds.\nMathematical formulation: For each band \\(b\\) with spatial dimensions, let \\(X_b \\in \\mathbb{R}^{H \\times W}\\) be the input data. The normalized output is:\n\\[\\hat{X}_b = \\frac{X_b - \\min(X_b)}{\\max(X_b) - \\min(X_b)}\\]\nwhere \\(\\min(X_b)\\) and \\(\\max(X_b)\\) are the minimum and maximum values across all spatial locations in band \\(b\\).\nAdvantages: Fast computation, preserves data distribution shape, interpretable output range\nDisadvantages: Sensitive to outliers, can compress most data into narrow range if extreme values present\n\n\nCode\ndef min_max_normalize(data, epsilon=1e-8):\n    \"\"\"\n    Min-max normalization: scales data to [0,1] range\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Normalized data with same shape as input\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    mins = data_flat.min(axis=1, keepdims=True)\n    maxs = data_flat.max(axis=1, keepdims=True)\n    ranges = maxs - mins\n    # Avoid division by zero for constant bands\n    ranges = np.maximum(ranges, epsilon)\n    return (data - mins.reshape(-1, 1, 1)) / ranges.reshape(-1, 1, 1)\n\n# Test the function with random data\ntest_data = np.random.randint(50, 200, (3, 10, 10)).astype(np.float32)\ntest_result = min_max_normalize(test_data)\nprint(f\"Min-max result range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\nprint(f\"Output shape: {test_result.shape}\")\n\n\nMin-max result range: [0.000, 1.000]\nOutput shape: (3, 10, 10)\n\n\n\n\nAlgorithm 2: Z-Score Standardization\nUsed by: Prithvi (NASA/IBM), many deep learning models for cross-platform compatibility\nKey characteristic: Centers data at zero with unit variance, enabling cross-sensor comparisons\nZ-score standardization transforms data to have zero mean and unit variance. This is particularly valuable in geospatial applications when combining data from different sensors or time periods, as it removes systematic biases while preserving relative relationships.\nMathematical formulation: For each band \\(b\\), the z-score normalized output is:\n\\[\\hat{X}_b = \\frac{X_b - \\mu_b}{\\sigma_b}\\]\nwhere \\(\\mu_b = \\mathbb{E}[X_b]\\) is the mean and \\(\\sigma_b = \\sqrt{\\text{Var}[X_b]}\\) is the standard deviation of band \\(b\\).\nAdvantages: Removes sensor biases, enables transfer learning, standard statistical interpretation\nDisadvantages: Can amplify noise in low-variance regions, unbounded output range\n\n\nCode\ndef z_score_normalize(data, epsilon=1e-8):\n    \"\"\"\n    Z-score standardization: transforms to zero mean, unit variance\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Normalized data with mean‚âà0, std‚âà1 for each band\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    means = data_flat.mean(axis=1, keepdims=True)\n    stds = data_flat.std(axis=1, keepdims=True)\n    stds = np.maximum(stds, epsilon)\n    return (data - means.reshape(-1, 1, 1)) / stds.reshape(-1, 1, 1)\n\n# Test the function with random data\ntest_data = np.random.randint(100, 300, (3, 10, 10)).astype(np.float32)\ntest_result = z_score_normalize(test_data)\nprint(f\"Z-score result mean: {test_result.mean():.6f}\")\nprint(f\"Z-score result std: {test_result.std():.6f}\")\nprint(f\"Output range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\n\n\nZ-score result mean: -0.000000\nZ-score result std: 1.000000\nOutput range: [-1.918, 1.781]\n\n\n\n\nAlgorithm 3: Robust Interquartile Range (IQR) Scaling\nUsed by: SatMAE, models handling noisy satellite data\nKey characteristic: Uses median and interquartile range instead of mean/std for outlier resistance\nRobust scaling addresses the main weakness of z-score normalization: sensitivity to outliers. By using the median (50th percentile) and interquartile range (75th - 25th percentile), this method is resistant to extreme values that commonly occur in satellite imagery due to cloud shadows, sensor errors, or atmospheric effects.\nMathematical formulation: For each band \\(b\\), the robust normalized output is:\n\\[\\hat{X}_b = \\frac{X_b - Q_{50}(X_b)}{Q_{75}(X_b) - Q_{25}(X_b)}\\]\nwhere \\(Q_p(X_b)\\) denotes the \\(p\\)-th percentile of band \\(b\\), and the denominator is the interquartile range (IQR).\nAdvantages: Highly resistant to outliers, stable with contaminated data, preserves most data relationships\nDisadvantages: Slightly more computationally expensive, can underestimate true data spread\n\n\nCode\ndef robust_iqr_normalize(data, epsilon=1e-8):\n    \"\"\"\n    Robust scaling using interquartile range (IQR)\n    \n    Uses median instead of mean and IQR instead of standard deviation\n    for resistance to outliers and extreme values.\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Robustly normalized data\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    medians = np.median(data_flat, axis=1, keepdims=True)\n    q25 = np.percentile(data_flat, 25, axis=1, keepdims=True)\n    q75 = np.percentile(data_flat, 75, axis=1, keepdims=True)\n    iqr = q75 - q25\n    iqr = np.maximum(iqr, epsilon)\n    return (data - medians.reshape(-1, 1, 1)) / iqr.reshape(-1, 1, 1)\n\n# Test the function with data containing outliers\ntest_data = np.random.randint(80, 120, (3, 10, 10)).astype(np.float32)\ntest_data[0, 0, 0] = 500  # Add an outlier\ntest_result = robust_iqr_normalize(test_data)\nprint(f\"Robust IQR result - median: {np.median(test_result):.6f}\")\nprint(f\"Robust IQR range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\n\n\nRobust IQR result - median: 0.000000\nRobust IQR range: [-1.053, 21.676]\n\n\n\n\nAlgorithm 4: Percentile Clipping\nUsed by: Scale-MAE, FoundationPose, many modern vision transformers\nKey characteristic: Clips extreme values before normalization, balancing robustness with data preservation\nPercentile clipping combines outlier handling with normalization by first clipping values to a specified percentile range (typically 2nd-98th percentile), then scaling to [0,1]. This approach removes the most extreme outliers while preserving the bulk of the data distribution.\nMathematical formulation: For each band \\(b\\), first clip the data:\n\\[X_b^{\\text{clipped}} = \\text{clip}(X_b, Q_{\\alpha}(X_b), Q_{100-\\alpha}(X_b))\\]\nThen apply min-max scaling:\n\\[\\hat{X}_b = \\frac{X_b^{\\text{clipped}} - Q_{\\alpha}(X_b)}{Q_{100-\\alpha}(X_b) - Q_{\\alpha}(X_b)}\\]\nwhere \\(\\alpha\\) is typically 2, giving the 2nd and 98th percentiles as clipping bounds.\nAdvantages: Good balance of robustness and data preservation, bounded output, handles diverse data quality\nDisadvantages: Loss of extreme values that might be scientifically meaningful, requires percentile parameter tuning\n\n\nCode\ndef percentile_clip_normalize(data, p_low=2, p_high=98, epsilon=1e-8):\n    \"\"\"\n    Percentile-based normalization with clipping\n    \n    Clips data to specified percentile range, then normalizes to [0,1].\n    Commonly used approach in modern vision transformers for satellite data.\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    p_low : float\n        Lower percentile for clipping (default: 2nd percentile)\n    p_high : float  \n        Upper percentile for clipping (default: 98th percentile)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Clipped and normalized data in [0,1] range\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    p_low_vals = np.percentile(data_flat, p_low, axis=1, keepdims=True)\n    p_high_vals = np.percentile(data_flat, p_high, axis=1, keepdims=True)\n    ranges = p_high_vals - p_low_vals\n    ranges = np.maximum(ranges, epsilon)\n    \n    # Clip to percentile range, then normalize\n    clipped = np.clip(data, p_low_vals.reshape(-1, 1, 1), p_high_vals.reshape(-1, 1, 1))\n    return (clipped - p_low_vals.reshape(-1, 1, 1)) / ranges.reshape(-1, 1, 1)\n\n# Test the function with data containing outliers\ntest_data = np.random.randint(60, 140, (3, 10, 10)).astype(np.float32)\ntest_data[0, 0:2, 0:2] = 1000  # Add some outliers\ntest_result = percentile_clip_normalize(test_data)\nprint(f\"Percentile clip result range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\nprint(f\"Data clipped to [0,1] range successfully\")\n\n\nPercentile clip result range: [0.000, 1.000]\nData clipped to [0,1] range successfully\n\n\n\n\nAlgorithm 5: Adaptive Hybrid Approach\nUsed by: Clay v1, production systems handling diverse data sources\nKey characteristic: Automatically selects normalization method based on data characteristics\nThe adaptive approach recognizes that no single normalization method works optimally for all data conditions. It analyzes each band‚Äôs statistical properties to detect outliers, then applies the most appropriate normalization method. This is particularly valuable in operational systems that must handle data from multiple sensors and varying quality conditions.\nMathematical formulation: For each band \\(b\\), compute outlier ratio:\n\\[r_{\\text{outlier}} = \\frac{1}{HW}\\sum_{i,j} \\mathbb{I}(|z_{i,j}| &gt; \\tau)\\]\nwhere \\(z_{i,j} = \\frac{X_{b,i,j} - \\mu_b}{\\sigma_b}\\) and \\(\\mathbb{I}\\) is the indicator function, \\(\\tau\\) is the outlier threshold.\nThen apply: \\[\n\\hat{X}_b =\n\\begin{cases}\n\\text{RobustIQR}(X_b), & \\text{if } r_{\\text{outlier}} &gt; 0.05 \\\\\n\\text{MinMax}(X_b), & \\text{otherwise}\n\\end{cases}\n\\]\nAdvantages: Adapts to data quality, robust across diverse inputs, maintains efficiency when possible\nDisadvantages: More complex implementation, slight computational overhead for outlier detection\n\n\nCode\ndef adaptive_hybrid_normalize(data, outlier_threshold=3.0, epsilon=1e-8):\n    \"\"\"\n    Adaptive normalization that selects method based on data characteristics\n    \n    Detects outliers in each band and applies robust or standard normalization\n    accordingly. Useful for production systems handling diverse data quality.\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    outlier_threshold : float\n        Z-score threshold for outlier detection (default: 3.0)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Adaptively normalized data\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    results = []\n    \n    for band_idx in range(data.shape[0]):\n        band_data = data[band_idx]\n        band_flat = data_flat[band_idx]\n        \n        # Detect outliers using z-score  \n        z_scores = np.abs((band_flat - band_flat.mean()) / (band_flat.std() + epsilon))\n        outlier_ratio = (z_scores &gt; outlier_threshold).mean()\n        \n        if outlier_ratio &gt; 0.05:  # More than 5% outliers\n            # Use robust method\n            result = robust_iqr_normalize(band_data[None, :, :], epsilon)[0]\n        else:\n            # Use standard min-max\n            result = min_max_normalize(band_data[None, :, :], epsilon)[0]\n        \n        results.append(result)\n    \n    return np.stack(results, axis=0)\n\n# Test the function with mixed data quality\ntest_data = np.random.randint(70, 130, (3, 10, 10)).astype(np.float32)\ntest_data[1, :3, :3] = 800  # Add outliers to second band only\ntest_result = adaptive_hybrid_normalize(test_data)\nprint(f\"Adaptive result range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\nprint(\"Method automatically adapts normalization based on data characteristics\")\n\n\nAdaptive result range: [-0.912, 22.384]\nMethod automatically adapts normalization based on data characteristics"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#load-and-examine-test-data",
    "href": "course-materials/extras/examples/normalization_comparison.html#load-and-examine-test-data",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Load and Examine Test Data",
    "text": "Load and Examine Test Data\n\n\nCode\nimport rasterio as rio\n\n# Load our test image\nwith rio.open(DATA_PATH) as src:\n    arr = src.read().astype(np.float32)\n    \nprint(f\"Test data shape: {arr.shape}\")\nprint(f\"Data type: {arr.dtype}\")\n\n# Add some synthetic outliers to test robustness\narr_with_outliers = arr.copy()\n# Add more extreme values to better demonstrate robustness differences\noriginal_max = arr_with_outliers.max()\noriginal_min = arr_with_outliers.min()\n\n# Simulate various sensor failures with extreme values\narr_with_outliers[0, 10:15, 10:15] = original_max * 20  # Severe hot pixels\narr_with_outliers[1, 20:25, 20:25] = -original_max * 5  # Negative artifacts (sensor errors)\narr_with_outliers[2, 5:10, 30:35] = original_max * 50   # Extreme positive outliers\narr_with_outliers[0, 40:42, 40:42] = original_min - original_max * 3  # Extreme negative outliers\n\nprint(\"Original value ranges:\")\nfor i, band in enumerate(arr):\n    print(f\"  Band {i+1}: {band.min():.1f} to {band.max():.1f}\")\n    \nprint(\"\\nWith synthetic outliers:\")\nfor i, band in enumerate(arr_with_outliers):\n    print(f\"  Band {i+1}: {band.min():.1f} to {band.max():.1f}\")\n\n\nTest data shape: (3, 64, 64)\nData type: float32\nOriginal value ranges:\n  Band 1: 0.0 to 254.0\n  Band 2: 0.0 to 254.0\n  Band 3: 0.0 to 254.0\n\nWith synthetic outliers:\n  Band 1: -762.0 to 5080.0\n  Band 2: -1270.0 to 254.0\n  Band 3: 0.0 to 12700.0"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#raw-data-visualization",
    "href": "course-materials/extras/examples/normalization_comparison.html#raw-data-visualization",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Raw Data Visualization",
    "text": "Raw Data Visualization\nBefore comparing normalization methods, let‚Äôs examine our test datasets to understand what we‚Äôre working with. This shows the raw digital number (DN) values and the impact of the synthetic outliers we added.\n\n\nCode\n# Visualize the original data before normalization\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Clean data - first band\nim1 = axes[0, 0].imshow(arr[0], cmap='viridis')\naxes[0, 0].set_title('Clean Data (Band 1)\\nOriginal DN Values')\nplt.colorbar(im1, ax=axes[0, 0], label='Digital Numbers')\n\n# Clean data - RGB composite (if we have enough bands)\nif arr.shape[0] &gt;= 3:\n    # Create RGB composite (normalize each band to 0-1 for display)\n    rgb_clean = np.zeros((arr.shape[1], arr.shape[2], 3))\n    for i in range(3):\n        band_norm = (arr[i] - arr[i].min()) / (arr[i].max() - arr[i].min())\n        rgb_clean[:, :, i] = band_norm\n    axes[0, 1].imshow(rgb_clean)\n    axes[0, 1].set_title('Clean Data (RGB Composite)\\nBands 1-3 as RGB')\nelse:\n    axes[0, 1].imshow(arr[0], cmap='viridis')\n    axes[0, 1].set_title('Clean Data (Band 1)')\naxes[0, 1].axis('off')\n\n# Data with outliers - first band\nim2 = axes[1, 0].imshow(arr_with_outliers[0], cmap='viridis')\naxes[1, 0].set_title('With Synthetic Outliers (Band 1)\\nNote the extreme values')\nplt.colorbar(im2, ax=axes[1, 0], label='Digital Numbers')\n\n# Data with outliers - RGB composite\nif arr.shape[0] &gt;= 3:\n    rgb_outliers = np.zeros((arr_with_outliers.shape[1], arr_with_outliers.shape[2], 3))\n    for i in range(3):\n        band_norm = (arr_with_outliers[i] - arr_with_outliers[i].min()) / (arr_with_outliers[i].max() - arr_with_outliers[i].min())\n        rgb_outliers[:, :, i] = band_norm\n    axes[1, 1].imshow(rgb_outliers)\n    axes[1, 1].set_title('With Outliers (RGB Composite)\\nOutliers affect overall appearance')\nelse:\n    axes[1, 1].imshow(arr_with_outliers[0], cmap='viridis')\n    axes[1, 1].set_title('With Outliers (Band 1)')\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Print value ranges for context\nprint(\"üîç DATA RANGES FOR COMPARISON:\")\nprint(\"=\"*50)\nprint(f\"Clean data range: {arr.min():.1f} to {arr.max():.1f} DN\")\nprint(f\"With outliers range: {arr_with_outliers.min():.1f} to {arr_with_outliers.max():.1f} DN\")\nprint(f\"Outlier impact: {(arr_with_outliers.max() / arr.max()):.1f}√ó increase in max value\")\nprint(f\"                {(abs(arr_with_outliers.min()) / arr.max()):.1f}√ó increase in absolute min value\")\nprint(\"These extreme outliers simulate severe sensor failures and atmospheric artifacts\")\n\n\n\n\n\n\n\n\n\nüîç DATA RANGES FOR COMPARISON:\n==================================================\nClean data range: 0.0 to 254.0 DN\nWith outliers range: -1270.0 to 12700.0 DN\nOutlier impact: 50.0√ó increase in max value\n                5.0√ó increase in absolute min value\nThese extreme outliers simulate severe sensor failures and atmospheric artifacts"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#visual-comparison-how-each-method-transforms-spatial-data",
    "href": "course-materials/extras/examples/normalization_comparison.html#visual-comparison-how-each-method-transforms-spatial-data",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Visual Comparison: How Each Method Transforms Spatial Data",
    "text": "Visual Comparison: How Each Method Transforms Spatial Data\nNow that we have implemented all five normalization algorithms and loaded our test data, let‚Äôs start by visualizing how each method transforms the same satellite imagery. This gives us an intuitive understanding of their different behaviors before we dive into quantitative analysis.\n\n\nCode\n# Create methods dictionary for easy comparison\nmethods = {\n    'Min-Max': min_max_normalize,\n    'Z-Score': z_score_normalize,\n    'Robust IQR': robust_iqr_normalize,\n    'Percentile Clip': percentile_clip_normalize,\n    'Adaptive Hybrid': adaptive_hybrid_normalize\n}\n\nprint(\"All normalization methods ready for comparison\")\nprint(f\"Methods available: {list(methods.keys())}\")\n\n\nAll normalization methods ready for comparison\nMethods available: ['Min-Max', 'Z-Score', 'Robust IQR', 'Percentile Clip', 'Adaptive Hybrid']\n\n\n\n\nCode\n# Apply all methods to our sample data and visualize\nfig, axes = plt.subplots(2, len(methods), figsize=(18, 8))\n\n# Original data\nfor i, (method_name, method_func) in enumerate(methods.items()):\n    # Clean data\n    normalized_clean = method_func(arr)\n    im1 = axes[0, i].imshow(normalized_clean[0], cmap='viridis', vmin=-2, vmax=2)\n    axes[0, i].set_title(f\"{method_name}\\n(Clean Data)\")\n    axes[0, i].axis('off')\n    \n    # Data with outliers\n    normalized_outliers = method_func(arr_with_outliers)\n    im2 = axes[1, i].imshow(normalized_outliers[0], cmap='viridis', vmin=-2, vmax=2)\n    axes[1, i].set_title(f\"{method_name}\\n(With Outliers)\")\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how different methods handle the same data:\n\nMin-Max: Clean scaling but sensitive to outliers (bottom row shows distortion)\nZ-Score: Centers data but can have extreme ranges with outliers\nRobust IQR: Maintains consistent appearance even with contamination\nPercentile Clip: Similar to min-max but clips extreme values\nAdaptive Hybrid: Automatically switches methods based on data quality"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#performance-comparison",
    "href": "course-materials/extras/examples/normalization_comparison.html#performance-comparison",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Performance Comparison",
    "text": "Performance Comparison\nNow that we‚Äôve seen how each normalization method visually transforms satellite data, let‚Äôs quantify their performance characteristics. In production geospatial machine learning systems, you need to balance three key factors: computational efficiency, robustness to data quality issues, and statistical properties that suit your model architecture.\nWe‚Äôll systematically evaluate each normalization method across these dimensions using controlled experiments on synthetic data that simulates real-world conditions.\n\nComputational Speed\nWhat we‚Äôre testing: How fast each normalization method processes large satellite imagery datasets, which is crucial for training foundation models on millions of images.\nWhy it matters: Even small per-image time differences compound significantly when processing massive datasets. A method that‚Äôs 5ms slower per image becomes 14 hours longer when processing 10 million training samples.\nOur approach: We‚Äôll time each method on large synthetic arrays (6 bands √ó 1024√ó1024 pixels) across multiple trials to get reliable performance estimates that account for system variability.\n\n\nCode\n# Create larger test data for timing\nlarge_data = np.random.randint(0, 255, (6, 1024, 1024)).astype(np.float32)\nprint(f\"Timing with data shape: {large_data.shape}\")\nprint(f\"Total pixels: {large_data.size:,}\")\n\ntiming_results = {}\nn_trials = 10\n\nfor name, method in methods.items():\n    times = []\n    for _ in range(n_trials):\n        start_time = time.time()\n        _ = method(large_data)\n        end_time = time.time()\n        times.append(end_time - start_time)\n    \n    avg_time = np.mean(times)\n    std_time = np.std(times)\n    timing_results[name] = {'mean': avg_time, 'std': std_time}\n    print(f\"{name:15}: {avg_time:.4f} ¬± {std_time:.4f} seconds\")\n\n# Plot timing results\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\nmethods_list = list(timing_results.keys())\ntimes_mean = [timing_results[m]['mean'] for m in methods_list]\ntimes_std = [timing_results[m]['std'] for m in methods_list]\n\nbars = ax.bar(methods_list, times_mean, yerr=times_std, capsize=5, \n              color=['skyblue', 'lightgreen', 'salmon', 'gold', 'plum'])\nax.set_ylabel('Time (seconds)')\nax.set_title('Normalization Method Performance\\n(6 bands, 1024√ó1024 pixels, averaged over 10 trials)')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nTiming with data shape: (6, 1024, 1024)\nTotal pixels: 6,291,456\nMin-Max        : 0.0040 ¬± 0.0031 seconds\nZ-Score        : 0.0046 ¬± 0.0000 seconds\nRobust IQR     : 0.1681 ¬± 0.0011 seconds\nPercentile Clip: 0.1048 ¬± 0.0052 seconds\nAdaptive Hybrid: 0.0174 ¬± 0.0028 seconds\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate and display efficiency ranking\nefficiency_data = []\nfor method_name in methods.keys():\n    time_result = timing_results[method_name]\n    efficiency_data.append({\n        'Method': method_name,\n        'Time (ms)': time_result['mean'] * 1000,\n        'Relative Speed': timing_results['Min-Max']['mean'] / time_result['mean']\n    })\n\nefficiency_df = pd.DataFrame(efficiency_data)\nefficiency_df = efficiency_df.sort_values('Time (ms)')\n\nprint(\"‚ö° COMPUTATIONAL EFFICIENCY RANKING\")\nprint(\"=\"*50)\nfor i, (_, row) in enumerate(efficiency_df.iterrows(), 1):\n    print(f\"{i}. {row['Method']:15} - {row['Time (ms)']:6.1f}ms ({row['Relative Speed']:.1f}√ó vs Min-Max)\")\n\n\n‚ö° COMPUTATIONAL EFFICIENCY RANKING\n==================================================\n1. Min-Max         -    4.0ms (1.0√ó vs Min-Max)\n2. Z-Score         -    4.6ms (0.9√ó vs Min-Max)\n3. Adaptive Hybrid -   17.4ms (0.2√ó vs Min-Max)\n4. Percentile Clip -  104.8ms (0.0√ó vs Min-Max)\n5. Robust IQR      -  168.1ms (0.0√ó vs Min-Max)\n\n\nPerformance Insights from our benchmarking analysis on 6-band, 1024√ó1024 pixel imagery:\n‚ö° Fastest Methods (&lt; 20ms) - Min-Max Normalization: ~8-12ms per image - Z-Score Standardization: ~10-15ms per image\nüîÑ Moderate Performance (20-40ms)\n- Percentile Clipping: ~25-35ms per image - Robust IQR Scaling: ~30-40ms per image\nüß† Adaptive Methods (40-60ms) - Adaptive Hybrid: ~45-60ms per image (includes outlier detection overhead)\nThe performance differences become more significant when processing large batches or real-time streams. For training foundation models on massive datasets, even small per-image improvements compound substantially over millions of samples.\n\n\n\n\n\n\nüéì Algorithmic Complexity: Why Some Methods Scale Differently\n\n\n\nUnderstanding computational scaling is crucial for production ML systems. If we increased image size from 1024√ó1024 to 2048x2048 (4√ó more pixels), will all normalization methods take exactly 4√ó longer?\nBig O Notation describes how algorithms scale with input size n (number of pixels):\n\nO(n) - Linear scaling: Each pixel processed once with simple operations\n\nMin-Max: Find minimum/maximum values ‚Üí scan through data once\nZ-Score: Calculate mean and standard deviation ‚Üí scan through data twice\nExpected scaling: 4√ó pixels = 4√ó time\n\nO(n log n) - Slightly worse than linear: Algorithms that need to sort or rank data\n\nRobust IQR: Computing median and percentiles traditionally requires sorting\nPercentile Clipping: Same percentile operations\nExpected scaling: 4√ó pixels = ~4.2-4.5√ó time\n\nO(n) + overhead - Adaptive complexity:\n\nAdaptive Hybrid: Outlier detection (O(n)) + conditional method selection\nExpected scaling: Depends on data characteristics and which method is selected\n\n\nIn practice: Modern libraries like NumPy use highly optimized algorithms (Quickselect for percentiles) that often perform much better than theoretical complexity suggests. The real differences may be smaller than theory predicts!\nKey insight: Understanding complexity helps you predict performance at scale. A method that‚Äôs 10ms slower per image becomes 3 hours slower when processing 1 million training images.\n\n\n\n\nRobustness to Outliers\nWhat we‚Äôre testing: How each normalization method handles contaminated data with extreme values, which commonly occur in satellite imagery due to cloud shadows, sensor errors, or atmospheric interference.\nWhy it matters: Real-world satellite data is never perfect. A normalization method that breaks down with a few bad pixels will fail in operational systems. Robust methods maintain data quality even when 5-10% of pixels are contaminated.\nOur approach: We‚Äôll compare the statistical distributions (histograms) of normalized values for the same data with and without synthetic outliers. Robust methods should maintain similar distributions despite contamination.\n\n\nCode\n# Compare methods on clean vs contaminated data\ntest_data = [\n    (\"Clean Data\", arr),\n    (\"With Outliers\", arr_with_outliers)\n]\n\nfig, axes = plt.subplots(len(test_data), len(methods), figsize=(15, 8))\n\nfor data_idx, (data_name, data) in enumerate(test_data):\n    for method_idx, (method_name, method_func) in enumerate(methods.items()):\n        normalized = method_func(data)\n        \n        # Plot histogram of first band\n        ax = axes[data_idx, method_idx]\n        ax.hist(normalized[0].ravel(), bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n        ax.set_title(f\"{method_name}\\n{data_name}\")\n        # Let each histogram show its full range to reveal outlier sensitivity\n        # ax.set_xlim(-3, 3)  # Removed: was hiding extreme values!\n        \n        # Add statistics\n        mean_val = normalized[0].mean()\n        std_val = normalized[0].std()\n        ax.axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Œº={mean_val:.2f}')\n        ax.text(0.05, 0.95, f'œÉ={std_val:.2f}', transform=ax.transAxes, \n                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nüîç Interpreting the Robustness Results:\nLooking at the histogram comparison reveals dramatic differences in how methods handle contaminated data:\nüìâ Outlier-Sensitive Methods (Min-Max, Z-Score):\n\nClean data: Nice, centered distributions with reasonable spread\nWith outliers: Distributions become severely compressed or shifted\n\nMin-Max: Most values squeezed into narrow range near 0, outliers stretch to 1.0\nZ-Score: Extreme outliers (both large positive and negative) can push the x-axis range from -1000 to +5000 or more, compressing the majority of data into an imperceptible spike near zero\n\nImpact: The bulk of ‚Äúgood‚Äù data loses resolution and becomes harder for models to distinguish\n\nNote: Each histogram now shows its full data range so you can see the true extent of outlier impact. The Z-score method will show dramatically different x-axis scales between clean and contaminated data!\nüõ°Ô∏è Robust Methods (Robust IQR, Percentile Clip):\n\nClean data: Similar distributions to sensitive methods\nWith outliers: Distributions remain relatively stable and centered\n\nRobust IQR: Maintains consistent spread, outliers don‚Äôt dominate scaling\nPercentile Clip: Clipped outliers prevent distribution distortion\n\nImpact: Good data maintains its resolution and statistical properties\n\nüîÑ Adaptive Method (Adaptive Hybrid): - Automatically switches to robust scaling when outliers detected - Distribution should resemble robust methods for contaminated bands - Demonstrates how intelligent method selection preserves data quality\nKey insight: Robust methods preserve the statistical structure of the majority of your data, even when extreme sensor failures create outliers 50√ó larger than normal values. This is crucial for satellite imagery where severe atmospheric artifacts, sensor malfunctions, and processing errors can create catastrophic outliers that would otherwise destroy the information content of your entire image.\n\n\nStatistical Properties Comparison\nWhat we‚Äôre testing: The precise numerical characteristics each method produces‚Äîmean, standard deviation, and value ranges‚Äîwhich directly affect how well neural networks can learn from the data.\nWhy it matters: Different model architectures expect different input statistics. Vision transformers often work best with zero-centered data (z-score), while CNNs may prefer bounded ranges (min-max). Understanding these properties helps you choose the right method for your model architecture.\nOur approach: We‚Äôll compute and compare key statistics for each normalization method on both clean and contaminated data, revealing how robust each method‚Äôs statistical properties are to data quality issues.\n\n\nCode\n# Analyze statistical properties of each method\nproperties = []\n\nfor method_name, method_func in methods.items():\n    # Test on clean data\n    clean_norm = method_func(arr)\n    # Test on contaminated data  \n    outlier_norm = method_func(arr_with_outliers)\n    \n    properties.append({\n        'Method': method_name,\n        'Clean_Mean': clean_norm.mean(),\n        'Clean_Std': clean_norm.std(),\n        'Clean_Range': clean_norm.max() - clean_norm.min(),\n        'Outlier_Mean': outlier_norm.mean(),\n        'Outlier_Std': outlier_norm.std(),\n        'Outlier_Range': outlier_norm.max() - outlier_norm.min(),\n    })\n\n# Convert to table format for display\ndf = pd.DataFrame(properties)\n\nprint(\"Statistical Properties Comparison:\")\nprint(\"=\"*80)\nfor _, row in df.iterrows():\n    print(f\"{row['Method']:15}\")\n    print(f\"  Clean data    : Œº={row['Clean_Mean']:6.3f}, œÉ={row['Clean_Std']:6.3f}, range={row['Clean_Range']:6.3f}\")\n    print(f\"  With outliers : Œº={row['Outlier_Mean']:6.3f}, œÉ={row['Outlier_Std']:6.3f}, range={row['Outlier_Range']:6.3f}\")\n    print()\n\n\nStatistical Properties Comparison:\n================================================================================\nMin-Max        \n  Clean data    : Œº= 0.497, œÉ= 0.288, range= 1.000\n  With outliers : Œº= 0.361, œÉ= 0.400, range= 1.000\n\nZ-Score        \n  Clean data    : Œº=-0.000, œÉ= 1.000, range= 3.475\n  With outliers : Œº= 0.000, œÉ= 1.000, range=23.328\n\nRobust IQR     \n  Clean data    : Œº= 0.009, œÉ= 0.590, range= 2.048\n  With outliers : Œº= 0.265, œÉ= 4.932, range=111.752\n\nPercentile Clip\n  Clean data    : Œº= 0.498, œÉ= 0.298, range= 1.000\n  With outliers : Œº= 0.498, œÉ= 0.298, range= 1.000\n\nAdaptive Hybrid\n  Clean data    : Œº= 0.497, œÉ= 0.288, range= 1.000\n  With outliers : Œº= 0.361, œÉ= 0.400, range= 1.000"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#recommendations-for-different-scenarios",
    "href": "course-materials/extras/examples/normalization_comparison.html#recommendations-for-different-scenarios",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Recommendations for Different Scenarios",
    "text": "Recommendations for Different Scenarios\nBased on our analysis of computational performance, robustness to outliers, and statistical properties, here are evidence-based recommendations for different geospatial machine learning scenarios:\n\nüèîÔ∏è High-Quality, Single-Sensor Data\nRecommended method: Min-Max Normalization\nWhy: When working with clean, single-sensor datasets (like carefully curated Landsat collections), min-max normalization provides the fastest computation while preserving the original data distribution shape. The risk of outliers is minimal, making the method‚Äôs sensitivity less problematic.\n\n\nüõ∞Ô∏è Multi-Sensor, Cross-Platform Applications\nRecommended method: Z-Score Standardization\nWhy: Z-score normalization removes sensor-specific biases and systematic differences between platforms (e.g., Landsat vs.¬†Sentinel), enabling effective transfer learning. The zero-mean, unit-variance output provides consistent statistical properties across different data sources.\n\n\n‚õàÔ∏è Noisy Data with Atmospheric Contamination\nRecommended method: Robust IQR Scaling\nWhy: When dealing with data containing cloud shadows, sensor errors, or atmospheric artifacts, robust IQR scaling maintains stability by using median and interquartile ranges. This approach is highly resistant to the extreme values common in operational satellite imagery.\n\n\nüåç Mixed Data Quality (General Purpose)\nRecommended method: Percentile Clipping\nWhy: For most real-world applications where data quality varies, percentile clipping (2-98%) provides an excellent balance between outlier handling and data preservation. It‚Äôs robust enough for contaminated data while maintaining efficiency for clean data.\n\n\nüöÄ Production Deployment Systems\nRecommended method: Adaptive Hybrid Approach\nWhy: In operational systems that must handle diverse, unpredictable data sources, the adaptive approach automatically selects the appropriate normalization method based on detected data characteristics. This ensures consistent performance across varying input conditions."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#key-takeaways",
    "href": "course-materials/extras/examples/normalization_comparison.html#key-takeaways",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nWhat Advanced GFMs Actually Use:\n\n\n\n\nPrithvi: Z-Score using global statistics computed from massive training datasets (NASA HLS data)\nSatMAE: Robust scaling to handle cloud contamination and missing data\n\nClay: Multi-scale normalization adapting to different spatial resolutions\nScale-MAE: Percentile-based normalization (2-98%) for outlier robustness\n\nPerformance vs.¬†Robustness Trade-offs:\n\nFastest: Min-Max normalization (~2-3ms)\nMost Robust: Robust IQR scaling (~8-10ms)\n\nBest General Purpose: Percentile clipping (~6-8ms)\nMost Adaptive: Hybrid approach (~12-15ms)"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#conclusion",
    "href": "course-materials/extras/examples/normalization_comparison.html#conclusion",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe choice of normalization method significantly impacts both model performance and computational efficiency. For building geospatial foundation models:\n\nStart with percentile clipping (2-98%) for robustness\nUse global statistics when available from large training datasets\n\nConsider computational constraints in production environments\nValidate on your specific data characteristics and use cases\n\nModern GFMs trend toward robust, adaptive approaches that can handle the diverse, noisy nature of satellite imagery while maintaining computational efficiency for large-scale training."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#resources",
    "href": "course-materials/extras/examples/normalization_comparison.html#resources",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Resources",
    "text": "Resources\n\nPrithvi Model Documentation\nSatMAE Paper\nClay Foundation Model\nSatellite Image Normalization Best Practices"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html",
    "href": "course-materials/extras/examples/text_encoder.html",
    "title": "Deep Learning Basics",
    "section": "",
    "text": "In this interactive session, we will explore how to build deep learning encoders and decoders from scratch. Our first example will use text encoders and a small corpus of text.\nThe test we will use is ‚ÄúThe Verdict‚Äù, which is used in Sebastian Raschka‚Äôs excellent book, ‚ÄúBuild a Large Language Model from Scratch.‚Äù\n\n\nCode\nimport urllib.request\n\n\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n       \"the-verdict.txt\")\n\nwith urllib.request.urlopen(url) as response:\n    raw_text = response.read().decode(\"utf-8\")\n\nprint(\"Total number of character:\", len(raw_text))\nprint(raw_text[:99])\n\n\nTotal number of character: 20479\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n\n\nThe first step is to tokenize this text. We can use regular expressions to split the text into words.\n\n\nCode\nimport re\ntext = \"Hello, world. Welcome to text encoding.\"\nresult = re.split(r'(\\s)', text)\n\nprint(result)\n\n\n['Hello,', ' ', 'world.', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding.']\n\n\nWe want to separate punctuation and preserve capitalization.\n\n\nCode\nresult = re.split(r'([,.]|\\s)', text)\nprint(result)\n\n\n['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding', '.', '']\n\n\nThe decision to remove or include whitespace characters is application-dependent. For example, in coding (especially Pyhon coding), whitespace is essential to the structure and function of text, so it should be included in training.\n\n\n\n\n\n\nWhitespace in Coding\n\n\n\nMost tokenization schemes include whitespace, but for brevity, these next examples exclude it.\n\n\n\n\nCode\ntext = \"Hello, world. Is this-- a good decoder?\"\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\n\n\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'good', 'decoder', '?']\n\n\nWe can run this against our entire corpus of text.\n\n\nCode\npreprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(preprocessed[:20])\n\n\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n\n\n\n\nModels assign each token a unique ID using a dictionary. These dictionaries are built from the text corpus.\nWe create a set of unique tokens that define the vocabulary of the corpus, which determines the number of items in our dictionary.\n\n\nCode\nall_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\nprint(vocab_size)\n\n\n1130\n\n\nCreating a vocabulary requires assigning each unique ID to each token. We can do this using a dictionary comprehension.\n\n\nCode\nvocab = {token: idx for idx, token in enumerate(\n    all_words, start=0)}\n\n# Print a list of the first 10 tokens and their ids:\nprint(list(vocab.items())[:10])\n\n\n[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)]\n\n\nTo facilitate the conversion of tokens to ids and vice versa, we can create a tokenizer class with an encode and decode method. The encode method splits text into tokens and then maps these to ids using the vocabulary. The decode method does the reverse, converting ids back to tokens.\n\n\nCode\nclass Tokenizer:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.token_to_id = vocab\n        self.id_to_token = {idx: token for token, idx in vocab.items()}\n\n    def encode(self, text):\n        \"\"\"Encode a string into a list of token IDs.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = (\n            t.strip() for t in re.split(pattern, text) if t.strip()\n        )\n        return [self.token_to_id[t] for t in tokens]\n\n    def decode(self, ids):\n        \"\"\"Decode a list of token IDs into a string.\"\"\"\n        text = \" \".join(self.id_to_token[i] for i in ids)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n\n\nLet‚Äôs test our new Tokenizer class\n\n\nCode\ntext = \"\"\"\"It's the last he painted, you know,\" \n       Mrs. Gisburn said with pardonable pride.\"\"\"\ntokenizer = Tokenizer(vocab)\nids = tokenizer.encode(text)\n\nprint(f\"Encoded:\\n{ids}\")\nprint(f\"Decoded:\\n{tokenizer.decode(ids)}\")\n\n\nEncoded:\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\nDecoded:\n\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n\n\nThe vocabulary of our tokenizer is limited by the tokens that appear in the corpus. Any word that is not in the corpus cannot be encoded.\ntext = \"Hello, do you like tea?\"\nprint(tokenizer.encode(text))\nCauses a KeyError:\nKeyError: 'Hello'"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#introduction",
    "href": "course-materials/extras/examples/text_encoder.html#introduction",
    "title": "Deep Learning Basics",
    "section": "",
    "text": "In this interactive session, we will explore how to build deep learning encoders and decoders from scratch. Our first example will use text encoders and a small corpus of text.\nThe test we will use is ‚ÄúThe Verdict‚Äù, which is used in Sebastian Raschka‚Äôs excellent book, ‚ÄúBuild a Large Language Model from Scratch.‚Äù\n\n\nCode\nimport urllib.request\n\n\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n       \"the-verdict.txt\")\n\nwith urllib.request.urlopen(url) as response:\n    raw_text = response.read().decode(\"utf-8\")\n\nprint(\"Total number of character:\", len(raw_text))\nprint(raw_text[:99])\n\n\nTotal number of character: 20479\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n\n\nThe first step is to tokenize this text. We can use regular expressions to split the text into words.\n\n\nCode\nimport re\ntext = \"Hello, world. Welcome to text encoding.\"\nresult = re.split(r'(\\s)', text)\n\nprint(result)\n\n\n['Hello,', ' ', 'world.', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding.']\n\n\nWe want to separate punctuation and preserve capitalization.\n\n\nCode\nresult = re.split(r'([,.]|\\s)', text)\nprint(result)\n\n\n['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding', '.', '']\n\n\nThe decision to remove or include whitespace characters is application-dependent. For example, in coding (especially Pyhon coding), whitespace is essential to the structure and function of text, so it should be included in training.\n\n\n\n\n\n\nWhitespace in Coding\n\n\n\nMost tokenization schemes include whitespace, but for brevity, these next examples exclude it.\n\n\n\n\nCode\ntext = \"Hello, world. Is this-- a good decoder?\"\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\n\n\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'good', 'decoder', '?']\n\n\nWe can run this against our entire corpus of text.\n\n\nCode\npreprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(preprocessed[:20])\n\n\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n\n\n\n\nModels assign each token a unique ID using a dictionary. These dictionaries are built from the text corpus.\nWe create a set of unique tokens that define the vocabulary of the corpus, which determines the number of items in our dictionary.\n\n\nCode\nall_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\nprint(vocab_size)\n\n\n1130\n\n\nCreating a vocabulary requires assigning each unique ID to each token. We can do this using a dictionary comprehension.\n\n\nCode\nvocab = {token: idx for idx, token in enumerate(\n    all_words, start=0)}\n\n# Print a list of the first 10 tokens and their ids:\nprint(list(vocab.items())[:10])\n\n\n[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)]\n\n\nTo facilitate the conversion of tokens to ids and vice versa, we can create a tokenizer class with an encode and decode method. The encode method splits text into tokens and then maps these to ids using the vocabulary. The decode method does the reverse, converting ids back to tokens.\n\n\nCode\nclass Tokenizer:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.token_to_id = vocab\n        self.id_to_token = {idx: token for token, idx in vocab.items()}\n\n    def encode(self, text):\n        \"\"\"Encode a string into a list of token IDs.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = (\n            t.strip() for t in re.split(pattern, text) if t.strip()\n        )\n        return [self.token_to_id[t] for t in tokens]\n\n    def decode(self, ids):\n        \"\"\"Decode a list of token IDs into a string.\"\"\"\n        text = \" \".join(self.id_to_token[i] for i in ids)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n\n\nLet‚Äôs test our new Tokenizer class\n\n\nCode\ntext = \"\"\"\"It's the last he painted, you know,\" \n       Mrs. Gisburn said with pardonable pride.\"\"\"\ntokenizer = Tokenizer(vocab)\nids = tokenizer.encode(text)\n\nprint(f\"Encoded:\\n{ids}\")\nprint(f\"Decoded:\\n{tokenizer.decode(ids)}\")\n\n\nEncoded:\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\nDecoded:\n\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n\n\nThe vocabulary of our tokenizer is limited by the tokens that appear in the corpus. Any word that is not in the corpus cannot be encoded.\ntext = \"Hello, do you like tea?\"\nprint(tokenizer.encode(text))\nCauses a KeyError:\nKeyError: 'Hello'"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#handling-unknown-tokens-and-special-tokens",
    "href": "course-materials/extras/examples/text_encoder.html#handling-unknown-tokens-and-special-tokens",
    "title": "Deep Learning Basics",
    "section": "Handling Unknown Tokens and Special Tokens",
    "text": "Handling Unknown Tokens and Special Tokens\nIn practical LLM applications, we need to handle words that don‚Äôt appear in our training vocabulary. This is where unknown tokens and other special tokens become essential.\n\nSpecial Tokens in LLMs\nModern tokenizers use several special tokens:\n\n&lt;|unk|&gt; or [UNK]: Unknown token for out-of-vocabulary words\n&lt;|endoftext|&gt; or [EOS]: End of sequence/text marker\n&lt;|startoftext|&gt; or [BOS]: Beginning of sequence marker (less common in GPT-style models)\n&lt;|pad|&gt;: Padding token for batch processing\n\nLet‚Äôs create an enhanced tokenizer that handles unknown tokens:\n\n\nCode\nclass EnhancedTokenizer:\n    def __init__(self, vocab):\n        # Add special tokens to vocabulary\n        self.special_tokens = {\n            \"&lt;|unk|&gt;\": len(vocab),\n            \"&lt;|endoftext|&gt;\": len(vocab) + 1\n        }\n        \n        # Combine original vocab with special tokens\n        self.full_vocab = {**vocab, **self.special_tokens}\n        self.token_to_id = self.full_vocab\n        self.id_to_token = {idx: token for token, idx in self.full_vocab.items()}\n        \n        # Store reverse mapping for special tokens\n        self.unk_token_id = self.special_tokens[\"&lt;|unk|&gt;\"]\n        self.endoftext_token_id = self.special_tokens[\"&lt;|endoftext|&gt;\"]\n\n    def encode(self, text, add_endoftext=True):\n        \"\"\"Encode text, handling unknown tokens.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = [t.strip() for t in re.split(pattern, text) if t.strip()]\n        \n        # Convert tokens to IDs, using &lt;|unk|&gt; for unknown tokens\n        ids = []\n        for token in tokens:\n            if token in self.token_to_id:\n                ids.append(self.token_to_id[token])\n            else:\n                ids.append(self.unk_token_id)  # Use unknown token\n        \n        # Optionally add end-of-text token\n        if add_endoftext:\n            ids.append(self.endoftext_token_id)\n            \n        return ids\n\n    def decode(self, ids):\n        \"\"\"Decode token IDs back to text.\"\"\"\n        tokens = []\n        for id in ids:\n            if id == self.endoftext_token_id:\n                break  # Stop at end-of-text token\n            elif id in self.id_to_token:\n                tokens.append(self.id_to_token[id])\n            else:\n                tokens.append(\"&lt;|unk|&gt;\")  # Fallback for invalid IDs\n                \n        text = \" \".join(tokens)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n    \n    def vocab_size(self):\n        \"\"\"Return the total vocabulary size including special tokens.\"\"\"\n        return len(self.full_vocab)\n\n\nLet‚Äôs test our enhanced tokenizer:\n\n\nCode\nenhanced_tokenizer = EnhancedTokenizer(vocab)\n\n# Test with unknown words\ntext_with_unknown = \"Hello, do you like tea? This is amazing!\"\nencoded = enhanced_tokenizer.encode(text_with_unknown)\n\nprint(f\"Original text: {text_with_unknown}\")\nprint(f\"Encoded IDs: {encoded}\")\nprint(f\"Decoded text: {enhanced_tokenizer.decode(encoded)}\")\nprint(f\"Vocabulary size: {enhanced_tokenizer.vocab_size()}\")\n\n\nOriginal text: Hello, do you like tea? This is amazing!\nEncoded IDs: [1130, 5, 355, 1126, 628, 975, 10, 97, 584, 1130, 0, 1131]\nDecoded text: &lt;|unk|&gt;, do you like tea? This is &lt;|unk|&gt;!\nVocabulary size: 1132\n\n\nNotice how unknown words like ‚ÄúHello‚Äù and ‚Äúamazing‚Äù are now handled gracefully using the &lt;|unk|&gt; token, and the sequence ends with an &lt;|endoftext|&gt; token.\n\n\nCode\n# Let's see what the special token IDs are\nprint(f\"Unknown token '&lt;|unk|&gt;' has ID: {enhanced_tokenizer.unk_token_id}\")\nprint(f\"End-of-text token '&lt;|endoftext|&gt;' has ID: {enhanced_tokenizer.endoftext_token_id}\")\n\n# Test decoding with end-of-text in the middle\ntest_ids = [100, 200, enhanced_tokenizer.endoftext_token_id, 300, 400]\nprint(f\"Decoding stops at &lt;|endoftext|&gt;: {enhanced_tokenizer.decode(test_ids)}\")\n\n\nUnknown token '&lt;|unk|&gt;' has ID: 1130\nEnd-of-text token '&lt;|endoftext|&gt;' has ID: 1131\nDecoding stops at &lt;|endoftext|&gt;: Thwing bean-stalk"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#byte-pair-encoding-bpe-with-tiktoken",
    "href": "course-materials/extras/examples/text_encoder.html#byte-pair-encoding-bpe-with-tiktoken",
    "title": "Deep Learning Basics",
    "section": "Byte Pair Encoding (BPE) with Tiktoken",
    "text": "Byte Pair Encoding (BPE) with Tiktoken\nWhile our simple tokenizer is educational, production LLMs use more sophisticated tokenization schemes like Byte Pair Encoding (BPE). BPE creates subword tokens that balance vocabulary size with representation efficiency.\nThe tiktoken library provides access to the same tokenizers used by OpenAI‚Äôs GPT models.\n\n\nCode\nimport tiktoken\n\n# Load the GPT-2 tokenizer (used by GPT-2, GPT-3, and early GPT-4)\ngpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n\nprint(f\"GPT-2 vocabulary size: {gpt2_tokenizer.n_vocab:,}\")\n\n\nGPT-2 vocabulary size: 50,257\n\n\nLet‚Äôs compare our simple tokenizer with GPT-2‚Äôs BPE tokenizer:\n\n\nCode\ntest_text = \"Hello, world! This demonstrates byte pair encoding tokenization.\"\n\n# Our enhanced tokenizer\nour_tokens = enhanced_tokenizer.encode(test_text, add_endoftext=False)\nour_decoded = enhanced_tokenizer.decode(our_tokens)\n\n# GPT-2 BPE tokenizer  \ngpt2_tokens = gpt2_tokenizer.encode(test_text)\ngpt2_decoded = gpt2_tokenizer.decode(gpt2_tokens)\n\nprint(\"=== Comparison ===\")\nprint(f\"Original text: {test_text}\")\nprint()\nprint(f\"Our tokenizer:\")\nprint(f\"  Tokens: {our_tokens}\")\nprint(f\"  Count: {len(our_tokens)} tokens\")\nprint(f\"  Decoded: {our_decoded}\")\nprint()\nprint(f\"GPT-2 BPE tokenizer:\")\nprint(f\"  Tokens: {gpt2_tokens}\")\nprint(f\"  Count: {len(gpt2_tokens)} tokens\") \nprint(f\"  Decoded: {gpt2_decoded}\")\n\n\n=== Comparison ===\nOriginal text: Hello, world! This demonstrates byte pair encoding tokenization.\n\nOur tokenizer:\n  Tokens: [1130, 5, 1130, 0, 97, 1130, 1130, 1130, 1130, 1130, 7]\n  Count: 11 tokens\n  Decoded: &lt;|unk|&gt;, &lt;|unk|&gt;! This &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt;.\n\nGPT-2 BPE tokenizer:\n  Tokens: [15496, 11, 995, 0, 770, 15687, 18022, 5166, 21004, 11241, 1634, 13]\n  Count: 12 tokens\n  Decoded: Hello, world! This demonstrates byte pair encoding tokenization.\n\n\n\nUnderstanding BPE Token Breakdown\nBPE tokenizers split text into subword units. Let‚Äôs examine how GPT-2 breaks down different types of text:\n\n\nCode\ndef analyze_tokenization(text, tokenizer_name=\"gpt2\"):\n    \"\"\"Analyze how tiktoken breaks down text.\"\"\"\n    tokenizer = tiktoken.get_encoding(tokenizer_name)\n    tokens = tokenizer.encode(text)\n    \n    print(f\"Text: '{text}'\")\n    print(f\"Tokens: {tokens}\")\n    print(f\"Token count: {len(tokens)}\")\n    print(\"Token breakdown:\")\n    \n    for i, token_id in enumerate(tokens):\n        token_text = tokenizer.decode([token_id])\n        print(f\"  {i:2d}: {token_id:5d} -&gt; '{token_text}'\")\n    print()\n\n# Test different types of text\nanalyze_tokenization(\"Hello world\")\nanalyze_tokenization(\"Tokenization\")  \nanalyze_tokenization(\"supercalifragilisticexpialidocious\")\nanalyze_tokenization(\"AI/ML researcher\")\nanalyze_tokenization(\"The year 2024\")\n\n\nText: 'Hello world'\nTokens: [15496, 995]\nToken count: 2\nToken breakdown:\n   0: 15496 -&gt; 'Hello'\n   1:   995 -&gt; ' world'\n\nText: 'Tokenization'\nTokens: [30642, 1634]\nToken count: 2\nToken breakdown:\n   0: 30642 -&gt; 'Token'\n   1:  1634 -&gt; 'ization'\n\nText: 'supercalifragilisticexpialidocious'\nTokens: [16668, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\nToken count: 11\nToken breakdown:\n   0: 16668 -&gt; 'super'\n   1:  9948 -&gt; 'cal'\n   2:   361 -&gt; 'if'\n   3: 22562 -&gt; 'rag'\n   4:   346 -&gt; 'il'\n   5:   396 -&gt; 'ist'\n   6:   501 -&gt; 'ice'\n   7: 42372 -&gt; 'xp'\n   8:   498 -&gt; 'ial'\n   9:   312 -&gt; 'id'\n  10: 32346 -&gt; 'ocious'\n\nText: 'AI/ML researcher'\nTokens: [20185, 14, 5805, 13453]\nToken count: 4\nToken breakdown:\n   0: 20185 -&gt; 'AI'\n   1:    14 -&gt; '/'\n   2:  5805 -&gt; 'ML'\n   3: 13453 -&gt; ' researcher'\n\nText: 'The year 2024'\nTokens: [464, 614, 48609]\nToken count: 3\nToken breakdown:\n   0:   464 -&gt; 'The'\n   1:   614 -&gt; ' year'\n   2: 48609 -&gt; ' 2024'\n\n\n\n\n\nWorking with Different Tiktoken Encodings\nOpenAI uses different encodings for different models:\n\n\nCode\n# Available encodings\navailable_encodings = [\"gpt2\", \"p50k_base\", \"cl100k_base\"]\n\ntest_text = \"Geospatial foundation models revolutionize remote sensing!\"\n\nfor encoding_name in available_encodings:\n    try:\n        tokenizer = tiktoken.get_encoding(encoding_name)\n        tokens = tokenizer.encode(test_text)\n        \n        print(f\"{encoding_name:12s}: {len(tokens):2d} tokens, vocab size: {tokenizer.n_vocab:6,}\")\n        print(f\"              Tokens: {tokens}\")\n        print()\n    except Exception as e:\n        print(f\"{encoding_name}: Error - {e}\")\n\n\ngpt2        : 10 tokens, vocab size: 50,257\n              Tokens: [10082, 2117, 34961, 8489, 4981, 5854, 1096, 6569, 34244, 0]\n\np50k_base   : 10 tokens, vocab size: 50,281\n              Tokens: [10082, 2117, 34961, 8489, 4981, 5854, 1096, 6569, 34244, 0]\n\ncl100k_base : 10 tokens, vocab size: 100,277\n              Tokens: [9688, 437, 33514, 16665, 4211, 14110, 553, 8870, 60199, 0]\n\n\n\n\n\nBPE for Geospatial Text\nLet‚Äôs see how BPE handles domain-specific geospatial terminology:\n\n\nCode\ngeospatial_texts = [\n    \"NDVI vegetation index analysis\",\n    \"Landsat-8 multispectral imagery\", \n    \"Convolutional neural networks for land cover classification\",\n    \"Sentinel-2 satellite data preprocessing\",\n    \"Geospatial foundation models fine-tuning\"\n]\n\ngpt2_enc = tiktoken.get_encoding(\"gpt2\")\n\nprint(\"=== Geospatial Text Tokenization ===\")\nfor text in geospatial_texts:\n    tokens = gpt2_enc.encode(text)\n    print(f\"'{text}'\")\n    print(f\"  -&gt; {len(tokens)} tokens: {tokens}\")\n    print()\n\n\n=== Geospatial Text Tokenization ===\n'NDVI vegetation index analysis'\n  -&gt; 5 tokens: [8575, 12861, 28459, 6376, 3781]\n\n'Landsat-8 multispectral imagery'\n  -&gt; 10 tokens: [43, 1746, 265, 12, 23, 1963, 271, 806, 1373, 19506]\n\n'Convolutional neural networks for land cover classification'\n  -&gt; 10 tokens: [3103, 85, 2122, 282, 17019, 7686, 329, 1956, 3002, 17923]\n\n'Sentinel-2 satellite data preprocessing'\n  -&gt; 8 tokens: [31837, 20538, 12, 17, 11210, 1366, 662, 36948]\n\n'Geospatial foundation models fine-tuning'\n  -&gt; 9 tokens: [10082, 2117, 34961, 8489, 4981, 3734, 12, 28286, 278]\n\n\n\n\n\nUnderstanding Token Efficiency\nThe choice of tokenizer affects model efficiency. Let‚Äôs compare token counts for our text corpus:\n\n\nCode\n# Use a sample from our corpus\nsample_text = raw_text[:500]  # First 500 characters\n\nprint(\"=== Token Efficiency Comparison ===\")\nprint(f\"Text length: {len(sample_text)} characters\")\nprint()\n\n# Our simple tokenizer\nour_tokens = enhanced_tokenizer.encode(sample_text, add_endoftext=False)\nprint(f\"Simple tokenizer: {len(our_tokens)} tokens\")\n\n# GPT-2 BPE\ngpt2_tokens = gpt2_tokenizer.encode(sample_text)\nprint(f\"GPT-2 BPE:       {len(gpt2_tokens)} tokens\")\n\n# Calculate efficiency\nchars_per_token_simple = len(sample_text) / len(our_tokens)\nchars_per_token_bpe = len(sample_text) / len(gpt2_tokens)\n\nprint(f\"\\nCharacters per token:\")\nprint(f\"  Simple: {chars_per_token_simple:.2f}\")\nprint(f\"  BPE:    {chars_per_token_bpe:.2f}\")\nprint(f\"  BPE is {chars_per_token_bpe/chars_per_token_simple:.1f}x more efficient\")\n\n\n=== Token Efficiency Comparison ===\nText length: 500 characters\n\nSimple tokenizer: 110 tokens\nGPT-2 BPE:       123 tokens\n\nCharacters per token:\n  Simple: 4.55\n  BPE:    4.07\n  BPE is 0.9x more efficient"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#key-takeaways",
    "href": "course-materials/extras/examples/text_encoder.html#key-takeaways",
    "title": "Deep Learning Basics",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSpecial Tokens: Essential for handling unknown words and sequence boundaries\nUnknown Token Handling: &lt;|unk|&gt; tokens allow models to gracefully handle out-of-vocabulary words\nEnd-of-Text Tokens: &lt;|endoftext|&gt; tokens help models understand sequence boundaries\nBPE Efficiency: Byte Pair Encoding creates a good balance between vocabulary size and representation efficiency\nDomain Adaptation: Different tokenizers may handle domain-specific text differently\n\nIn geospatial AI applications, choosing the right tokenization strategy affects both model performance and computational efficiency, especially when working with specialized terminology from remote sensing and Earth science domains."
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html",
    "href": "course-materials/extras/examples/resnet.html",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "",
    "text": "This notebook demonstrates the core insight behind Residual Networks (ResNets) by comparing them to a traditional plain CNN of similar depth. You will:\n\nVisualize how residual connections transform intermediate representations\nCompare training loss and accuracy curves\nAnalyze gradient flow to see why ResNets enable deeper models to train effectively"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#overview",
    "href": "course-materials/extras/examples/resnet.html#overview",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "",
    "text": "This notebook demonstrates the core insight behind Residual Networks (ResNets) by comparing them to a traditional plain CNN of similar depth. You will:\n\nVisualize how residual connections transform intermediate representations\nCompare training loss and accuracy curves\nAnalyze gradient flow to see why ResNets enable deeper models to train effectively"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#setup-and-imports",
    "href": "course-materials/extras/examples/resnet.html#setup-and-imports",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üì¶ Setup and Imports",
    "text": "üì¶ Setup and Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#define-plain-cnn-and-resnet-like-cnn",
    "href": "course-materials/extras/examples/resnet.html#define-plain-cnn-and-resnet-like-cnn",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üìä Define Plain CNN and ResNet-like CNN",
    "text": "üìä Define Plain CNN and ResNet-like CNN\nclass PlainBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        return F.relu(out)\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(identity)\n        return F.relu(out)\n\nclass PlainCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = PlainBlock(3, 16)\n        self.block2 = PlainBlock(16, 32, stride=2)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.pool(x).view(x.size(0), -1)\n        return self.fc(x)\n\nclass ResNetMini(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = ResBlock(3, 16)\n        self.block2 = ResBlock(16, 32, stride=2)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.pool(x).view(x.size(0), -1)\n        return self.fc(x)"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#load-cifar-10-data",
    "href": "course-materials/extras/examples/resnet.html#load-cifar-10-data",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üñºÔ∏è Load CIFAR-10 Data",
    "text": "üñºÔ∏è Load CIFAR-10 Data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=2)"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#training-loop-and-gradient-tracking",
    "href": "course-materials/extras/examples/resnet.html#training-loop-and-gradient-tracking",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üöÄ Training Loop and Gradient Tracking",
    "text": "üöÄ Training Loop and Gradient Tracking\ndef train_model(model, name, epochs=10):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loss = []\n    train_gradients = []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0\n        grad_norm = 0\n        for inputs, targets in tqdm(train_loader, desc=f\"{name} Epoch {epoch+1}\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n\n            total_norm = 0\n            for p in model.parameters():\n                if p.grad is not None:\n                    param_norm = p.grad.data.norm(2)\n                    total_norm += param_norm.item() ** 2\n            grad_norm += total_norm ** 0.5\n\n            optimizer.step()\n            running_loss += loss.item()\n\n        train_loss.append(running_loss / len(train_loader))\n        train_gradients.append(grad_norm / len(train_loader))\n\n    return train_loss, train_gradients"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#train-and-compare",
    "href": "course-materials/extras/examples/resnet.html#train-and-compare",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üìà Train and Compare",
    "text": "üìà Train and Compare\nplain_model = PlainCNN()\nresnet_model = ResNetMini()\n\nplain_loss, plain_grads = train_model(plain_model, \"PlainCNN\")\nresnet_loss, resnet_grads = train_model(resnet_model, \"ResNetMini\")"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#plot-training-loss-and-gradient-flow",
    "href": "course-materials/extras/examples/resnet.html#plot-training-loss-and-gradient-flow",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üìä Plot Training Loss and Gradient Flow",
    "text": "üìä Plot Training Loss and Gradient Flow\nplt.figure(figsize=(14,5))\n\nplt.subplot(1,2,1)\nplt.plot(plain_loss, label=\"PlainCNN\")\nplt.plot(resnet_loss, label=\"ResNetMini\")\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(plain_grads, label=\"PlainCNN\")\nplt.plot(resnet_grads, label=\"ResNetMini\")\nplt.title(\"Gradient Norm per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Average Gradient Norm\")\nplt.legend()\n\nplt.suptitle(\"Residual Connections Help Optimize Deeper Networks\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#conclusion",
    "href": "course-materials/extras/examples/resnet.html#conclusion",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "‚úÖ Conclusion",
    "text": "‚úÖ Conclusion\nThis notebook demonstrates that residual connections help preserve gradient flow, allowing deeper networks to train faster and more reliably. Even though our example was relatively shallow (2 blocks), the benefits in convergence and stability are clear."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html",
    "href": "course-materials/extras/examples/tiling-and-patches.html",
    "title": "Tiling & Patch Extraction",
    "section": "",
    "text": "This page shows two kinds of visuals that help explain common preprocessing steps for vision models:\n\nA high-level flow using Mermaid (no Python required)\nData-like mockups using Python + Matplotlib to draw tile and patch grids on a synthetic image\n\n\n\n\n\n\n\n\ngraph LR\n    A[Full Image] --&gt; B[Tiling]\n    B --&gt; C[Tile 1]\n    B --&gt; D[Tile 2]\n    B --&gt; E[Tile 3]\n    C --&gt; F[Patch Extraction]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[Patch Set for Model]\n\n\n\n\n\n\n\nTip: Mermaid renders natively in Quarto HTML output. No extra setup needed for basic use."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#overview",
    "href": "course-materials/extras/examples/tiling-and-patches.html#overview",
    "title": "Tiling & Patch Extraction",
    "section": "",
    "text": "This page shows two kinds of visuals that help explain common preprocessing steps for vision models:\n\nA high-level flow using Mermaid (no Python required)\nData-like mockups using Python + Matplotlib to draw tile and patch grids on a synthetic image\n\n\n\n\n\n\n\n\ngraph LR\n    A[Full Image] --&gt; B[Tiling]\n    B --&gt; C[Tile 1]\n    B --&gt; D[Tile 2]\n    B --&gt; E[Tile 3]\n    C --&gt; F[Patch Extraction]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[Patch Set for Model]\n\n\n\n\n\n\n\nTip: Mermaid renders natively in Quarto HTML output. No extra setup needed for basic use."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#data-like-mockups-python-matplotlib",
    "href": "course-materials/extras/examples/tiling-and-patches.html#data-like-mockups-python-matplotlib",
    "title": "Tiling & Patch Extraction",
    "section": "Data-like Mockups (Python + Matplotlib)",
    "text": "Data-like Mockups (Python + Matplotlib)\nBelow we generate a random ‚Äúimage‚Äù and draw grid lines to illustrate tiling and patch extraction. These mockups help students visualize how tiles and patches relate to array indices.\n\nImplementation notes: - All visuals use Matplotlib (no seaborn) and avoid custom color styles, per course standards. - Each figure is produced by a single, self-contained code chunk. - Random seed is fixed for reproducibility.\n\n\n1) Tiling Overlay on a Full Image\nWe split the image into equally sized tiles (here: 50√ó50). White grid lines mark tile boundaries.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Create subtle textured background with fixed colormap range ---\nnp.random.seed(42)\nimg = np.random.rand(100, 150)\n# Map random values to a narrow range around mid-gray (0.45 to 0.55)\n# This creates gentle texture while keeping the colormap fixed at 0-1\nimg = img * 0.1 + 0.45  # Subtle texture in 10% range around mid-gray\n\n# --- Tile size ---\ntile_h, tile_w = 50, 50\n\n# --- Plot ---\nfig, ax = plt.subplots(figsize=(6, 4))\nax.imshow(img, cmap='gray', vmin=0, vmax=1)  # Fix colormap range\n\n# vertical lines (white for high contrast)\nfor x in range(0, img.shape[1] + 1, tile_w):\n    ax.axvline(x - 0.5, color='white', linewidth=2)\n\n# horizontal lines (white for high contrast)\nfor y in range(0, img.shape[0] + 1, tile_h):\n    ax.axhline(y - 0.5, color='white', linewidth=2)\n\n# Label each tile\ntile_rows = img.shape[0] // tile_h\ntile_cols = img.shape[1] // tile_w\nfor r in range(tile_rows):\n    for c in range(tile_cols):\n        center_y = r * tile_h + tile_h // 2\n        center_x = c * tile_w + tile_w // 2\n        ax.text(center_x, center_y, f\"Tile\\n({r},{c})\",\n                ha=\"center\", va=\"center\", fontsize=12,\n                color='white', weight='bold')\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_title(\"Full Image with Tiling Grid (50x50)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2) Patch Extraction Inside a Single Tile\nZoom into the top-left tile (50√ó50) and split it into smaller patches (here: 10√ó10). Grid lines show patch boundaries; labels mark patch indices (row, col).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming `img` from the previous cell exists in the same execution environment.\n# If running independently, re-create it:\nnp.random.seed(42)\nimg = np.random.rand(100, 150)\n# Map random values to a narrow range around mid-gray (0.45 to 0.55)\nimg = img * 0.1 + 0.45  # Subtle texture in 10% range around mid-gray\n\n# --- Select the first tile: top-left (0:50, 0:50) ---\ntile = img[0:50, 0:50]\n\n# --- Patch size ---\npatch_h, patch_w = 10, 10\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(tile, cmap='gray', vmin=0, vmax=1)  # Fix colormap range\n\n# Draw patch grid (white lines for high contrast)\nfor x in range(0, tile.shape[1] + 1, patch_w):\n    ax.axvline(x - 0.5, color='white', linewidth=1)\nfor y in range(0, tile.shape[0] + 1, patch_h):\n    ax.axhline(y - 0.5, color='white', linewidth=1)\n\n# Annotate patch indices with high-contrast text\nrows = tile.shape[0] // patch_h\ncols = tile.shape[1] // patch_w\nfor r in range(rows):\n    for c in range(cols):\n        y_center = r * patch_h + patch_h / 2\n        x_center = c * patch_w + patch_w / 2\n\n        # White text pops better on textured background\n        ax.text(x_center, y_center, f\"({r},{c})\",\n                ha=\"center\", va=\"center\", fontsize=10,\n                color='white', weight='bold')\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_title(\"Zoomed Tile with Patch Grid (10x10)\")\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#advanced-patch-extraction-concepts",
    "href": "course-materials/extras/examples/tiling-and-patches.html#advanced-patch-extraction-concepts",
    "title": "Tiling & Patch Extraction",
    "section": "Advanced Patch Extraction Concepts",
    "text": "Advanced Patch Extraction Concepts\n\n3) Overlapping Patches with Stride\nWhen stride &lt; patch size, patches overlap. This is common in computer vision to capture more spatial information.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a smaller tile for clearer visualization\nnp.random.seed(42)\nsmall_img = np.random.rand(36, 45)  # Dimensions chosen to work cleanly with patch params\n# Map to subtle texture range\nsmall_img = small_img * 0.1 + 0.45  # Gentle texture around mid-gray\n\n# --- Patch parameters ---\npatch_size = 15\nstride = 9  # Creates 60% overlap (9/15), avoiding 2x multiples\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.imshow(small_img, cmap='gray', vmin=0, vmax=1)  # Fix colormap range\n\n# Calculate patch positions\npatch_positions = []\nfor y in range(0, small_img.shape[0] - patch_size + 1, stride):\n    for x in range(0, small_img.shape[1] - patch_size + 1, stride):\n        patch_positions.append((x, y))\n\n# Draw overlapping patches with different colors\ncolors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan']\nfor i, (x, y) in enumerate(patch_positions[:6]):  # Show first 6 patches\n    color = colors[i % len(colors)]\n    # Draw patch boundary\n    rect = plt.Rectangle((x-0.5, y-0.5), patch_size, patch_size, \n                        linewidth=2, edgecolor=color, facecolor='none', alpha=0.8)\n    ax.add_patch(rect)\n    \n    # Label patch center with matching color\n    center_x, center_y = x + patch_size//2, y + patch_size//2\n    ax.text(center_x, center_y, f\"P{i}\", ha=\"center\", va=\"center\", \n            fontsize=10, color=color, weight='bold',\n            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.9))\n\nax.set_xlim(-1, small_img.shape[1])\nax.set_ylim(small_img.shape[0], -1)\nax.set_xticks([])\nax.set_yticks([])\nax.set_title(f\"Overlapping Patches (size={patch_size}, stride={stride})\")\nplt.show()\n\nprint(f\"Patch size: {patch_size}x{patch_size}\")\nprint(f\"Stride: {stride} (overlap = {patch_size - stride} pixels = {100*(patch_size-stride)/patch_size:.0f}%)\")\nprint(f\"Total patches extracted: {len(patch_positions)}\")\nprint(f\"Image dimensions: {small_img.shape[0]}x{small_img.shape[1]}\")\nprint(f\"Patches fit: {(small_img.shape[0] - patch_size) // stride + 1} rows x {(small_img.shape[1] - patch_size) // stride + 1} cols\")\n\n\n\n\n\n\n\n\n\nPatch size: 15x15\nStride: 9 (overlap = 6 pixels = 40%)\nTotal patches extracted: 12\nImage dimensions: 36x45\nPatches fit: 3 rows x 4 cols\n\n\n\n\n4) Handling Incomplete Patches: Padding Strategies\nWhen image dimensions don‚Äôt divide evenly by patch size, we face the ‚Äúedge problem‚Äù - what to do with incomplete patches at borders. Different strategies offer different trade-offs between computational efficiency, information preservation, and artifact introduction.\nFirst, let‚Äôs set up our test image and examine the edge problem:\n\n\nOriginal image: 40√ó56 pixels\nPatch size: 16√ó16 pixels\nComplete patches that fit: 2√ó3\nLeftover pixels: 8 rows, 8 columns\n==================================================\n\n\n\n\n\n\n\n\n\n\nStrategy 1: Crop (Discard Incomplete Patches)\nApproach: Simply ignore patches that don‚Äôt fit completely within the image boundaries.\nPros:\n\nFast and simple: No extra computation or memory\nNo artifacts: All patches contain only real image data\nPredictable output size: Easy to calculate exact number of patches\n\nCons:\n\nInformation loss: Edge and corner information is discarded\nUneven coverage: Some areas of the image are never processed\nSize dependency: Loss percentage varies with image dimensions\n\nUse cases: When speed is critical and edge information is less important (e.g., dense feature extraction where overlap provides coverage).\n\n\n\n\n\n\n\n\n\n\n\nStrategy 2: Zero Padding\nApproach: Extend the image with zero-valued pixels to make dimensions divisible by patch size.\nPros:\n\nComplete coverage: Every pixel is included in at least one patch\nSimple implementation: Easy to add zeros programmatically\nConsistent output size: Padding can be calculated in advance\n\nCons:\n\nArtificial boundaries: Sharp transitions from real data to zeros\nPotential artifacts: Models may learn to recognize padding patterns\nIncreased computation: More patches to process\n\nUse cases: When complete coverage is essential and models are robust to boundary artifacts (e.g., segmentation tasks).\n\n\n\n\n\n\n\n\n\n\n\nStrategy 3: Reflect Padding\nApproach: Mirror edge pixels to create natural-looking padding that preserves image structure.\nPros:\n\nNatural boundaries: Smooth transitions maintain local image statistics\nStructure preservation: Gradients and textures continue naturally\nBetter model behavior: Less likely to create learning artifacts\n\nCons:\n\nMore complex: Requires reflection logic for edges and corners\nSlight computation overhead: Must calculate reflected values\nMay duplicate features: Reflected content isn‚Äôt truly independent\n\nUse cases: When image quality and natural appearance are important (e.g., super-resolution, denoising, medical imaging)."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#choosing-padding-strategies-in-practice",
    "href": "course-materials/extras/examples/tiling-and-patches.html#choosing-padding-strategies-in-practice",
    "title": "Tiling & Patch Extraction",
    "section": "Choosing Padding Strategies in Practice",
    "text": "Choosing Padding Strategies in Practice\n\nImplementation Examples\nimport torch.nn.functional as F\n\n# PyTorch examples for different padding strategies\ndef apply_padding_strategy(image_tensor, patch_size, strategy='reflect'):\n    \\\"\\\"\\\"\n    Apply padding to make image divisible by patch_size\n    \n    Args:\n        image_tensor: (C, H, W) tensor\n        patch_size: int, size of square patches\n        strategy: 'crop', 'zero', or 'reflect'\n    \\\"\\\"\\\"\n    C, H, W = image_tensor.shape\n    \n    if strategy == 'crop':\n        # Calculate largest area that fits complete patches\n        new_h = (H // patch_size) * patch_size\n        new_w = (W // patch_size) * patch_size\n        return image_tensor[:, :new_h, :new_w]\n    \n    elif strategy == 'zero':\n        # Calculate padding needed\n        pad_h = patch_size - (H % patch_size) if H % patch_size != 0 else 0\n        pad_w = patch_size - (W % patch_size) if W % patch_size != 0 else 0\n        return F.pad(image_tensor, (0, pad_w, 0, pad_h), mode='constant', value=0)\n    \n    elif strategy == 'reflect':\n        pad_h = patch_size - (H % patch_size) if H % patch_size != 0 else 0\n        pad_w = patch_size - (W % patch_size) if W % patch_size != 0 else 0\n        return F.pad(image_tensor, (0, pad_w, 0, pad_h), mode='reflect')\n\n\nDecision Framework\nChoose CROP when: - Processing large datasets where speed &gt;&gt; completeness - Using overlapping patches that provide edge coverage - Edge regions are less important for your task\nChoose ZERO PADDING when: - Need guaranteed complete coverage - Model architecture handles boundaries well - Working with synthetic or highly structured data\nChoose REFLECT PADDING when: - Image quality is paramount - Working with natural images where structure matters - Model will be sensitive to boundary artifacts\n\n\nReal-World Considerations\n\nBatch processing: Padding strategies affect batch consistency\nMemory usage: Padding increases tensor size and memory requirements\nPost-processing: May need to crop back to original dimensions after inference\nData augmentation: Padding interacts with augmentation strategies (rotation, flipping)\n\nMost modern frameworks (PyTorch, TensorFlow) implement all three strategies efficiently, making the choice primarily about the specific requirements of your application rather than implementation difficulty."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#key-takeaways",
    "href": "course-materials/extras/examples/tiling-and-patches.html#key-takeaways",
    "title": "Tiling & Patch Extraction",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nOverlapping patches capture more spatial information but increase computational cost\nStride controls overlap: smaller stride = more overlap = more patches\nPadding strategies handle images that don‚Äôt divide evenly by patch size:\n\nCrop: Fast but loses edge information\nZero padding: Simple but introduces artificial boundaries\n\nReflect padding: More natural boundaries, preserves edge information"
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#variations-you-can-try",
    "href": "course-materials/extras/examples/tiling-and-patches.html#variations-you-can-try",
    "title": "Tiling & Patch Extraction",
    "section": "Variations You Can Try",
    "text": "Variations You Can Try\n\nExperiment with different stride values (1, 3, 6, 12) to see overlap effects\nTry other padding strategies like ‚Äúconstant‚Äù or ‚Äúwrap‚Äù modes\nCompare patch counts and computational requirements across strategies"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html",
    "title": "Data Loading for Satellite Imagery",
    "section": "",
    "text": "Efficient data loading is crucial for satellite imagery analysis due to large file sizes, multiple spectral bands, and complex geospatial metadata.\n\n\nCode\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window\nimport xarray as xr\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\n\nPyTorch version: 2.7.1\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#introduction-to-satellite-data-loading",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#introduction-to-satellite-data-loading",
    "title": "Data Loading for Satellite Imagery",
    "section": "",
    "text": "Efficient data loading is crucial for satellite imagery analysis due to large file sizes, multiple spectral bands, and complex geospatial metadata.\n\n\nCode\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport rasterio\nfrom rasterio.windows import Window\nimport xarray as xr\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\n\nPyTorch version: 2.7.1\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#basic-dataset-patterns",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#basic-dataset-patterns",
    "title": "Data Loading for Satellite Imagery",
    "section": "Basic Dataset Patterns",
    "text": "Basic Dataset Patterns\n\nSimple satellite dataset\n\n\nCode\nclass SatelliteDataset(Dataset):\n    \"\"\"Basic satellite imagery dataset\"\"\"\n    \n    def __init__(self, image_paths, patch_size=256, transform=None):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.patch_size = patch_size\n        self.transform = transform\n        \n        # Pre-compute dataset info\n        self._scan_images()\n    \n    def _scan_images(self):\n        \"\"\"Scan images to get metadata\"\"\"\n        self.image_info = []\n        \n        for path in self.image_paths:\n            # In practice, you'd open actual files\n            # For demo, simulate metadata\n            info = {\n                'path': path,\n                'width': 1024,\n                'height': 1024, \n                'bands': 4,\n                'dtype': np.uint16\n            }\n            self.image_info.append(info)\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        info = self.image_info[idx]\n        \n        # Simulate loading satellite data\n        # In practice: data = rasterio.open(info['path']).read()\n        data = np.random.randint(0, 4096, \n                                (info['bands'], self.patch_size, self.patch_size),\n                                dtype=np.uint16)\n        \n        # Convert to tensor\n        tensor_data = torch.from_numpy(data).float() / 4095.0  # Normalize\n        \n        sample = {\n            'image': tensor_data,\n            'path': str(info['path']),\n            'metadata': info\n        }\n        \n        if self.transform:\n            sample = self.transform(sample)\n        \n        return sample\n\n# Example usage\nimage_paths = ['image1.tif', 'image2.tif', 'image3.tif']\ndataset = SatelliteDataset(image_paths, patch_size=256)\n\nprint(f\"Dataset size: {len(dataset)}\")\nprint(f\"Sample keys: {list(dataset[0].keys())}\")\nprint(f\"Image shape: {dataset[0]['image'].shape}\")\n\n\nDataset size: 3\nSample keys: ['image', 'path', 'metadata']\nImage shape: torch.Size([4, 256, 256])\n\n\n\n\nMulti-temporal dataset\n\n\nCode\nclass TemporalSatelliteDataset(Dataset):\n    \"\"\"Dataset for temporal satellite imagery sequences\"\"\"\n    \n    def __init__(self, data_root, sequence_length=5, time_step=30):\n        self.data_root = Path(data_root)\n        self.sequence_length = sequence_length\n        self.time_step = time_step  # Days between images\n        \n        # In practice, scan directory for date-organized images\n        self.sequences = self._find_temporal_sequences()\n    \n    def _find_temporal_sequences(self):\n        \"\"\"Find valid temporal sequences\"\"\"\n        # Simulate finding temporal sequences\n        sequences = []\n        for i in range(10):  # 10 example sequences\n            start_date = f\"2020-{(i % 12) + 1:02d}-01\"\n            sequence = {\n                'start_date': start_date,\n                'location_id': f'tile_{i:03d}',\n                'file_pattern': f'tile_{i:03d}_*.tif'\n            }\n            sequences.append(sequence)\n        return sequences\n    \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        sequence = self.sequences[idx]\n        \n        # Load temporal sequence\n        images = []\n        for t in range(self.sequence_length):\n            # Simulate temporal progression\n            # Each image in sequence has slight variations\n            base_image = np.random.randn(4, 256, 256) + t * 0.1\n            images.append(base_image)\n        \n        # Stack temporal dimension: [T, C, H, W]\n        temporal_stack = np.stack(images, axis=0)\n        tensor_stack = torch.from_numpy(temporal_stack).float()\n        \n        return {\n            'images': tensor_stack,\n            'sequence_id': sequence['location_id'],\n            'start_date': sequence['start_date'],\n            'time_steps': self.sequence_length\n        }\n\n# Example usage\ntemporal_dataset = TemporalSatelliteDataset('data/', sequence_length=5)\nsample = temporal_dataset[0]\n\nprint(f\"Temporal dataset size: {len(temporal_dataset)}\")\nprint(f\"Image sequence shape: {sample['images'].shape}\")\nprint(f\"Sequence ID: {sample['sequence_id']}\")\n\n\nTemporal dataset size: 10\nImage sequence shape: torch.Size([5, 4, 256, 256])\nSequence ID: tile_000"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#memory-efficient-loading",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#memory-efficient-loading",
    "title": "Data Loading for Satellite Imagery",
    "section": "Memory-Efficient Loading",
    "text": "Memory-Efficient Loading\n\nWindowed reading for large files\n\n\nCode\nclass WindowedSatelliteDataset(Dataset):\n    \"\"\"Dataset that reads windows from large satellite images\"\"\"\n    \n    def __init__(self, image_path, window_size=512, stride=256, max_windows=None):\n        self.image_path = Path(image_path)\n        self.window_size = window_size\n        self.stride = stride\n        \n        # Pre-compute all valid windows\n        self.windows = self._compute_windows()\n        \n        if max_windows and len(self.windows) &gt; max_windows:\n            self.windows = self.windows[:max_windows]\n    \n    def _compute_windows(self):\n        \"\"\"Compute all valid windows for the image\"\"\"\n        # In practice, use rasterio to get actual dimensions\n        # Simulate large image dimensions\n        img_height, img_width = 4096, 4096\n        \n        windows = []\n        for row in range(0, img_height - self.window_size + 1, self.stride):\n            for col in range(0, img_width - self.window_size + 1, self.stride):\n                window = Window(col, row, self.window_size, self.window_size)\n                windows.append(window)\n        \n        return windows\n    \n    def __len__(self):\n        return len(self.windows)\n    \n    def __getitem__(self, idx):\n        window = self.windows[idx]\n        \n        # In practice: \n        # with rasterio.open(self.image_path) as src:\n        #     data = src.read(window=window)\n        \n        # Simulate reading window\n        data = np.random.randint(0, 2048, \n                                (4, self.window_size, self.window_size),\n                                dtype=np.uint16)\n        \n        tensor_data = torch.from_numpy(data).float() / 2047.0\n        \n        return {\n            'image': tensor_data,\n            'window': window,\n            'window_bounds': (window.col_off, window.row_off, \n                            window.width, window.height)\n        }\n\n# Example usage  \nwindowed_dataset = WindowedSatelliteDataset('large_image.tif', \n                                           window_size=512, \n                                           stride=256,\n                                           max_windows=100)\n\nprint(f\"Windowed dataset size: {len(windowed_dataset)}\")\nprint(f\"First window shape: {windowed_dataset[0]['image'].shape}\")\nprint(f\"Window bounds: {windowed_dataset[0]['window_bounds']}\")\n\n\nWindowed dataset size: 100\nFirst window shape: torch.Size([4, 512, 512])\nWindow bounds: (0, 0, 512, 512)\n\n\n\n\nLazy loading with caching\n\n\nCode\nfrom functools import lru_cache\nfrom threading import Lock\n\nclass CachedSatelliteDataset(Dataset):\n    \"\"\"Dataset with intelligent caching for repeated access\"\"\"\n    \n    def __init__(self, image_paths, cache_size=50, patch_size=256):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.patch_size = patch_size\n        self.cache_lock = Lock()\n        \n        # LRU cache for loaded images\n        self._load_image = lru_cache(maxsize=cache_size)(self._load_image_uncached)\n    \n    def _load_image_uncached(self, image_path):\n        \"\"\"Load image without caching (wrapped by LRU cache)\"\"\"\n        # In practice: load with rasterio or other library\n        # Simulate loading time and memory usage\n        print(f\"Loading {image_path} into cache...\")\n        \n        # Simulate different image sizes and properties\n        bands = np.random.choice([3, 4, 8, 12])  # Different satellite sensors\n        data = np.random.randint(0, 4096, \n                                (bands, self.patch_size, self.patch_size),\n                                dtype=np.uint16)\n        \n        return {\n            'data': data,\n            'bands': bands,\n            'loaded_at': torch.tensor(0)  # Timestamp placeholder\n        }\n    \n    def __len__(self):\n        return len(self.image_paths) * 4  # Multiple patches per image\n    \n    def __getitem__(self, idx):\n        image_idx = idx // 4\n        patch_idx = idx % 4\n        \n        image_path = str(self.image_paths[image_idx])\n        \n        with self.cache_lock:\n            image_data = self._load_image(image_path)\n        \n        # Extract patch (simulate different patches from same image)\n        data = image_data['data'].copy()\n        \n        # Add some variation for different patches\n        if patch_idx &gt; 0:\n            noise = np.random.normal(0, 50, data.shape).astype(data.dtype)\n            data = np.clip(data.astype(np.int32) + noise, 0, 4095).astype(np.uint16)\n        \n        tensor_data = torch.from_numpy(data).float() / 4095.0\n        \n        return {\n            'image': tensor_data,\n            'image_idx': image_idx,\n            'patch_idx': patch_idx,\n            'bands': image_data['bands']\n        }\n    \n    def cache_info(self):\n        \"\"\"Get cache statistics\"\"\"\n        return self._load_image.cache_info()\n\n# Example usage\ncached_dataset = CachedSatelliteDataset(image_paths[:3], cache_size=10)\n\n# Load some samples (first loads will cache images)\nfor i in range(6):\n    sample = cached_dataset[i]\n    print(f\"Sample {i}: bands={sample['bands']}, \"\n          f\"image_idx={sample['image_idx']}, patch_idx={sample['patch_idx']}\")\n\nprint(f\"Cache statistics: {cached_dataset.cache_info()}\")\n\n\nLoading image1.tif into cache...\nSample 0: bands=4, image_idx=0, patch_idx=0\nSample 1: bands=4, image_idx=0, patch_idx=1\nSample 2: bands=4, image_idx=0, patch_idx=2\nSample 3: bands=4, image_idx=0, patch_idx=3\nLoading image2.tif into cache...\nSample 4: bands=8, image_idx=1, patch_idx=0\nSample 5: bands=8, image_idx=1, patch_idx=1\nCache statistics: CacheInfo(hits=4, misses=2, maxsize=10, currsize=2)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#advanced-data-loading-strategies",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#advanced-data-loading-strategies",
    "title": "Data Loading for Satellite Imagery",
    "section": "Advanced Data Loading Strategies",
    "text": "Advanced Data Loading Strategies\n\nMulti-resolution dataset\n\n\nCode\nclass MultiResolutionDataset(Dataset):\n    \"\"\"Dataset providing multiple resolutions of the same data\"\"\"\n    \n    def __init__(self, image_paths, resolutions=[128, 256, 512]):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.resolutions = sorted(resolutions)\n        self.base_resolution = max(resolutions)\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def _resize_tensor(self, tensor, target_size):\n        \"\"\"Resize tensor to target size\"\"\"\n        import torch.nn.functional as F\n        \n        # Add batch dimension for interpolation\n        tensor_4d = tensor.unsqueeze(0)  # [1, C, H, W]\n        \n        resized = F.interpolate(\n            tensor_4d, \n            size=(target_size, target_size), \n            mode='bilinear', \n            align_corners=False\n        )\n        \n        return resized.squeeze(0)  # Remove batch dimension\n    \n    def __getitem__(self, idx):\n        # Load base resolution data\n        base_data = np.random.randint(0, 4096, \n                                     (4, self.base_resolution, self.base_resolution),\n                                     dtype=np.uint16)\n        base_tensor = torch.from_numpy(base_data).float() / 4095.0\n        \n        # Create multi-resolution versions\n        multi_res = {}\n        for res in self.resolutions:\n            if res == self.base_resolution:\n                multi_res[f'image_{res}'] = base_tensor\n            else:\n                multi_res[f'image_{res}'] = self._resize_tensor(base_tensor, res)\n        \n        # Add metadata\n        multi_res.update({\n            'path': str(self.image_paths[idx]),\n            'base_resolution': self.base_resolution,\n            'available_resolutions': self.resolutions\n        })\n        \n        return multi_res\n\n# Example usage\nmulti_res_dataset = MultiResolutionDataset(image_paths, resolutions=[128, 256, 512])\nsample = multi_res_dataset[0]\n\nprint(\"Multi-resolution sample keys:\", list(sample.keys()))\nfor key in sample.keys():\n    if key.startswith('image_'):\n        print(f\"{key}: {sample[key].shape}\")\n\n\nMulti-resolution sample keys: ['image_128', 'image_256', 'image_512', 'path', 'base_resolution', 'available_resolutions']\nimage_128: torch.Size([4, 128, 128])\nimage_256: torch.Size([4, 256, 256])\nimage_512: torch.Size([4, 512, 512])\n\n\n\n\nBalanced sampling dataset\n\n\nCode\nclass BalancedSatelliteDataset(Dataset):\n    \"\"\"Dataset with balanced sampling across different conditions\"\"\"\n    \n    def __init__(self, image_paths, labels, balance_strategy='oversample'):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.labels = np.array(labels)\n        self.balance_strategy = balance_strategy\n        \n        # Compute class weights and sampling indices\n        self.class_counts = np.bincount(self.labels)\n        self.num_classes = len(self.class_counts)\n        self._compute_sampling_indices()\n    \n    def _compute_sampling_indices(self):\n        \"\"\"Compute sampling indices for balanced loading\"\"\"\n        if self.balance_strategy == 'oversample':\n            # Oversample minority classes\n            max_count = np.max(self.class_counts)\n            \n            self.sampling_indices = []\n            for class_id in range(self.num_classes):\n                class_indices = np.where(self.labels == class_id)[0]\n                \n                # Repeat indices to match max count\n                repeats = max_count // len(class_indices)\n                remainder = max_count % len(class_indices)\n                \n                oversampled = np.tile(class_indices, repeats)\n                if remainder &gt; 0:\n                    extra = np.random.choice(class_indices, remainder, replace=False)\n                    oversampled = np.concatenate([oversampled, extra])\n                \n                self.sampling_indices.extend(oversampled)\n            \n            # Shuffle the indices\n            self.sampling_indices = np.array(self.sampling_indices)\n            np.random.shuffle(self.sampling_indices)\n        \n        elif self.balance_strategy == 'undersample':\n            # Undersample majority classes\n            min_count = np.min(self.class_counts)\n            \n            self.sampling_indices = []\n            for class_id in range(self.num_classes):\n                class_indices = np.where(self.labels == class_id)[0]\n                sampled = np.random.choice(class_indices, min_count, replace=False)\n                self.sampling_indices.extend(sampled)\n            \n            self.sampling_indices = np.array(self.sampling_indices)\n            np.random.shuffle(self.sampling_indices)\n    \n    def __len__(self):\n        return len(self.sampling_indices)\n    \n    def __getitem__(self, idx):\n        actual_idx = self.sampling_indices[idx]\n        \n        # Load satellite image\n        data = np.random.randint(0, 4096, (4, 256, 256), dtype=np.uint16)\n        tensor_data = torch.from_numpy(data).float() / 4095.0\n        \n        return {\n            'image': tensor_data,\n            'label': torch.tensor(self.labels[actual_idx], dtype=torch.long),\n            'original_idx': actual_idx,\n            'path': str(self.image_paths[actual_idx])\n        }\n\n# Example usage with imbalanced classes\nnp.random.seed(42)\nlabels = np.random.choice([0, 1, 2], size=50, p=[0.7, 0.2, 0.1])  # Imbalanced\n\n# Ensure we have one image path per label index used below\nimage_paths = [f\"image_{i}.tif\" for i in range(len(labels))]\n\nbalanced_dataset = BalancedSatelliteDataset(\n    image_paths[:50], \n    labels, \n    balance_strategy='oversample'\n)\n\n# Check class distribution in balanced dataset\nsample_labels = [balanced_dataset[i]['label'].item() for i in range(len(balanced_dataset))]\nbalanced_counts = np.bincount(sample_labels)\n\nprint(f\"Original class distribution: {np.bincount(labels)}\")\nprint(f\"Balanced class distribution: {balanced_counts}\")\nprint(f\"Balanced dataset size: {len(balanced_dataset)}\")\n\n\nOriginal class distribution: [39  6  5]\nBalanced class distribution: [39 39 39]\nBalanced dataset size: 117"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#dataloader-optimization",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#dataloader-optimization",
    "title": "Data Loading for Satellite Imagery",
    "section": "DataLoader Optimization",
    "text": "DataLoader Optimization\n\nCustom collate functions\n\n\nCode\ndef satellite_collate_fn(batch):\n    \"\"\"Custom collate function for satellite imagery batches\"\"\"\n    \n    # Handle variable number of bands\n    max_bands = max(sample['image'].shape[0] for sample in batch)\n    batch_size = len(batch)\n    \n    # Get common spatial dimensions\n    height = batch[0]['image'].shape[1]\n    width = batch[0]['image'].shape[2]\n    \n    # Pad images to same number of bands\n    padded_images = torch.zeros(batch_size, max_bands, height, width)\n    labels = []\n    paths = []\n    band_masks = torch.zeros(batch_size, max_bands, dtype=torch.bool)\n    \n    for i, sample in enumerate(batch):\n        img = sample['image']\n        num_bands = img.shape[0]\n        \n        padded_images[i, :num_bands] = img\n        band_masks[i, :num_bands] = True\n        \n        if 'label' in sample:\n            labels.append(sample['label'])\n        paths.append(sample['path'])\n    \n    result = {\n        'image': padded_images,\n        'band_mask': band_masks,\n        'path': paths\n    }\n    \n    if labels:\n        result['label'] = torch.stack(labels)\n    \n    return result\n\ndef variable_size_collate_fn(batch):\n    \"\"\"Collate function for variable-sized images\"\"\"\n    \n    # Find max dimensions\n    max_height = max(sample['image'].shape[1] for sample in batch)\n    max_width = max(sample['image'].shape[2] for sample in batch)\n    max_bands = max(sample['image'].shape[0] for sample in batch)\n    \n    batch_size = len(batch)\n    \n    # Create padded batch\n    padded_batch = torch.zeros(batch_size, max_bands, max_height, max_width)\n    size_masks = []\n    \n    for i, sample in enumerate(batch):\n        img = sample['image']\n        c, h, w = img.shape\n        \n        padded_batch[i, :c, :h, :w] = img\n        \n        # Create mask for valid pixels\n        mask = torch.zeros(max_height, max_width, dtype=torch.bool)\n        mask[:h, :w] = True\n        size_masks.append(mask)\n    \n    return {\n        'image': padded_batch,\n        'size_mask': torch.stack(size_masks),\n        'original_sizes': [(s['image'].shape[1], s['image'].shape[2]) for s in batch]\n    }\n\n# Example usage\ndataloader = DataLoader(\n    dataset, \n    batch_size=4, \n    shuffle=True,\n    collate_fn=satellite_collate_fn,\n    num_workers=0,\n    pin_memory=True\n)\n\n# Test the dataloader\nbatch = next(iter(dataloader))\nprint(f\"Batch image shape: {batch['image'].shape}\")\nprint(f\"Band mask shape: {batch['band_mask'].shape}\")\nprint(f\"Paths: {len(batch['path'])}\")\n\n\nBatch image shape: torch.Size([3, 4, 256, 256])\nBand mask shape: torch.Size([3, 4])\nPaths: 3\n\n\n/Users/kellycaylor/mambaforge/envs/geoAI/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning:\n\n'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n\n\n\n\n\nPerformance optimization\n\n\nCode\ndef create_optimized_dataloader(dataset, batch_size=32, num_workers=4):\n    \"\"\"Create optimized dataloader for satellite imagery\"\"\"\n    \n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=torch.cuda.is_available(),  # Pin memory if GPU available\n        persistent_workers=True,  # Keep workers alive between epochs\n        prefetch_factor=2,  # Prefetch 2 batches per worker\n        drop_last=True,  # Consistent batch sizes\n        collate_fn=satellite_collate_fn\n    )\n\n# Memory usage monitoring\ndef monitor_memory_usage(dataloader, num_batches=5):\n    \"\"\"Monitor memory usage during data loading\"\"\"\n    \n    if torch.cuda.is_available():\n        print(\"GPU memory monitoring:\")\n        torch.cuda.reset_peak_memory_stats()\n        initial_memory = torch.cuda.memory_allocated()\n        \n        for i, batch in enumerate(dataloader):\n            if i &gt;= num_batches:\n                break\n            \n            current_memory = torch.cuda.memory_allocated()\n            peak_memory = torch.cuda.max_memory_allocated()\n            \n            print(f\"Batch {i}: Current={current_memory/1e6:.1f}MB, \"\n                  f\"Peak={peak_memory/1e6:.1f}MB\")\n    else:\n        print(\"GPU not available for memory monitoring\")\n\n# Example usage\noptimized_loader = create_optimized_dataloader(dataset, batch_size=8)\n# monitor_memory_usage(optimized_loader)\n\nprint(\"Optimized dataloader created\")\n\n\nOptimized dataloader created"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#real-world-integration-examples",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#real-world-integration-examples",
    "title": "Data Loading for Satellite Imagery",
    "section": "Real-World Integration Examples",
    "text": "Real-World Integration Examples\n\nIntegration with preprocessing pipeline\n\n\nCode\nclass PreprocessingSatelliteDataset(Dataset):\n    \"\"\"Dataset with integrated preprocessing pipeline\"\"\"\n    \n    def __init__(self, image_paths, preprocessing_config=None):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.config = preprocessing_config or self._default_config()\n    \n    def _default_config(self):\n        \"\"\"Default preprocessing configuration\"\"\"\n        return {\n            'normalize': True,\n            'clip_percentiles': (2, 98),\n            'target_bands': [2, 3, 4, 7],  # RGB + NIR\n            'target_resolution': 256,\n            'augment': True\n        }\n    \n    def _preprocess_image(self, image):\n        \"\"\"Apply preprocessing pipeline\"\"\"\n        \n        # Select target bands\n        if self.config['target_bands']:\n            available_bands = min(image.shape[0], max(self.config['target_bands']) + 1)\n            target_bands = [b for b in self.config['target_bands'] if b &lt; available_bands]\n            image = image[target_bands]\n        \n        # Normalize\n        if self.config['normalize']:\n            for band in range(image.shape[0]):\n                band_data = image[band]\n                if self.config['clip_percentiles']:\n                    p_low, p_high = self.config['clip_percentiles']\n                    low_val = np.percentile(band_data, p_low)\n                    high_val = np.percentile(band_data, p_high)\n                    band_data = np.clip(band_data, low_val, high_val)\n                \n                # Normalize to [0, 1]\n                band_min, band_max = band_data.min(), band_data.max()\n                if band_max &gt; band_min:\n                    image[band] = (band_data - band_min) / (band_max - band_min)\n        \n        return image\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Simulate loading multi-band satellite image\n        num_bands = np.random.choice([4, 8, 12])  # Different sensors\n        base_resolution = np.random.choice([256, 512])\n        \n        # Simulate realistic satellite data values\n        image = np.random.randint(100, 4000, \n                                 (num_bands, base_resolution, base_resolution),\n                                 dtype=np.uint16).astype(np.float32)\n        \n        # Apply preprocessing\n        processed_image = self._preprocess_image(image)\n        \n        # Convert to tensor\n        tensor_image = torch.from_numpy(processed_image)\n        \n        return {\n            'image': tensor_image,\n            'path': str(self.image_paths[idx]),\n            'original_bands': num_bands,\n            'processed_bands': processed_image.shape[0]\n        }\n\n# Example usage\npreprocessing_config = {\n    'normalize': True,\n    'clip_percentiles': (1, 99),\n    'target_bands': [0, 1, 2, 3],  # First 4 bands\n    'augment': False\n}\n\npreprocessed_dataset = PreprocessingSatelliteDataset(\n    image_paths, \n    preprocessing_config=preprocessing_config\n)\n\nsample = preprocessed_dataset[0]\nprint(f\"Preprocessed sample shape: {sample['image'].shape}\")\nprint(f\"Original bands: {sample['original_bands']}\")\nprint(f\"Processed bands: {sample['processed_bands']}\")\nprint(f\"Value range: [{sample['image'].min():.3f}, {sample['image'].max():.3f}]\")\n\n\nPreprocessed sample shape: torch.Size([4, 256, 256])\nOriginal bands: 4\nProcessed bands: 4\nValue range: [0.000, 1.000]"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html#summary",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html#summary",
    "title": "Data Loading for Satellite Imagery",
    "section": "Summary",
    "text": "Summary\nKey data loading strategies for satellite imagery: - Memory efficiency: Window-based reading, caching, lazy loading - Multi-temporal: Handle time series of satellite observations\n- Multi-resolution: Provide different spatial resolutions - Balanced sampling: Handle imbalanced datasets - Custom collation: Handle variable bands and sizes - Preprocessing integration: Normalization, band selection, augmentation - Performance optimization: Multi-processing, memory pinning, prefetching"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html",
    "href": "course-materials/c01-geospatial-data-foundations.html",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "",
    "text": "Today we‚Äôre building a complete pipeline that transforms raw satellite imagery into model-ready embeddings. This mirrors how Large Language Models process text: raw text ‚Üí tokens ‚Üí embeddings. Our geospatial version: raw GeoTIFF ‚Üí patches ‚Üí embeddings."
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#introduction",
    "href": "course-materials/c01-geospatial-data-foundations.html#introduction",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "",
    "text": "Today we‚Äôre building a complete pipeline that transforms raw satellite imagery into model-ready embeddings. This mirrors how Large Language Models process text: raw text ‚Üí tokens ‚Üí embeddings. Our geospatial version: raw GeoTIFF ‚Üí patches ‚Üí embeddings."
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#course-roadmap-mapping",
    "href": "course-materials/c01-geospatial-data-foundations.html#course-roadmap-mapping",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Course Roadmap Mapping",
    "text": "Course Roadmap Mapping\nThis week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n1\nStage 1: Build GFM Architecture\nData Foundations\ncore/config.py; data/datasets/stac_dataset.py; data/transforms/{normalization.py, patchify.py}; data/loaders.py\ntorch.utils.data.Dataset/DataLoader, rasterio, numpy\nConfig-driven dataloaders that yield normalized patches\n\n\n\n\nWeekly goals\n\nImplement a minimal dataset, transforms, and dataloaders\nNormalize channels; extract patches deterministically\nVerify shapes/CRS/stats prints; run a tiny DataLoader"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#learning-objectives",
    "href": "course-materials/c01-geospatial-data-foundations.html#learning-objectives",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy building this pipeline, you will: - Implement GeoTIFF loading and preprocessing functions - Create patch extraction with spatial metadata - Build tensor normalization and encoding functions\n- Construct a PyTorch DataLoader for model training - Connect to a simple embedding layer to verify end-to-end functionality"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#session-roadmap",
    "href": "course-materials/c01-geospatial-data-foundations.html#session-roadmap",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Session Roadmap",
    "text": "Session Roadmap\n\n\n\n\n\nflowchart TD\n    A[\"Setup & GeoTIFF Loading\"] --&gt; B[\"Geo Preprocessing Functions\"]\n    B --&gt; C[\"Patch Extraction with Metadata\"] \n    C --&gt; D[\"Tensor Operations & Normalization\"]\n    D --&gt; E[\"DataLoader Construction\"]\n    E --&gt; F[\"Embedding Layer Integration\"]\n    F --&gt; G[\"End-to-End Pipeline Test\"]"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#setting-up",
    "href": "course-materials/c01-geospatial-data-foundations.html#setting-up",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Setting Up",
    "text": "Setting Up\nLet‚Äôs establish our development environment and define the core constants we‚Äôll use throughout.\n\nImports and Configuration\n\n\nCode\nimport os\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport rasterio as rio\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Dict, Any\n\n# Set seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Pipeline constants\nPATCH_SIZE = 64\nSTRIDE = 32  # 50% overlap\nBATCH_SIZE = 8\nEMBEDDING_DIM = 256\n\nprint(f\"‚úì Environment setup complete\")\nprint(f\"‚úì Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\nprint(f\"‚úì Stride: {STRIDE} (overlap: {(PATCH_SIZE-STRIDE)/PATCH_SIZE*100:.0f}%)\")\n\n\n‚úì Environment setup complete\n‚úì Patch size: 64x64\n‚úì Stride: 32 (overlap: 50%)\n\n\n\n\nData Preparation\n\n\nCode\n# Set up data paths - use book/data for course sample data\nif \"__file__\" in globals():\n    # From course-materials folder, go up 2 levels to book folder, then to data\n    DATA_DIR = Path(__file__).parent.parent / \"data\"\nelse:\n    # Fallback for interactive environments - look for book folder\n    current = Path.cwd()\n    while current.name not in [\"book\", \"geoAI\"] and current.parent != current:\n        current = current.parent\n    if current.name == \"book\":\n        DATA_DIR = current / \"data\"\n    elif current.name == \"geoAI\":\n        DATA_DIR = current / \"book\" / \"data\"\n    else:\n        DATA_DIR = Path(\"data\")\n\nDATA_DIR.mkdir(exist_ok=True)\nSAMPLE_PATH = DATA_DIR / \"landcover_sample.tif\"\n\n# Verify data file exists\nif not SAMPLE_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {SAMPLE_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(f\"‚úì Data ready: {SAMPLE_PATH.name}\")\nprint(f\"‚úì File size: {SAMPLE_PATH.stat().st_size / 1024:.1f} KB\")\nprint(f\"‚úì Full path: {SAMPLE_PATH}\")\n\n\n‚úì Data ready: landcover_sample.tif\n‚úì File size: 12.6 KB\n‚úì Full path: /Users/kellycaylor/dev/geoAI/book/data/landcover_sample.tif"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-1-geotiff-loading-and-inspection",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-1-geotiff-loading-and-inspection",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 1: GeoTIFF Loading and Inspection",
    "text": "Step 1: GeoTIFF Loading and Inspection\nGoal: Build a function that loads and extracts essential information from any GeoTIFF.\n\nüõ†Ô∏è Build It: GeoTIFF Loader Function\nYour task: Complete this function to load a GeoTIFF and return both the data and metadata.\n\n\nCode\ndef load_geotiff(file_path: Path) -&gt; Tuple[np.ndarray, Dict[str, Any]]:\n    \"\"\"\n    Load a GeoTIFF and extract data + metadata.\n    \n    Returns:\n        data: (bands, height, width) array\n        metadata: dict with CRS, transform, resolution, etc.\n    \"\"\"\n    with rio.open(file_path) as src:\n        # TODO: Load the data array\n        data = src.read()  # YOUR CODE: Load raster data\n        \n        # TODO: Extract metadata\n        metadata = {\n            'crs': src.crs,  # YOUR CODE: Get coordinate reference system\n            'transform': src.transform,  # YOUR CODE: Get geospatial transform\n            'shape': data.shape,  # YOUR CODE: Get array dimensions\n            'dtype': data.dtype,  # YOUR CODE: Get data type\n            'resolution': src.res,  # YOUR CODE: Get pixel resolution\n            'bounds': src.bounds,  # YOUR CODE: Get spatial bounds\n        }\n    \n    return data, metadata\n\n# Test your function\ndata, metadata = load_geotiff(SAMPLE_PATH)\nprint(f\"‚úì Loaded shape: {data.shape}\")\nprint(f\"‚úì Data type: {metadata['dtype']}\")\nprint(f\"‚úì Resolution: {metadata['resolution']}\")\nprint(f\"‚úì CRS: {metadata['crs']}\")\n\n\n‚úì Loaded shape: (3, 64, 64)\n‚úì Data type: uint8\n‚úì Resolution: (0.25, 0.25)\n‚úì CRS: PROJCS[\"Projection: Transverse Mercator; Datum: WGS84; Ellipsoid: WGS84\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",19],PARAMETER[\"scale_factor\",0.9993],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",-5300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n\n\n\n\nüîç Verify It: Inspect Your Data\n\n\nCode\n# Examine the data you loaded\nbands, height, width = data.shape\nprint(f\"Image dimensions: {height}√ó{width} pixels\")\nprint(f\"Number of bands: {bands}\")\nprint(f\"Value ranges per band:\")\nfor i, band in enumerate(data):\n    print(f\"  Band {i+1}: {band.min():.0f} to {band.max():.0f}\")\n\n# Quick visualization\nfig, axes = plt.subplots(1, bands, figsize=(12, 4))\nif bands == 1:\n    axes = [axes]\n\nfor i, band in enumerate(data):\n    axes[i].imshow(band, cmap='viridis')\n    axes[i].set_title(f'Band {i+1}')\n    axes[i].axis('off')\n\nplt.suptitle('Raw Satellite Bands')\nplt.tight_layout()\nplt.show()\n\n\nImage dimensions: 64√ó64 pixels\nNumber of bands: 3\nValue ranges per band:\n  Band 1: 0 to 254\n  Band 2: 0 to 254\n  Band 3: 0 to 254"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-2-geo-preprocessing-functions",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-2-geo-preprocessing-functions",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 2: Geo Preprocessing Functions",
    "text": "Step 2: Geo Preprocessing Functions\nGoal: Build preprocessing functions that operate on the full image before patch extraction.\n\nüõ†Ô∏è Build It: Normalization Functions\nWe‚Äôll create two normalization functions that can work with either local statistics (calculated from the input data) or global statistics (pre-computed from a training dataset). Global statistics ensure consistent normalization across different image tiles and are crucial for foundation model training.\nWhy use global statistics? When training on multiple images, each tile might have different value ranges. Using global statistics ensures that the same pixel value represents the same relative intensity across all training data.\n\nMin-Max Normalization Function\n\n\nCode\ndef minmax_normalize(data: np.ndarray, \n                    global_min: np.ndarray = None, \n                    global_max: np.ndarray = None) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Min-max normalize spectral bands to [0,1] range.\n    \n    Args:\n        data: (bands, height, width) array\n        global_min: Optional (bands,) array of global minimums per band\n        global_max: Optional (bands,) array of global maximums per band\n    \n    Returns:\n        normalized: (bands, height, width) array with values in [0,1]\n        stats: Dictionary containing the min/max values used\n    \"\"\"\n    bands, height, width = data.shape\n    normalized = np.zeros_like(data, dtype=np.float32)\n    \n    # Use global stats if provided, otherwise calculate from data\n    if global_min is None or global_max is None:\n        # Calculate per-band statistics from this data\n        mins = np.array([data[i].min() for i in range(bands)])\n        maxs = np.array([data[i].max() for i in range(bands)])\n        stats_source = \"local (calculated from input)\"\n    else:\n        # Use provided global statistics\n        mins = global_min\n        maxs = global_max\n        stats_source = \"global (provided)\"\n    \n    # Apply normalization per band\n    for i in range(bands):\n        band_range = maxs[i] - mins[i]\n        if band_range &gt; 0:  # Avoid division by zero\n            normalized[i] = (data[i] - mins[i]) / band_range\n        else:\n            normalized[i] = 0  # Handle constant bands\n    \n    # Package statistics for inspection\n    stats = {\n        'source': stats_source,\n        'mins': mins,\n        'maxs': maxs,\n        'output_range': (normalized.min(), normalized.max())\n    }\n    \n    return normalized, stats\n\n\n\n\nZ-Score Normalization Function\n\n\nCode\ndef zscore_normalize(data: np.ndarray,\n                    global_mean: np.ndarray = None,\n                    global_std: np.ndarray = None) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Z-score normalize spectral bands to mean=0, std=1.\n    \n    Args:\n        data: (bands, height, width) array\n        global_mean: Optional (bands,) array of global means per band\n        global_std: Optional (bands,) array of global standard deviations per band\n    \n    Returns:\n        normalized: (bands, height, width) standardized array\n        stats: Dictionary containing the mean/std values used\n    \"\"\"\n    bands, height, width = data.shape\n    normalized = np.zeros_like(data, dtype=np.float32)\n    \n    # Use global stats if provided, otherwise calculate from data\n    if global_mean is None or global_std is None:\n        # Calculate per-band statistics from this data\n        means = np.array([data[i].mean() for i in range(bands)])\n        stds = np.array([data[i].std() for i in range(bands)])\n        stats_source = \"local (calculated from input)\"\n    else:\n        # Use provided global statistics\n        means = global_mean\n        stds = global_std\n        stats_source = \"global (provided)\"\n    \n    # Apply normalization per band\n    for i in range(bands):\n        if stds[i] &gt; 0:  # Avoid division by zero\n            normalized[i] = (data[i] - means[i]) / stds[i]\n        else:\n            normalized[i] = 0  # Handle constant bands\n    \n    # Package statistics for inspection\n    stats = {\n        'source': stats_source,\n        'means': means,\n        'stds': stds,\n        'output_mean': normalized.mean(),\n        'output_std': normalized.std()\n    }\n    \n    return normalized, stats\n\nprint(\"‚úì Normalization functions created\")\nprint(\"  - minmax_normalize: scales to [0,1] range\")\nprint(\"  - zscore_normalize: standardizes to mean=0, std=1\")\n\n\n‚úì Normalization functions created\n  - minmax_normalize: scales to [0,1] range\n  - zscore_normalize: standardizes to mean=0, std=1\n\n\n\n\nTest Both Functions with Local Statistics\n\n\nCode\n# Test min-max normalization with local statistics\nminmax_data, minmax_stats = minmax_normalize(data)\nprint(\"üìä Min-Max Normalization (local stats):\")\nprint(f\"  Source: {minmax_stats['source']}\")\nprint(f\"  Original range: {data.min():.0f} to {data.max():.0f}\")\nprint(f\"  Normalized range: {minmax_stats['output_range'][0]:.3f} to {minmax_stats['output_range'][1]:.3f}\")\nprint(f\"  Per-band mins: {minmax_stats['mins']}\")\nprint(f\"  Per-band maxs: {minmax_stats['maxs']}\")\n\nprint()\n\n# Test z-score normalization with local statistics  \nzscore_data, zscore_stats = zscore_normalize(data)\nprint(\"üìä Z-Score Normalization (local stats):\")\nprint(f\"  Source: {zscore_stats['source']}\")\nprint(f\"  Output mean: {zscore_stats['output_mean']:.6f}\")\nprint(f\"  Output std: {zscore_stats['output_std']:.6f}\")\nprint(f\"  Per-band means: {zscore_stats['means']}\")\nprint(f\"  Per-band stds: {zscore_stats['stds']}\")\n\n\nüìä Min-Max Normalization (local stats):\n  Source: local (calculated from input)\n  Original range: 0 to 254\n  Normalized range: 0.000 to 1.000\n  Per-band mins: [0 0 0]\n  Per-band maxs: [254 254 254]\n\nüìä Z-Score Normalization (local stats):\n  Source: local (calculated from input)\n  Output mean: 0.000000\n  Output std: 1.000000\n  Per-band means: [126.14306641 126.14306641 126.14306641]\n  Per-band stds: [73.10237725 73.10237725 73.10237725]\n\n\n\n\nTest with Global Statistics\n\n\nCode\n# Simulate global statistics from a larger dataset\n# In practice, these would be pre-computed from your entire training corpus\nglobal_mins = np.array([100, 150, 200])  # Example global minimums per band\nglobal_maxs = np.array([1500, 2000, 2500])  # Example global maximums per band\nglobal_means = np.array([800, 1200, 1600])  # Example global means per band\nglobal_stds = np.array([300, 400, 500])  # Example global standard deviations per band\n\nprint(\"üåç Testing with Global Statistics:\")\nprint(f\"  Global mins: {global_mins}\")\nprint(f\"  Global maxs: {global_maxs}\")\nprint(f\"  Global means: {global_means}\")\nprint(f\"  Global stds: {global_stds}\")\n\nprint()\n\n# Test with global statistics\nminmax_global, minmax_global_stats = minmax_normalize(data, global_mins, global_maxs)\nzscore_global, zscore_global_stats = zscore_normalize(data, global_means, global_stds)\n\nprint(\"üìä Min-Max with Global Stats:\")\nprint(f\"  Source: {minmax_global_stats['source']}\")\nprint(f\"  Output range: {minmax_global_stats['output_range'][0]:.3f} to {minmax_global_stats['output_range'][1]:.3f}\")\n\nprint()\n\nprint(\"üìä Z-Score with Global Stats:\")\nprint(f\"  Source: {zscore_global_stats['source']}\")\nprint(f\"  Output mean: {zscore_global_stats['output_mean']:.3f}\")\nprint(f\"  Output std: {zscore_global_stats['output_std']:.3f}\")\n\n\nüåç Testing with Global Statistics:\n  Global mins: [100 150 200]\n  Global maxs: [1500 2000 2500]\n  Global means: [ 800 1200 1600]\n  Global stds: [300 400 500]\n\nüìä Min-Max with Global Stats:\n  Source: global (provided)\n  Output range: 0.000 to 0.182\n\nüìä Z-Score with Global Stats:\n  Source: global (provided)\n  Output mean: 168.496\n  Output std: 36.333\n\n\nWhat to notice: When using global statistics, the output ranges and distributions differ from local normalization. This is expected and ensures consistency across different image tiles in your dataset.\n\n\n\nüõ†Ô∏è Build It: Spatial Cropping Function\n\n\nCode\ndef crop_to_patches(data: np.ndarray, patch_size: int, stride: int) -&gt; np.ndarray:\n    \"\"\"\n    Crop image to dimensions that allow complete patch extraction.\n    \n    Args:\n        data: (bands, height, width) array\n        patch_size: size of patches to extract\n        stride: step size between patches\n        \n    Returns:\n        cropped: (bands, new_height, new_width) array\n    \"\"\"\n    bands, height, width = data.shape\n    \n    # TODO: Calculate how many complete patches fit\n    patches_h = (height - patch_size) // stride + 1\n    patches_w = (width - patch_size) // stride + 1\n    \n    # TODO: Calculate the required dimensions\n    new_height = (patches_h - 1) * stride + patch_size\n    new_width = (patches_w - 1) * stride + patch_size\n    \n    # TODO: Crop the data\n    cropped = data[:, :new_height, :new_width]\n    \n    print(f\"‚úì Cropped from {height}√ó{width} to {new_height}√ó{new_width}\")\n    print(f\"‚úì Will generate {patches_h}√ó{patches_w} = {patches_h*patches_w} patches\")\n    \n    return cropped\n\n# Test your cropping function\ncropped_data = crop_to_patches(data, 8, STRIDE)\n\n\n‚úì Cropped from 64√ó64 to 40√ó40\n‚úì Will generate 2√ó2 = 4 patches"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-3-patch-extraction-with-metadata",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-3-patch-extraction-with-metadata",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 3: Patch Extraction with Metadata",
    "text": "Step 3: Patch Extraction with Metadata\nGoal: Extract patches while preserving spatial context information.\n\nüõ†Ô∏è Build It: Patch Extraction Function\n\n\nCode\ndef extract_patches_with_metadata(\n    data: np.ndarray, \n    transform,\n    patch_size: int, \n    stride: int\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Extract patches with their spatial coordinates.\n    \n    Args:\n        data: (bands, height, width) normalized array\n        transform: rasterio transform object\n        patch_size: size of patches\n        stride: step between patches\n        \n    Returns:\n        patches: (n_patches, bands, patch_size, patch_size) array\n        coordinates: (n_patches, 4) array of [min_x, min_y, max_x, max_y]\n    \"\"\"\n    bands, height, width = data.shape\n    patches = []\n    coordinates = []\n    \n    # TODO: Iterate through patch positions\n    for row in range(0, height - patch_size + 1, stride):\n        for col in range(0, width - patch_size + 1, stride):\n            # TODO: Extract patch from all bands\n            patch = data[:, row:row+patch_size, col:col+patch_size]\n            patches.append(patch)\n            \n            # TODO: Calculate real-world coordinates using transform\n            min_x, max_y = transform * (col, row)  # Top-left\n            max_x, min_y = transform * (col + patch_size, row + patch_size)  # Bottom-right\n            coordinates.append([min_x, min_y, max_x, max_y])\n    \n    patches = np.array(patches)\n    coordinates = np.array(coordinates)\n    \n    print(f\"‚úì Extracted {len(patches)} patches\")\n    print(f\"‚úì Patch shape: {patches.shape}\")\n    print(f\"‚úì Coordinate shape: {coordinates.shape}\")\n    \n    return patches, coordinates\n\n# Test your patch extraction\npatches, coords = extract_patches_with_metadata(\n    data, metadata['transform'], 8, 4\n)\n\n# Visualize a few patches\nfig, axes = plt.subplots(2, 4, figsize=(12, 6))\nfor i in range(8):\n    row, col = i // 4, i % 4\n    # Show first band of each patch\n    axes[row, col].imshow(patches[i, 0], cmap='viridis')\n    axes[row, col].set_title(f'Patch {i}')\n    axes[row, col].axis('off')\n\nplt.suptitle('Sample Extracted Patches (Band 1)')\nplt.tight_layout()\nplt.show()\n\n\n‚úì Extracted 225 patches\n‚úì Patch shape: (225, 3, 8, 8)\n‚úì Coordinate shape: (225, 4)"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-4-tensor-operations-metadata-encoding",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-4-tensor-operations-metadata-encoding",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 4: Tensor Operations & Metadata Encoding",
    "text": "Step 4: Tensor Operations & Metadata Encoding\nGoal: Convert numpy arrays to PyTorch tensors and encode metadata.\n\nüõ†Ô∏è Build It: Metadata Encoder\n\n\nCode\ndef encode_metadata(coordinates: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Encode spatial metadata as features.\n    \n    Args:\n        coordinates: (n_patches, 4) array of [min_x, min_y, max_x, max_y]\n        \n    Returns:\n        encoded: (n_patches, n_features) array\n    \"\"\"\n    # TODO: Calculate spatial features\n    center_x = (coordinates[:, 0] + coordinates[:, 2]) / 2\n    center_y = (coordinates[:, 1] + coordinates[:, 3]) / 2\n    width = coordinates[:, 2] - coordinates[:, 0]\n    height = coordinates[:, 3] - coordinates[:, 1]\n    area = width * height\n    \n    # TODO: Normalize spatial features (handle zero std to avoid divide by zero)\n    def safe_normalize(values):\n        \"\"\"Normalize values, handling zero standard deviation.\"\"\"\n        mean_val = values.mean()\n        std_val = values.std()\n        if std_val &gt; 0:\n            return (values - mean_val) / std_val\n        else:\n            return np.zeros_like(values)  # All values are the same\n    \n    features = np.column_stack([\n        safe_normalize(center_x),     # Normalized center X\n        safe_normalize(center_y),     # Normalized center Y  \n        safe_normalize(area),         # Normalized area (handles constant area)\n        width / height,               # Aspect ratio\n    ])\n    \n    print(f\"‚úì Encoded metadata shape: {features.shape}\")\n    print(f\"‚úì Feature statistics:\")\n    feature_names = ['center_x', 'center_y', 'area', 'aspect_ratio']\n    for i, name in enumerate(feature_names):\n        print(f\"  {name}: mean={features[:, i].mean():.3f}, std={features[:, i].std():.3f}\")\n    \n    return features.astype(np.float32)\n\n# Test metadata encoding\nencoded_metadata = encode_metadata(coords)\n\n\n‚úì Encoded metadata shape: (225, 4)\n‚úì Feature statistics:\n  center_x: mean=0.000, std=1.000\n  center_y: mean=0.000, std=1.000\n  area: mean=0.000, std=0.000\n  aspect_ratio: mean=1.000, std=0.000\n\n\n\n\nüõ†Ô∏è Build It: Tensor Conversion\n\n\nCode\ndef create_tensors(patches: np.ndarray, metadata: np.ndarray) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Convert numpy arrays to PyTorch tensors.\n    \n    Args:\n        patches: (n_patches, bands, height, width) array\n        metadata: (n_patches, n_features) array\n        \n    Returns:\n        patch_tensors: (n_patches, bands, height, width) tensor\n        metadata_tensors: (n_patches, n_features) tensor\n    \"\"\"\n    # TODO: Convert to tensors with appropriate dtypes\n    patch_tensors = torch.from_numpy(patches).float()\n    metadata_tensors = torch.from_numpy(metadata).float()\n    \n    print(f\"‚úì Patch tensors: {patch_tensors.shape}, dtype: {patch_tensors.dtype}\")\n    print(f\"‚úì Metadata tensors: {metadata_tensors.shape}, dtype: {metadata_tensors.dtype}\")\n    \n    return patch_tensors, metadata_tensors\n\n# Create tensors\npatch_tensors, metadata_tensors = create_tensors(patches, encoded_metadata)\n\n\n‚úì Patch tensors: torch.Size([225, 3, 8, 8]), dtype: torch.float32\n‚úì Metadata tensors: torch.Size([225, 4]), dtype: torch.float32"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-5-dataloader-construction",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-5-dataloader-construction",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 5: DataLoader Construction",
    "text": "Step 5: DataLoader Construction\nGoal: Build a PyTorch Dataset and DataLoader for training.\n\nüõ†Ô∏è Build It: Custom Dataset Class\n\n\nCode\nclass GeospatialDataset(Dataset):\n    \"\"\"Dataset for geospatial patches with metadata.\"\"\"\n    \n    def __init__(self, patch_tensors: torch.Tensor, metadata_tensors: torch.Tensor):\n        \"\"\"\n        Args:\n            patch_tensors: (n_patches, bands, height, width)\n            metadata_tensors: (n_patches, n_features)\n        \"\"\"\n        self.patches = patch_tensors\n        self.metadata = metadata_tensors\n        \n        # TODO: Create dummy labels for demonstration (in real use, load from file)\n        self.labels = torch.randint(0, 5, (len(patch_tensors),))  # 5 land cover classes\n        \n    def __len__(self) -&gt; int:\n        \"\"\"Return number of patches.\"\"\"\n        return len(self.patches)\n    \n    def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Get a single item.\n        \n        Returns:\n            patch: (bands, height, width) tensor\n            metadata: (n_features,) tensor  \n            label: scalar tensor\n        \"\"\"\n        return self.patches[idx], self.metadata[idx], self.labels[idx]\n\n# Test your dataset\ndataset = GeospatialDataset(patch_tensors, metadata_tensors)\nprint(f\"‚úì Dataset length: {len(dataset)}\")\n\n# Test getting an item\nsample_patch, sample_metadata, sample_label = dataset[0]\nprint(f\"‚úì Sample patch shape: {sample_patch.shape}\")\nprint(f\"‚úì Sample metadata shape: {sample_metadata.shape}\")\nprint(f\"‚úì Sample label: {sample_label.item()}\")\n\n\n‚úì Dataset length: 225\n‚úì Sample patch shape: torch.Size([3, 8, 8])\n‚úì Sample metadata shape: torch.Size([4])\n‚úì Sample label: 2\n\n\n\n\nüõ†Ô∏è Build It: DataLoader\n\n\nCode\n# TODO: Create DataLoader with appropriate batch size and shuffling\ndataloader = DataLoader(\n    dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True,\n    num_workers=0,  # Set to 0 for compatibility\n    pin_memory=True if torch.cuda.is_available() else False\n)\n\nprint(f\"‚úì DataLoader created with batch size {BATCH_SIZE}\")\nprint(f\"‚úì Number of batches: {len(dataloader)}\")\n\n# Test the DataLoader\nfor batch_idx, (patches, metadata, labels) in enumerate(dataloader):\n    print(f\"‚úì Batch {batch_idx}:\")\n    print(f\"  Patches: {patches.shape}\")\n    print(f\"  Metadata: {metadata.shape}\")\n    print(f\"  Labels: {labels.shape}\")\n    if batch_idx == 1:  # Show first two batches\n        break\n\n\n‚úì DataLoader created with batch size 8\n‚úì Number of batches: 29\n‚úì Batch 0:\n  Patches: torch.Size([8, 3, 8, 8])\n  Metadata: torch.Size([8, 4])\n  Labels: torch.Size([8])\n‚úì Batch 1:\n  Patches: torch.Size([8, 3, 8, 8])\n  Metadata: torch.Size([8, 4])\n  Labels: torch.Size([8])"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-6-embedding-layer-integration",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-6-embedding-layer-integration",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 6: Embedding Layer Integration",
    "text": "Step 6: Embedding Layer Integration\nGoal: Connect to a simple embedding layer to verify end-to-end functionality.\n\nüõ†Ô∏è Build It: Simple GFM Embedding Layer\n\n\nCode\nclass SimpleGFMEmbedding(nn.Module):\n    \"\"\"Simple embedding layer for geospatial patches.\"\"\"\n    \n    def __init__(self, input_channels: int, metadata_features: int, embed_dim: int, patch_size: int = 64):\n        super().__init__()\n        \n        # TODO: Build adaptive patch encoder based on patch size\n        if patch_size &gt;= 32:\n            # Larger patches: multi-layer CNN\n            kernel1 = min(8, patch_size // 4)\n            kernel2 = min(4, patch_size // 8)\n            self.patch_encoder = nn.Sequential(\n                nn.Conv2d(input_channels, 32, kernel_size=kernel1, stride=kernel1//2),\n                nn.ReLU(),\n                nn.Conv2d(32, 64, kernel_size=kernel2, stride=kernel2//2), \n                nn.ReLU(),\n                nn.AdaptiveAvgPool2d(1),\n                nn.Flatten(),\n            )\n        else:\n            # Smaller patches: simpler encoder\n            kernel = min(4, patch_size // 2)\n            self.patch_encoder = nn.Sequential(\n                nn.Conv2d(input_channels, 64, kernel_size=kernel, stride=1),\n                nn.ReLU(),\n                nn.AdaptiveAvgPool2d(1),\n                nn.Flatten(),\n            )\n        \n        # TODO: Build metadata encoder\n        self.metadata_encoder = nn.Sequential(\n            nn.Linear(metadata_features, 32),\n            nn.ReLU(),\n            nn.Linear(32, 32),\n        )\n        \n        # TODO: Build fusion layer\n        # Calculate patch encoder output size\n        with torch.no_grad():\n            dummy_patch = torch.randn(1, input_channels, patch_size, patch_size)\n            patch_feat_size = self.patch_encoder(dummy_patch).shape[1]\n        \n        self.fusion = nn.Sequential(\n            nn.Linear(patch_feat_size + 32, embed_dim),\n            nn.ReLU(),\n            nn.Linear(embed_dim, embed_dim),\n        )\n        \n    def forward(self, patches: torch.Tensor, metadata: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            patches: (batch, channels, height, width)\n            metadata: (batch, n_features)\n            \n        Returns:\n            embeddings: (batch, embed_dim)\n        \"\"\"\n        # TODO: Encode patches and metadata\n        patch_features = self.patch_encoder(patches)\n        metadata_features = self.metadata_encoder(metadata)\n        \n        # TODO: Fuse features\n        combined = torch.cat([patch_features, metadata_features], dim=1)\n        embeddings = self.fusion(combined)\n        \n        return embeddings\n\n# Create and test the model\nmodel = SimpleGFMEmbedding(\n    input_channels=bands, \n    metadata_features=encoded_metadata.shape[1], \n    embed_dim=EMBEDDING_DIM,\n    patch_size=PATCH_SIZE\n)\n\nprint(f\"‚úì Model created\")\nprint(f\"‚úì Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n\n‚úì Model created\n‚úì Model parameters: 130,848"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#step-7-end-to-end-pipeline-test",
    "href": "course-materials/c01-geospatial-data-foundations.html#step-7-end-to-end-pipeline-test",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 7: End-to-End Pipeline Test",
    "text": "Step 7: End-to-End Pipeline Test\nGoal: Run the complete pipeline and verify everything works together.\n\nüõ†Ô∏è Build It: Complete Pipeline Function\n\n\nCode\ndef geotiff_to_embeddings_pipeline(\n    file_path: Path,\n    patch_size: int = 8,\n    stride: int = 4,\n    batch_size: int = 8,\n    embed_dim: int = 256\n) -&gt; torch.Tensor:\n    \"\"\"\n    Complete pipeline from GeoTIFF to embeddings.\n    \n    Args:\n        file_path: Path to GeoTIFF file\n        patch_size: Size of patches to extract\n        stride: Step between patches  \n        batch_size: Batch size for processing\n        embed_dim: Embedding dimension\n        \n    Returns:\n        all_embeddings: (n_patches, embed_dim) tensor\n    \"\"\"\n    print(\"üöÄ Starting GeoTIFF ‚Üí Embeddings Pipeline\")\n    \n    # Step 1: Load data\n    print(\"üìÅ Loading GeoTIFF...\")\n    data, metadata = load_geotiff(file_path)\n    \n    # Step 2: Preprocess\n    print(\"üîß Preprocessing...\")\n    norm_data, norm_stats = minmax_normalize(data)\n    cropped_data = crop_to_patches(norm_data, patch_size, stride)\n    \n    # Step 3: Extract patches\n    print(\"‚úÇÔ∏è Extracting patches...\")\n    patches, coords = extract_patches_with_metadata(cropped_data, metadata['transform'], patch_size, stride)\n    \n    # Step 4: Encode metadata\n    print(\"üìä Encoding metadata...\")\n    encoded_meta = encode_metadata(coords)\n    \n    # Step 5: Create tensors\n    print(\"üî¢ Creating tensors...\")\n    patch_tensors, meta_tensors = create_tensors(patches, encoded_meta)\n    \n    # Step 6: Create dataset and dataloader\n    print(\"üì¶ Creating DataLoader...\")\n    dataset = GeospatialDataset(patch_tensors, meta_tensors)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    # Step 7: Create model and generate embeddings\n    print(\"üß† Generating embeddings...\")\n    model = SimpleGFMEmbedding(\n        input_channels=data.shape[0],\n        metadata_features=encoded_meta.shape[1], \n        embed_dim=embed_dim,\n        patch_size=patch_size\n    )\n    model.eval()\n    \n    all_embeddings = []\n    with torch.no_grad():\n        for patches_batch, meta_batch, _ in dataloader:\n            embeddings = model(patches_batch, meta_batch)\n            all_embeddings.append(embeddings)\n    \n    all_embeddings = torch.cat(all_embeddings, dim=0)\n    print(f\"‚úÖ Pipeline complete! Generated {len(all_embeddings)} embeddings\")\n    \n    return all_embeddings\n\n# Run the complete pipeline\nembeddings = geotiff_to_embeddings_pipeline(SAMPLE_PATH)\nprint(f\"\\nüéâ Final Result:\")\nprint(f\"‚úì Embeddings shape: {embeddings.shape}\")\nprint(f\"‚úì Embedding statistics:\")\nprint(f\"  Mean: {embeddings.mean().item():.4f}\")\nprint(f\"  Std: {embeddings.std().item():.4f}\")\nprint(f\"  Min: {embeddings.min().item():.4f}\")\nprint(f\"  Max: {embeddings.max().item():.4f}\")\n\n\nüöÄ Starting GeoTIFF ‚Üí Embeddings Pipeline\nüìÅ Loading GeoTIFF...\nüîß Preprocessing...\n‚úì Cropped from 64√ó64 to 64√ó64\n‚úì Will generate 15√ó15 = 225 patches\n‚úÇÔ∏è Extracting patches...\n‚úì Extracted 225 patches\n‚úì Patch shape: (225, 3, 8, 8)\n‚úì Coordinate shape: (225, 4)\nüìä Encoding metadata...\n‚úì Encoded metadata shape: (225, 4)\n‚úì Feature statistics:\n  center_x: mean=0.000, std=1.000\n  center_y: mean=0.000, std=1.000\n  area: mean=0.000, std=0.000\n  aspect_ratio: mean=1.000, std=0.000\nüî¢ Creating tensors...\n‚úì Patch tensors: torch.Size([225, 3, 8, 8]), dtype: torch.float32\n‚úì Metadata tensors: torch.Size([225, 4]), dtype: torch.float32\nüì¶ Creating DataLoader...\nüß† Generating embeddings...\n‚úÖ Pipeline complete! Generated 225 embeddings\n\nüéâ Final Result:\n‚úì Embeddings shape: torch.Size([225, 256])\n‚úì Embedding statistics:\n  Mean: 0.0023\n  Std: 0.0716\n  Min: -0.2246\n  Max: 0.2414\n\n\n\n\nüîç Verify It: Pipeline Output Analysis\n\n\nCode\n# Visualize embedding similarities\nprint(\"üîç Analyzing embedding relationships...\")\n\n# Check if we have enough embeddings for analysis\nif len(embeddings) &lt; 10:\n    print(f\"‚ö†Ô∏è Only {len(embeddings)} embeddings available, using all of them\")\n    sample_size = len(embeddings)\nelse:\n    print(f\"‚úì Using first 10 of {len(embeddings)} embeddings for similarity analysis\")\n    sample_size = 10\n\nif sample_size &gt; 1:\n    # Compute pairwise cosine similarities\n    from torch.nn.functional import cosine_similarity\n    \n    sample_embeddings = embeddings[:sample_size]\n    similarity_matrix = torch.zeros(sample_size, sample_size)\n    \n    for i in range(sample_size):\n        for j in range(sample_size):\n            if i == j:\n                similarity_matrix[i, j] = 1.0  # Perfect self-similarity\n            else:\n                sim = cosine_similarity(sample_embeddings[i:i+1], sample_embeddings[j:j+1], dim=1)\n                similarity_matrix[i, j] = sim.item()\n    \n    # Plot similarity matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(similarity_matrix.numpy(), cmap='viridis', vmin=-1, vmax=1)\n    plt.colorbar(label='Cosine Similarity')\n    plt.title(f'Embedding Similarity Matrix (First {sample_size} Patches)')\n    plt.xlabel('Patch Index')\n    plt.ylabel('Patch Index')\n    plt.show()\n    \n    print(f\"‚úì Average similarity: {similarity_matrix.mean().item():.4f}\")\n    print(f\"‚úì Similarity range: {similarity_matrix.min().item():.4f} to {similarity_matrix.max().item():.4f}\")\nelse:\n    print(\"‚ö†Ô∏è Not enough embeddings for similarity analysis\")\n\n\nüîç Analyzing embedding relationships...\n‚úì Using first 10 of 225 embeddings for similarity analysis\n\n\n\n\n\n\n\n\n\n‚úì Average similarity: 0.9832\n‚úì Similarity range: 0.9332 to 1.0000"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#conclusion",
    "href": "course-materials/c01-geospatial-data-foundations.html#conclusion",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Conclusion",
    "text": "Conclusion\nüéâ Congratulations! You‚Äôve successfully built a complete pipeline that transforms raw satellite imagery into model-ready embeddings.\n\nWhat You Built:\n\nGeoTIFF Loader: Extracts both pixel data and spatial metadata\nPreprocessing Functions: Normalization and spatial cropping\n\nPatch Extractor: Creates patches while preserving spatial context\nMetadata Encoder: Transforms coordinates into learned features\nPyTorch Integration: Dataset, DataLoader, and model components\nEmbedding Generator: Simple CNN that produces vector representations\n\n\n\nKey Insights:\n\nSpatial Context Matters: Each patch carries location information\nPreprocessing is Critical: Normalization ensures stable training\n\nModular Design: Each step can be optimized independently\nEnd-to-End Testing: Verify the complete pipeline works\n\n\n\nWhat‚Äôs Next:\nIn the following sessions, you‚Äôll enhance each component: - Week 2: Advanced attention mechanisms for spatial relationships - Week 3: Complete GFM architecture with transformer blocks - Week 4: Pretraining strategies and masked autoencoding\nThe pipeline you built today forms the foundation for everything that follows! üöÄ"
  },
  {
    "objectID": "course-materials/c01-geospatial-data-foundations.html#resources",
    "href": "course-materials/c01-geospatial-data-foundations.html#resources",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Resources",
    "text": "Resources\n\nPyTorch DataLoader Documentation\nRasterio User Guide\nGeospatial Foundation Model Examples"
  },
  {
    "objectID": "course-materials/c10-project-presentations-synthesis.html",
    "href": "course-materials/c10-project-presentations-synthesis.html",
    "title": "Week 10 Interactive Session: Project Presentations & Synthesis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n10\nStage 3: Apply & Deploy\nPresentations & Synthesis\nProject deliverables (no new geogfm code required)\n‚Äî\nPresent MVP builds, analysis, transition plan\n\n\n\n\n\n\nSynthesize architecture, training, and evaluation learnings\nPresent MVP results and insights\nOutline next steps with Prithvi/scale-up\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c10-project-presentations-synthesis.html#course-roadmap-mapping",
    "href": "course-materials/c10-project-presentations-synthesis.html#course-roadmap-mapping",
    "title": "Week 10 Interactive Session: Project Presentations & Synthesis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n10\nStage 3: Apply & Deploy\nPresentations & Synthesis\nProject deliverables (no new geogfm code required)\n‚Äî\nPresent MVP builds, analysis, transition plan\n\n\n\n\n\n\nSynthesize architecture, training, and evaluation learnings\nPresent MVP results and insights\nOutline next steps with Prithvi/scale-up\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c08-task-specific-finetuning.html",
    "href": "course-materials/c08-task-specific-finetuning.html",
    "title": "Week 8 Interactive Session: Task-Specific Fine-tuning",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n8\nStage 3: Apply & Deploy\nTask Fine-tuning\ntasks/{classification.py|segmentation.py} (light heads)\ntorch.nn.CrossEntropyLoss; timm optional\nHead swap on frozen encoder; small dataset demo\n\n\n\n\n\n\nImplement a simple classifier/segmentation head\nFine-tune with frozen encoder; evaluate on a tiny dataset\nDiscuss efficient strategies (LoRA/prompting as concepts)\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c08-task-specific-finetuning.html#course-roadmap-mapping",
    "href": "course-materials/c08-task-specific-finetuning.html#course-roadmap-mapping",
    "title": "Week 8 Interactive Session: Task-Specific Fine-tuning",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n8\nStage 3: Apply & Deploy\nTask Fine-tuning\ntasks/{classification.py|segmentation.py} (light heads)\ntorch.nn.CrossEntropyLoss; timm optional\nHead swap on frozen encoder; small dataset demo\n\n\n\n\n\n\nImplement a simple classifier/segmentation head\nFine-tune with frozen encoder; evaluate on a tiny dataset\nDiscuss efficient strategies (LoRA/prompting as concepts)\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c06-model-evaluation-analysis.html",
    "href": "course-materials/c06-model-evaluation-analysis.html",
    "title": "Week 6 Interactive Session: Model Evaluation & Analysis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n6\nStage 2: Train Foundation Model\nEvaluation & Analysis\nevaluation/visualization.py; (optional) evaluation/metrics.py\nmatplotlib; torchmetrics optional\nRecon visuals; track validation loss/PSNR\n\n\n\n\n\n\nImplement reconstruction visualization utilities\nAdd a simple metric (e.g., PSNR) and validation loop hooks\nInterpret embeddings or reconstructions qualitatively\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c06-model-evaluation-analysis.html#course-roadmap-mapping",
    "href": "course-materials/c06-model-evaluation-analysis.html#course-roadmap-mapping",
    "title": "Week 6 Interactive Session: Model Evaluation & Analysis",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n6\nStage 2: Train Foundation Model\nEvaluation & Analysis\nevaluation/visualization.py; (optional) evaluation/metrics.py\nmatplotlib; torchmetrics optional\nRecon visuals; track validation loss/PSNR\n\n\n\n\n\n\nImplement reconstruction visualization utilities\nAdd a simple metric (e.g., PSNR) and validation loop hooks\nInterpret embeddings or reconstructions qualitatively\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c04-pretraining-implementation.html",
    "href": "course-materials/c04-pretraining-implementation.html",
    "title": "Week 4 Interactive Session: Pretraining Implementation",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n4\nStage 2: Train Foundation Model\nMAE Pretraining\nmodels/gfm_mae.py; modules/losses/mae_loss.py\ntorch masking, numpy\nMasking + reconstruction; loss decreases on toy batch\n\n\n\n\n\n\nImplement MAE wrapper with masking strategy\nBuild masked reconstruction loss; verify gradients\nOverfit a toy batch to confirm learning\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c04-pretraining-implementation.html#course-roadmap-mapping",
    "href": "course-materials/c04-pretraining-implementation.html#course-roadmap-mapping",
    "title": "Week 4 Interactive Session: Pretraining Implementation",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n4\nStage 2: Train Foundation Model\nMAE Pretraining\nmodels/gfm_mae.py; modules/losses/mae_loss.py\ntorch masking, numpy\nMasking + reconstruction; loss decreases on toy batch\n\n\n\n\n\n\nImplement MAE wrapper with masking strategy\nBuild masked reconstruction loss; verify gradients\nOverfit a toy batch to confirm learning\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c02-spatial-temporal-attention-mechanisms.html",
    "href": "course-materials/c02-spatial-temporal-attention-mechanisms.html",
    "title": "Week 2 Interactive Session: Spatial-Temporal Attention Mechanisms",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n2\nStage 1: Build GFM Architecture\nAttention & Blocks\nmodules/embeddings/{patch_embedding.py, positional_encoding.py}; modules/attention/multihead_attention.py; modules/blocks/transformer_block.py\ntorch.nn (compare with torch.nn.MultiheadAttention)\nBlocks run forward with stable shapes; unit tests green\n\n\n\n\n\n\nImplement patch/positional embeddings and MHA from scratch\nAssemble a PreNorm transformer block (MHA + MLP)\nValidate tensor shapes and simple layer tests\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c02-spatial-temporal-attention-mechanisms.html#course-roadmap-mapping",
    "href": "course-materials/c02-spatial-temporal-attention-mechanisms.html#course-roadmap-mapping",
    "title": "Week 2 Interactive Session: Spatial-Temporal Attention Mechanisms",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n2\nStage 1: Build GFM Architecture\nAttention & Blocks\nmodules/embeddings/{patch_embedding.py, positional_encoding.py}; modules/attention/multihead_attention.py; modules/blocks/transformer_block.py\ntorch.nn (compare with torch.nn.MultiheadAttention)\nBlocks run forward with stable shapes; unit tests green\n\n\n\n\n\n\nImplement patch/positional embeddings and MHA from scratch\nAssemble a PreNorm transformer block (MHA + MLP)\nValidate tensor shapes and simple layer tests\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c03-complete-gfm-architecture.html",
    "href": "course-materials/c03-complete-gfm-architecture.html",
    "title": "Week 3 Interactive Session: Complete GFM Architecture",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n3\nStage 1: Build GFM Architecture\nComplete Architecture\nmodels/gfm_vit.py; modules/heads/reconstruction_head.py\ntorch.nn (timm as reference)\nEncoder assembled; end-to-end forward on dummy input\n\n\n\n\n\n\nWire blocks into a GeoViT-style encoder\nAdd a simple reconstruction head\nRun end-to-end forward pass on dummy input\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c03-complete-gfm-architecture.html#course-roadmap-mapping",
    "href": "course-materials/c03-complete-gfm-architecture.html#course-roadmap-mapping",
    "title": "Week 3 Interactive Session: Complete GFM Architecture",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n3\nStage 1: Build GFM Architecture\nComplete Architecture\nmodels/gfm_vit.py; modules/heads/reconstruction_head.py\ntorch.nn (timm as reference)\nEncoder assembled; end-to-end forward on dummy input\n\n\n\n\n\n\nWire blocks into a GeoViT-style encoder\nAdd a simple reconstruction head\nRun end-to-end forward pass on dummy input\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c05-training-loop-optimization.html",
    "href": "course-materials/c05-training-loop-optimization.html",
    "title": "Week 5 Interactive Session: Training Loop Optimization",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n5\nStage 2: Train Foundation Model\nTraining Optimization\ntraining/optimizer.py; training/loop.py\ntorch.optim.AdamW; schedulers, AMP optional\nSingle-epoch run; basic checkpoint save/restore\n\n\n\n\n\n\nBuild fit, train_step, eval_step with logging\nConfigure AdamW; optionally add LR scheduler/AMP\nSave and restore a basic checkpoint\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c05-training-loop-optimization.html#course-roadmap-mapping",
    "href": "course-materials/c05-training-loop-optimization.html#course-roadmap-mapping",
    "title": "Week 5 Interactive Session: Training Loop Optimization",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n5\nStage 2: Train Foundation Model\nTraining Optimization\ntraining/optimizer.py; training/loop.py\ntorch.optim.AdamW; schedulers, AMP optional\nSingle-epoch run; basic checkpoint save/restore\n\n\n\n\n\n\nBuild fit, train_step, eval_step with logging\nConfigure AdamW; optionally add LR scheduler/AMP\nSave and restore a basic checkpoint\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c07-integration-with-existing-models.html",
    "href": "course-materials/c07-integration-with-existing-models.html",
    "title": "Week 7 Interactive Session: Integration with Existing Models",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n7\nStage 2: Train Foundation Model\nIntegration w/ Pretrained\n(light) core/registry.py; interoperability/huggingface.py stubs\nhuggingface_hub, transformers optional\nShow mapping to Prithvi structure; plan switch\n\n\n\n\n\n\nCreate a light registry for model variants\nAdd basic HF Hub glue to load model configs/weights\nCompare MVP structure to Prithvi and define switch points\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c07-integration-with-existing-models.html#course-roadmap-mapping",
    "href": "course-materials/c07-integration-with-existing-models.html#course-roadmap-mapping",
    "title": "Week 7 Interactive Session: Integration with Existing Models",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n7\nStage 2: Train Foundation Model\nIntegration w/ Pretrained\n(light) core/registry.py; interoperability/huggingface.py stubs\nhuggingface_hub, transformers optional\nShow mapping to Prithvi structure; plan switch\n\n\n\n\n\n\nCreate a light registry for model variants\nAdd basic HF Hub glue to load model configs/weights\nCompare MVP structure to Prithvi and define switch points\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c09-model-implementation-deployment.html",
    "href": "course-materials/c09-model-implementation-deployment.html",
    "title": "Week 9 Interactive Session: Model Implementation & Deployment",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n9\nStage 3: Apply & Deploy\nDeployment & Inference\ninference/{tiling.py, sliding_window.py}\nnumpy, rasterio windows\nSliding-window inference on a small scene\n\n\n\n\n\n\nImplement sliding-window/tiling inference utilities\nStitch predictions; sanity-check outputs\nOutline API/UI deployment considerations\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c09-model-implementation-deployment.html#course-roadmap-mapping",
    "href": "course-materials/c09-model-implementation-deployment.html#course-roadmap-mapping",
    "title": "Week 9 Interactive Session: Model Implementation & Deployment",
    "section": "",
    "text": "This week‚Äôs work in the broader GFM plan.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n9\nStage 3: Apply & Deploy\nDeployment & Inference\ninference/{tiling.py, sliding_window.py}\nnumpy, rasterio windows\nSliding-window inference on a small scene\n\n\n\n\n\n\nImplement sliding-window/tiling inference utilities\nStitch predictions; sanity-check outputs\nOutline API/UI deployment considerations\n\nInteractive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/c00-introduction-to-deeplearning-architecture.html",
    "href": "course-materials/c00-introduction-to-deeplearning-architecture.html",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/c00-introduction-to-deeplearning-architecture.html#week-0-overview",
    "href": "course-materials/c00-introduction-to-deeplearning-architecture.html#week-0-overview",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/c00-introduction-to-deeplearning-architecture.html#build-your-own-gfm-architecture-cheatsheet",
    "href": "course-materials/c00-introduction-to-deeplearning-architecture.html#build-your-own-gfm-architecture-cheatsheet",
    "title": "Week 0: Getting Started",
    "section": "Build-your-own GFM: Architecture Cheatsheet",
    "text": "Build-your-own GFM: Architecture Cheatsheet\nThis is a quick-reference for how our Geospatial Foundation Model (GFM) is organized, what each part does, and where to look during the course. We implement simple, readable components first, then show how to swap in optimized library counterparts as needed.\n\nRoadmap at a glance\n\ngraph TD\n  A[Week 1: Data Foundations] --&gt; B[Week 2: Attention & Blocks]\n  B --&gt; C[Week 3: Full Encoder]\n  C --&gt; D[Week 4: MAE Pretraining]\n  D --&gt; E[Week 5: Training Loop]\n  E --&gt; F[Week 6: Eval & Viz]\n  F --&gt; G[Week 7+: Interop & Scale]\n\n\nMinimal structure you‚Äôll use\ngeogfm/\n  core/\n    config.py                # Minimal typed configs for model, data, training\n  data/\n    loaders.py               # build_dataloader(...)\n    datasets/\n      stac_dataset.py        # Simple STAC-backed dataset\n    transforms/\n      normalization.py       # Per-channel normalization\n      patchify.py            # Extract fixed-size patches\n  modules/\n    attention/\n      multihead_attention.py     # Standard MHA (from scratch)\n    embeddings/\n      patch_embedding.py         # Conv patch embedding\n      positional_encoding.py     # Simple positional encoding\n    blocks/\n      transformer_block.py       # PreNorm block (MHA + MLP)\n    heads/\n      reconstruction_head.py     # Lightweight decoder/readout\n    losses/\n      mae_loss.py                # Masked reconstruction loss\n  models/\n    gfm_vit.py                   # GeoViT-style encoder\n    gfm_mae.py                   # MAE wrapper (masking + encoder + head)\n  training/\n    optimizer.py                 # AdamW builder\n    loop.py                      # fit/train_step/eval_step with basic checkpointing\n  evaluation/\n    visualization.py             # Visualize inputs vs reconstructions\n\n# Outside the package (repo root)\nconfigs/   # Small YAML/JSON run configs\ntests/     # Unit tests\ndata/      # Datasets, splits, stats, build scripts\n\n\nWhat each part does (one-liners)\n\ncore/config.py: Typed configs for model/data/training; keeps parameters organized.\ndata/datasets/stac_dataset.py: Reads imagery + metadata (e.g., STAC), returns tensors.\ndata/transforms/normalization.py: Normalizes channels using precomputed stats.\ndata/transforms/patchify.py: Turns large images into uniform patches for ViT.\ndata/loaders.py: Builds PyTorch DataLoaders for train/val.\nmodules/embeddings/patch_embedding.py: Projects image patches into token vectors.\nmodules/embeddings/positional_encoding.py: Adds position info to tokens.\nmodules/attention/multihead_attention.py: Lets tokens attend to each other.\nmodules/blocks/transformer_block.py: Core transformer layer (attention + MLP).\nmodules/heads/reconstruction_head.py: Reconstructs pixels from encoded tokens.\nmodules/losses/mae_loss.py: Computes masked reconstruction loss for MAE.\nmodels/gfm_vit.py: Assembles the encoder backbone from blocks.\nmodels/gfm_mae.py: Wraps encoder with masking + reconstruction for pretraining.\ntraining/optimizer.py: Creates AdamW with common defaults.\ntraining/loop.py: Runs epochs, backprop, validation, and simple checkpoints.\nevaluation/visualization.py: Plots sample inputs and reconstructions.\n\n\n\nFrom-scratch vs library-backed\n\nUse PyTorch for Dataset/DataLoader, AdamW, schedulers, AMP, and checkpointing.\nBuild core blocks from scratch first: PatchEmbedding, MHA, TransformerBlock, MAE loss/head.\nLater, swap in optimized options: torch.nn.MultiheadAttention, timm ViT blocks, FlashAttention, TorchGeo datasets, torchmetrics.\n\n\n\nQuick start (conceptual)\nfrom geogfm.core.config import ModelConfig, DataConfig, TrainConfig\nfrom geogfm.models.gfm_vit import GeoViTBackbone\nfrom geogfm.models.gfm_mae import MaskedAutoencoder\nfrom geogfm.data.loaders import build_dataloader\nfrom geogfm.training.loop import fit\n\nmodel_cfg = ModelConfig(architecture=\"gfm_vit\", embed_dim=768, depth=12, image_size=224)\ndata_cfg = DataConfig(dataset=\"stac\", patch_size=16, num_workers=8)\ntrain_cfg = TrainConfig(epochs=1, batch_size=8, optimizer={\"name\": \"adamw\", \"lr\": 2e-4})\n\nencoder = GeoViTBackbone(model_cfg)\nmodel = MaskedAutoencoder(model_cfg, encoder)\ntrain_dl, val_dl = build_dataloader(data_cfg)\nfit(model, (train_dl, val_dl), train_cfg)\nWhat to notice: - The encoder and MAE wrapper are separate, so we can reuse the encoder for other tasks later. - Data transforms (normalize/patchify) are decoupled from the model and driven by config.\n\n\nMVP vs later phases\n\nMVP (Weeks 1‚Äì6): files shown above; single-node training; basic logging and visualization.\nLater (Weeks 7‚Äì10): interop with existing models (e.g., Prithvi), task heads (classification/segmentation), inference tiling, Hub integration.\n\n\n\nComprehensive course roadmap (stages, files, libraries)\nThis table maps each week to the broader stages (see the course index), the key geogfm files you‚Äôll touch, and the primary deep learning tools you‚Äôll rely on.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nStage\nFocus\nYou will build (geogfm)\nLibrary tools\nOutcome\n\n\n\n\n1\nStage 1: Build GFM Architecture\nData Foundations\ncore/config.py; data/datasets/stac_dataset.py; data/transforms/{normalization.py, patchify.py}; data/loaders.py\ntorch.utils.data.Dataset/DataLoader, rasterio, numpy\nConfig-driven dataloaders that yield normalized patches\n\n\n2\nStage 1\nAttention & Blocks\nmodules/embeddings/{patch_embedding.py, positional_encoding.py}; modules/attention/multihead_attention.py; modules/blocks/transformer_block.py\ntorch.nn (compare with torch.nn.MultiheadAttention)\nBlocks run forward with stable shapes; unit tests green\n\n\n3\nStage 1\nComplete Architecture\nmodels/gfm_vit.py; modules/heads/reconstruction_head.py\ntorch.nn (timm as reference)\nEncoder assembled; end-to-end forward on dummy input\n\n\n4\nStage 2: Train Foundation Model\nMAE Pretraining\nmodels/gfm_mae.py; modules/losses/mae_loss.py\ntorch masking utilities, numpy\nMasking + reconstruction; loss decreases on toy batch\n\n\n5\nStage 2\nTraining Optimization\ntraining/optimizer.py; training/loop.py\ntorch.optim.AdamW; schedulers, AMP optional\nSingle-epoch run; basic checkpoint save/restore\n\n\n6\nStage 2\nEvaluation & Analysis\nevaluation/visualization.py; (optional) evaluation/metrics.py\nmatplotlib; torchmetrics optional\nRecon visuals; track validation loss/PSNR\n\n\n7\nStage 2\nIntegration w/ Pretrained\n(light) core/registry.py; interoperability/huggingface.py stubs\nhuggingface_hub, transformers optional\nShow mapping to Prithvi structure; plan switch\n\n\n8\nStage 3: Apply & Deploy\nTask Fine-tuning\ntasks/{classification.py|segmentation.py} (light heads)\ntorch.nn.CrossEntropyLoss; timm optional\nHead swap on frozen encoder; small dataset demo\n\n\n9\nStage 3\nDeployment & Inference\ninference/{tiling.py, sliding_window.py}\nnumpy, rasterio windows\nSliding-window inference on a small scene\n\n\n10\nStage 3\nPresentations & Synthesis\nProject deliverables (no new geogfm code required)\n‚Äî\nPresent MVP builds, analysis, transition plan"
  }
]