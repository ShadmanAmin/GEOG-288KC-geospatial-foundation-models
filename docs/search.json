[
  {
    "objectID": "example_course/course-materials/cheatsheets/seaborn.html",
    "href": "example_course/course-materials/cheatsheets/seaborn.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Seaborn is a powerful Python data visualization library built on top of Matplotlib. It provides a high-level interface for drawing attractive statistical graphics. Seaborn is particularly useful for creating complex visualizations with just a few lines of code.\nKey features: - Built-in themes for attractive plots - Statistical plot types - Integration with Pandas DataFrames - Automatic estimation and plotting of statistical models"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/seaborn.html#introduction-to-seaborn",
    "href": "example_course/course-materials/cheatsheets/seaborn.html#introduction-to-seaborn",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Seaborn is a powerful Python data visualization library built on top of Matplotlib. It provides a high-level interface for drawing attractive statistical graphics. Seaborn is particularly useful for creating complex visualizations with just a few lines of code.\nKey features: - Built-in themes for attractive plots - Statistical plot types - Integration with Pandas DataFrames - Automatic estimation and plotting of statistical models"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/seaborn.html#setting-up-seaborn",
    "href": "example_course/course-materials/cheatsheets/seaborn.html#setting-up-seaborn",
    "title": "EDS 217 Cheatsheet",
    "section": "Setting up Seaborn",
    "text": "Setting up Seaborn\nTo use Seaborn, you need to import it along with other necessary libraries:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Set the style for all plots\nsns.set_style(\"whitegrid\")"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/seaborn.html#common-plot-types-in-environmental-data-science",
    "href": "example_course/course-materials/cheatsheets/seaborn.html#common-plot-types-in-environmental-data-science",
    "title": "EDS 217 Cheatsheet",
    "section": "Common Plot Types in Environmental Data Science",
    "text": "Common Plot Types in Environmental Data Science\nFor these examples, we’ll use various built-in datasets provided by Seaborn. These datasets are included with the library and are commonly used for demonstration purposes.\n\n1. Scatter Plots\nUseful for showing relationships between two continuous variables.\n\n\nCode\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips.head())\n\n\n   total_bill   tip     sex smoker  day    time  size\n0       16.99  1.01  Female     No  Sun  Dinner     2\n1       10.34  1.66    Male     No  Sun  Dinner     3\n2       21.01  3.50    Male     No  Sun  Dinner     3\n3       23.68  3.31    Male     No  Sun  Dinner     2\n4       24.59  3.61  Female     No  Sun  Dinner     4\n\n\n\n\nCode\n# Basic scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='total_bill', y='tip', data=tips)\nplt.title('Relationship between Total Bill and Tip')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Add hue for a third variable\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='total_bill', y='tip', hue='time', data=tips)\nplt.title('Relationship between Total Bill and Tip, colored by Time of Day')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe ‘tips’ dataset contains information about restaurant bills and tips. It’s a classic dataset used for demonstrating various plotting techniques.\n\n\n2. Line Plots\nIdeal for time series data or showing trends.\n\n\nCode\n# Load the flights dataset\nflights = sns.load_dataset(\"flights\")\nprint(flights.head())\n\n\n   year month  passengers\n0  1949   Jan         112\n1  1949   Feb         118\n2  1949   Mar         132\n3  1949   Apr         129\n4  1949   May         121\n\n\n\n\nCode\n# Basic line plot (uncertainty bounds calculated auto-magically by grouping rows containing the same year!)\nplt.figure(figsize=(12, 6))\nsns.lineplot(x='year', y='passengers', data=flights)\nplt.title('Number of Passengers over Time')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Multiple lines with confidence intervals\nplt.figure(figsize=(12, 6))\nsns.lineplot(x='year', y='passengers', hue='month', data=flights)\nplt.title('Number of Passengers over Time, by Month')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe ‘flights’ dataset contains information about passenger numbers for flights over time. It’s useful for demonstrating time series visualizations.\n\n\n3. Bar Plots\nGreat for comparing quantities across different categories.\n\n\nCode\n# Load the titanic dataset\ntitanic = sns.load_dataset(\"titanic\")\nprint(titanic.head())\n\n\n   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n0         0       3    male  22.0      1      0   7.2500        S  Third   \n1         1       1  female  38.0      1      0  71.2833        C  First   \n2         1       3  female  26.0      0      0   7.9250        S  Third   \n3         1       1  female  35.0      1      0  53.1000        S  First   \n4         0       3    male  35.0      0      0   8.0500        S  Third   \n\n     who  adult_male deck  embark_town alive  alone  \n0    man        True  NaN  Southampton    no  False  \n1  woman       False    C    Cherbourg   yes  False  \n2  woman       False  NaN  Southampton   yes   True  \n3  woman       False    C  Southampton   yes  False  \n4    man        True  NaN  Southampton    no   True  \n\n\n\n\nCode\n# Basic bar plot\nplt.figure(figsize=(10, 6))\nsns.barplot(x='class', y='fare', data=titanic)\nplt.title('Average Fare by Passenger Class')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Grouped bar plot\nplt.figure(figsize=(10, 6))\nsns.barplot(x='class', y='fare', hue='sex', data=titanic)\nplt.title('Average Fare by Passenger Class and Sex')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe ‘titanic’ dataset contains information about passengers on the Titanic, including their class, sex, age, and survival status.\n\n\n4. Box Plots\nUseful for showing distribution of data across categories.\n\n\nCode\n# Basic box plot\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='class', y='age', data=titanic)\nplt.title('Distribution of Age by Passenger Class')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Add individual data points\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='class', y='age', data=titanic)\nsns.swarmplot(x='class', y='age', data=titanic, color=\".25\", size=3)\nplt.title('Distribution of Age by Passenger Class with Individual Points')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5. Violin Plots\nSimilar to box plots but show the full distribution of data.\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.violinplot(x='class', y='age', data=titanic)\nplt.title('Distribution of Age by Passenger Class (Violin Plot)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n6. Heatmaps\nExcellent for visualizing correlation matrices or gridded data.\n\n\nCode\n# Load the penguins dataset\npenguins = sns.load_dataset(\"penguins\")\nprint(penguins.head(5))\n\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen            39.1           18.7              181.0   \n1  Adelie  Torgersen            39.5           17.4              186.0   \n2  Adelie  Torgersen            40.3           18.0              195.0   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen            36.7           19.3              193.0   \n\n   body_mass_g     sex  \n0       3750.0    Male  \n1       3800.0  Female  \n2       3250.0  Female  \n3          NaN     NaN  \n4       3450.0  Female  \n\n\n\n\nCode\n# Correlation heatmap\nplt.figure(figsize=(10, 8))\ncorrelation = penguins.select_dtypes(include=[np.number]).corr()\nsns.heatmap(correlation, annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Penguin Measurements')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe ‘penguins’ dataset contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/seaborn.html#using-seaborn-for-data-exploration-with-dataframes",
    "href": "example_course/course-materials/cheatsheets/seaborn.html#using-seaborn-for-data-exploration-with-dataframes",
    "title": "EDS 217 Cheatsheet",
    "section": "Using Seaborn for Data Exploration with DataFrames",
    "text": "Using Seaborn for Data Exploration with DataFrames\nSeaborn is particularly powerful when working with Pandas DataFrames, as it can automatically infer variable types and choose appropriate plot types.\n\nQuick Data Overview\nRecall the structure of the penguins dataframe, which has a combination of measured and categorical values:\n\n\nCode\nprint(penguins.head())\n\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen            39.1           18.7              181.0   \n1  Adelie  Torgersen            39.5           17.4              186.0   \n2  Adelie  Torgersen            40.3           18.0              195.0   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen            36.7           19.3              193.0   \n\n   body_mass_g     sex  \n0       3750.0    Male  \n1       3800.0  Female  \n2       3250.0  Female  \n3          NaN     NaN  \n4       3450.0  Female  \n\n\nWe can explore the distribution of every numerical variable as well as the pair-wise relationship between all the variables in a dataframe using pairplot and can use a categorical variable to further organize the data within each plot using the hue argument.\n\n\nCode\n# Get a quick overview of numerical variables\nsns.pairplot(penguins, hue='species')\nplt.suptitle('Overview of Penguin Measurements by Species', y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Visualize distributions of all numerical variables\nsns.displot(penguins['bill_length_mm'], kind='kde')\nplt.suptitle('Distribution of Penguin Bill Lengths',y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships\n\n\nCode\n# Explore relationship between variables\nsns.relplot(data=penguins, x='bill_length_mm', y='bill_depth_mm', hue='species', style='sex')\nplt.title('Relationship between Bill Length and Depth')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Facet plots for multi-dimensional exploration\ng = sns.FacetGrid(penguins, col='species', row='sex')\ng.map(sns.scatterplot, 'bill_length_mm', 'bill_depth_mm')\ng.add_legend()\nplt.suptitle('Bill Measurements by Species and Sex', y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCategorical Data Exploration\n\n\nCode\n# Compare distributions across categories\nsns.catplot(data=penguins, kind='box', x='species', y='body_mass_g', hue='sex')\nplt.title('Body Mass Distribution by Species and Sex')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Count plots for categorical variables\nsns.countplot(data=penguins, x='island', hue='species')\nplt.title('Penguin Species Count by Island')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTime Series Exploration\n\n\nCode\n# Visualize trends over time\nsns.lineplot(data=flights, x='year', y='passengers', hue='month')\nplt.title('Passenger Numbers by Year and Month')\nplt.show()"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/seaborn.html#key-points",
    "href": "example_course/course-materials/cheatsheets/seaborn.html#key-points",
    "title": "EDS 217 Cheatsheet",
    "section": "Key Points",
    "text": "Key Points\n\nSeaborn works seamlessly with Pandas DataFrames, making it easy to create plots directly from your data.\nThe library offers a wide range of plot types suitable for various environmental data science tasks.\nSeaborn’s statistical plotting functions (like regplot and lmplot) can automatically fit and visualize linear regressions.\nThe pairplot function and FacetGrid class are powerful for creating multi-panel plots to explore complex datasets.\nSeaborn’s themes and color palettes help create publication-quality figures with minimal customization."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/seaborn.html#resources",
    "href": "example_course/course-materials/cheatsheets/seaborn.html#resources",
    "title": "EDS 217 Cheatsheet",
    "section": "Resources",
    "text": "Resources\n\nSeaborn Official Documentation\nSeaborn Tutorial\nSeaborn Example Gallery\nSeaborn Examples at the Python Graph Gallery\nVisualization in Seaborn (Jake VanderPlas)\n\nRemember, while Seaborn is powerful, it’s built on matplotlib. For more customization options, you might need to use matplotlib functions directly."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This cheatsheet provides a quick reference for common operations on Pandas Series. It’s designed for beginning data science students who are just starting to work with Pandas."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#introduction",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#introduction",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This cheatsheet provides a quick reference for common operations on Pandas Series. It’s designed for beginning data science students who are just starting to work with Pandas."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#importing-pandas",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#importing-pandas",
    "title": "EDS 217 Cheatsheet",
    "section": "Importing Pandas",
    "text": "Importing Pandas\nAlways start by importing pandas:\n\n\nCode\nimport pandas as pd"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#creating-a-series",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#creating-a-series",
    "title": "EDS 217 Cheatsheet",
    "section": "Creating a Series",
    "text": "Creating a Series\n\nFrom a list\n\n\nCode\ndata = [1, 2, 3, 4, 5]\ns = pd.Series(data)\nprint(s)\n\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\n\n\nFrom a dictionary\n\n\nCode\ndata = {'a': 0., 'b': 1., 'c': 2.}\ns = pd.Series(data)\nprint(s)\n\n\na    0.0\nb    1.0\nc    2.0\ndtype: float64\n\n\n\n\nWith custom index\n\n\nCode\ns = pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e'])\nprint(s)\n\n\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#basic-series-information",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#basic-series-information",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Series Information",
    "text": "Basic Series Information\n\n\nCode\n# Display the first few elements\nprint(s.head())\n\n# Get basic information about the Series\nprint(s.info())\n\n# Get summary statistics\nprint(s.describe())\n\n# Get index\nprint(s.index)\n\n# Get values\nprint(s.values)\n\n\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64\n&lt;class 'pandas.core.series.Series'&gt;\nIndex: 5 entries, a to e\nSeries name: None\nNon-Null Count  Dtype\n--------------  -----\n5 non-null      int64\ndtypes: int64(1)\nmemory usage: 80.0+ bytes\nNone\ncount    5.000000\nmean     3.000000\nstd      1.581139\nmin      1.000000\n25%      2.000000\n50%      3.000000\n75%      4.000000\nmax      5.000000\ndtype: float64\nIndex(['a', 'b', 'c', 'd', 'e'], dtype='object')\n[1 2 3 4 5]"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#selecting-data",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#selecting-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Selecting Data",
    "text": "Selecting Data\n\nBy label\n\n\nCode\n# Select a single element\nelement = s['a']\n\n# Select multiple elements\nsubset = s[['a', 'c', 'e']]\n\n\n\n\nBy position\n\n\nCode\n# Select by integer index (direct selection is being deprecated)\nfirst_element = s.iloc[0]\n\n# Select a slice by index (still okay):\nsubset = s[1:4]\n\n# Select a slice (using iloc)\nsubset = s.iloc[1:4]\n\n\n\n\nBy condition\n\n\nCode\n# Select elements greater than 2\ngreater_than_two = s[s &gt; 2]"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#basic-data-manipulation",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#basic-data-manipulation",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Data Manipulation",
    "text": "Basic Data Manipulation\n\nUpdating values\n\n\nCode\ns['a'] = 10\n\n\n\n\nRemoving elements\n\n\nCode\ns=s.drop(labels=['a'])\n\n\n\n\nAdding elements to a Series\n\n\nCode\nanother_series = pd.Series(\n    [3, 4, 5], \n    index=['f', 'g', 'h']\n)\ns = pd.concat([s, another_series])\n\n\nThe pd.concat() command (short for concatenate) is now the preferred method for extending series.\n\n\n\n\n\n\nImportant\n\n\n\nThe pd.concat() command takes a list of pd.Series objects to concatenate. This means you must create a pd.Series of new values to extend an existing pd.Series.\n\n\n\n\nUpdating elements based on their value using mask\n\n\nCode\nprint(s)\ns = s.mask(s &gt; 2, s * 2)\nprint(s)\n\n\nb    2\nc    3\nd    4\ne    5\nf    3\ng    4\nh    5\ndtype: int64\nb     2\nc     6\nd     8\ne    10\nf     6\ng     8\nh    10\ndtype: int64\n\n\nThis line uses the greater than (&gt;) logical operator within the mask() function to update the series. It will double the values in series where the condition s &gt; 5 is True, while leaving other values unchanged.\n\n\nReplacing elements based on their value using where\n\n\nCode\nprint(s)\ns = s.where(s &lt; 8, 12)\nprint(s)\n\n\nb     2\nc     6\nd     8\ne    10\nf     6\ng     8\nh    10\ndtype: int64\nb     2\nc     6\nd    12\ne    12\nf     6\ng    12\nh    12\ndtype: int64\n\n\nThis line of code will update the values in series where condition is False (i.e. where s is not less than 8), replacing them with 12. The values where condition is True will remain unchanged.\n\n\nApplying functions\n\nApplying a newly-defined function\n\n\nCode\ndef squared(x):\n    ''' Square the value of x '''\n    return x**2\n\n# Apply a function to each element\ns_squared = s.apply(squared)\n\n\n\n\nApplying a lambda (temporary) function\n\n\nCode\ns_squared = s.apply(lambda x: x**2)\n\n\nLambda functions allow for doing transformations with temporary functions instead of having to define functions seperately. They are good for quick, 1-off transformations.\n\n\n\nHandling missing values\n\n\nCode\n# Drop missing values\ns_cleaned = s.dropna()\n\n# Fill missing values\ns_filled = s.fillna(0)  # Fills with 0"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#basic-calculations",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#basic-calculations",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Calculations",
    "text": "Basic Calculations\n\n\nCode\n# Calculate mean\nmean_value = s.mean()\n\n# Calculate sum\nsum_value = s.sum()\n\n# Calculate maximum\nmax_value = s.max()"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#sorting",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#sorting",
    "title": "EDS 217 Cheatsheet",
    "section": "Sorting",
    "text": "Sorting\n\n\nCode\nprint(s)\n# Sort by values\ns_sorted = s.sort_values()\n\n# Sort by index\ns_sorted_by_index = s.sort_index()\n\n\nb     2\nc     6\nd    12\ne    12\nf     6\ng    12\nh    12\ndtype: int64"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#reindexing",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#reindexing",
    "title": "EDS 217 Cheatsheet",
    "section": "Reindexing",
    "text": "Reindexing\n\n\nCode\nprint(f\"Original Series:\\n{s}\\n\", sep='')\nnew_index = ['a', 'c', 'b', 'f', 'e', 'd', 'g', 'h', 'i', 'b']\ns_reindexed = s.reindex(new_index)\nprint(f\"Re-indexed series:\\n{s_reindexed}\")\n\n\nOriginal Series:\nb     2\nc     6\nd    12\ne    12\nf     6\ng    12\nh    12\ndtype: int64\n\nRe-indexed series:\na     NaN\nc     6.0\nb     2.0\nf     6.0\ne    12.0\nd    12.0\ng    12.0\nh    12.0\ni     NaN\nb     2.0\ndtype: float64\n\n\n\nthe reindex() command takes an ordered list that specifies the indicies that should be used to make a new pd.Series object. The list of indicies supplied to reindex() must have some indicies in common with the existing index. Indicies that do not appear in the existing Series will be set to NaN in the new Series. Repetition of indicies is allowed."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#combining-series",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#combining-series",
    "title": "EDS 217 Cheatsheet",
    "section": "Combining Series",
    "text": "Combining Series\n\n\nCode\ns1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\ns2 = pd.Series([4, 5, 6], index=['d', 'e', 'f'])\ns_combined = pd.concat([s1, s2])"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#converting-to-other-data-types",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#converting-to-other-data-types",
    "title": "EDS 217 Cheatsheet",
    "section": "Converting to Other Data Types",
    "text": "Converting to Other Data Types\n\n\nCode\n# To list\nlist_data = s.tolist()\n\n# To dictionary\ndict_data = s.to_dict()\n\n# To DataFrame\ndf = s.to_frame()"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_series.html#further-learning",
    "href": "example_course/course-materials/cheatsheets/pandas_series.html#further-learning",
    "title": "EDS 217 Cheatsheet",
    "section": "Further Learning",
    "text": "Further Learning\nFor more advanced operations and in-depth explanations, check out these resources:\n\nPandas Official Documentation\n10 Minutes to Pandas\nPython for Data Analysis by Wes McKinney\n\nRemember, practice is key! Try these operations with different datasets to become more comfortable with Pandas Series."
  },
  {
    "objectID": "example_course/course-materials/lectures/pandas_workflow.html#data-loading",
    "href": "example_course/course-materials/lectures/pandas_workflow.html#data-loading",
    "title": "Efficient Data Workflows in Pandas",
    "section": "1. Data Loading",
    "text": "1. Data Loading\nImportance\nLoading data properly is essential! It serves as the foundation for all subsequent operations.\nCommon Operations\n\nread_csv()\nread_excel()\nread_sql()"
  },
  {
    "objectID": "example_course/course-materials/lectures/pandas_workflow.html#data-cleaning",
    "href": "example_course/course-materials/lectures/pandas_workflow.html#data-cleaning",
    "title": "Efficient Data Workflows in Pandas",
    "section": "2. Data Cleaning",
    "text": "2. Data Cleaning\nImportance\nData cleaning is crucial for correcting or removing inaccurate, corrupted, or irrelevant data from the dataset.\nCommon Operations\n\ndropna(): Removes rows or columns with missing values.\nfillna(): Fills missing values.\nastype(): Converts column data types.\nrename(): Renames columns or index names.\ndrop(): Removes rows or columns that match given labels."
  },
  {
    "objectID": "example_course/course-materials/lectures/pandas_workflow.html#data-transformation",
    "href": "example_course/course-materials/lectures/pandas_workflow.html#data-transformation",
    "title": "Efficient Data Workflows in Pandas",
    "section": "3. Data Transformation",
    "text": "3. Data Transformation\nImportance\nData transformation involves modifying data to prepare it for analysis, which may include filtering, sorting, or adding new columns.\nCommon Operations\n\nquery(): Filter DataFrame using a query expression string.\nassign(): Add new columns or overwrite existing ones.\napply(): Apply a function to rows or columns.\nsort_values(): Sort by the values of columns."
  },
  {
    "objectID": "example_course/course-materials/lectures/pandas_workflow.html#combining-data",
    "href": "example_course/course-materials/lectures/pandas_workflow.html#combining-data",
    "title": "Efficient Data Workflows in Pandas",
    "section": "4. Combining Data",
    "text": "4. Combining Data\nImportance\nCombining data is essential when you need to enrich or expand your dataset through additions from other data sources.\nCommon Operations\n\nmerge(): Combines DataFrames based on keys.\njoin(): Joins DataFrames using index or key.\nconcat(): Concatenates DataFrames along an axis."
  },
  {
    "objectID": "example_course/course-materials/lectures/pandas_workflow.html#grouping-and-aggregation",
    "href": "example_course/course-materials/lectures/pandas_workflow.html#grouping-and-aggregation",
    "title": "Efficient Data Workflows in Pandas",
    "section": "5. Grouping and Aggregation",
    "text": "5. Grouping and Aggregation\nImportance\nGrouping and aggregation are critical for summarizing data, which can help in identifying patterns or performing segment-wise analysis.\nCommon Operations\n\ngroupby(): Group data by columns for aggregation.\nsum(): Sum values across rows/columns.\nmean(): Calculate mean of values across rows/columns.\naggregate(): Apply functions to groups, reducing dimensions."
  },
  {
    "objectID": "example_course/course-materials/lectures/pandas_workflow.html#data-summarization",
    "href": "example_course/course-materials/lectures/pandas_workflow.html#data-summarization",
    "title": "Efficient Data Workflows in Pandas",
    "section": "6. Data Summarization",
    "text": "6. Data Summarization\nImportance\nData summarization provides a quick look into the dataset, which is helpful for initial analyses and decision-making.\nCommon Operations\n\ndescribe()\nvalue_counts()"
  },
  {
    "objectID": "example_course/course-materials/lectures/pandas_workflow.html#outputexport",
    "href": "example_course/course-materials/lectures/pandas_workflow.html#outputexport",
    "title": "Efficient Data Workflows in Pandas",
    "section": "7. Output/Export",
    "text": "7. Output/Export\nImportance\nThe final step in any data analysis workflow is to save or export your results, making them available for sharing or further processing.\nCommon Operations\n\nto_csv()\nto_excel()\nto_json()"
  },
  {
    "objectID": "example_course/course-materials/lectures/pandas_workflow.html#combining-the-workflow-with-method-chaining",
    "href": "example_course/course-materials/lectures/pandas_workflow.html#combining-the-workflow-with-method-chaining",
    "title": "Efficient Data Workflows in Pandas",
    "section": "Combining the Workflow with Method Chaining",
    "text": "Combining the Workflow with Method Chaining\nIntroduction to Method Chaining\nMethod chaining allows combining multiple operations into a single, coherent expression. It enhances readability and efficiency."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#getting-started",
    "title": "Interactive Session 6B",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#learning-objectives",
    "href": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#learning-objectives",
    "title": "Interactive Session 6B",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nConcatenate DataFrames from separate files\nHandle mismatched column names when merging data\nJoin datasets using a common key\nUnderstand different types of joins (inner, outer, left, right)\nLearn about more advanced sorting methods for organizing data"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#introduction-to-merging-and-joining",
    "href": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#introduction-to-merging-and-joining",
    "title": "Interactive Session 6B",
    "section": "Introduction to Merging and Joining",
    "text": "Introduction to Merging and Joining\nIn environmental data science, it’s common to work with data from multiple sources. We often need to combine these datasets for comprehensive analysis. Pandas provides powerful tools for merging and joining data."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#concatenating-dataframes",
    "href": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#concatenating-dataframes",
    "title": "Interactive Session 6B",
    "section": "Concatenating DataFrames",
    "text": "Concatenating DataFrames\nConcatenation is useful when you have multiple datasets with the same structure, such as data collected over different time periods or from different locations.\nLet’s start with an example of concatenating weather data from two stations:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\nLet’s create a couple of simple dataframes containing date, temperature, and humidity data:\n\n✏️ Try it. Copy the cell below to your notebook and run it.\n\n\n\nCode\n# Create sample DataFrames\nstation1_data = pd.DataFrame({\n    'date': pd.date_range(start='2023-01-01', periods=5),\n    'temperature': [20, 22, 21, 23, 22],\n    'humidity': [50, 48, 52, 51, 49]\n})\n\nstation2_data = pd.DataFrame({\n    'date': pd.date_range(start='2023-01-06', periods=5),\n    'temperature': [19, 20, 22, 21, 23],\n    'humidity': [53, 50, 47, 49, 48]\n})\n\nprint(\"Station 1 Data:\")\nprint(station1_data)\nprint(\"\\nStation 2 Data:\")\nprint(station2_data)\n\n\nStation 1 Data:\n        date  temperature  humidity\n0 2023-01-01           20        50\n1 2023-01-02           22        48\n2 2023-01-03           21        52\n3 2023-01-04           23        51\n4 2023-01-05           22        49\n\nStation 2 Data:\n        date  temperature  humidity\n0 2023-01-06           19        53\n1 2023-01-07           20        50\n2 2023-01-08           22        47\n3 2023-01-09           21        49\n4 2023-01-10           23        48\n\n\nUse pd.concat() to combine these two dataframes into a single one:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Concatenate the DataFrames\ncombined_data = pd.concat([station1_data, station2_data], ignore_index=True)\n\nprint(\"\\nCombined Data:\")\nprint(combined_data)\n\n\n\nCombined Data:\n        date  temperature  humidity\n0 2023-01-01           20        50\n1 2023-01-02           22        48\n2 2023-01-03           21        52\n3 2023-01-04           23        51\n4 2023-01-05           22        49\n5 2023-01-06           19        53\n6 2023-01-07           20        50\n7 2023-01-08           22        47\n8 2023-01-09           21        49\n9 2023-01-10           23        48\n\n\nIn this example, we used pd.concat() to combine data from two stations. The ignore_index=True parameter resets the index of the combined DataFrame. The most common use of concatenation is when downloading or working with multiple datafiles containing the same data and structure (e.g. annual rainfall data, monthly weather data, etc…)"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#handling-mismatched-column-names",
    "href": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#handling-mismatched-column-names",
    "title": "Interactive Session 6B",
    "section": "Handling Mismatched Column Names",
    "text": "Handling Mismatched Column Names\nSometimes, datasets may have consistent data types but mismatched column names. Let’s look at an example where we have temperature data with different date column names.\n\n✏️ Try it. Copy the cell below to your notebook and run it.\n\n\n\nCode\n# Create sample DataFrames with mismatched column names\ndf1 = pd.DataFrame({\n    'date': pd.date_range(start='2023-01-01', periods=3),\n    'temp_celsius': [20, 22, 21]\n})\n\ndf2 = pd.DataFrame({\n    'DATE': pd.date_range(start='2023-01-04', periods=3),\n    'temp_celsius': [23, 22, 24]\n})\n\nprint(\"DataFrame 1:\")\nprint(df1)\nprint(\"\\nDataFrame 2:\")\nprint(df2)\n\n\nDataFrame 1:\n        date  temp_celsius\n0 2023-01-01            20\n1 2023-01-02            22\n2 2023-01-03            21\n\nDataFrame 2:\n        DATE  temp_celsius\n0 2023-01-04            23\n1 2023-01-05            22\n2 2023-01-06            24\n\n\n\nRenaming columns to facilitate concatenation\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Rename columns before concatenating\ndf2 = df2.rename(columns={'DATE': 'date'})\n\n# Concatenate the DataFrames\ncombined_df = pd.concat([df1, df2], ignore_index=True)\n\nprint(\"\\nCombined Data:\")\nprint(combined_df)\n\n\n\nCombined Data:\n        date  temp_celsius\n0 2023-01-01            20\n1 2023-01-02            22\n2 2023-01-03            21\n3 2023-01-04            23\n4 2023-01-05            22\n5 2023-01-06            24\n\n\nHere, we used the rename() method to standardize the column names before concatenation."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#joining-data-with-a-common-key",
    "href": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#joining-data-with-a-common-key",
    "title": "Interactive Session 6B",
    "section": "Joining Data with a Common Key",
    "text": "Joining Data with a Common Key\nOften, we need to combine datasets that share a common key, such as a field site ID or a species name. Pandas provides several join operations: inner, outer, left, and right.\nLet’s look at an example where we join species occurrence data with site information:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create sample DataFrames\nspecies_data = pd.DataFrame({\n    'site_id': ['A', 'B', 'A', 'C', 'B'],\n    'species': ['Oak', 'Pine', 'Maple', 'Birch', 'Oak'],\n    'count': [10, 5, 8, 3, 7]\n})\n\nsite_info = pd.DataFrame({\n    'site_id': ['A', 'B', 'C', 'D'],\n    'elevation': [100, 200, 150, 180],\n    'soil_type': ['loam', 'clay', 'sandy', 'loam']\n})\n\nprint(\"Species Data:\")\nprint(species_data)\nprint(\"\\nSite Info:\")\nprint(site_info)\n\n\nSpecies Data:\n  site_id species  count\n0       A     Oak     10\n1       B    Pine      5\n2       A   Maple      8\n3       C   Birch      3\n4       B     Oak      7\n\nSite Info:\n  site_id  elevation soil_type\n0       A        100      loam\n1       B        200      clay\n2       C        150     sandy\n3       D        180      loam\n\n\n\nPerforming an inner join to combine specieis and site data\n\n\nCode\n# Perform an inner join\nmerged_data = pd.merge(species_data, site_info, on='site_id', how='inner')\n\nprint(\"\\nMerged Data (Inner Join):\")\nprint(merged_data)\n\n\n\nMerged Data (Inner Join):\n  site_id species  count  elevation soil_type\n0       A     Oak     10        100      loam\n1       B    Pine      5        200      clay\n2       A   Maple      8        100      loam\n3       C   Birch      3        150     sandy\n4       B     Oak      7        200      clay\n\n\nIn this example, we used pd.merge() to perform an inner join on the ‘site_id’ column. This join keeps only the rows where the site_id exists in both DataFrames.\n\n\nDifferent Types of Joins\nLet’s explore other types of joins:\n\nLeft Join: Keeps all rows from the left DataFrame (species_data) and matching rows from the right DataFrame (site_info).\nRight Join: Keeps all rows from the right DataFrame (site_info) and matching rows from the left DataFrame (species_data).\nOuter Join: Keeps all rows from both DataFrames, filling in NaN where there’s no match."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#understanding-join-types-in-depth",
    "href": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#understanding-join-types-in-depth",
    "title": "Interactive Session 6B",
    "section": "Understanding Join Types in Depth",
    "text": "Understanding Join Types in Depth\nWhen working with environmental data, understanding different join types is crucial. Let’s explore these in more detail, with a focus on their applications in environmental data science.\n\nInner Join: The Most Common Join\n\n\n\n\n\n\nNote\n\n\n\nInner join is the most frequently used join type in data analysis, including environmental studies. It returns only the rows that have matching values in both DataFrames.\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Example of Inner Join\nspecies_observations = pd.DataFrame({\n    'site_id': ['A', 'B', 'C', 'D', 'E'],\n    'species': ['Oak', 'Pine', 'Maple', 'Birch', 'Cedar']\n})\n\nsite_characteristics = pd.DataFrame({\n    'site_id': ['A', 'B', 'C', 'F', 'G'],\n    'soil_type': ['Loam', 'Clay', 'Sandy', 'Silt', 'Peat']\n})\n\ninner_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='inner')\nprint(\"Inner Join Result:\")\nprint(inner_join_result)\n\n\nInner Join Result:\n  site_id species soil_type\n0       A     Oak      Loam\n1       B    Pine      Clay\n2       C   Maple     Sandy\n\n\nWhen to use Inner Join: - When you want to analyze only the data points that have complete information in both datasets. - For example, studying the relationship between soil types and tree species, but only for sites where you have both pieces of information.\n\n\nLeft Join: Keeping All Data from the Primary Dataset\nLeft join keeps all rows from the left (primary) DataFrame and matching rows from the right DataFrame. It’s useful when you want to retain all records from your main dataset.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Example of Left Join\nleft_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='left')\nprint(\"Left Join Result:\")\nprint(left_join_result)\n\n\nLeft Join Result:\n  site_id species soil_type\n0       A     Oak      Loam\n1       B    Pine      Clay\n2       C   Maple     Sandy\n3       D   Birch       NaN\n4       E   Cedar       NaN\n\n\nWhen to use Left Join: - When you want to keep all observations from your primary dataset, even if some don’t have matching information in the secondary dataset. - For example, retaining all species observations, even for sites without soil type data.\n\n\nRight Join: Less Common, but Useful in Specific Cases\nRight join is less common but can be useful in certain scenarios. It keeps all rows from the right DataFrame and matching rows from the left.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Example of Right Join\nright_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='right')\nprint(\"Right Join Result:\")\nprint(right_join_result)\n\n\nRight Join Result:\n  site_id species soil_type\n0       A     Oak      Loam\n1       B    Pine      Clay\n2       C   Maple     Sandy\n3       F     NaN      Silt\n4       G     NaN      Peat\n\n\nWhen to use Right Join: - When you want to ensure all records from a secondary dataset are included. - For example, including all soil type data, even for sites where no species were observed.\n\n\nOuter Join: Combining All Data\nOuter join combines all rows from both DataFrames, filling in NaN where there’s no match. It’s useful for getting a complete view of all available data.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Example of Outer Join\nouter_join_result = pd.merge(species_observations, site_characteristics, on='site_id', how='outer')\nprint(\"Outer Join Result:\")\nprint(outer_join_result)\n\n\nOuter Join Result:\n  site_id species soil_type\n0       A     Oak      Loam\n1       B    Pine      Clay\n2       C   Maple     Sandy\n3       D   Birch       NaN\n4       E   Cedar       NaN\n5       F     NaN      Silt\n6       G     NaN      Peat\n\n\nWhen to use Outer Join: - When you want to see all available data from both datasets, identifying where information might be missing. - Useful for data exploration or when preparing a comprehensive dataset for further analysis.\n\n\nKey Considerations for Choosing Join Types\n\nData Completeness: Inner join for complete cases, outer join for all available data.\nAnalysis Requirements: Consider what missing data means for your analysis.\nData Quality: Outer joins can help identify data gaps or collection inconsistencies.\nPrimary Focus: Use left join when focusing on a primary dataset but want additional information where available.\n\n\n\nCommon Scenarios in Environmental Data Science\n\nSpecies Distribution Studies: Left join species observations (primary data) with environmental factors.\nComprehensive Ecosystem Analysis: Outer join multiple datasets (species, soil, climate) to get a full picture.\nHabitat Suitability Models: Inner join species presence with complete environmental data.\nLong-term Monitoring: Left join time-series observations with site metadata, keeping all temporal data.\n\nRemember, the choice of join can significantly impact your analysis results. Always consider the nature of your data and the questions you’re trying to answer when selecting a join type.\n\nPractice Exercise\nYou have two datasets: one containing river water quality measurements and another with information about sampling sites. Combine these datasets to create a comprehensive view of water quality across different sites.\n\n\nCode\n# Water quality data\nwater_quality = pd.DataFrame({\n    'site_code': ['RV01', 'RV02', 'RV01', 'RV03', 'RV02'],\n    'date': pd.date_range(start='2023-01-01', periods=5),\n    'pH': [7.2, 7.8, 7.3, 6.9, 7.7],\n    'dissolved_oxygen': [8.5, 7.9, 8.3, 7.2, 8.1]\n})\n\n# Site information\nsite_info = pd.DataFrame({\n    'site_code': ['RV01', 'RV02', 'RV03', 'RV04'],\n    'river_name': ['Blue River', 'Green Creek', 'Red Stream', 'Yellow Brook'],\n    'watershed': ['Alpine', 'Lowland', 'Lowland', 'Alpine']\n})\n\n\n\n\nCode\n# Your task:\n# 1. Merge the water quality data with site information\n# 2. Calculate the average pH and dissolved oxygen for each river\n# 3. Display the results sorted by average pH\n\n# Your code here"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#advanced-sorting-techniques",
    "href": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#advanced-sorting-techniques",
    "title": "Interactive Session 6B",
    "section": "Advanced Sorting Techniques",
    "text": "Advanced Sorting Techniques\nAfter merging datasets, you often need to sort the resulting DataFrame in more complex ways. Let’s explore some advanced sorting techniques that are particularly useful when working with merged data.\n\nSorting with a Custom Key or Function\nSometimes you need to sort based on a computed value rather than the raw data in a column. You can use the key parameter in sort_values() to apply a function to the column before sorting:\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Create a sample DataFrame\ndf = pd.DataFrame({\n    'site': ['A', 'B', 'C', 'D'],\n    'species': ['Oak', 'Pine', 'Maple', 'Birch'],\n    'height': [5.2, 12.7, 8.1, 9.9]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Sort by absolute difference from 10 meters\ndf_sorted = df.sort_values('height', key=lambda x: abs(x - 10))\nprint(\"\\nSorted by proximity to 10 meters:\")\nprint(df_sorted)\n\n\nOriginal DataFrame:\n  site species  height\n0    A     Oak     5.2\n1    B    Pine    12.7\n2    C   Maple     8.1\n3    D   Birch     9.9\n\nSorted by proximity to 10 meters:\n  site species  height\n3    D   Birch     9.9\n2    C   Maple     8.1\n1    B    Pine    12.7\n0    A     Oak     5.2\n\n\nThis is useful when you want to sort based on a specific criterion, like proximity to a target value.\n\n\nSorting with NaN Values\nWhen dealing with merged data, you might encounter NaN values. By default, Pandas sorts NaN values at the end:\n\n\nCode\n# Create a DataFrame with NaN values\ndf_nan = pd.DataFrame({\n    'site': ['A', 'B', 'C', 'D', 'E'],\n    'value': [5, np.nan, 3, np.nan, 1]\n})\n\nprint(\"DataFrame with NaN values:\")\nprint(df_nan)\n\n# Sort with NaN values\ndf_sorted_nan = df_nan.sort_values('value', na_position='first')\nprint(\"\\nSorted with NaN values first:\")\nprint(df_sorted_nan)\n\n\nDataFrame with NaN values:\n  site  value\n0    A    5.0\n1    B    NaN\n2    C    3.0\n3    D    NaN\n4    E    1.0\n\nSorted with NaN values first:\n  site  value\n1    B    NaN\n3    D    NaN\n4    E    1.0\n2    C    3.0\n0    A    5.0\n\n\nYou can control where NaN values appear using the na_position parameter.\n\n\nSorting a MultiIndex DataFrame\nWhen you perform certain merge operations or create pivot tables, you might end up with a MultiIndex DataFrame. Sorting these requires specifying which level to sort on:\n\n\nCode\n# Create a MultiIndex DataFrame\nmulti_df = pd.DataFrame({\n    'site': ['A', 'A', 'B', 'B'],\n    'year': [2022, 2023, 2022, 2023],\n    'value': [10, 12, 9, 11]\n}).set_index(['site', 'year'])\n\nprint(\"MultiIndex DataFrame:\")\nprint(multi_df)\n\n# Sort by the 'site' level of the index\nsorted_multi_df = multi_df.sort_index(level='site', ascending=False)\nprint(\"\\nSorted by 'site' level:\")\nprint(sorted_multi_df)\n\n# Sort by the 'value' column within each 'site'\nsorted_multi_df_values = multi_df.sort_values('value', ascending=False)\nprint(\"\\nSorted by 'value' within each 'site':\")\nprint(sorted_multi_df_values)\n\n\nMultiIndex DataFrame:\n           value\nsite year       \nA    2022     10\n     2023     12\nB    2022      9\n     2023     11\n\nSorted by 'site' level:\n           value\nsite year       \nB    2023     11\n     2022      9\nA    2023     12\n     2022     10\n\nSorted by 'value' within each 'site':\n           value\nsite year       \nA    2023     12\nB    2023     11\nA    2022     10\nB    2022      9\n\n\nThis is particularly useful when you’ve merged data that creates hierarchical relationships in your DataFrame.\n\n\nStable Sorting\nWhen sorting by multiple columns, you might want to preserve the relative order of rows with the same values. This is called stable sorting:\n\n\nCode\n# Create a DataFrame with duplicate values\ndf_duplicate = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'B', 'A'],\n    'subcategory': [1, 1, 2, 2, 3],\n    'value': [10, 20, 30, 40, 50]\n})\n\nprint(\"DataFrame with duplicate values:\")\nprint(df_duplicate)\n\n# Perform a stable sort\ndf_stable_sort = df_duplicate.sort_values(['category', 'subcategory'], kind='mergesort')\nprint(\"\\nStable sort result:\")\nprint(df_stable_sort)\n\n\nDataFrame with duplicate values:\n  category  subcategory  value\n0        A            1     10\n1        B            1     20\n2        A            2     30\n3        B            2     40\n4        A            3     50\n\nStable sort result:\n  category  subcategory  value\n0        A            1     10\n2        A            2     30\n4        A            3     50\n1        B            1     20\n3        B            2     40\n\n\nUsing kind='mergesort' ensures a stable sort, which can be important when preserving the original order matters within your sorted groups.\nThese advanced sorting techniques will help you effectively organize and analyze complex datasets resulting from merge operations or other data manipulations."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#key-points",
    "href": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#key-points",
    "title": "Interactive Session 6B",
    "section": "Key Points",
    "text": "Key Points\n\nConcatenation (pd.concat()) is useful for combining datasets with the same structure.\nAlways check and standardize column names before merging datasets.\nChoose the appropriate join type (inner, left, right, outer) based on your analysis needs.\nMerging on a common key allows you to combine information from different sources."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#resources",
    "href": "example_course/course-materials/interactive-sessions/6b_grouping_joining_sorting_2_old.html#resources",
    "title": "Interactive Session 6B",
    "section": "Resources",
    "text": "Resources\n\nPandas Merging Documentation\nPandas Concat Documentation\n\nDon’t forget to check out our EDS 217 Cheatsheet on Merging and Joining for quick reference!\n\nEnd interactive session 6B"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#getting-started",
    "title": "Interactive Session 6A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#introduction",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#introduction",
    "title": "Interactive Session 6A",
    "section": "Introduction",
    "text": "Introduction\nIn this session, we’ll explore essential data manipulation and analysis techniques in pandas, focusing on some simple examples. We’ll cover sorting, grouping, joining, working with dates, and applying custom transformations to data."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#setup",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#setup",
    "title": "Interactive Session 6A",
    "section": "Setup",
    "text": "Setup\nLet’s start by importing the necessary libraries and creating a sample dataset:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a sample dataset of species observations\nnp.random.seed(42) \ndates = pd.date_range(start='2023-01-01', periods=100)\ndata = {\n    'date': dates,\n    'site': np.random.choice(['Forest', 'Grassland', 'Wetland'], 100),\n    'species': np.random.choice(['Oak', 'Maple', 'Pine', 'Birch'], 100),\n    'count': np.random.randint(1, 20, 100),\n    'temperature': np.random.normal(15, 5, 100)\n}\ndf = pd.DataFrame(data)\nprint(df.head())\n\n\n        date     site species  count  temperature\n0 2023-01-01  Wetland   Birch      7     9.043483\n1 2023-01-02   Forest   Birch      1    18.282768\n2 2023-01-03  Wetland   Birch      1    10.126592\n3 2023-01-04  Wetland    Pine     13    18.935423\n4 2023-01-05   Forest    Pine      9    20.792978"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#sorting-data",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#sorting-data",
    "title": "Interactive Session 6A",
    "section": "1. Sorting Data",
    "text": "1. Sorting Data\nSorting is crucial in data analysis for identifying extremes, patterns, and organizing your data for subsequent analyses.\n\nBasic Sorting\n\n\nCode\n# Sort by species count\ndf_sorted = df.sort_values('count', ascending=False)\nprint(df_sorted.head())\n\n\n         date       site species  count  temperature\n12 2023-01-13     Forest     Oak     19    10.552428\n33 2023-02-03  Grassland   Maple     19    19.281994\n53 2023-02-23     Forest   Maple     19    20.677828\n55 2023-02-25    Wetland   Birch     19    18.256956\n81 2023-03-23     Forest   Birch     19    19.262167\n\n\n\n\nMulti-column Sorting\n\n\nCode\n# Sort by site and then by species count\ndf_multi_sorted = df.sort_values(['site', 'count'], ascending=[True, False])\nprint(df_multi_sorted.head())\n\n\n         date    site species  count  temperature\n12 2023-01-13  Forest     Oak     19    10.552428\n53 2023-02-23  Forest   Maple     19    20.677828\n81 2023-03-23  Forest   Birch     19    19.262167\n61 2023-03-03  Forest     Oak     18    15.409371\n95 2023-04-06  Forest   Maple     18    20.162326"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#grouping-and-aggregating-data",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#grouping-and-aggregating-data",
    "title": "Interactive Session 6A",
    "section": "2. Grouping and Aggregating Data",
    "text": "2. Grouping and Aggregating Data\nGrouping allows us to analyze data at different ecological levels, from individual species to entire ecosystems.\n\nBasic Groupby\n\n\nCode\n# Sum of species counts by site\nsite_counts = df.groupby('site')['count'].sum()\nprint(site_counts)\n\n\nsite\nForest       311\nGrassland    336\nWetland      248\nName: count, dtype: int64\n\n\n\n\nMultiple Aggregations\n\n\nCode\n# Multiple stats by site\nsite_stats = df.groupby('site').agg({\n    'count': ['sum', 'mean', 'max'],\n    'species': 'nunique',\n    'temperature': 'mean'\n})\nprint(site_stats)\n\n\n          count               species temperature\n            sum      mean max nunique        mean\nsite                                             \nForest      311  9.424242  19       4   16.527332\nGrassland   336  9.333333  19       4   15.540037\nWetland     248  8.000000  19       4   14.528127"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#joining-data",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#joining-data",
    "title": "Interactive Session 6A",
    "section": "3. Joining Data",
    "text": "3. Joining Data\nJoining data allows us to combine information from datasets.\n\n\nCode\n# Create a second DataFrame with site characteristics\nsite_data = pd.DataFrame({\n    'site': ['Forest', 'Grassland', 'Wetland'],\n    'soil_pH': [6.5, 7.2, 6.8],\n    'annual_rainfall': [1200, 800, 1500]\n})\n\n# Perform an inner join\nmerged_df = pd.merge(df, site_data, on='site', how='inner')\nprint(merged_df.head())\n\n\n        date     site species  count  temperature  soil_pH  annual_rainfall\n0 2023-01-01  Wetland   Birch      7     9.043483      6.8             1500\n1 2023-01-02   Forest   Birch      1    18.282768      6.5             1200\n2 2023-01-03  Wetland   Birch      1    10.126592      6.8             1500\n3 2023-01-04  Wetland    Pine     13    18.935423      6.8             1500\n4 2023-01-05   Forest    Pine      9    20.792978      6.5             1200\n\n\nTypes of joins and when to use them:\n\nInner Join: Use when you want to combine two datasets based on a common key, keeping only the records that have matches in both datasets. This is useful when you’re interested in analyzing only the data points that have complete information across both sources.\nLeft Join: Use when you want to keep all records from the left (primary) dataset and match them with records from the right (secondary) dataset where possible. This is helpful when you want to preserve all information from your main dataset while enriching it with additional data where available.\nRight Join: Similar to a left join, but keeps all records from the right dataset. This is less common but can be useful when you want to ensure all records from a secondary dataset are included, even if they don’t have corresponding entries in the primary dataset.\nOuter Join: Use when you want to combine all unique records from both datasets, regardless of whether they have matches or not. This creates a comprehensive dataset that includes all information from both sources, filling in with null values where there’s no match.\n\nUse cases: - Combining observation data with site characteristics - Merging disparate datasets that share a location or timestamp."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#working-with-dates",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#working-with-dates",
    "title": "Interactive Session 6A",
    "section": "4. Working with Dates",
    "text": "4. Working with Dates\nDate manipulation is crucial for analyzing seasonal patterns, long-term trends, and time-sensitive events.\n\n\nCode\n# Set date as index\ndf.set_index('date', inplace=True)\nprint(df.head())\n\n# Resample to monthly data\nmonthly_counts = df.resample('M')['count'].sum()\nprint(monthly_counts.head())\n\n\n               site species  count  temperature\ndate                                           \n2023-01-01  Wetland   Birch      7     9.043483\n2023-01-02   Forest   Birch      1    18.282768\n2023-01-03  Wetland   Birch      1    10.126592\n2023-01-04  Wetland    Pine     13    18.935423\n2023-01-05   Forest    Pine      9    20.792978\ndate\n2023-01-31    275\n2023-02-28    232\n2023-03-31    297\n2023-04-30     91\nFreq: ME, Name: count, dtype: int64\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_40183/881548494.py:6: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n  monthly_counts = df.resample('M')['count'].sum()\n\n\n\nUnderstanding inplace=True\nThe inplace=True parameter modifies the original DataFrame directly:\n\n\nCode\n# Without inplace=True (creates a new DataFrame)\ndf_new = df.reset_index()\nprint(\"\\nNew DataFrame with reset index:\")\nprint(df_new.head())\nprint(\"\\nOriginal DataFrame (unchanged):\")\nprint(df.head())\n\n# With inplace=True (modifies the original DataFrame)\ndf.reset_index(inplace=True)\nprint(\"\\nOriginal DataFrame after reset_index(inplace=True):\")\nprint(df.head())\n\n\n\nNew DataFrame with reset index:\n        date     site species  count  temperature\n0 2023-01-01  Wetland   Birch      7     9.043483\n1 2023-01-02   Forest   Birch      1    18.282768\n2 2023-01-03  Wetland   Birch      1    10.126592\n3 2023-01-04  Wetland    Pine     13    18.935423\n4 2023-01-05   Forest    Pine      9    20.792978\n\nOriginal DataFrame (unchanged):\n               site species  count  temperature\ndate                                           \n2023-01-01  Wetland   Birch      7     9.043483\n2023-01-02   Forest   Birch      1    18.282768\n2023-01-03  Wetland   Birch      1    10.126592\n2023-01-04  Wetland    Pine     13    18.935423\n2023-01-05   Forest    Pine      9    20.792978\n\nOriginal DataFrame after reset_index(inplace=True):\n        date     site species  count  temperature\n0 2023-01-01  Wetland   Birch      7     9.043483\n1 2023-01-02   Forest   Birch      1    18.282768\n2 2023-01-03  Wetland   Birch      1    10.126592\n3 2023-01-04  Wetland    Pine     13    18.935423\n4 2023-01-05   Forest    Pine      9    20.792978\n\n\nWhen to use inplace=True: - When preprocessing large datasets to save memory - In data cleaning pipelines for time series - When you’re sure you won’t need the original version of the data\nWhen not to use inplace=True: - When you need to preserve the original dataset for comparison - In functions where you want to return a modified copy without altering the input - When working with shared datasets that other parts of your analysis might depend on\n\n\nDate Filtering and Analysis\n\n\nCode\n# Filter by date range (e.g., spring season)\nspring_data = df[(df['date'] &gt;= '2023-03-01') & (df['date'] &lt; '2023-06-01')]\nprint(spring_data.head())\n\n# Extract date components\ndf['month'] = df['date'].dt.month\ndf['day_of_year'] = df['date'].dt.dayofyear\nprint(df.head())\n\n\n         date       site species  count  temperature\n59 2023-03-01    Wetland   Birch      8    13.815907\n60 2023-03-02  Grassland    Pine      7    12.573182\n61 2023-03-03     Forest     Oak     18    15.409371\n62 2023-03-04  Grassland   Birch      8    26.573293\n63 2023-03-05  Grassland     Oak      1     5.663674\n        date     site species  count  temperature  month  day_of_year\n0 2023-01-01  Wetland   Birch      7     9.043483      1            1\n1 2023-01-02   Forest   Birch      1    18.282768      1            2\n2 2023-01-03  Wetland   Birch      1    10.126592      1            3\n3 2023-01-04  Wetland    Pine     13    18.935423      1            4\n4 2023-01-05   Forest    Pine      9    20.792978      1            5\n\n\nWhen to use date manipulation: - Analyzing seasonal patterns - Studying time-specific events (e.g., flowering times, migration patterns) - Creating time-based features for models - Aligning climate data with other observations (resampling)"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#using-df.apply-to-transform-data",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#using-df.apply-to-transform-data",
    "title": "Interactive Session 6A",
    "section": "5. Using df.apply() to transform data",
    "text": "5. Using df.apply() to transform data\nThe apply() function allows you to apply custom calculations to your data.\n\n\nCode\n# Apply a function to categorize temperature\ndef categorize_temperature(value):\n    if value &lt; 10:\n        return 'Cold'\n    elif value &lt; 20:\n        return 'Moderate'\n    else:\n        return 'Warm'\n\ndf['temp_category'] = df['temperature'].apply(categorize_temperature)\nprint(df.head())\n\n# Apply a function to calculate biodiversity index\ndef simpson_diversity(row):\n    n = row['count']\n    N = df.loc[df['site'] == row['site'], 'count'].sum()\n    return 1 - (n * (n - 1)) / (N * (N - 1))\n\ndf['simpson_index'] = df.apply(simpson_diversity, axis=1)\nprint(df.head())\n\n\n        date     site species  count  temperature  month  day_of_year  \\\n0 2023-01-01  Wetland   Birch      7     9.043483      1            1   \n1 2023-01-02   Forest   Birch      1    18.282768      1            2   \n2 2023-01-03  Wetland   Birch      1    10.126592      1            3   \n3 2023-01-04  Wetland    Pine     13    18.935423      1            4   \n4 2023-01-05   Forest    Pine      9    20.792978      1            5   \n\n  temp_category  \n0          Cold  \n1      Moderate  \n2      Moderate  \n3      Moderate  \n4          Warm  \n        date     site species  count  temperature  month  day_of_year  \\\n0 2023-01-01  Wetland   Birch      7     9.043483      1            1   \n1 2023-01-02   Forest   Birch      1    18.282768      1            2   \n2 2023-01-03  Wetland   Birch      1    10.126592      1            3   \n3 2023-01-04  Wetland    Pine     13    18.935423      1            4   \n4 2023-01-05   Forest    Pine      9    20.792978      1            5   \n\n  temp_category  simpson_index  \n0          Cold       0.999314  \n1      Moderate       1.000000  \n2      Moderate       1.000000  \n3      Moderate       0.997453  \n4          Warm       0.999253  \n\n\nWhen to use apply(): - Calculating complex indices (e.g., biodiversity measures) - Applying models to observational data - Implementing custom data cleaning rules - Performing category-specific calculations\nRemember, while apply() is versatile, it can be slower than vectorized operations for large datasets. Always consider if there’s a vectorized alternative, especially when working with big datasets."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#reshaping-dataframes-with-pivot-tables",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#reshaping-dataframes-with-pivot-tables",
    "title": "Interactive Session 6A",
    "section": "6. Reshaping DataFrames with Pivot Tables",
    "text": "6. Reshaping DataFrames with Pivot Tables\nPivot tables provide a way to reshape data and calculate aggregations in one step.\n\nHow Pivot Tables Work\n\nReshaping Data: Pivot tables reshape data by turning unique values from one column into multiple columns.\nAggregation: They perform aggregations on a specified value column for each unique group created by the new columns.\nIndex and Columns: You specify which column to use as the new index, which to use as new columns, and which to aggregate.\n\nThe idea is very similar to the df.pivot command: \nThe main difference between df.pivot and df.pivot_table is that df.pivot_table includes aggregation.\nLet’s see an example:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a sample DataFrame\ndata = {\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'city': ['New York', 'Los Angeles', 'New York', 'Los Angeles'],\n    'temperature': [32, 68, 28, 72]\n}\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n\nOriginal DataFrame:\n         date         city  temperature\n0  2023-01-01     New York           32\n1  2023-01-01  Los Angeles           68\n2  2023-01-02     New York           28\n3  2023-01-02  Los Angeles           72\n\n\n\nUsing df.pivot to rotate the dataframe:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\npivot = pd.pivot(df, values='temperature', index='date', columns='city')\nprint(\"\\nPivot:\")\nprint(pivot)\n\n\n\nPivot:\ncity        Los Angeles  New York\ndate                             \n2023-01-01           68        32\n2023-01-02           72        28\n\n\n\n\nUsing df.pivot_table to create a pivot table:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a pivot table\npivot_table = df.pivot_table(values='temperature', index='date', columns='city', aggfunc='mean')\nprint(\"\\nPivot Table:\")\nprint(pivot_table)\n\n\n\nPivot Table:\ncity        Los Angeles  New York\ndate                             \n2023-01-01         68.0      32.0\n2023-01-02         72.0      28.0\n\n\nIn this example: - ‘date’ becomes the index - ‘city’ values become new columns - ‘temperature’ values are aggregated (mean) for each date-city combination\n\n\n\n\n\n\nNote\n\n\n\nIn this example, the result of our pivot and pivot_table commands are essentially the same. Why is that the case? When would we expect different results from these two commands?\n\n\n\n\n\nKey Features of Pivot Tables\n\nHandling Duplicates: If there are multiple values for a given index-column combination, an aggregation function (like mean, sum, count) must be specified.\nMissing Data: Pivot tables can reveal missing data, often filling these gaps with NaN.\nMulti-level Index: You can create multi-level indexes and columns for more complex reorganizations.\nFlexibility: You can pivot on multiple columns and use multiple value columns.\n\nPivot tables are especially useful for: - Summarizing large datasets - Creating cross-tabulations - Preparing data for visualization - Identifying patterns or trends across categories\nRemember, while pivot tables are powerful, they work best with well-structured data and clear categorical variables.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a pivot table\npivot = df.pivot_table(values='temperature', index='city', columns='date', aggfunc='mean')\nprint(pivot)\n\n\ndate         2023-01-01  2023-01-02\ncity                               \nLos Angeles        68.0        72.0\nNew York           32.0        28.0\n\n\nTry creating a pivot table that shows the maximum humidity for each city and date.\n\n✏️ Try it. Add the cell below to your notebook and then provide your code.\n\n\n\nCode\n# Your code here"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#key-points",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#key-points",
    "title": "Interactive Session 6A",
    "section": "Key Points",
    "text": "Key Points\n\nGrouping allows us to split data based on categories and perform operations on each group.\nThe groupby() function is the primary tool for grouping in Pandas.\nWe can apply various aggregation functions to grouped data.\nMulti-level grouping creates a hierarchical index.\nPivot tables offer a way to reshape and aggregate data simultaneously."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#conclusion",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting.html#conclusion",
    "title": "Interactive Session 6A",
    "section": "Conclusion",
    "text": "Conclusion\nThese techniques form the foundation of data manipulation and analysis in pandas. By understanding when and how to use each method, you can efficiently process and gain insights from datasets. Next you’ll applying these concepts to different scenarios to strengthen your skills in environmental data science."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day4.html#introduction",
    "href": "example_course/course-materials/eod-practice/eod-day4.html#introduction",
    "title": "Day 4: Tasks & activities",
    "section": "Introduction",
    "text": "Introduction\nThis end-of-day session is focused on using pandas for loading, visualizing, and analyzing marine microplastics data. This session is designed to help you become more comfortable with the pandas library, equipping you with the skills needed to perform data analysis effectively.\nThe National Oceanic and Atmospheric Administration, via its National Centers for Environmental Information has an entire section related to marine microplastics – that is, microplastics found in water — at https://www.ncei.noaa.gov/products/microplastics.\nWe will be working with a recent download of the entire marine microplastics dataset. The url for this data is located here:\n\n\nCode\nurl = 'https://ucsb.box.com/shared/static/dnnu59jsnkymup6o8aaovdywrtxiy3a9.csv'\n\n\nObjective: Write your own notebook that contains a simple DataFrame exploration as well as some basic grouping, filtering, and aggregation, and visualization… all within the pandas library."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day4.html#loading-the-data",
    "href": "example_course/course-materials/eod-practice/eod-day4.html#loading-the-data",
    "title": "Day 4: Tasks & activities",
    "section": "1. Loading the Data",
    "text": "1. Loading the Data\nObjective: Learn to load data into a pandas DataFrame and display the first few records.\n\nTask 1.1 Import the pandas library.\n\n\nTask 1.2: Load the dataset into a dataframe named df from the provided URL into a pandas DataFrame.\n\n\n\n\n\n\nTip\n\n\n\nI’ve already taken a look at this data set and noticed there was a column with sample date called Date. We can use the parse_date option of the read_csv() function to convert values in the Dates column of the csv into datetime objects in pandas while reading the file.\n\n\n\n\nTask 1.3:\n\nDisplay the first five rows of the DataFrame to get an initial understanding of the data structure."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day4.html#exploring-the-data",
    "href": "example_course/course-materials/eod-practice/eod-day4.html#exploring-the-data",
    "title": "Day 4: Tasks & activities",
    "section": "2. Exploring the Data",
    "text": "2. Exploring the Data\n\nTask 2.1:\n\nDisplay summary statistics of the dataset to understand the central tendency and variability.\n\n\n\nTask 2.2:\n\nCheck the data types of each column and identify if there are any missing values.\nRemove any records that are missing a value in the Oceans\n\n\n\n\n\n\n\nUsing ~ to invert built-in function results\n\n\n\nThe ~ operator inverts a list of Boolean values (switches True to False and vice versa).\nThis operator isn’t useful for most selection operations because you can just use == and != to invert selection criteria. However, the ~ operator becomes very handy when there is a need to invert the results of a built-in function.\nFor example, the use of the ~ operator and isnull() combine to create an efficient way to filter dataframes where the value of a df[column] is not isnull():\ndf_valid = df[~(df['column'].isnull())].copy()\nNote that the results of the built-in function - df['column'].isnull() need to be wrapped in ( ) for the ~ operator to work properly."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day4.html#grouping-the-data",
    "href": "example_course/course-materials/eod-practice/eod-day4.html#grouping-the-data",
    "title": "Day 4: Tasks & activities",
    "section": "3. Grouping the Data",
    "text": "3. Grouping the Data\n\nTask 3.1:\n\nCreate a groupby object called oceans that groups the data in df according to the value of the Oceans column.\n\n\n\nTask 3.2:\n\nDetermine the total number of Measurements taken from each Ocean.\n\n\n\nTask 3.3:\n\nDetermine the average value of Measurement taken from each Ocean."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day4.html#filtering-the-data-and-aggregating-the-data",
    "href": "example_course/course-materials/eod-practice/eod-day4.html#filtering-the-data-and-aggregating-the-data",
    "title": "Day 4: Tasks & activities",
    "section": "4. Filtering the Data and Aggregating the data",
    "text": "4. Filtering the Data and Aggregating the data\n\nTask 4.1:\n\nFilter the data to a new df (called df2) that only contains rows where the Unit of measurement is pieces/m3\n\n\n\nTask 4.2:\n\nUse the groupby and the max() command to determine the Maximum value of pieces/m3 measured for each Ocean"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day4.html#simple-plots-in-pandas",
    "href": "example_course/course-materials/eod-practice/eod-day4.html#simple-plots-in-pandas",
    "title": "Day 4: Tasks & activities",
    "section": "5. Simple Plots in pandas",
    "text": "5. Simple Plots in pandas\n\nTask 5.1:\n\nMake a histogram of the latitude of every sample in your filtered dataframe using the DataFrame plot command.\n\n\n\nTask 5.2:\n\nMake a new dataframe (df3) from your filtered dataframe (df2) that contains only rows where Measurement is greater than zero.\n\n\n\n\n\n\n\nImportant\n\n\n\nUsing .copy() when filtering a dataframe ensures that you’re working with a new DataFrame, not a view of the original. This is especially important when you’re filtering data and then modifying the result, which is common in data science workflows.\n\n\n\n\nTask 5.3\n\nCreate a new column in df3 that contains the log10 of Measurements.\n\n\n\n\n\n\n\nTip\n\n\n\nThe numpy library has a log10() function that you will find useful for this step!\n\n\n\n\nTask 5.4\n\nMake a histogram of the log-transformed values in df3"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day4.html#conclusion",
    "href": "example_course/course-materials/eod-practice/eod-day4.html#conclusion",
    "title": "Day 4: Tasks & activities",
    "section": "Conclusion",
    "text": "Conclusion\n🎉 Congratulations, you’re officially doing python data science! 🎉\nBe sure to save your notebook and add comments and reflections at the end of your notebook before heading out for the day.\n\nEnd Activity Session (Day 4)"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#getting-started",
    "title": "Interactive Session 4A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#introduction-to-pandas-dataframes",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#introduction-to-pandas-dataframes",
    "title": "Interactive Session 4A",
    "section": "Introduction to pandas DataFrames",
    "text": "Introduction to pandas DataFrames\nIn this interactive session, we’ll explore the fundamental concepts of pandas DataFrames, their relationship to Series, and some essential methods for working with them."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#instructions",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#instructions",
    "title": "Interactive Session 4A",
    "section": "Instructions",
    "text": "Instructions\nWe will work through this material together, writing a new notebook as we go.\n\n\n\n\n\n\nNote\n\n\n\n🐍     This symbol designates an important note about Python structure, syntax, or another quirk.\n\n\n\n✏️     This symbol designates code you should add to your notebook and run."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#setting-up-our-environment",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#setting-up-our-environment",
    "title": "Interactive Session 4A",
    "section": "Setting up our environment",
    "text": "Setting up our environment\nLet’s start by importing the necessary libraries:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#understanding-series",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#understanding-series",
    "title": "Interactive Session 4A",
    "section": "Understanding Series",
    "text": "Understanding Series\nBefore diving into DataFrames, let’s briefly review pandas Series, as they form the building blocks of DataFrames.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Creating a Series\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\nprint(s)\n\n\n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64\n\n\nA Series is a one-dimensional labeled array that can hold data of any type."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#introduction-to-dataframes",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#introduction-to-dataframes",
    "title": "Interactive Session 4A",
    "section": "Introduction to DataFrames",
    "text": "Introduction to DataFrames\nA DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of it as a table or a spreadsheet-like structure.\nLet’s create a simple DataFrame:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Creating a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4],\n    'B': pd.date_range(start='2023-01-01', periods=4),\n    'C': pd.Series(1, index=list(range(4)), dtype='float32'),\n    'D': np.array([3] * 4, dtype='int32'),\n    'E': pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n    'F': 'foo'\n})\n\nprint(df)\n\n\n   A          B    C  D      E    F\n0  1 2023-01-01  1.0  3   test  foo\n1  2 2023-01-02  1.0  3  train  foo\n2  3 2023-01-03  1.0  3   test  foo\n3  4 2023-01-04  1.0  3  train  foo\n\n\nHere, we’ve created a DataFrame with different types of data: integers, dates, floats, categories, and strings."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#dataframes-and-series-relationship",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#dataframes-and-series-relationship",
    "title": "Interactive Session 4A",
    "section": "DataFrames and Series Relationship",
    "text": "DataFrames and Series Relationship\nEach column in a DataFrame is a Series. You can access a column like this:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df['A'])\n\n\n0    1\n1    2\n2    3\n3    4\nName: A, dtype: int64\n\n\nThis returns a Series object. You can confirm this:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(type(df['A']))\n\n\n&lt;class 'pandas.core.series.Series'&gt;"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#investigating-dataframe-structure",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#investigating-dataframe-structure",
    "title": "Interactive Session 4A",
    "section": "Investigating DataFrame Structure",
    "text": "Investigating DataFrame Structure\npandas provides several methods to investigate the structure of a DataFrame:\n\n1. Shape\nTo get the dimensions of the DataFrame:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.shape)\n\n\n(4, 6)\n\n\nThis returns a tuple where the first element is the number of rows, and the second is the number of columns.\n\n\n2. Info\nFor a concise summary of the DataFrame:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4 entries, 0 to 3\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   A       4 non-null      int64         \n 1   B       4 non-null      datetime64[ns]\n 2   C       4 non-null      float32       \n 3   D       4 non-null      int32         \n 4   E       4 non-null      category      \n 5   F       4 non-null      object        \ndtypes: category(1), datetime64[ns](1), float32(1), int32(1), int64(1), object(1)\nmemory usage: 288.0+ bytes\n\n\nThis method prints information about the DataFrame including the index dtype and columns, non-null values and memory usage.\n\n\n3. Dtypes\nTo see the data types of each column:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.dtypes)\n\n\nA             int64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object\n\n\n\n\n4. Describe\nTo see some basic statistical details of numerical columns:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.describe())\n\n\n              A                    B    C    D\ncount  4.000000                    4  4.0  4.0\nmean   2.500000  2023-01-02 12:00:00  1.0  3.0\nmin    1.000000  2023-01-01 00:00:00  1.0  3.0\n25%    1.750000  2023-01-01 18:00:00  1.0  3.0\n50%    2.500000  2023-01-02 12:00:00  1.0  3.0\n75%    3.250000  2023-01-03 06:00:00  1.0  3.0\nmax    4.000000  2023-01-04 00:00:00  1.0  3.0\nstd    1.290994                  NaN  0.0  0.0"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#accessing-dataframe-contents",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#accessing-dataframe-contents",
    "title": "Interactive Session 4A",
    "section": "Accessing DataFrame Contents",
    "text": "Accessing DataFrame Contents\nThere are multiple ways to access data within a DataFrame. The easiest and most direct way is to use the head() and tail() command. The head() method shows the first (tail() shows the last) 10 values by default (if no argument is provided). If you provide a number to the command, then that is the number of values returned.\n\n1. Head and Tail\nTo view the first or last few rows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.head(2))  # First 2 rows\nprint(\"\\n\")\nprint(df.tail(2))  # Last 2 rows\n\n\n   A          B    C  D      E    F\n0  1 2023-01-01  1.0  3   test  foo\n1  2 2023-01-02  1.0  3  train  foo\n\n\n   A          B    C  D      E    F\n2  3 2023-01-03  1.0  3   test  foo\n3  4 2023-01-04  1.0  3  train  foo\n\n\n\n\n2. Indexing\nYou can access rows and columns using various indexing methods:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Selecting a single column\nprint(df['A'])\n\n# Selecting multiple columns\nprint(df[['A', 'B']])\n\n# Selecting rows by position\nprint(df.iloc[0])  # First row\n\n# Selecting rows by label\nprint(df.loc[0])  # First row (assuming default integer index)\n\n# Selecting both rows and columns\nprint(df.loc[0, 'A'])  # Value at first row of column 'A'\n\n\n0    1\n1    2\n2    3\n3    4\nName: A, dtype: int64\n   A          B\n0  1 2023-01-01\n1  2 2023-01-02\n2  3 2023-01-03\n3  4 2023-01-04\nA                      1\nB    2023-01-01 00:00:00\nC                    1.0\nD                      3\nE                   test\nF                    foo\nName: 0, dtype: object\nA                      1\nB    2023-01-01 00:00:00\nC                    1.0\nD                      3\nE                   test\nF                    foo\nName: 0, dtype: object\n1"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#basic-dataframe-operations",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#basic-dataframe-operations",
    "title": "Interactive Session 4A",
    "section": "Basic DataFrame Operations",
    "text": "Basic DataFrame Operations\nLet’s look at some basic operations you can perform on DataFrames:\n\n1. Adding a new column\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndf['G'] = df['A'] + df['D']\nprint(df)\n\n\n   A          B    C  D      E    F  G\n0  1 2023-01-01  1.0  3   test  foo  4\n1  2 2023-01-02  1.0  3  train  foo  5\n2  3 2023-01-03  1.0  3   test  foo  6\n3  4 2023-01-04  1.0  3  train  foo  7\n\n\n\n\n2. Applying functions to columns\nLet’s define a simpe function and use apply to apply that function to every row in a column:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndef square(x):\n    return x ** 2\n\ndf['A_squared'] = df['A'].apply(square)\nprint(df)\n\n\n   A          B    C  D      E    F  G  A_squared\n0  1 2023-01-01  1.0  3   test  foo  4          1\n1  2 2023-01-02  1.0  3  train  foo  5          4\n2  3 2023-01-03  1.0  3   test  foo  6          9\n3  4 2023-01-04  1.0  3  train  foo  7         16\n\n\n\n\n3. Basic statistics\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df['A'].mean())  # Mean of column A\nprint(df['D'].sum())   # Sum of column D\n\n\n2.5\n12"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#conclusion",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes_original.html#conclusion",
    "title": "Interactive Session 4A",
    "section": "Conclusion",
    "text": "Conclusion\nThis session introduced you to the basics of pandas DataFrames, their relationship to Series, and some fundamental methods for exploring and manipulating them.\nIn future sessions, we’ll dive deeper into more advanced operations like selection, filtering, grouping, and data cleaning.\nRemember, DataFrames are powerful tools for data manipulation and analysis, built upon the concept of Series.\nUnderstanding their structure and basic operations is crucial for effective data analysis with pandas.\n\nPandas Cheat Sheet\nDataFrame Cheatsheet\n\n\nEnd interactive session 4A"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/random_numbers.html",
    "href": "example_course/course-materials/cheatsheets/random_numbers.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "For information on the previous np.random API and its use cases, please refer to the NumPy documentation on legacy random generation: NumPy Legacy Random Generation\nThis cheatsheet focuses on the modern Generator-based approach to random number generation in NumPy.\n\n\n\n\nCode\nimport numpy as np\n\n\n\n\n\n\n\nCode\n# Create a Generator with the default BitGenerator\nrng = np.random.default_rng()\n\n# Create a Generator with a specific seed\nrng_seeded = np.random.default_rng(seed=42)\n\n\n\n\n\n\n\n\n\nCode\n# Single random float\nprint(rng.random())\n\n# Array of random floats\nprint(rng.random(5))\n\n\n0.1587752984013584\n[0.6803629  0.37911992 0.27807791 0.76383091 0.63573615]\n\n\n\n\n\n\n\nCode\n# Single random integer from 0 to 10 (inclusive)\nprint(rng.integers(11))\n\n# Array of random integers from 1 to 100 (inclusive)\nprint(rng.integers(1, 101, size=5))\n\n\n7\n[53 20 30 44 41]\n\n\n\n\n\n\n\nCode\n# Single value from standard normal distribution\nprint(rng.standard_normal())\n\n# Array from normal distribution with mean=0, std=1\nprint(rng.normal(loc=0, scale=1, size=5))\n\n\n0.6468359466300964\n[-0.31137764  0.18056671  0.53194635  1.36360984 -0.52374801]\n\n\n\n\n\n\n\n\nCode\n# Random choice from an array\narr = np.array([1, 2, 3, 4, 5])\nprint(rng.choice(arr))\n\n# Random sample without replacement\nprint(rng.choice(arr, size=3, replace=False))\n\n\n2\n[3 2 1]\n\n\n\n\n\n\n\nCode\narr = np.arange(10)\nrng.shuffle(arr)\nprint(arr)\n\n\n[5 7 4 8 2 6 9 3 1 0]\n\n\n\n\n\nGenerators provide methods for many other distributions:\n\n\nCode\n# Poisson distribution\nprint(rng.poisson(lam=5, size=3))\n\n# Exponential distribution\nprint(rng.exponential(scale=1.0, size=3))\n\n# Binomial distribution\nprint(rng.binomial(n=10, p=0.5, size=3))\n\n\n[5 5 8]\n[0.18983469 0.31953811 1.68140815]\n[5 4 6]\n\n\n\n\n\nGenerators can fill existing arrays, which can be more efficient:\n\n\nCode\narr = np.empty(5)\nrng.random(out=arr)\nprint(arr)\n\n\n[0.64436064 0.82549213 0.34414789 0.69527507 0.86218693]\n\n\n\n\n\nYou can use different Bit Generators with varying statistical qualities:\n\n\nCode\nfrom numpy.random import PCG64, MT19937\n\nrng_pcg = np.random.Generator(PCG64())\nrng_mt = np.random.Generator(MT19937())\n\nprint(\"PCG64:\", rng_pcg.random())\nprint(\"MT19937:\", rng_mt.random())\n\n\nPCG64: 0.17483451679680462\nMT19937: 0.4790870868216416\n\n\n\n\n\n\n\nCode\n# Save state\nstate = rng.bit_generator.state\n\n# Generate some numbers\nprint(\"Original:\", rng.random(3))\n\n# Restore state and regenerate\nrng.bit_generator.state = state\nprint(\"Restored:\", rng.random(3))\n\n\nOriginal: [0.26224457 0.44779407 0.50033989]\nRestored: [0.26224457 0.44779407 0.50033989]\n\n\n\n\n\nYou can create independent generators from an existing one:\n\n\nCode\nchild1, child2 = rng.spawn(2)\nprint(\"Child 1:\", child1.random())\nprint(\"Child 2:\", child2.random())\n\n\nChild 1: 0.32761207840863094\nChild 2: 0.9971788752210861\n\n\n\n\n\nGenerators are designed to be thread-safe and support “jumping” ahead in the sequence:\n\n\nCode\nrng = np.random.Generator(PCG64())\nrng.bit_generator.advance(1000)  # Jump ahead 1000 steps\n\n\n&lt;numpy.random._pcg64.PCG64 at 0x1b1406c40&gt;\n\n\n\n\n\n\nUse default_rng() to create a Generator unless you have specific requirements for a different Bit Generator.\nSet a seed for reproducibility in scientific computations and testing.\nUse the spawn() method to create independent generators for parallel processing.\nWhen performance is critical, consider using the out parameter to fill existing arrays.\nFor very long periods or when security is important, consider using the PCG64DXSM Bit Generator.\n\nRemember, Generators provide a more robust, flexible, and future-proof approach to random number generation in NumPy. They offer better statistical properties and are designed to work well in both single-threaded and multi-threaded environments."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/random_numbers.html#numpy-generator-based-random-number-generation-cheatsheet",
    "href": "example_course/course-materials/cheatsheets/random_numbers.html#numpy-generator-based-random-number-generation-cheatsheet",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "For information on the previous np.random API and its use cases, please refer to the NumPy documentation on legacy random generation: NumPy Legacy Random Generation\nThis cheatsheet focuses on the modern Generator-based approach to random number generation in NumPy.\n\n\n\n\nCode\nimport numpy as np\n\n\n\n\n\n\n\nCode\n# Create a Generator with the default BitGenerator\nrng = np.random.default_rng()\n\n# Create a Generator with a specific seed\nrng_seeded = np.random.default_rng(seed=42)\n\n\n\n\n\n\n\n\n\nCode\n# Single random float\nprint(rng.random())\n\n# Array of random floats\nprint(rng.random(5))\n\n\n0.1587752984013584\n[0.6803629  0.37911992 0.27807791 0.76383091 0.63573615]\n\n\n\n\n\n\n\nCode\n# Single random integer from 0 to 10 (inclusive)\nprint(rng.integers(11))\n\n# Array of random integers from 1 to 100 (inclusive)\nprint(rng.integers(1, 101, size=5))\n\n\n7\n[53 20 30 44 41]\n\n\n\n\n\n\n\nCode\n# Single value from standard normal distribution\nprint(rng.standard_normal())\n\n# Array from normal distribution with mean=0, std=1\nprint(rng.normal(loc=0, scale=1, size=5))\n\n\n0.6468359466300964\n[-0.31137764  0.18056671  0.53194635  1.36360984 -0.52374801]\n\n\n\n\n\n\n\n\nCode\n# Random choice from an array\narr = np.array([1, 2, 3, 4, 5])\nprint(rng.choice(arr))\n\n# Random sample without replacement\nprint(rng.choice(arr, size=3, replace=False))\n\n\n2\n[3 2 1]\n\n\n\n\n\n\n\nCode\narr = np.arange(10)\nrng.shuffle(arr)\nprint(arr)\n\n\n[5 7 4 8 2 6 9 3 1 0]\n\n\n\n\n\nGenerators provide methods for many other distributions:\n\n\nCode\n# Poisson distribution\nprint(rng.poisson(lam=5, size=3))\n\n# Exponential distribution\nprint(rng.exponential(scale=1.0, size=3))\n\n# Binomial distribution\nprint(rng.binomial(n=10, p=0.5, size=3))\n\n\n[5 5 8]\n[0.18983469 0.31953811 1.68140815]\n[5 4 6]\n\n\n\n\n\nGenerators can fill existing arrays, which can be more efficient:\n\n\nCode\narr = np.empty(5)\nrng.random(out=arr)\nprint(arr)\n\n\n[0.64436064 0.82549213 0.34414789 0.69527507 0.86218693]\n\n\n\n\n\nYou can use different Bit Generators with varying statistical qualities:\n\n\nCode\nfrom numpy.random import PCG64, MT19937\n\nrng_pcg = np.random.Generator(PCG64())\nrng_mt = np.random.Generator(MT19937())\n\nprint(\"PCG64:\", rng_pcg.random())\nprint(\"MT19937:\", rng_mt.random())\n\n\nPCG64: 0.17483451679680462\nMT19937: 0.4790870868216416\n\n\n\n\n\n\n\nCode\n# Save state\nstate = rng.bit_generator.state\n\n# Generate some numbers\nprint(\"Original:\", rng.random(3))\n\n# Restore state and regenerate\nrng.bit_generator.state = state\nprint(\"Restored:\", rng.random(3))\n\n\nOriginal: [0.26224457 0.44779407 0.50033989]\nRestored: [0.26224457 0.44779407 0.50033989]\n\n\n\n\n\nYou can create independent generators from an existing one:\n\n\nCode\nchild1, child2 = rng.spawn(2)\nprint(\"Child 1:\", child1.random())\nprint(\"Child 2:\", child2.random())\n\n\nChild 1: 0.32761207840863094\nChild 2: 0.9971788752210861\n\n\n\n\n\nGenerators are designed to be thread-safe and support “jumping” ahead in the sequence:\n\n\nCode\nrng = np.random.Generator(PCG64())\nrng.bit_generator.advance(1000)  # Jump ahead 1000 steps\n\n\n&lt;numpy.random._pcg64.PCG64 at 0x1b1406c40&gt;\n\n\n\n\n\n\nUse default_rng() to create a Generator unless you have specific requirements for a different Bit Generator.\nSet a seed for reproducibility in scientific computations and testing.\nUse the spawn() method to create independent generators for parallel processing.\nWhen performance is critical, consider using the out parameter to fill existing arrays.\nFor very long periods or when security is important, consider using the PCG64DXSM Bit Generator.\n\nRemember, Generators provide a more robust, flexible, and future-proof approach to random number generation in NumPy. They offer better statistical properties and are designed to work well in both single-threaded and multi-threaded environments."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/chart_customization.html",
    "href": "example_course/course-materials/cheatsheets/chart_customization.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, color='red', linestyle='--', linewidth=2, marker='o', markersize=5)\nplt.title('Customized Line Plot', fontsize=16)\nplt.xlabel('X-axis', fontsize=12)\nplt.ylabel('Y-axis', fontsize=12)\nplt.grid(True, linestyle=':')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(-1.5, 1.5)\nplt.xticks(np.arange(0, 6, 1))\nplt.yticks(np.arange(-1.5, 1.6, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, np.sin(x), label='sin(x)')\nplt.plot(x, np.cos(x), label='cos(x)')\nplt.legend(fontsize=12, loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.title('Sine Wave', fontsize=16, fontweight='bold')\nplt.text(5, 0.5, 'Peak', fontsize=14, color='red')\nplt.annotate('Trough', xy=(3*np.pi/2, -1), xytext=(6, -0.5),\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.show()"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/chart_customization.html#matplotlib-customization",
    "href": "example_course/course-materials/cheatsheets/chart_customization.html#matplotlib-customization",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, color='red', linestyle='--', linewidth=2, marker='o', markersize=5)\nplt.title('Customized Line Plot', fontsize=16)\nplt.xlabel('X-axis', fontsize=12)\nplt.ylabel('Y-axis', fontsize=12)\nplt.grid(True, linestyle=':')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.xlim(0, 5)\nplt.ylim(-1.5, 1.5)\nplt.xticks(np.arange(0, 6, 1))\nplt.yticks(np.arange(-1.5, 1.6, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, np.sin(x), label='sin(x)')\nplt.plot(x, np.cos(x), label='cos(x)')\nplt.legend(fontsize=12, loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\nplt.title('Sine Wave', fontsize=16, fontweight='bold')\nplt.text(5, 0.5, 'Peak', fontsize=14, color='red')\nplt.annotate('Trough', xy=(3*np.pi/2, -1), xytext=(6, -0.5),\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.show()"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/chart_customization.html#seaborn-customization",
    "href": "example_course/course-materials/cheatsheets/chart_customization.html#seaborn-customization",
    "title": "EDS 217 Cheatsheet",
    "section": "Seaborn Customization",
    "text": "Seaborn Customization\n\nSetting the Style\n\n\nCode\nimport seaborn as sns\nimport pandas as pd\n\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"deep\")\n\n\n\n\nLoading and Preparing Data\n\n\nCode\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Display the first few rows and data types\nprint(tips.head())\nprint(\"\\nData Types:\")\nprint(tips.dtypes)\n\n# Select only numeric columns for correlation\nnumeric_columns = tips.select_dtypes(include=[np.number]).columns\ntips_numeric = tips[numeric_columns]\n\n\n   total_bill   tip     sex smoker  day    time  size\n0       16.99  1.01  Female     No  Sun  Dinner     2\n1       10.34  1.66    Male     No  Sun  Dinner     3\n2       21.01  3.50    Male     No  Sun  Dinner     3\n3       23.68  3.31    Male     No  Sun  Dinner     2\n4       24.59  3.61  Female     No  Sun  Dinner     4\n\nData Types:\ntotal_bill     float64\ntip            float64\nsex           category\nsmoker        category\nday           category\ntime          category\nsize             int64\ndtype: object\n\n\n\n\nCustomizing a Scatter Plot\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"time\", size=\"size\")\nplt.title(\"Tips vs Total Bill\", fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing a Box Plot\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=tips, x=\"day\", y=\"total_bill\", palette=\"Set3\")\nplt.title(\"Total Bill by Day\", fontsize=16)\nplt.show()\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_39918/2996973332.py:2: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=tips, x=\"day\", y=\"total_bill\", palette=\"Set3\")\n\n\n\n\n\n\n\n\n\n\n\nCustomizing a Heatmap (Correlation of Numeric Columns)\n\n\nCode\ncorr = tips_numeric.corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", linewidths=0.5)\nplt.title(\"Correlation Heatmap of Numeric Columns\", fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing a Pair Plot\n\n\nCode\nsns.pairplot(tips, hue=\"time\", palette=\"husl\", height=2.5, \n             vars=[\"total_bill\", \"tip\", \"size\"])\nplt.suptitle(\"Pair Plot of Tips Dataset\", y=1.02, fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing a Regression Plot\n\n\nCode\nplt.figure(figsize=(10, 6))\nsns.regplot(data=tips, x=\"total_bill\", y=\"tip\", scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\nplt.title(\"Regression Plot: Tip vs Total Bill\", fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing a Categorical Plot\n\n\nCode\nplt.figure(figsize=(12, 6))\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\", hue=\"sex\", kind=\"violin\", split=True)\nplt.title(\"Distribution of Total Bill by Day and Sex\", fontsize=16)\nplt.show()\n\n\n&lt;Figure size 1152x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nRemember, you can always combine Matplotlib and Seaborn customizations for even more control over your visualizations!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#getting-started",
    "title": "Interactive Session 7A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S)."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#introduction",
    "href": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#introduction",
    "title": "Interactive Session 7A",
    "section": "Introduction",
    "text": "Introduction\nThere are extensive options for plotting in Python – some favorites include statistical visualizations in Seaborn and interactive plots for web applications in Bokeh. The original and fundamental library for visualizations in Python, however, is matplotlib`.\nMatplotlib was the first plotting library developed for Python and remains the most widely used library for data visualization in the Python community. Designed to resemble graphics in MATLAB, matplotlib is reminiscent of MATLAB in both appearance and functionality. As a result, it is not the easiest library to work with, and deviates from the object-oriented syntax we are familiar with in Python.\nThis session will serve as an introduction to plotting in Python using matplotlib. The nature of matplotlib – and figure-making in general – is such that the easiest way to learn is by following examples. As such, this session is structured a bit differently than the others, so be sure to look carefully at the coded examples. Finally, the best way to learn advanced functions and find help with matplotlib is by exploring the examples in the gallery.\n\nInstructions\nWe will work through this notebook together. To run a cell, click on the cell and press “Shift” + “Enter” or click the “Run” button in the toolbar at the top.\n\n\n\n\n\n\nNote\n\n\n\n🐍     This symbol designates an important note about Python structure, syntax, or another quirk."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#introduction-to-matplotlib",
    "href": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#introduction-to-matplotlib",
    "title": "Interactive Session 7A",
    "section": "Introduction to matplotlib",
    "text": "Introduction to matplotlib\n\n\n\n\nmatplotlib logo\n\n\n\nAs always, we will begin by importing the required libraries and packages. For plotting, itself, we will use a module of the matplotlib library called pyplot. The pyplot module consists of a collection of functions to display and edit figures. As you advance with Python and with data analysis, you may want to explore additional features of matplotlib, but pyplot will suit the vast majority of your plotting needs at this stage.\nThe standard import statement for matplotlib.pyplot is:\nimport matplotlib.pyplot as plt\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nAnatomy of a matplotlib plot\nThe core components of a matplotlib plot are the Figure and the Axes. The Figure is the overall window upon which all components are drawn. It serves as the blank container for plots, as well as other things, such as a legend, color bar, etc. You can (and will) create multiple independent figures, and each figure can hold multiple Axes. To the figure you will add Axes, the area where the data are actually plotted and where associated ticks, labels, etc. live.\nWhen working with a single plot, we will mostly deal with the Figure object and its routines, but we will see the Axes become important as we increase the complexity of our plots.\n\n\n\n\nObject heirarchy in matplotlib\n\n\n\n\n\nBasic plotting\nWe will start with the most basic plotting routines: plt.plot() and plt.scatter(). The first, plt.plot(), is used to generate a connected line plot (with optional markers for individual data points). plt.scatter(), as the name suggests, is used to generate a scatter plot.\nEach time you want to create a new figure, it is wise to first initialize a new instance of the matplotlib.figure.Figure class on which to plot our data. While this is not required to display the plot, if you subsequently plot additional data without a new Figure instance, all data will be plotted on the same figure. For example, let’s generate a few functions, \\(y_{\\sin} = \\sin{(x)}\\) and \\(y_{\\cos} = \\cos{(x)}\\):\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Generate a 1D array with 300 points between -5 and 5\nx = np.linspace(-5,5,300)\n# Generate sine wave\nysin = np.sin(x)\n# Generate cosine wave\nycos = np.cos(x)\n\n\nWe can plot these on the same figure without instancing plt.figure() as follows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Plot sine wave\nplt.plot(x,ysin)\n# Plot cosine wave\nplt.plot(x,ycos)\n\n\n\n\n\n\n\n\n\nTo create multiple graphs in separate figure windows, however, you need to create new Figure instances as follows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Plot sine wave\nfig1 = plt.figure()\nplt.plot(x,ysin)\n\n# Plot cosine wave\nfig2 = plt.figure()\nplt.plot(x,ycos)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis also allows you to access the Figure object later by refering to the variable fig. Thus, even when you want to plot all data on a single plot, it is best to always start by initializing a new Figure.\nTo generate a scatter plot instead of a line, we can use plt.scatter():\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Generate new x and y with fewer points for legibility\n# np.linspace(lower, upper, n): \n#     Creates n points between lower and upper, including both bounds.\n\nxscat = np.linspace(-5,5,25)\nyscat = np.sin(xscat)\n\n# Plot sine function as scatter plot\nplt.scatter(xscat,yscat)\n\n\n\n\n\n\n\n\n\nYou can also create a scatter plot using plt.plot() with keyword arguments, which allow you to change things like the color, style, and size of the lines and markers. We will explore some of these keyword arguments in the next section."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#plt.plot-keyword-arguments",
    "href": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#plt.plot-keyword-arguments",
    "title": "Interactive Session 7A",
    "section": "plt.plot() Keyword arguments",
    "text": "plt.plot() Keyword arguments\nIn addition to the required x and y parameters, there are a number of optional keyword arguments that can be passed to the matplotlib plotting functions. Here, we will consider a few of the most useful: color, marker, and linestyle.\n\nColors\nThe first thing you might wish to control is the color of your plot. Matplotlib accepts several different color definitions to the color keyword argument, which is a feature of most plotting functions.\nFirst, colors can be passed as strings according to their HTML/CSS names. For example:\n\nUsing HTML string names to assign colors\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Specifying color with a string:\ny = ysin\nplt.plot(x, y, 'green')\n\n\n\n\n\n\n\n\n\nIn total, there are 140 colors allowed in HTML; their names are shown below.\n\n\n\n\nHTML color names\n\n\n\nAs you can see in the image above, the basic colors can also be defined by a single-letter shortcut. These are shown in the table below.\n\n\n\nLetter code\nColor name\n\n\n\n\n'r'\nred\n\n\n'g'\ngreen\n\n\n'b'\nblue\n\n\n'c'\ncyan\n\n\n'm'\nmagenta\n\n\n'y'\nyellow\n\n\n'k'\nblack\n\n\n'w'\nwhite\n\n\n\n\n\nUsing RGB(A) tuples to assign colors\nAnother way of specifying colors is to use an RGB(A) tuple, where the brightness of each channel (R, G, or B, which correspond to red, green, and blue) is given as a float between 0 and 1.\n\n\n\n\n\n\nUsing alpha for transparency\n\n\n\nAn optional fourth value, A or alpha, value can be passed to specify the opacity of the line or marker.\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Specifying color with an RGB tuple:\nplt.plot(x, y, color=(0.2,0.7,1.0))\n\n\n\n\n\n\n\n\n\nA grayscale value can be used by passing a number between 0 and 1 as a string. In this representation, '0.0' corresponds to black and '1.0' corresponds to white.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Specifying greyscale with a intensity value [0-1]:\nplt.plot(x, y, color='0.25')\n\n\n\n\n\n\n\n\n\n\n\nUsing hex codes to define colors\nAnother way to define colors is to use color hex codes, which represent colors as hexadecimals ranging from 0 to FF. Color hex codes consist of a hash character # followed by six hex values (e.g. #AFD645). Hex codes must be passed as strings (e.g. '#AFD645') in matplotlib and are perhaps the most flexible way to select colors.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Specifying color with a hex code:\nplt.plot(x, y, color='#C6E2FF')\n\n\n\n\n\n\n\n\n\n\n\n\nLinestyles\nUsing the linestyle keyword argument, you can change the style of the line plotted using plt.plot(). These can be specified either by their name or a shortcut. A few of the style options (and their matplotlib shortcuts) are shown in the table below. To see a full list of linestyle options, see the docs.\n\n\n\nShort code\nLine style\n\n\n\n\n'-'\nsolid\n\n\n'--'\ndashed\n\n\n':'\ndotted\n\n\n'-.'\ndashdot\n\n\n\nAs we’ve already seen, the default linestyle is solid. The syntax for changing a line’s style is:\nplt.plot(x, y, linestyle='dashed')\nor, more commonly:\nplt.plot(x, y, linestyle='--')\nLet’s adjust the style of our waveform plot using the linestyle keyword argument.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave with different colors + linestyles\nplt.plot(x, np.sin(x - 0), color='darkblue', linestyle='-')\nplt.plot(x, np.sin(x - 1), color='m', linestyle='dashed')\nplt.plot(x, np.sin(x - 2), color=(0.0,0.8,0.81), linestyle=':') \nplt.plot(x, np.sin(x - 3), color='0.65', linestyle='solid')\nplt.plot(x, np.sin(x - 4), color='#B8D62E', linestyle='-.')\n\n\n\n\n\n\n\n\n\n\nMarkers\nMarkers can be used in plt.plot() and plt.scatter(). There are several available markers in matplotlib, and you can also define your own. A few of the most useful are shown in the table below.\n\n\n\nMarker code\nSymbol\nDescription\n\n\n\n\n'o'\n●\ncircle\n\n\n'.'\n⋅\npoint\n\n\n'*'\n★\nstar\n\n\n'+'\n\\(+\\)\nplus\n\n\n'x'\n\\(\\times\\)\nx\n\n\n'^'\n▲\ntriangle\n\n\n's'\n◼\nsquare\n\n\n\nNote that unlike color and linestyle, the marker keyword argument only accepts a code to specify the marker style.\nplt.scatter(x, y, marker='+')\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave as scatter plot with different colors + markers\nplt.scatter(xscat, yscat-0, color='darkblue', marker='o')\nplt.scatter(xscat, yscat-1, color='m', marker='.')\nplt.scatter(xscat, yscat-2, color=(0.0,0.8,0.81), marker='+')\nplt.scatter(xscat, yscat-3, color='0.65', marker='*')\nplt.scatter(xscat, yscat-4, color='#B8D62E', marker='s')\n\n\n\n\n\n\n\n\n\nUsing the marker keyword argument with the plt.plot() function creates a connected line plot, where the data points are designated by markers and connected by lines.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave with different colors + markers\nplt.plot(xscat, np.sin(xscat - 0), color='darkblue', marker='o')\nplt.plot(xscat, np.sin(xscat - 1), color='m', marker='.')\nplt.plot(xscat, np.sin(xscat - 2), color=(0.0,0.8,0.81), marker='+')\nplt.plot(xscat, np.sin(xscat - 3), color='0.65', marker='*')\nplt.plot(xscat, np.sin(xscat - 4), color='#B8D62E', marker='s')\n\n\n\n\n\n\n\n\n\n\n\nExplicit definitions vs. shortcuts\nUp to now, we have used explicit definitions to specify keyword arguments. While this is generally preferable, matplotlib does allow color, linestyle, and marker codes to be combined into a single, non-keyword argument. For example:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Plot a dashed red line\nplt.plot(x, y, 'r--')\n\n\n\n\n\n\n\n\n\nSeveral examples are presented in the cell below.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave with different colors + markers\nplt.plot(xscat, yscat-0, 'b-o')    # Solid blue line with circle markers\nplt.plot(xscat, yscat-1, 'm--*')   # Dashed magenta line with star markers\nplt.plot(xscat, yscat-2, 'c+')     # Cyan plus markers\nplt.plot(xscat, yscat-3, 'k')      # Solid black line\nplt.plot(xscat, yscat-4, 'y-s')    # Solid yellow line with square markers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs you can see, the downside of this method is that you are limited to the eight colors that have a single-letter code. To use other colors, you must use explicitly defined keyword arguments.\n\n\nIn addition to those we explored in this section, other useful keyword arguments include linewidth and markersize, which do exactly what you’d expect them to do. For a full list of keyword arguments (you should know what’s coming by now), see the docs."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#axes-settings",
    "href": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#axes-settings",
    "title": "Interactive Session 7A",
    "section": "Axes settings",
    "text": "Axes settings\nNext, we will explore how to scale and annotate a plot using axes routines that control what goes on around the edges of the plot.\n\nLimits\nBy default, matplotlib will attempt to determine x- and y-axis limits, which usually work pretty well. Sometimes, however, it is useful to have finer control. The simplest way to adjust the display limits is to use the plt.xlim() and plt.ylim() methods.\nIn the example below, adjust the numbers (these can be int or float values) to see how the plot changes.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set axis limits\nplt.xlim(-5,5)\nplt.ylim(-2,2)\n\n\n\n\n\n\n\n\n\n\n\nTicks and Tick Labels\nYou may also find it useful to adjust the ticks and/or tick labels that matplotlib displays by default. The plt.xticks() and plt.yticks() methods allow you to control the locations of both the ticks and the labels on the x- and y-axes, respectively. Both methods accept two list or array-like arguments, as well as optional keyword arguments. The first corresponds to the ticks, while the second controls the tick labels.\n# Set x-axis ticks at 0, 0.25, 0.5, 0.75, 1.0 with all labeled\nplt.xticks([0,0.25,0.5,0.75,1.0])\n# Set y-axis ticks from 0 to 100 with ticks on 10s and labels on 20s\nplt.yticks(np.arange(0,101,10),['0','','20','','40','','60','','80','','100'])\n\n\n\n\n\n\nImportant\n\n\n\nIf the labels are not specified, all ticks will be labeled accordingly. To only label certain ticks, you must pass a list with empty strings in the location of the ticks you wish to leave unlabeled (or the ticks will be labeled in order).\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n\n([&lt;matplotlib.axis.YTick at 0x1b2b66450&gt;,\n  &lt;matplotlib.axis.YTick at 0x1b2b7eb10&gt;,\n  &lt;matplotlib.axis.YTick at 0x1b2b3bd50&gt;,\n  &lt;matplotlib.axis.YTick at 0x1b2b44450&gt;,\n  &lt;matplotlib.axis.YTick at 0x1b2b72e10&gt;],\n [Text(0, -1.0, '−1.0'),\n  Text(0, -0.5, '−0.5'),\n  Text(0, 0.0, '0.0'),\n  Text(0, 0.5, '0.5'),\n  Text(0, 1.0, '1.0')])\n\n\n\n\n\n\n\n\n\nAs with any plot, it is imperative to include x- and y-axis labels. This can be done by passing strings to the plt.xlabel() and plt.ylabel() methods:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\n\n\nText(0, 0.5, 'y-axis')\n\n\n\n\n\n\n\n\n\nA nice feature about matplotlib is that it supports TeX formatting for mathematical expressions. This is quite useful for displaying equations, exponents, units, and other mathematical operators. The syntax for TeX expressions is 'r$TeX expression here$'. For example, we can display the axis labels as \\(x\\) and \\(\\sin{(x)}\\) as follows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$\\sin{(x)}$')\n\n\nText(0, 0.5, '$\\\\sin{(x)}$')\n\n\n\n\n\n\n\n\n\n\n\nTitles\nAdding a title to your plot is analogous to labeling the x- and y-axes. The plt.title() method allows you to set the title of your plot by passing a string:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\nplt.plot(x, ycos, color='#B8D62E')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\n\n# Set title\nplt.title('Sinusoidal functions')\n\n\nText(0.5, 1.0, 'Sinusoidal functions')\n\n\n\n\n\n\n\n\n\n\n\nLegends\nWhen multiple datasets are plotted on the same axes it is often useful to include a legend that labels each line or set of points. Matplotlib has a quick way of displaying a legend using the plt.legend() method. There are multiple ways of specifying the label for each dataset; I prefer to pass a list of strings to plt.legend():\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\nplt.plot(x, ycos, color='#B8D62E')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\n\n# Set title\nplt.title('Sinusoidal functions')\n\n# Legend\nplt.legend(labels=['sin(x)','cos(x)'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnother way of setting the data labels is to use the label keyword argument in the plt.plot() (or plt.scatter()) function:\n# Plot data\nplt.plot(x1, y1, label='Data1')\nplt.plot(x2, y2, label='Data2')\n\n# Legend\nplt.legend()\nNote that you must still run plt.legend() to display the legend.\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, label='sin(x)', color='darkblue')\nplt.plot(x, ycos, label='cos(x)', color='#B8D62E')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\n\n# Set title\nplt.title('Sinusoidal functions')\n\n# Legend\nplt.legend()"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#subplots-multiple-axes",
    "href": "example_course/course-materials/interactive-sessions/7a_visualizations_1.html#subplots-multiple-axes",
    "title": "Interactive Session 7A",
    "section": "Subplots + multiple axes",
    "text": "Subplots + multiple axes\nNow that we’ve established the basics of plotting in matplotlib, let’s get a bit more complicated. Oftentimes, you may want to plot data on multiple axes within the same figure. The easiest way to do this in matplotlib is to use the plt.subplot() function, which takes three non-keyword arguments: nrows, ncols, and index. nrows and ncols correspond to the total number of rows and columns of the entire figure, while index refers to the index position of the current axes. Importantly (and annoyingly), the index for subplots starts in the upper left corner at 1 (not 0)!. The image below contains a few examples of how matplotlib arranges subplots.\n\n\n\n\nmatplotlib subplots\n\n\n\nThe most explicit way of adding subplots is to use the fig.add_subplot() command to initialize new axes as variables. This allows you to access each Axes object later to plot data and adjust the axes parameters.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig = plt.figure()\n# Add four axes\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\n\n\n\n\n\n\n\n\n\nTo plot data, we use ax.plot() or ax.scatter(). These methods are analogous to plt.plot() and plt.scatter(), but they act on individual Axes, rather than the Figure object.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig = plt.figure()\n# Add four axes\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\n\n# Plot data\n# Plot sine wave with different colors on different axes\nax1.plot(x, np.sin(x - 0), color='darkblue')\nax2.plot(x, np.sin(x - 1), color='m')\nax3.plot(x, np.sin(x - 2), color=(0.0,0.8,0.81))\nax4.plot(x, np.sin(x - 4), color='#B8D62E')\n\n\n\n\n\n\n\n\n\n\nFigure vs. Axes methods\nPerhaps the trickiest part about subplots – and Axes methods in general – is adjusting the axes settings. While most Figure functions translate directly Axes methods (e.g. plt.plot() \\(\\rightarrow\\) ax.plot(), plt.legend() \\(\\rightarrow\\) ax.legend()), commands to set limits, ticks, labels, and titles are slightly modified. Some important Figure methods and their Axes counterparts are shown in the table below.\n\n\n\nFigure command\nAxes command\n\n\n\n\nplt.xlabel()\nax.set_xlabel()\n\n\nplt.ylabel()\nax.set_ylabel()\n\n\nplt.xlim()\nax.set_xlim()\n\n\nplt.ylim()\nax.set_ylim()\n\n\nplt.xticks()\nax.set_xticks()\n\n\nplt.yticks()\nax.set_yticks()\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese commands are different primarily because the Figure functions are inherited from MATLAB syntax (remember, matplotlib was design to work exactly like matlab functions), while the Axes functions were developed later and are object-oriented. Generally, the arguments are similar – if not identical – between the two.\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Initialize empty figure\nfig = plt.figure()\n# Add four axes\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\n\n# Plot data\n# Plot sine wave with different colors on different axes\nax1.plot(x, np.sin(x - 0), color='darkblue')\nax2.plot(x, np.sin(x - 1), color='m')\nax3.plot(x, np.sin(x - 2), color=(0.0,0.8,0.81))\nax4.plot(x, np.sin(x - 4), color='#B8D62E')\n\n# Set axes limits, labels, + ticks\nfor i,ax in enumerate([ax1,ax2,ax3,ax4]):\n    # i is the list index, but subplots count from 1.\n    # so make a new variable to keep track of subplot number:\n    subplot_number =  i + 1 \n    # Set x limits \n    ax.set_xlim(-5,5)\n    # Set title\n    ax.set_title(f'$\\sin{{(x - {i})}}$')\n    # Only label x ticks and x-axis on bottom row\n    if subplot_number &lt; 3:\n        ax.set_xticklabels([])\n    else:\n        ax.set_xlabel('x')\n    # Only label y ticks and y-axis on left column\n    if subplot_number == 1 or subplot_number == 3:\n        ax.set_ylabel('y')\n    else:\n        ax.set_yticklabels([])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the last example, we included a command, plt.tight_layout(), which automatically formats the figure to fit the window. This is most useful when using an IDE with a separate plotting window, rather than with in-line plots like those in a notebook. To get a sense of what plt.tight_layout() does, try re-running the above cell with this command commented out.\n\n\nTo go beyond regularly gridded subplots and create subplots that span multiple rows and/or columns, check out GridSpec.\n\nEnd interactive session 7A"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day1.html#objective",
    "href": "example_course/course-materials/eod-practice/eod-day1.html#objective",
    "title": "Day 1: Tasks & activities",
    "section": "Objective",
    "text": "Objective\nIn this exercise, you will work with climate data using the Python data science workflow. You’ll load the data into a pandas DataFrame, perform basic exploration and cleaning, and create visualizations. This hands-on practice will help you understand how Python can be used for data analysis, with comparisons to similar tasks in R. Think of this as a movie trailer for the skills you’ll build over the next week.\n\n🎬 “Coming Attractions” Approach\n\nYour job: Copy, paste, and run the code exactly as written\nOur job: Show you what’s happening (not how it works yet!)\nThe goal: Get excited about what you’ll learn and see the big picture\n\n\n\n\n\n\n\nDon’t Panic! 🚀\n\n\n\nYou’re not expected to understand every line of code today. By next Friday, you’ll know exactly how all of this works. For now, just enjoy the ride and see what’s possible!"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day1.html#background-and-data-source",
    "href": "example_course/course-materials/eod-practice/eod-day1.html#background-and-data-source",
    "title": "Day 1: Tasks & activities",
    "section": "Background and Data Source",
    "text": "Background and Data Source\nOur data comes from the Arctic Long Term Ecological Research station. The Arctic Long Term Ecological Research (ARC LTER) site is part of a network of sites established by the National Science Foundation to support long-term ecologicalLooking South of Toolik Field Station research in the United States. The research site is located in the foothills region of the Brooks Range, North Slope of Alaska (68° 38’N, 149° 36.4’W, elevation 720 m). The Arctic LTER project’s goal is to understand and predict the effects of environmental change on arctic landscapes, both natural and anthropogenic. Researchers at the site use long-term monitoring and surveys of natural variation of ecosystem characteristics, experimental manipulation of ecosystems (years to decades) and modeling at ecosystem and watershed scales to gain an understanding of the controls of ecosystem structure and function. The data and insights gained are provided to federal, Alaska state and North Slope Borough officials who regulate the lands on the North Slope and through this web site.\nWe will be using some basic weather data downloaded from Toolik Station:\n\nToolik Station Meteorological Data: toolik_weather.csv Shaver, G. 2019. A multi-year DAILY weather file for the Toolik Field Station at Toolik Lake, AK starting 1988 to present. ver 4. Environmental Data Initiative. https://doi.org/10.6073/pasta/ce0f300cdf87ec002909012abefd9c5c (Accessed 2021-08-08).\n\nI have already downloaded this data and placed in our course repository, where we can access it easily using its github raw url.\nLet’s dive into the exercise!\n\n🗓️ When You’ll Master These Skills\n\n\n\nWhat you’ll see today\nWhen you’ll learn it\nWhat we’ll cover\n\n\n\n\nimport pandas as pd\nDay 3-4\nData structures and DataFrames\n\n\npd.read_csv()\nDay 4\nLoading data from files\n\n\ndf.head(), df.info()\nDay 4\nData exploration methods\n\n\ndf.groupby()\nDay 6\nData aggregation and grouping\n\n\nplt.plot(), plt.bar()\nDay 7\nData visualization"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day1.html#instructions",
    "href": "example_course/course-materials/eod-practice/eod-day1.html#instructions",
    "title": "Day 1: Tasks & activities",
    "section": "Instructions",
    "text": "Instructions\n\nSetup and Data Loading\n\nOpen JupyterLab and Start a New Notebook\n\n\n\nImport Libraries\n\nImport the necessary libraries to work with data (pandas) and create plots (matplotlib.pyplot). Use the standard python conventions that import pandas as pd and import matplotlib.pyplot as plt\n\n\n🎬 Copy and paste this code:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nWhat just happened? We imported two powerful libraries! pandas is like Excel but supercharged for data analysis, and matplotlib creates beautiful(ish) plots. 🎓 Coming up: You’ll learn about Python imports and libraries on Days 2-3.\n\n\nLoad the Data\nOur data is located at:\nhttps://raw.githubusercontent.com/environmental-data-science/eds217-day0-comp/main/data/raw_data/toolik_weather.csv\n\nCreate a variable called url that stores the URL provided above as a string.\nUse the pandas library’s read_csv() function from pandas to load the data from the URL into a new DataFrame called df. Any pandas function will always be called using the pd object and dot notation: pd.read_csv().\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe read_csv() function can do a ton of different things, but today all you need to know is that it can take a url to a csv file as it’s only input.\n\n\n🎬 Copy and paste this code:\nurl = 'https://raw.githubusercontent.com/environmental-data-science/eds217-day0-comp/main/data/raw_data/toolik_weather.csv'\ndf = pd.read_csv(url)\n\n\n\n\n\n\nR vs Python: Data Loading\n\n\n\nThis is just like df &lt;- read.csv(url) in R! Both pandas DataFrames and R data.frames are tabular data structures. The main syntax difference is Python’s dot notation: pd.read_csv() vs R’s read.csv(). Both can read directly from URLs, which is incredibly convenient for reproducible research!\n\n\nWhat just happened? We loaded over 15,000 rows of climate data from the internet in one line! The data is now stored in a “DataFrame” called df. 🎓 Coming up: Day 4 will teach you all about loading and working with data files.\n\n\n\nData Exploration\n\nPreview the Data\n\nUse the head() method to display the first few rows of the DataFrame df.\n\n\n🎬 Copy and paste this code:\ndf.head()\n\n\n\n\n\n\nNote\n\n\n\nBecause the head() function is a method of a DataFrame, you will call it using dot notation and the dataframe you just created: df.head()\n\n\nWhat just happened? We previewed the first 5 rows of our 15,000+ row dataset! You can see daily weather measurements from Alaska. 🎓 Coming up: Day 4 morning will teach you data exploration methods like this.\n\n\n\n\n\n\nR vs Python: Data Exploration\n\n\n\nThis is exactly like head(df) in R! The key difference is Python’s object-oriented approach: df.head() vs R’s functional approach head(df). Both show you the first few rows, but Python treats the DataFrame as an object that has methods (like .head()) built into it.\n\n\n\n\nCheck for Data Quality\n\nUse the isnull() method combined with sum() to count missing values in each column.\n\n\n🎬 Copy and paste this code:\ndf.isnull().sum()\nWhat just happened? We checked every column for missing data! Looks like our temperature data is complete (0 missing values), which is great. 🎓 Coming up: Day 5 will teach you all about data cleaning and handling missing values.\n\n\n\n\n\n\nR vs Python: Missing Data Check\n\n\n\nIn R, you’d use sum(is.na(df)) to count missing values. Python uses df.isnull().sum() - notice the chaining of methods! This reads left-to-right: “take the DataFrame, check for null values, then sum them up.” Both approaches give you the count of missing values per column.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should see that the Daily_AirTemp_Mean_C doesn’t have any missing values. This means we can skip the usual step of dealing with missing data. We’ll learn these tools in Python and Pandas later in the course.\n\n\n\n\nGet Data Summary Statistics and Data Descriptions\n\nUse the describe() method to generate summary statistics for numerical columns.\nUse the info() method to get an overview of the DataFrame, including data types and non-null counts. Just like the head() function, these are methods associated with your df object, so you call them with dot notation.\n\n\n🎬 Copy and paste this code:\ndf.describe()\ndf.info()\nWhat just happened? We got instant statistics and information about our entire dataset! You can see temperature ranges, averages, and data types. 🎓 Coming up: Day 4 will teach you how to explore and understand your datasets.\n\n\n\n\n\n\nR vs Python: Data Summary\n\n\n\nThese are like summary(df) and str(df) in R. Python’s .describe() gives you the statistical summary (like summary()) while .info() shows the structure (like str()). Notice how Python uses dot notation - the DataFrame object has these methods built in, whereas R uses separate functions that take the data frame as input.\n\n\n\n\n\n\nData Analysis\n\nCalculate Monthly Average Temperature\n\nNow for some real data analysis - let’s find average temperatures by month!\n🎬 Copy and paste this code:\nmonthly = df.groupby('Month')\nmonthly_means = monthly['Daily_AirTemp_Mean_C'].mean()\nWhat just happened? We grouped 15,000+ daily temperature readings by month and calculated averages! This turned years of daily data into 12 monthly summaries. 🎓 Coming up: Day 6 will teach you all about grouping and aggregating data like this.\n\n\n\n\n\n\nR vs Python: Data Grouping\n\n\n\nThis is exactly like using df %&gt;% group_by(Month) %&gt;% summarize(mean_temp = mean(Daily_AirTemp_Mean_C)) in dplyr! Both approaches group data and calculate statistics. Python’s syntax is df.groupby('column')['target_column'].function(), while R uses the pipe operator %&gt;% to chain operations. Both are powerful for data aggregation!\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can do analysis on a specific column in a dataframe using [column_nanme] notation: my_df[\"column A\"].mean() would give the average value of “column A” (if there was a column with that name in the dataframe). In the coming days, we will spend a lot of time learning how to select and subset data in dataframes!\n\n\n\n\nPlot Monthly Average Temperature\n\nTime to turn numbers into pictures! Let’s plot the monthly temperature patterns.\n🎬 Copy and paste this code:\nplt.plot(monthly_means)\nNow let’s make it even better with labels:\nmonths = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nplt.bar(months, monthly_means)\nWhat just happened? You used a basic plotting function to make a data visualization. The bar chart clearly shows Alaska’s extreme seasonal temperature differences. 🎓 Coming up: Day 7 will teach you how to create amazing visualizations and customize them.\n\n\n\n\n\n\nR vs Python: Data Visualization\n\n\n\nThis is like creating plots with ggplot(df, aes(x=Month, y=temp)) + geom_bar() in R! Python’s matplotlib uses a more direct approach: plt.plot() and plt.bar() create plots immediately. Both are powerful - ggplot2 uses a “grammar of graphics” approach while matplotlib is more imperative. You’ll learn both have their strengths!\n\n\n\n\nAnalyze Temperature Trends Over Years\n\nLet’s explore how temperatures have changed over the decades!\n🎬 Copy and paste this code:\nyear = df.groupby('Year')\nyearly_means = year['Daily_AirTemp_Mean_C'].mean()\nplt.plot(yearly_means)\nAnd as a bar chart:\nyear_list = df['Year'].unique()\nplt.bar(year_list, yearly_means)\nWhat just happened? You analyzed climate trends across multiple decades! You can see how Arctic temperatures have varied over time - real climate science! 🎓 Coming up: This combines Day 6 skills (grouping data) with Day 7 skills (visualization).\n\n\n\n\n\n\nR vs Python: Time Series Analysis\n\n\n\nThis is just like grouping by year in R and plotting the results! Whether you use df %&gt;% group_by(Year) %&gt;% summarize() in R or df.groupby('Year').mean() in Python, you’re doing the same analytical thinking. The syntax differs, but the data science concepts are identical.\n\n\n\n\nSaving Analyses and Figures\nData scientists always save their analyses for future use.\n🎬 Copy and paste this code:\nmonthly_means.to_csv(\"monthly_means.csv\", header=True)\nWhat just happened? You saved your analysis results to a file that you (or other scientists) can use later! This is how research becomes reproducible. 🎓 Coming up: Day 4 will teach you all about importing, exporting, and managing data files.\n\n\n\n\n\n\nR vs Python: Data Export\n\n\n\nThis is just like write.csv(monthly_means, \"monthly_means.csv\") in R! Python uses the object-oriented approach where the data (your Series monthly_means) has a method .to_csv() built into it. R uses a function that takes the data as input. Both create the exact same CSV file - just different syntax approaches to the same goal.\n\n\n\nExample to_csv() Output:\nIf you inspect the monthly_means.csv file using the file browser in JupyterLab, it will look something like this:\nMonth,Daily_AirTemp_Mean_C\n1,-20.561290322580643\n2,-23.94107142857143\n3,-17.806451612903224\n4,-15.25294117647059\n5,-0.8758190327613105\n6,8.76624"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day1.html#conclusion",
    "href": "example_course/course-materials/eod-practice/eod-day1.html#conclusion",
    "title": "Day 1: Tasks & activities",
    "section": "Conclusion",
    "text": "Conclusion\nWe will spend the rest of the course learning more about each of the steps we just went through. And of course, we have a lot more to learn about the essentials of the Python programming language over the next 8 days of class.\nTake some time now to reflect on what you’ve learned today, and to add some additional comments and notes in your code to follow up on in the coming days.\nBy the end of the course you will be writing your own Python data science workflows just like this one… hopefully many of the “code strangers” you’ve just met will have become good friends!\n\n🎉🎉 Congratulations! You made it to the end of a Python data science workflow…🎉🎉\n🎉🎉..and the end of the first day of EDS 217!! 🎉🎉\n\n\nEnd Activity Session (Day 1)"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/7b_visualizations_2.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/7b_visualizations_2.html#getting-started",
    "title": "Interactive Session 7B",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/7b_visualizations_2.html#using-the-seaborn-library",
    "href": "example_course/course-materials/interactive-sessions/7b_visualizations_2.html#using-the-seaborn-library",
    "title": "Interactive Session 7B",
    "section": "Using the Seaborn Library",
    "text": "Using the Seaborn Library\nThis session provides a deeper introduction to the Seaborn visualization library.\nSeaborn helps you explore and understand your data. Its plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots. Its dataset-oriented, declarative API lets you focus on what the different elements of your plots mean, rather than on the details of how to draw them.\nHere’s an example of seaborne’s capabilities.\n\n\nCode\n%matplotlib inline\n# Import seaborn\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"sex\", style=\"smoker\", size=\"size\",\n)\n\ntips.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBehind the scenes, seaborn uses matplotlib to draw its plots. The plot above shows the relationship between five variables in the built-in tips dataset using a single call to the seaborn function relplot().\nNotice that you only need to provide the names of the variables and their roles in the plot.\nThis interface is different from matplotlib, in that you do not need to specify attributes of the plot elements in terms of the color values or marker codes.\nBehind the scenes, seaborn handled the translation from values in the dataframe to arguments that matplotlib understands. This declarative approach lets you stay focused on the questions that you want to answer, rather than on the details of how to control matplotlib.\n\nSeaborn relplot()\nThe function relplot() is named that way because it is designed to visualize many different statistical relationships. While scatter plots are often effective, relationships where one variable represents a measure of time are better represented by a line. The relplot() function has a convenient kind parameter that lets you easily switch to this alternate representation:\n\n\nCode\ndots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n\n\n\n\n\n\n\n\n\nIf you compare the two calls to relplot() in the two examples so far, you will see that the size and style parameters are used in both the scatter plots (first example) and line plots (second example). However, they affect the two visualizations differently.\nIn a scatter plot, the size and style arguments affect marker area and symbol representation.\nIn a line plot, the size and style arguments alter the line width and dashing.\nAllowing the same arguments (syntax) to change meaning (semantics) across different contexts is more characteristic of natural languages than formal ones. In this case, seaborn is attempting to allow you to write in a “grammar of graphics”, which is the same concept underlying ggplot created by Hadley Wickham.\nThe benefit of adopting this less formal specification is that you do not need to worry about as many syntax details and instead can focus more on the overall structure of the plot and the information you want it to convey.\n\n\nComparing matplotlib to seaborn\nA focus of today’s activities is translation, so let’s look at translating some of the examples from yesterday’s matplotlib exercise into seaborn.\nFirst, as always, let’s import our important packages:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nBasic line plots (sns.lineplot)\nLet’s use a couple of common, predictable functions for an example, \\(y_{\\sin} = \\sin{(x)}\\) and \\(y_{\\cos} = \\cos{(x)}\\):\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Generate a 1D array with 300 points between -5 and 5\nx = np.linspace(-5,5,300)\n# Generate sine wave\nysin = np.sin(x)\n# Generate cosine wave\nycos = np.cos(x)\n\n# Now let's make a dataframe from these arrays:\ndf = pd.DataFrame({\n    'x': x,\n    'ysin': ysin,\n    'ycos': ycos\n    })\n\n\nWe can plot these on the same figure without instancing plt.figure() as follows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Plot sine wave\nplt.plot(x,ysin)\n# Plot cosine wave\nplt.plot(x,ycos)\n\n\n\n\n\n\n\n\n\nSeaborn uses the lineplot command to plot line plots:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nsns.lineplot(data=df,x='x',y='ysin')\nsns.lineplot(data=df,x='x',y='ycos')\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with real data\n\n\nWorking with real-world data usually complicates things, and plotting is no exception. In particular, working with time series can get a bit messy. Let’s take a look at some data on solar radiation as an example.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Import data\nbsrn = pd.read_csv(\n    'https://bit.ly/bsrn_data',\n    index_col=0,\n    parse_dates=True\n    )\n\nprint(bsrn.head())\n\n\n                     H_m  SWD_Wm2  STD_SWD  DIR_Wm2  STD_DIR  DIF_Wm2  \\\nDATE                                                                    \n2019-10-01 00:00:00    2     -3.0      0.0      0.0      0.0     -3.0   \n2019-10-01 00:01:00    2     -3.0      0.0      0.0      0.0     -3.0   \n2019-10-01 00:02:00    2     -3.0      0.0      0.0      0.0     -3.0   \n2019-10-01 00:03:00    2     -3.0      0.0      0.0      0.0     -3.0   \n2019-10-01 00:04:00    2     -3.0      0.0      0.0      0.0     -3.0   \n\n                     STD_DIF  LWD_Wm2  STD_LWD  SWU_Wm2  LWU_Wm2  T_degC  \\\nDATE                                                                       \n2019-10-01 00:00:00      0.0    300.0      0.1        0      383    16.2   \n2019-10-01 00:01:00      0.0    300.0      0.3        0      383    16.4   \n2019-10-01 00:02:00      0.0    300.0      0.2        0      383    16.5   \n2019-10-01 00:03:00      0.0    300.0      0.1        0      383    16.5   \n2019-10-01 00:04:00      0.0    300.0      0.1        0      383    16.8   \n\n                       RH  P_hPa  \nDATE                              \n2019-10-01 00:00:00  30.7    966  \n2019-10-01 00:01:00  30.7    966  \n2019-10-01 00:02:00  30.5    966  \n2019-10-01 00:03:00  30.4    966  \n2019-10-01 00:04:00  30.5    966  \n\n\nNow that we’ve imported our data, let’s make a quick plot of incoming shortwave radiation over time.\n\n✏️ Apply it. Translate the cell below into seaborn using the sns.lineplot command.\n\n\n\nCode\n# # Initialize empty figure\n# fig = plt.figure()\n# # Plot incoming SW radiation\n# plt.plot(bsrn.index,bsrn.SWD_Wm2)\n# # Label y-axis\n# plt.ylabel(r'Incoming SW radiation (W m$^{-2}$)')\n\n\nYou should end up with something that looks like this:\n\n\nText(0, 0.5, 'Incoming SW radiation (W m$^{-2}$)')\n\n\n\n\n\n\n\n\n\nThe x-axis looks rather messy because the tick labels are timestamps, which are, by nature, very long. Luckily, there a few approaches you can use to wrangle your x-axis labels when working with long timeseries.\n\n\nTechnique 1: Use figsize to alter the aspect ratio and layout.\nThe default figure size and aspect ratio aren’t that great for long time series, which are usually a wider aspect ratio (think old TV shows vs. widescreen movies). So, often, you can make things work a lot better by just tinkering with the figure size using the plt.figure() command and a figsize argument:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Example 1: Without explicitly setting figsize\nsns.lineplot(data=bsrn, x='DATE', y='SWD_Wm2')\nplt.ylabel(r'Incoming SW radiation (W m$^{-2}$)')\nplt.title('Default Figure Size')\nplt.show()\n\n# Example 2: With explicitly setting figsize\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=bsrn, x='DATE', y='SWD_Wm2')\nplt.ylabel(r'Incoming SW radiation (W m$^{-2}$)')\nplt.title('Custom Figure Size (12x6 inches)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe wider aspect ratio creates more room for your crowded x-axes labels! However, you can still see that the labels are still almost running together on the far right of the figure.\n\n\nTechnique 2: Rotate X-axis Labels\nAnother simple approach is to rotate the x-axis labels:\n\n\nCode\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=bsrn, x='DATE', y='SWD_Wm2')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTechnique 3: Use Fewer X-axis Labels\nIf your dataset spans a long time period, you might want to show fewer labels:\n\n\nCode\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=bsrn, x='DATE', y='SWD_Wm2')\n\n# Show only 5 evenly spaced labels\nplt.gca().xaxis.set_major_locator(plt.MaxNLocator(5))\n\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTechnique 4: Use a Time-based Moving Average\nIf you have too many data points, you might want to resample your data. Here we create a new column from the dataframe index (which contains our dates). We then resample the dataframe to get hourly averages of the data and plot these instead of the raw data, which is collected every minute\n\n\nCode\n# Ensure DATE is a datetime column\nbsrn['DATE'] = pd.to_datetime(bsrn.index)\n\n# Resample to monthly mean\nbsrn_monthly = bsrn.resample('h', on='DATE').mean().reset_index()\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=bsrn_monthly, x='DATE', y='SWD_Wm2')\n\nplt.xticks(rotation=45, ha='right')\nplt.title('BSRN Hourly Average Time Series')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdvanced Axis Labeling: Date Locators and Formatters\nWhile beyond the scope of this course, date locators and formatters in Matplotlib are powerful tools for customizing time-based axes in your plots. They allow you to:\n\nControl which dates are shown on the axis (locators)\nDetermine how those dates are displayed (formatters)\n\nThese tools are particularly useful when working with time series data spanning different time scales (e.g., hours, days, months, years).\nIf you’re interested in learning more about these advanced techniques, here are some helpful resources:\n\nMatplotlib Date Formatting\n\nOfficial examples of date formatting in Matplotlib\n\nDate tick labels\n\nConcise date formatters in Matplotlib\n\nCustomizing Date Ticks\n\nExamples of customizing date ticks\n\nPython Graph Gallery - Time Series\n\nVarious examples of time series plots, some using date locators and formatters\n\n\nRemember, while these tools can create more polished and precise time-based plots, the techniques we’ve covered in this course are sufficient for many basic time series visualizations.\n\n\n\n✏️ Try it. Add a cell to your notebook and add code for the following exercise."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/7b_visualizations_2.html#practice",
    "href": "example_course/course-materials/interactive-sessions/7b_visualizations_2.html#practice",
    "title": "Interactive Session 7B",
    "section": "📚  Practice ",
    "text": "📚  Practice \nPlot temperature and relative humidity (ideally using subplots) over the month of October 2019 at the BSRN station. Be sure to format the timestamps and include axis labels, a title, and a legend, if necessary.\n\n\nCode\n# Add your code here!\n\n# Step 1: Filter the dataframe to October 2019. You can use .loc to filter, as the index of the dataframe is already a datetime. Just filter from the start to end date that you want.\n\n# Step 2: Create a figure with two subplots using the `plt.subplots` command.\n\n\nHere’s one way your plots may turn out:\n\n\n\n\n\n\n\n\n\n\nEnd interactive session 7B"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day7.html#introduction",
    "href": "example_course/course-materials/eod-practice/eod-day7.html#introduction",
    "title": "Day 7: Tasks & activities",
    "section": "Introduction",
    "text": "Introduction\nToday, we will look at the latest plant hardiness zone map distributed in 2023 by the US Department of Agriculture (USDA), and how it differs from the previous map, which came out in 2012.\nThe map is meant to help gardeners, and people interested in gardening and plants, know which plants will survive and thrive in different parts of the United States. The main data point used by the map is the mean low temperature recorded (in Fahrenheit) for a given location. Locations with similar mean low temperatures, within a 10-degree range, are categorized as being in the same zone. Each zone is then further divided into two sub-zones, with a 5-degree range for minimum temperatures.\nThe latest zone map showed that for many locations, the minimum temperature had risen somewhat. This doesn’t come as a massive surprise, given trends in climate change, but I thought that it would be interesting to explore the data and understand just where things had changed.\nThe data was collected by the PRISM research group at Oregon State University (https://prism.oregonstate.edu/projects/plant_hardiness_zones.php). A new data set was just released on November 15th by the US Department of Agriculture, at https://www.ars.usda.gov/news-events/news/research-news/2023/usda-unveils-updated-plant-hardiness-zone-map/.\nThe data is available in a variety of formats, but I decided that it would probably be easiest to work with it via zip codes, because they are spread out through the entire country. In order to figure out just where the zip codes are located, we will download an additional data set, a map of zip codes along with location information for each one, joining it together with our other data.\n\nDatasets\nYou will work with three CSV datasets:\nFirst, we’ll use the latest (2023) Plant Hardiness Zone report from https://prism.oregonstate.edu/projects/plant_hardiness_zones.php . The data comes in several formats and parts; we’ll use the CSV file that provides us with data per US zip code:\nhttps://prism.oregonstate.edu/projects/phm_data/phzm_us_zipcode_2023.csv\nNext, we’ll download data from the previous survey in 2012, described at https://prism.oregonstate.edu/projects/plant_hardiness_zones_2012.php :\nhttps://prism.oregonstate.edu/projects/public/phm/2012/phm_us_zipcode_2012.csv\nFinally, we’ll download and work with a CSV file containing US zip codes:\nhttp://uszipcodelist.com/zip_code_database.csv"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day7.html#tasks",
    "href": "example_course/course-materials/eod-practice/eod-day7.html#tasks",
    "title": "Day 7: Tasks & activities",
    "section": "Tasks",
    "text": "Tasks\n\n1. Setup\nFirst, import pandas, matplotlib, and seaborn and load the three datasets.\nNext, display the first few rows and print out the dataset info to get an idea of the contents of each dataset.\nYou may have noticed that the zipcodes were read in as integers rather than strings, and therefore might not be 5 digits long. Ensure the zipcode or zip column in all datasets is a 5-character string, filling in any zeros that were dropped.\nCombine the 2012 and 2023 data together by adding a year column and then stacking them together.\nIn the combined plant hardiness dataframe, create two new columns, trange_min and trange_max, containing the min and max temperatures of the trange column. Remove the original trange column.\nHint: use str.split() to split the trange strings where they have spaces and retrieve the first and last components (min and max, respectively)"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day7.html#tasks-1",
    "href": "example_course/course-materials/eod-practice/eod-day7.html#tasks-1",
    "title": "Day 7: Tasks & activities",
    "section": "Tasks",
    "text": "Tasks\n\n2. Exploration and visualization\nOn average, how much has the minimum temperature in a zip code changed from 2012 to 2023?\nMerge together the combined plant hardiness dataset and the zipcode dataset by zipcode. This will give us more informtaion in the plant hardiness dataset, such as the latitude and longitude for each zipcode.\nCreate two scatter plot where the x axis is the longitude, the y axis is the latitude, the color is based on the minimum temperature in 2012 for one and 2023 for the other. Only look at longitude &lt; -60.\nNow create a single scatter plot where you look at the difference between the minimum temperature in 2012 and 2023. Only look at longitude &lt; -60. Color any zipcodes where you do not have information from both years in grey.\nCreate a bar plot showing the top 10 states where the average minimum temperature increased the most. Label your axes appropriately.\n\nEnd Activity Session (Day 7)"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day1-draft.html#welcome-to-your-data-science-future",
    "href": "example_course/course-materials/eod-practice/eod-day1-draft.html#welcome-to-your-data-science-future",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "🎯 Welcome to Your Data Science Future!",
    "text": "🎯 Welcome to Your Data Science Future!\nToday’s Mission: Get a sneak peek at where you’re headed! You’ll copy and run Python code to see what’s possible with data science. Think of this as a movie trailer for the skills you’ll build over the next week.\n\n🎬 “Coming Attractions” Approach\n\nYour job: Copy, paste, and run the code exactly as written\nOur job: Show you what’s happening (not how it works yet!)\nThe goal: Get excited about what you’ll learn and see the big picture\n\n\n\n\n\n\n\nDon’t Panic! 🚀\n\n\n\nYou’re not expected to understand every line of code today. By next Friday, you’ll know exactly how all of this works. For now, just enjoy the ride and see what’s possible!"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day1-draft.html#when-youll-master-these-skills",
    "href": "example_course/course-materials/eod-practice/eod-day1-draft.html#when-youll-master-these-skills",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "🗓️ When You’ll Master These Skills",
    "text": "🗓️ When You’ll Master These Skills\n\n\n\nWhat you’ll see today\nWhen you’ll learn it\nWhat we’ll cover\n\n\n\n\nimport pandas as pd\nDay 3-4\nData structures and DataFrames\n\n\npd.read_csv()\nDay 4\nLoading data from files\n\n\ndf.head(), df.info()\nDay 4\nData exploration methods\n\n\ndf.groupby()\nDay 6\nData aggregation and grouping\n\n\nplt.plot(), plt.bar()\nDay 7\nData visualization"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day1-draft.html#background-and-data-source",
    "href": "example_course/course-materials/eod-practice/eod-day1-draft.html#background-and-data-source",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "Background and Data Source",
    "text": "Background and Data Source\nOur data comes from the Arctic Long Term Ecological Research station. The Arctic Long Term Ecological Research (ARC LTER) site is part of a network of sites established by the National Science Foundation to support long-term ecological research in the United States. The research site is located in the foothills region of the Brooks Range, North Slope of Alaska (68° 38’N, 149° 36.4’W, elevation 720 m).\nThe Arctic LTER project’s goal is to understand and predict the effects of environmental change on arctic landscapes, both natural and anthropogenic. Researchers at the site use long-term monitoring and surveys of natural variation of ecosystem characteristics, experimental manipulation of ecosystems (years to decades) and modeling at ecosystem and watershed scales to gain an understanding of the controls of ecosystem structure and function. The data and insights gained are provided to federal, Alaska state and North Slope Borough officials who regulate the lands on the North Slope and through this web site.\nWe will be using some basic weather data downloaded from Toolik Station:\n\nToolik Station Meteorological Data: toolik_weather.csv Shaver, G. 2019. A multi-year DAILY weather file for the Toolik Field Station at Toolik Lake, AK starting 1988 to present. ver 4. Environmental Data Initiative. https://doi.org/10.6073/pasta/ce0f300cdf87ec002909012abefd9c5c (Accessed 2021-08-08).\n\nI have already downloaded this data and placed in our course repository, where we can access it easily using its GitHub raw URL.\nLet’s dive into your preview of data science with Python!"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day1-draft.html#instructions",
    "href": "example_course/course-materials/eod-practice/eod-day1-draft.html#instructions",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "Instructions",
    "text": "Instructions\n\n🚀 Step 1: Import the Magic Tools\nFirst, we’ll import the libraries that make data science possible in Python.\n🎬 Copy and paste this code:\nimport pandas as pd\nimport matplotlib.pyplot as plt\n🎭 What just happened? We imported two powerful libraries! pandas is like Excel but supercharged for data analysis, and matplotlib creates beautiful(ish) plots. 🎓 Coming up: You’ll learn about Python imports and libraries on Days 2-3.\n\n\n\n🚀 Step 2: Load Real Climate Data\nNow we’ll load 15,000+ rows of Arctic weather data in just one line!\nOur data is located at: https://raw.githubusercontent.com/environmental-data-science/eds217-day0-comp/main/data/raw_data/toolik_weather.csv\n🎬 Copy and paste this code:\nurl = 'https://raw.githubusercontent.com/environmental-data-science/eds217-day0-comp/main/data/raw_data/toolik_weather.csv'\ndf = pd.read_csv(url)\n🎭 What just happened? We loaded over 15,000 rows of climate data from the internet in one line! The data is now stored in a “DataFrame” called df. 🎓 Coming up: Day 4 will teach you all about loading and working with data files.\n\n\n\n\n\n\nR vs Python: Data Loading\n\n\n\nThis is just like df &lt;- read.csv(url) in R! Both pandas DataFrames and R data.frames are tabular data structures. The main syntax difference is Python’s dot notation: pd.read_csv() vs R’s read.csv(). Both can read directly from URLs, which is incredibly convenient for reproducible research!\n\n\n\n\n\n🚀 Step 3: Peek at the Data\nLet’s see what our Arctic climate data looks like!\n🎬 Copy and paste this code:\ndf.head()\n🎭 What just happened? We previewed the first 5 rows of our 15,000+ row dataset! You can see daily weather measurements from Alaska. 🎓 Coming up: Day 4 morning will teach you data exploration methods like this.\n\n\n\n\n\n\nR vs Python: Data Exploration\n\n\n\nThis is exactly like head(df) in R! The key difference is Python’s object-oriented approach: df.head() vs R’s functional approach head(df). Both show you the first few rows, but Python treats the DataFrame as an object that has methods (like .head()) built into it.\n\n\n\n\n\n🚀 Step 4: Check Data Quality\nReal data science involves checking for missing or problematic data.\n🎬 Copy and paste this code:\ndf.isnull().sum()\n🎭 What just happened? We checked every column for missing data! Looks like our temperature data is complete (0 missing values), which is great. 🎓 Coming up: Day 5 will teach you all about data cleaning and handling missing values.\n\n\n\n\n\n\nR vs Python: Missing Data Check\n\n\n\nIn R, you’d use sum(is.na(df)) to count missing values. Python uses df.isnull().sum() - notice the chaining of methods! This reads left-to-right: “take the DataFrame, check for null values, then sum them up.” Both approaches give you the count of missing values per column.\n\n\n\n\n\n🚀 Step 5: Get Data Summary Statistics\nLet’s get a statistical overview of our climate data.\n🎬 Copy and paste this code:\ndf.describe()\ndf.info()\n🎭 What just happened? We got instant statistics and information about our entire dataset! You can see temperature ranges, averages, and data types. 🎓 Coming up: Day 4 will teach you how to explore and understand your datasets.\n\n\n\n\n\n\nR vs Python: Data Summary\n\n\n\nThese are like summary(df) and str(df) in R. Python’s .describe() gives you the statistical summary (like summary()) while .info() shows the structure (like str()). Notice how Python uses dot notation - the DataFrame object has these methods built in, whereas R uses separate functions that take the data frame as input.\n\n\n\n\n\n🚀 Step 6: Calculate Monthly Averages\nNow for some real data analysis - let’s find average temperatures by month!\n🎬 Copy and paste this code:\nmonthly = df.groupby('Month')\nmonthly_means = monthly['Daily_AirTemp_Mean_C'].mean()\n🎭 What just happened? We grouped 15,000+ daily temperature readings by month and calculated averages! This turned years of daily data into 12 monthly summaries. 🎓 Coming up: Day 6 will teach you all about grouping and aggregating data like this.\n\n\n\n\n\n\nR vs Python: Data Grouping\n\n\n\nThis is exactly like using df %&gt;% group_by(Month) %&gt;% summarize(mean_temp = mean(Daily_AirTemp_Mean_C)) in dplyr! Both approaches group data and calculate statistics. Python’s syntax is df.groupby('column')['target_column'].function(), while R uses the pipe operator %&gt;% to chain operations. Both are powerful for data aggregation!\n\n\n\n\n\n🚀 Step 7: Create Your First Visualization\nTime to turn numbers into pictures! Let’s plot the monthly temperature patterns.\n🎬 Copy and paste this code:\nplt.plot(monthly_means)\nNow let’s make it even better with labels:\nmonths = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nplt.bar(months, monthly_means)\n🎭 What just happened? You created professional data visualizations! The bar chart clearly shows Alaska’s extreme seasonal temperature differences. 🎓 Coming up: Day 7 will teach you how to create amazing visualizations and customize them.\n\n\n\n\n\n\nR vs Python: Data Visualization\n\n\n\nThis is like creating plots with ggplot(df, aes(x=Month, y=temp)) + geom_bar() in R! Python’s matplotlib uses a more direct approach: plt.plot() and plt.bar() create plots immediately. Both are powerful - ggplot2 uses a “grammar of graphics” approach while matplotlib is more imperative. You’ll learn both have their strengths!\n\n\n\n\n\n🚀 Step 8: Analyze Climate Trends Over Time\nLet’s explore how temperatures have changed over the decades!\n🎬 Copy and paste this code:\nyear = df.groupby('Year')\nyearly_means = year['Daily_AirTemp_Mean_C'].mean()\nplt.plot(yearly_means)\nAnd as a bar chart:\nyear_list = df['Year'].unique()\nplt.bar(year_list, yearly_means)\n🎭 What just happened? You analyzed climate trends across multiple decades! You can see how Arctic temperatures have varied over time - real climate science! 🎓 Coming up: This combines Day 6 skills (grouping data) with Day 7 skills (visualization).\n\n\n\n\n\n\nR vs Python: Time Series Analysis\n\n\n\nThis is just like grouping by year in R and plotting the results! Whether you use df %&gt;% group_by(Year) %&gt;% summarize() in R or df.groupby('Year').mean() in Python, you’re doing the same analytical thinking. The syntax differs, but the data science concepts are identical.\n\n\n\n\n\n🚀 Step 9: Save Your Work\nData scientists always save their analyses for future use.\n🎬 Copy and paste this code:\nmonthly_means.to_csv(\"monthly_means.csv\", header=True)\n🎭 What just happened? You saved your analysis results to a file that you (or other scientists) can use later! This is how research becomes reproducible. 🎓 Coming up: Day 4 will teach you all about importing, exporting, and managing data files.\n\n\n\n\n\n\nR vs Python: Data Export\n\n\n\nThis is just like write.csv(monthly_means, \"monthly_means.csv\") in R! Python uses the object-oriented approach where the data (your Series monthly_means) has a method .to_csv() built into it. R uses a function that takes the data as input. Both create the exact same CSV file - just different syntax approaches to the same goal."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day1-draft.html#congratulations-youre-a-data-scientist",
    "href": "example_course/course-materials/eod-practice/eod-day1-draft.html#congratulations-youre-a-data-scientist",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "🎉 Congratulations! You’re a Data Scientist!",
    "text": "🎉 Congratulations! You’re a Data Scientist!\nYou just completed a real data science workflow: - ✅ Loaded real climate data from the internet - ✅ Explored and understood your dataset\n- ✅ Calculated meaningful statistics - ✅ Created professional visualizations - ✅ Analyzed climate trends over time - ✅ Saved your results for future use\n\n🧠 What You’ve Seen Today\nThis preview showed you the power and simplicity of Python for data science. In just a few lines of code, you: - Processed 15,000+ rows of climate data - Created multiple visualizations - Performed real climate analysis - Saved reproducible results\n\n\n🚀 What’s Next\nOver the next week, you’ll learn exactly how each line works: - Days 2-3: Python fundamentals (variables, data types, control flows) - the building blocks - Day 4: DataFrames and data loading (like today’s pd.read_csv()) - working with structured data - Day 5: Data cleaning and manipulation - making messy data analysis-ready - Day 6: Grouping and aggregation (like today’s groupby()) - summarizing and analyzing patterns - Day 7: Data visualization (like today’s plots) - creating publication-quality graphics\nBy Friday, you’ll understand every single line of code you ran today - and you’ll be able to write it all yourself!\n\n\n🔄 R Skills Transfer\nYour R experience gives you a huge advantage! You already understand: - Data structures (data.frames ↔︎ DataFrames) - Statistical thinking (same concepts, different syntax) - Data workflows (load → explore → analyze → visualize) - Reproducible research (scripts and documentation)\nPython will feel familiar because the thinking is the same - you’re just learning new syntax for concepts you already know!"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day1-draft.html#reflection-questions",
    "href": "example_course/course-materials/eod-practice/eod-day1-draft.html#reflection-questions",
    "title": "Day 1: Tasks & activities (DRAFT)",
    "section": "🤔 Reflection Questions",
    "text": "🤔 Reflection Questions\nTake a moment to think about: 1. What surprised you most about what Python can do with data? 2. Which part of the analysis was most interesting to you? 3. What questions do you have about how this all works? 4. How might you use these skills in your future research or work?\nAdd your thoughts in a new markdown cell below!\n\n\n\n\n\n\n\nRemember\n\n\n\nToday was about inspiration and seeing possibilities. Don’t worry if you don’t understand everything yet - that’s exactly what the rest of the course is for! You’re going to do amazing things with data science. 🚀\n\n\n\nEnd Activity Session (Day 1) - Welcome to Your Data Science Journey!"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3b_control_flows.html",
    "href": "example_course/course-materials/coding-colabs/3b_control_flows.html",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "",
    "text": "Here’s a quick reference for the basic control flow structures we’ll be using:\n# If statement\nif condition:\n    # code to run if condition is True\n\n# For loop\nfor item in sequence:\n    # code to run for each item\n\n# While loop\nwhile condition:\n    # code to run while condition is True\nHere’s our course cheatsheet on control flows:\n\nControl Flows Cheatsheet\n\nFeel free to refer to this cheatsheet throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3b_control_flows.html#quick-reference",
    "href": "example_course/course-materials/coding-colabs/3b_control_flows.html#quick-reference",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "",
    "text": "Here’s a quick reference for the basic control flow structures we’ll be using:\n# If statement\nif condition:\n    # code to run if condition is True\n\n# For loop\nfor item in sequence:\n    # code to run for each item\n\n# While loop\nwhile condition:\n    # code to run while condition is True\nHere’s our course cheatsheet on control flows:\n\nControl Flows Cheatsheet\n\nFeel free to refer to this cheatsheet throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3b_control_flows.html#exercise-overview",
    "href": "example_course/course-materials/coding-colabs/3b_control_flows.html#exercise-overview",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Exercise Overview",
    "text": "Exercise Overview\nIn this Coding Colab, you’ll practice using basic control flow structures in Python. Work through these simple tasks with your partner, discussing your approach as you go."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3b_control_flows.html#part-1-if-statements-20-minutes",
    "href": "example_course/course-materials/coding-colabs/3b_control_flows.html#part-1-if-statements-20-minutes",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Part 1: If Statements (20 minutes)",
    "text": "Part 1: If Statements (20 minutes)\n\nTask 1: Simple Weather Advice\nWrite a program that gives weather advice based on temperature:\n\nSet a variable temperature = 20\nUse an if-else statement to print advice:\n\nIf temperature is above 25, print “It’s a hot day, stay hydrated!”\nOtherwise, print “Enjoy the pleasant weather!”\n\n\n\n\nCode\ntemperature = 20\n\n# Your code here\n# Use an if-else statement to print weather advice\n\n\n\n\nTask 2: Grade Classifier\nCreate a program that assigns a letter grade based on a numerical score:\n\nSet a variable score = 85\nUse if-elif-else statements to print the letter grade:\n\n90 or above: “A”\n80-89: “B”\n70-79: “C”\n60-69: “D”\nBelow 60: “F”\n\n\n\n\nCode\nscore = 85\n\n# Your code here\n# Use if-elif-else statements to print the letter grade"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3b_control_flows.html#part-2-for-loops-20-minutes",
    "href": "example_course/course-materials/coding-colabs/3b_control_flows.html#part-2-for-loops-20-minutes",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Part 2: For Loops (20 minutes)",
    "text": "Part 2: For Loops (20 minutes)\n\nTask 3: Counting Sheep\nWrite a program that counts from 1 to 5, printing “sheep” after each number:\n\nUse a for loop with the range() function\nPrint each number followed by “sheep”\n\n\n\nCode\n# Your code here\n# Use a for loop to count sheep\n\n\n\n\nTask 4: Sum of Numbers\nCalculate the sum of numbers from 1 to 10:\n\nCreate a variable total = 0\nUse a for loop with the range() function to add each number to total\nAfter the loop, print the total\n\n\n\nCode\ntotal = 0\n\n# Your code here\n# Use a for loop to calculate the sum"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3b_control_flows.html#part-3-while-loops-15-minutes",
    "href": "example_course/course-materials/coding-colabs/3b_control_flows.html#part-3-while-loops-15-minutes",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Part 3: While Loops (15 minutes)",
    "text": "Part 3: While Loops (15 minutes)\n\nTask 5: Countdown\nCreate a simple countdown program:\n\nSet a variable countdown = 5\nUse a while loop to print the countdown from 5 to 1\nAfter each print, decrease the countdown by 1\nWhen the countdown reaches 0, print “Blast off!”\n\n\n\nCode\ncountdown = 5\n\n# Your code here\n# Use a while loop for the countdown"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3b_control_flows.html#conclusion-and-discussion-5-minutes",
    "href": "example_course/course-materials/coding-colabs/3b_control_flows.html#conclusion-and-discussion-5-minutes",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Conclusion and Discussion (5 minutes)",
    "text": "Conclusion and Discussion (5 minutes)\nWith your partner, briefly discuss:\n\nWhich control structure (if, for, or while) did you find easiest to use?\nCan you think of a real-life situation where you might use each of these control structures?\n\nRemember, it’s okay if you don’t finish all tasks. The goal is to practice and understand these concepts. Good luck and enjoy coding together!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#getting-started",
    "title": "Interactive Session 3C",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#introduction",
    "href": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#introduction",
    "title": "Interactive Session 3C",
    "section": "Introduction",
    "text": "Introduction\nIn this interactive session, we’ll explore the most essential aspects of NumPy arrays and Pandas Series. These fundamental data structures are crucial for data manipulation and analysis in Python. We’ll focus on the key concepts that are most relevant for beginning data scientists. We’re also going to assume that you will primarily work with Pandas DataFrames and Series, so we won’t spend too much time on the details of NumPy arrays.\nLet’s start by importing the necessary libraries:\n\n\nCode\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#numpy-arrays-the-foundation",
    "href": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#numpy-arrays-the-foundation",
    "title": "Interactive Session 3C",
    "section": "NumPy Arrays: The Foundation",
    "text": "NumPy Arrays: The Foundation\nNumPy arrays are the building blocks for many data structures in Python, including Pandas Series and DataFrames. Let’s explore their basic properties and operations.\n\nCreating NumPy Arrays\n\n\nCode\n# Create a 1D array\narr_1d = np.array([1, 2, 3, 4, 5])\nprint(\"1D array:\", arr_1d)\n\n# Create a 2D array\narr_2d = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"2D array:\\n\", arr_2d)\n\n\n1D array: [1 2 3 4 5]\n2D array:\n [[1 2 3]\n [4 5 6]]\n\n\n\n\nBasic Array Operations\n\n\nCode\n# Array arithmetic\nprint(\"Array + 2:\", arr_1d + 2)\nprint(\"Array * 2:\", arr_1d * 2)\n\n# Array statistics\nprint(\"Mean:\", np.mean(arr_1d))\nprint(\"Sum:\", np.sum(arr_1d))\n\n\nArray + 2: [3 4 5 6 7]\nArray * 2: [ 2  4  6  8 10]\nMean: 3.0\nSum: 15\n\n\nNow it’s your turn! Create a NumPy array of your favorite numbers and calculate its standard deviation (np.std())."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#pandas-series-building-on-numpy",
    "href": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#pandas-series-building-on-numpy",
    "title": "Interactive Session 3C",
    "section": "Pandas Series: Building on NumPy",
    "text": "Pandas Series: Building on NumPy\nPandas Series are one-dimensional labeled arrays built on top of NumPy arrays. They’re like a column in a spreadsheet or a single column of a DataFrame.\n\nCreating Pandas Series\n\n\nCode\n# Create a Series from a list\ns1 = pd.Series([1, 2, 3, 4, 5])\nprint(\"Series from list:\\n\", s1)\n\n# Create a Series with custom index\ns2 = pd.Series([10, 20, 30, 40, 50], index=['a', 'b', 'c', 'd', 'e'])\nprint(\"\\nSeries with custom index:\\n\", s2)\n\n\nSeries from list:\n 0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\nSeries with custom index:\n a    10\nb    20\nc    30\nd    40\ne    50\ndtype: int64\n\n\n\n\nBasic Series Operations\n\n\nCode\n# Accessing elements\nprint(\"Element at index 'c':\", s2['c'])\nprint(\"First three elements:\\n\", s2[:3])\n\n# Series arithmetic\nprint(\"\\nSeries + 5:\\n\", s2 + 5)\n\n# Series statistics\nprint(\"\\nMean:\", s2.mean())\nprint(\"Median:\", s2.median())\n\n\nElement at index 'c': 30\nFirst three elements:\n a    10\nb    20\nc    30\ndtype: int64\n\nSeries + 5:\n a    15\nb    25\nc    35\nd    45\ne    55\ndtype: int64\n\nMean: 30.0\nMedian: 30.0\n\n\nYour turn! Create a Pandas Series representing daily temperatures for a week. Use the days of the week as the index. Then, calculate and print the maximum temperature."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#comparing-numpy-arrays-and-pandas-series",
    "href": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#comparing-numpy-arrays-and-pandas-series",
    "title": "Interactive Session 3C",
    "section": "Comparing NumPy Arrays and Pandas Series",
    "text": "Comparing NumPy Arrays and Pandas Series\nLet’s explore some key differences and similarities between NumPy arrays and Pandas Series.\n\n\nCode\n# Create a NumPy array and a Pandas Series with the same data\nnp_arr = np.array([1, 2, 3, 4, 5])\npd_series = pd.Series([1, 2, 3, 4, 5])\n\nprint(\"NumPy array:\", np_arr)\nprint(\"Pandas Series:\\n\", pd_series)\n\n# Demonstrate label-based indexing in Pandas Series\npd_series.index = ['a', 'b', 'c', 'd', 'e']\nprint(\"\\nPandas Series with custom index:\\n\", pd_series)\nprint(\"Value at index 'c':\", pd_series['c'])\n\n# Show that NumPy operations work on Pandas Series\nprint(\"\\nSquare root of Pandas Series:\\n\", np.sqrt(pd_series))\n\n\nNumPy array: [1 2 3 4 5]\nPandas Series:\n 0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\nPandas Series with custom index:\n a    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64\nValue at index 'c': 3\n\nSquare root of Pandas Series:\n a    1.000000\nb    1.414214\nc    1.732051\nd    2.000000\ne    2.236068\ndtype: float64\n\n\nNow it’s your turn! Create a NumPy array and a Pandas Series, both containing the same data (use any numbers you like). Then, calculate the mean of both and compare the results. Are they the same? Why or why not?"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#conclusion",
    "href": "example_course/course-materials/interactive-sessions/3c_arrays_and_series.html#conclusion",
    "title": "Interactive Session 3C",
    "section": "Conclusion",
    "text": "Conclusion\nIn this session, we’ve covered the essential aspects of NumPy arrays and Pandas Series. Remember:\n\nNumPy arrays are the foundation for numerical computing in Python.\nPandas Series build on NumPy arrays, adding labeled axes and additional functionality.\nBoth support fast operations on entire datasets without explicit loops.\nPandas Series are particularly useful for labeled data and seamlessly integrate with DataFrames.\n\nAs you continue your journey in data science, you’ll find these structures invaluable for efficient data manipulation and analysis.\n\nEnd interactive session 3C"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/control_flows.html",
    "href": "example_course/course-materials/cheatsheets/control_flows.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nx = 10\nif x &gt; 0:\n    print(\"Positive\")\nelif x &lt; 0:\n    print(\"Negative\")\nelse:\n    print(\"Zero\")\n\n\nPositive"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/control_flows.html#conditional-statements",
    "href": "example_course/course-materials/cheatsheets/control_flows.html#conditional-statements",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nx = 10\nif x &gt; 0:\n    print(\"Positive\")\nelif x &lt; 0:\n    print(\"Negative\")\nelse:\n    print(\"Zero\")\n\n\nPositive"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/control_flows.html#loops",
    "href": "example_course/course-materials/cheatsheets/control_flows.html#loops",
    "title": "EDS 217 Cheatsheet",
    "section": "Loops",
    "text": "Loops\n\nfor loop\n\n\nCode\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor fruit in fruits:\n    print(fruit)\n\n\napple\nbanana\ncherry\n\n\n\n\nwhile loop\n\n\nCode\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1\n\n\n0\n1\n2\n3\n4"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/control_flows.html#loop-control",
    "href": "example_course/course-materials/cheatsheets/control_flows.html#loop-control",
    "title": "EDS 217 Cheatsheet",
    "section": "Loop Control",
    "text": "Loop Control\n\nbreak\n\n\nCode\nfor i in range(10):\n    if i == 5:\n        break\n    print(i)\n\n\n0\n1\n2\n3\n4\n\n\n\n\ncontinue\n\n\nCode\nfor i in range(5):\n    if i == 2:\n        continue\n    print(i)\n\n\n0\n1\n3\n4"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/control_flows.html#comprehensions",
    "href": "example_course/course-materials/cheatsheets/control_flows.html#comprehensions",
    "title": "EDS 217 Cheatsheet",
    "section": "Comprehensions",
    "text": "Comprehensions\n\nList Comprehension\n\n\nCode\nsquares = [x**2 for x in range(5)]\nprint(squares)\n\n\n[0, 1, 4, 9, 16]\n\n\n\n\nDictionary Comprehension\n\n\nCode\nsquares_dict = {x: x**2 for x in range(5)}\nprint(squares_dict)\n\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/control_flows.html#exception-handling",
    "href": "example_course/course-materials/cheatsheets/control_flows.html#exception-handling",
    "title": "EDS 217 Cheatsheet",
    "section": "Exception Handling",
    "text": "Exception Handling\n\ntry-except\n\n\nCode\ntry:\n    result = 10 / 0\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\n\n\nCannot divide by zero\n\n\n\n\ntry-except-else-finally\n\n\nCode\ntry:\n    x = 5\n    result = 10 / x\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\nelse:\n    print(f\"Result: {result}\")\nfinally:\n    print(\"Execution completed\")\n\n\nResult: 2.0\nExecution completed\n\n\nFor more detailed information on Python control flows, refer to the Python documentation on Control Flow Tools."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/sets.html",
    "href": "example_course/course-materials/cheatsheets/sets.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\n# Empty set\nempty_set = set()\nprint(f\"Empty set: {empty_set}\")\n\n# Set from a list\nset_from_list = set([1, 2, 3, 4, 5])\nprint(f\"Set from list: {set_from_list}\")\n\n# Set literal\nset_literal = {1, 2, 3, 4, 5}\nprint(f\"Set literal: {set_literal}\")\n\n\nEmpty set: set()\nSet from list: {1, 2, 3, 4, 5}\nSet literal: {1, 2, 3, 4, 5}"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/sets.html#creating-sets",
    "href": "example_course/course-materials/cheatsheets/sets.html#creating-sets",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\n# Empty set\nempty_set = set()\nprint(f\"Empty set: {empty_set}\")\n\n# Set from a list\nset_from_list = set([1, 2, 3, 4, 5])\nprint(f\"Set from list: {set_from_list}\")\n\n# Set literal\nset_literal = {1, 2, 3, 4, 5}\nprint(f\"Set literal: {set_literal}\")\n\n\nEmpty set: set()\nSet from list: {1, 2, 3, 4, 5}\nSet literal: {1, 2, 3, 4, 5}"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/sets.html#basic-operations",
    "href": "example_course/course-materials/cheatsheets/sets.html#basic-operations",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Operations",
    "text": "Basic Operations\n\n\nCode\ns = {1, 2, 3, 4, 5}\nprint(f\"Initial set: {s}\")\n\n# Add an element\ns.add(6)\nprint(f\"After adding 6: {s}\")\n\n# Remove an element\ns.remove(3)  # Raises KeyError if not found\nprint(f\"After removing 3: {s}\")\n\ns.discard(10)  # Doesn't raise error if not found\nprint(f\"After discarding 10 (not in set): {s}\")\n\n# Pop a random element\npopped = s.pop()\nprint(f\"Popped element: {popped}\")\nprint(f\"Set after pop: {s}\")\n\n# Check membership\nprint(f\"Is 2 in the set? {2 in s}\")\n\n# Clear the set\ns.clear()\nprint(f\"Set after clear: {s}\")\n\n\nInitial set: {1, 2, 3, 4, 5}\nAfter adding 6: {1, 2, 3, 4, 5, 6}\nAfter removing 3: {1, 2, 4, 5, 6}\nAfter discarding 10 (not in set): {1, 2, 4, 5, 6}\nPopped element: 1\nSet after pop: {2, 4, 5, 6}\nIs 2 in the set? True\nSet after clear: set()"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/sets.html#set-methods",
    "href": "example_course/course-materials/cheatsheets/sets.html#set-methods",
    "title": "EDS 217 Cheatsheet",
    "section": "Set Methods",
    "text": "Set Methods\n\n\nCode\na = {1, 2, 3}\nb = {3, 4, 5}\nprint(f\"Set a: {a}\")\nprint(f\"Set b: {b}\")\n\n\nSet a: {1, 2, 3}\nSet b: {3, 4, 5}\n\n\n\nUnion\n\n\nCode\nunion_set = a.union(b)\nprint(f\"Union of a and b: {union_set}\")\n\n\nUnion of a and b: {1, 2, 3, 4, 5}\n\n\n\n\nIntersection\n\n\nCode\nintersection_set = a.intersection(b)\nprint(f\"Intersection of a and b: {intersection_set}\")\n\n\nIntersection of a and b: {3}\n\n\n\n\nDifference\n\n\nCode\ndifference_set = a.difference(b)\nprint(f\"Difference of a and b: {difference_set}\")\n\n\nDifference of a and b: {1, 2}\n\n\n\n\nSymmetric difference\n\n\nCode\nsymmetric_difference_set = a.symmetric_difference(b)\nprint(f\"Symmetric difference of a and b: {symmetric_difference_set}\")\n\n\nSymmetric difference of a and b: {1, 2, 4, 5}\n\n\n\n\nSubset and superset\n\n\nCode\nis_subset = a.issubset(b)\nis_superset = a.issuperset(b)\nprint(f\"Is a a subset of b? {is_subset}\")\nprint(f\"Is a a superset of b? {is_superset}\")\n\n\nIs a a subset of b? False\nIs a a superset of b? False"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/5c_cleaning_data.html",
    "href": "example_course/course-materials/coding-colabs/5c_cleaning_data.html",
    "title": "Day 5: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, you will work together and apply your new data cleaning skills to a simple dataframe that has a suprising number of problems."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/5c_cleaning_data.html#introduction",
    "href": "example_course/course-materials/coding-colabs/5c_cleaning_data.html#introduction",
    "title": "Day 5: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, you will work together and apply your new data cleaning skills to a simple dataframe that has a suprising number of problems."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/5c_cleaning_data.html#resources",
    "href": "example_course/course-materials/coding-colabs/5c_cleaning_data.html#resources",
    "title": "Day 5: 🙌 Coding Colab",
    "section": "Resources",
    "text": "Resources\nHere’s our course cheatsheet on cleaning data:\n\nPandas Cleaning Cheatsheet\n\nFeel free to refer to this cheatsheet throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/5c_cleaning_data.html#setup",
    "href": "example_course/course-materials/coding-colabs/5c_cleaning_data.html#setup",
    "title": "Day 5: 🙌 Coding Colab",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries and load an example messy dataframe.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\nurl = 'https://bit.ly/messy_csv'\nmessy_df = pd.read_csv(url)"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/5c_cleaning_data.html#practical-exercise-cleaning-a-messy-environmental-dataset",
    "href": "example_course/course-materials/coding-colabs/5c_cleaning_data.html#practical-exercise-cleaning-a-messy-environmental-dataset",
    "title": "Day 5: 🙌 Coding Colab",
    "section": "Practical Exercise: Cleaning a Messy Environmental Dataset",
    "text": "Practical Exercise: Cleaning a Messy Environmental Dataset\nLet’s apply what we’ve learned so far to clean the messy environmental dataset.\nYour task is to clean this dataframe by\n\nRemoving duplicates\nHandling missing values (either fill or dropna to remove rows with missing data)\nEnsuring consistent data types (dates, strings)\nFormatting the ‘site’ column for consistency\nMaking sure all column names are lower case, without whitespace.\n\nTry to implement these steps using the techniques we’ve learned.\n\nEnd Coding Colab Session (Day 4)"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html",
    "href": "course-materials/cheatsheets/rasterio_basics.html",
    "title": "Working with Rasterio",
    "section": "",
    "text": "Rasterio is the essential Python library for reading and writing geospatial raster data.\n\n\nCode\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\n\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#introduction-to-rasterio",
    "href": "course-materials/cheatsheets/rasterio_basics.html#introduction-to-rasterio",
    "title": "Working with Rasterio",
    "section": "",
    "text": "Rasterio is the essential Python library for reading and writing geospatial raster data.\n\n\nCode\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\n\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#creating-sample-data",
    "href": "course-materials/cheatsheets/rasterio_basics.html#creating-sample-data",
    "title": "Working with Rasterio",
    "section": "Creating Sample Data",
    "text": "Creating Sample Data\nSince we may not have actual satellite imagery files, let’s create some sample raster data:\n\n\nCode\n# Create sample raster data\ndef create_sample_raster():\n    # Create a simple 100x100 raster with some pattern\n    height, width = 100, 100\n    \n    # Create coordinate grids\n    x = np.linspace(-2, 2, width)\n    y = np.linspace(-2, 2, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Create a pattern (e.g., circular pattern)\n    data = np.sin(np.sqrt(X**2 + Y**2)) * 255\n    data = data.astype(np.uint8)\n    \n    return data\n\nsample_data = create_sample_raster()\nprint(f\"Sample data shape: {sample_data.shape}\")\nprint(f\"Data type: {sample_data.dtype}\")\nprint(f\"Data range: {sample_data.min()} to {sample_data.max()}\")\n\n\nSample data shape: (100, 100)\nData type: uint8\nData range: 7 to 254"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#working-with-raster-arrays",
    "href": "course-materials/cheatsheets/rasterio_basics.html#working-with-raster-arrays",
    "title": "Working with Rasterio",
    "section": "Working with Raster Arrays",
    "text": "Working with Raster Arrays\n\nBasic array operations\n\n\nCode\n# Basic statistics\nprint(f\"Mean: {np.mean(sample_data):.2f}\")\nprint(f\"Standard deviation: {np.std(sample_data):.2f}\")\nprint(f\"Unique values: {len(np.unique(sample_data))}\")\n\n# Histogram of values\nplt.figure(figsize=(8, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Sample Raster Data')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.hist(sample_data.flatten(), bins=50, alpha=0.7)\nplt.title('Histogram of Values')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\nMean: 215.01\nStandard deviation: 46.13\nUnique values: 182\n\n\n\n\n\n\n\n\n\n\n\nArray masking and filtering\n\n\nCode\n# Create masks\nhigh_values = sample_data &gt; 200\nlow_values = sample_data &lt; 50\n\nprint(f\"Pixels with high values (&gt;200): {np.sum(high_values)}\")\nprint(f\"Pixels with low values (&lt;50): {np.sum(low_values)}\")\n\n# Apply mask\nmasked_data = np.where(high_values, sample_data, 0)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Original Data')\n\nplt.subplot(1, 3, 2)\nplt.imshow(high_values, cmap='gray')\nplt.title('High Value Mask')\n\nplt.subplot(1, 3, 3)\nplt.imshow(masked_data, cmap='viridis')\nplt.title('Masked Data')\n\nplt.tight_layout()\nplt.show()\n\n\nPixels with high values (&gt;200): 7364\nPixels with low values (&lt;50): 76"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#raster-metadata-and-transforms",
    "href": "course-materials/cheatsheets/rasterio_basics.html#raster-metadata-and-transforms",
    "title": "Working with Rasterio",
    "section": "Raster Metadata and Transforms",
    "text": "Raster Metadata and Transforms\n\nUnderstanding geospatial transforms\n\n\nCode\nfrom rasterio.transform import from_bounds\n\n# Create a sample transform (geographic coordinates)\nwest, south, east, north = -120.0, 35.0, -115.0, 40.0  # Bounding box\ntransform = from_bounds(west, south, east, north, 100, 100)\n\nprint(f\"Transform: {transform}\")\nprint(f\"Pixel width: {transform[0]}\")\nprint(f\"Pixel height: {abs(transform[4])}\")\n\n# Convert pixel coordinates to geographic coordinates\ndef pixel_to_geo(row, col, transform):\n    x, y = rasterio.transform.xy(transform, row, col)\n    return x, y\n\n# Example: center pixel\ncenter_row, center_col = 50, 50\ngeo_x, geo_y = pixel_to_geo(center_row, center_col, transform)\nprint(f\"Center pixel ({center_row}, {center_col}) -&gt; Geographic: ({geo_x:.2f}, {geo_y:.2f})\")\n\n\nTransform: | 0.05, 0.00,-120.00|\n| 0.00,-0.05, 40.00|\n| 0.00, 0.00, 1.00|\nPixel width: 0.05\nPixel height: 0.05\nCenter pixel (50, 50) -&gt; Geographic: (-117.47, 37.48)\n\n\n\n\nCreating profiles for writing rasters\n\n\nCode\n# Create a profile for writing raster data\nprofile = {\n    'driver': 'GTiff',\n    'dtype': 'uint8',\n    'nodata': None,\n    'width': 100,\n    'height': 100,\n    'count': 1,\n    'crs': 'EPSG:4326',  # WGS84 lat/lon\n    'transform': transform\n}\n\nprint(\"Raster profile:\")\nfor key, value in profile.items():\n    print(f\"  {key}: {value}\")\n\n\nRaster profile:\n  driver: GTiff\n  dtype: uint8\n  nodata: None\n  width: 100\n  height: 100\n  count: 1\n  crs: EPSG:4326\n  transform: | 0.05, 0.00,-120.00|\n| 0.00,-0.05, 40.00|\n| 0.00, 0.00, 1.00|"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#multi-band-operations",
    "href": "course-materials/cheatsheets/rasterio_basics.html#multi-band-operations",
    "title": "Working with Rasterio",
    "section": "Multi-band Operations",
    "text": "Multi-band Operations\n\nWorking with multi-band data\n\n\nCode\n# Create multi-band sample data (RGB-like)\ndef create_multiband_sample():\n    height, width = 100, 100\n    bands = 3\n    \n    # Create different patterns for each band\n    x = np.linspace(-2, 2, width)\n    y = np.linspace(-2, 2, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Band 1: Circular pattern\n    band1 = (np.sin(np.sqrt(X**2 + Y**2)) * 127 + 127).astype(np.uint8)\n    \n    # Band 2: Linear gradient\n    band2 = (np.linspace(0, 255, width) * np.ones((height, 1))).astype(np.uint8)\n    \n    # Band 3: Checkerboard pattern\n    band3 = ((X + Y) &gt; 0).astype(np.uint8) * 255\n    \n    return np.stack([band1, band2, band3])\n\nmultiband_data = create_multiband_sample()\nprint(f\"Multiband shape: {multiband_data.shape}\")\nprint(f\"Shape format: (bands, height, width)\")\n\n\nMultiband shape: (3, 100, 100)\nShape format: (bands, height, width)\n\n\n\n\nVisualizing multi-band data\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\n# Individual bands\nfor i in range(3):\n    row, col = i // 2, i % 2\n    axes[row, col].imshow(multiband_data[i], cmap='gray')\n    axes[row, col].set_title(f'Band {i+1}')\n\n# RGB composite (transpose for matplotlib)\nrgb_composite = np.transpose(multiband_data, (1, 2, 0))\naxes[1, 1].imshow(rgb_composite)\naxes[1, 1].set_title('RGB Composite')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#band-math-and-indices",
    "href": "course-materials/cheatsheets/rasterio_basics.html#band-math-and-indices",
    "title": "Working with Rasterio",
    "section": "Band Math and Indices",
    "text": "Band Math and Indices\n\nCalculate vegetation index (NDVI-like)\n\n\nCode\n# Simulate NIR and Red bands\nnir_band = multiband_data[0].astype(np.float32)\nred_band = multiband_data[1].astype(np.float32)\n\n# Calculate NDVI-like index\n# NDVI = (NIR - Red) / (NIR + Red)\nndvi = (nir_band - red_band) / (nir_band + red_band + 1e-8)  # Small value to avoid division by zero\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(nir_band, cmap='RdYlBu_r')\nplt.title('NIR-like Band')\nplt.colorbar()\n\nplt.subplot(1, 3, 2)\nplt.imshow(red_band, cmap='Reds')\nplt.title('Red-like Band')\nplt.colorbar()\n\nplt.subplot(1, 3, 3)\nplt.imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.title('NDVI-like Index')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {np.mean(ndvi):.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI range: -0.211 to 1.000\nNDVI mean: 0.353"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#resampling-and-reprojection-concepts",
    "href": "course-materials/cheatsheets/rasterio_basics.html#resampling-and-reprojection-concepts",
    "title": "Working with Rasterio",
    "section": "Resampling and Reprojection Concepts",
    "text": "Resampling and Reprojection Concepts\n\nUnderstanding resampling methods\n\n\nCode\nfrom scipy import ndimage\n\n# Demonstrate different resampling methods\noriginal = sample_data\n\n# Downsample (reduce resolution)\ndownsampled = ndimage.zoom(original, 0.5, order=1)  # Linear interpolation\n\n# Upsample (increase resolution) \nupsampled = ndimage.zoom(original, 2.0, order=1)  # Linear interpolation\n\nplt.figure(figsize=(15, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(original, cmap='viridis')\nplt.title(f'Original ({original.shape[0]}x{original.shape[1]})')\n\nplt.subplot(1, 3, 2)\nplt.imshow(downsampled, cmap='viridis')\nplt.title(f'Downsampled ({downsampled.shape[0]}x{downsampled.shape[1]})')\n\nplt.subplot(1, 3, 3)\nplt.imshow(upsampled, cmap='viridis')\nplt.title(f'Upsampled ({upsampled.shape[0]}x{upsampled.shape[1]})')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Original size: {original.shape}\")\nprint(f\"Downsampled size: {downsampled.shape}\")  \nprint(f\"Upsampled size: {upsampled.shape}\")\n\n\n\n\n\n\n\n\n\nOriginal size: (100, 100)\nDownsampled size: (50, 50)\nUpsampled size: (200, 200)"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#common-rasterio-patterns",
    "href": "course-materials/cheatsheets/rasterio_basics.html#common-rasterio-patterns",
    "title": "Working with Rasterio",
    "section": "Common Rasterio Patterns",
    "text": "Common Rasterio Patterns\n\nWorking with windows and blocks\n\n\nCode\n# Simulate processing large rasters in blocks\ndef process_in_blocks(data, block_size=50):\n    \"\"\"Process data in blocks to simulate handling large rasters\"\"\"\n    height, width = data.shape\n    processed = np.zeros_like(data)\n    \n    for row in range(0, height, block_size):\n        for col in range(0, width, block_size):\n            # Define window bounds\n            row_end = min(row + block_size, height)\n            col_end = min(col + block_size, width)\n            \n            # Extract block\n            block = data[row:row_end, col:col_end]\n            \n            # Process block (example: enhance contrast)\n            processed_block = np.clip(block * 1.2, 0, 255)\n            \n            # Write back to result\n            processed[row:row_end, col:col_end] = processed_block\n            \n            print(f\"Processed block: ({row}:{row_end}, {col}:{col_end})\")\n    \n    return processed\n\n# Process sample data in blocks\nprocessed_data = process_in_blocks(sample_data, block_size=25)\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Original')\n\nplt.subplot(1, 2, 2)\nplt.imshow(processed_data, cmap='viridis')\nplt.title('Processed (Enhanced)')\n\nplt.tight_layout()\nplt.show()\n\n\nProcessed block: (0:25, 0:25)\nProcessed block: (0:25, 25:50)\nProcessed block: (0:25, 50:75)\nProcessed block: (0:25, 75:100)\nProcessed block: (25:50, 0:25)\nProcessed block: (25:50, 25:50)\nProcessed block: (25:50, 50:75)\nProcessed block: (25:50, 75:100)\nProcessed block: (50:75, 0:25)\nProcessed block: (50:75, 25:50)\nProcessed block: (50:75, 50:75)\nProcessed block: (50:75, 75:100)\nProcessed block: (75:100, 0:25)\nProcessed block: (75:100, 25:50)\nProcessed block: (75:100, 50:75)\nProcessed block: (75:100, 75:100)"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#summary",
    "href": "course-materials/cheatsheets/rasterio_basics.html#summary",
    "title": "Working with Rasterio",
    "section": "Summary",
    "text": "Summary\nKey Rasterio concepts covered: - Reading and understanding raster data structure - Working with single and multi-band imagery - Coordinate transforms and geospatial metadata\n- Band math and index calculations - Resampling and processing techniques - Block-based processing for large datasets"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html",
    "href": "course-materials/cheatsheets/pytorch_tensors.html",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "",
    "text": "Code\nimport torch\nimport numpy as np\n\n# From Python lists\ntensor_from_list = torch.tensor([[1, 2], [3, 4]])\nprint(f\"From list: {tensor_from_list}\")\n\n# From NumPy arrays\nnumpy_array = np.array([[1, 2], [3, 4]])\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(f\"From numpy: {tensor_from_numpy}\")\n\n# Common initialization patterns\nzeros_tensor = torch.zeros(3, 3)\nones_tensor = torch.ones(2, 4)\nrandom_tensor = torch.randn(2, 3)  # Normal distribution\n\nprint(f\"Zeros: {zeros_tensor}\")\nprint(f\"Random: {random_tensor}\")\n\n\nFrom list: tensor([[1, 2],\n        [3, 4]])\nFrom numpy: tensor([[1, 2],\n        [3, 4]])\nZeros: tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nRandom: tensor([[ 0.8308, -1.3461,  1.1544],\n        [ 0.4209,  0.1841, -1.6572]])\n\n\n\n\n\n\n\nCode\n# Check tensor properties\nsample_tensor = torch.randn(3, 4, 5)\n\nprint(f\"Shape: {sample_tensor.shape}\")\nprint(f\"Size: {sample_tensor.size()}\")\nprint(f\"Data type: {sample_tensor.dtype}\")\nprint(f\"Device: {sample_tensor.device}\")\nprint(f\"Number of dimensions: {sample_tensor.ndim}\")\nprint(f\"Total elements: {sample_tensor.numel()}\")\n\n\nShape: torch.Size([3, 4, 5])\nSize: torch.Size([3, 4, 5])\nData type: torch.float32\nDevice: cpu\nNumber of dimensions: 3\nTotal elements: 60"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#creating-tensors",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#creating-tensors",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "",
    "text": "Code\nimport torch\nimport numpy as np\n\n# From Python lists\ntensor_from_list = torch.tensor([[1, 2], [3, 4]])\nprint(f\"From list: {tensor_from_list}\")\n\n# From NumPy arrays\nnumpy_array = np.array([[1, 2], [3, 4]])\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(f\"From numpy: {tensor_from_numpy}\")\n\n# Common initialization patterns\nzeros_tensor = torch.zeros(3, 3)\nones_tensor = torch.ones(2, 4)\nrandom_tensor = torch.randn(2, 3)  # Normal distribution\n\nprint(f\"Zeros: {zeros_tensor}\")\nprint(f\"Random: {random_tensor}\")\n\n\nFrom list: tensor([[1, 2],\n        [3, 4]])\nFrom numpy: tensor([[1, 2],\n        [3, 4]])\nZeros: tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nRandom: tensor([[ 0.8308, -1.3461,  1.1544],\n        [ 0.4209,  0.1841, -1.6572]])\n\n\n\n\n\n\n\nCode\n# Check tensor properties\nsample_tensor = torch.randn(3, 4, 5)\n\nprint(f\"Shape: {sample_tensor.shape}\")\nprint(f\"Size: {sample_tensor.size()}\")\nprint(f\"Data type: {sample_tensor.dtype}\")\nprint(f\"Device: {sample_tensor.device}\")\nprint(f\"Number of dimensions: {sample_tensor.ndim}\")\nprint(f\"Total elements: {sample_tensor.numel()}\")\n\n\nShape: torch.Size([3, 4, 5])\nSize: torch.Size([3, 4, 5])\nData type: torch.float32\nDevice: cpu\nNumber of dimensions: 3\nTotal elements: 60"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#gpu-operations",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#gpu-operations",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "GPU Operations",
    "text": "GPU Operations\n\nCheck GPU availability\n\n\nCode\n# Check if CUDA is available\ncuda_available = torch.cuda.is_available()\nprint(f\"CUDA available: {cuda_available}\")\n\nif cuda_available:\n    gpu_count = torch.cuda.device_count()\n    gpu_name = torch.cuda.get_device_name(0)\n    print(f\"Number of GPUs: {gpu_count}\")\n    print(f\"GPU name: {gpu_name}\")\nelse:\n    print(\"Running on CPU\")\n\n\nCUDA available: False\nRunning on CPU\n\n\n\n\nMoving tensors to GPU\n\n\nCode\n# Create a tensor\ncpu_tensor = torch.randn(3, 3)\nprint(f\"Original device: {cpu_tensor.device}\")\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpu_tensor = cpu_tensor.to(device)\nprint(f\"After moving to device: {gpu_tensor.device}\")\n\n# Alternative methods\nif torch.cuda.is_available():\n    # Method 2: .cuda() method\n    gpu_tensor2 = cpu_tensor.cuda()\n    print(f\"Using .cuda(): {gpu_tensor2.device}\")\n    \n    # Method 3: Create directly on GPU\n    direct_gpu = torch.randn(3, 3, device='cuda')\n    print(f\"Created on GPU: {direct_gpu.device}\")\n\n\nOriginal device: cpu\nAfter moving to device: cpu"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#basic-tensor-operations",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#basic-tensor-operations",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\n\nMathematical operations\n\n\nCode\n# Create sample tensors\na = torch.randn(3, 3)\nb = torch.randn(3, 3)\n\n# Element-wise operations\naddition = a + b\nsubtraction = a - b  \nmultiplication = a * b\ndivision = a / b\n\nprint(f\"Addition shape: {addition.shape}\")\nprint(f\"Subtraction mean: {subtraction.mean():.3f}\")\n\n# Matrix operations\nmatmul = torch.matmul(a, b)  # Matrix multiplication\ntranspose = a.t()  # Transpose\n\nprint(f\"Matrix multiplication shape: {matmul.shape}\")\nprint(f\"Transpose shape: {transpose.shape}\")\n\n\nAddition shape: torch.Size([3, 3])\nSubtraction mean: -0.533\nMatrix multiplication shape: torch.Size([3, 3])\nTranspose shape: torch.Size([3, 3])\n\n\n\n\nUseful tensor methods\n\n\nCode\n# Statistical operations\ndata = torch.randn(4, 5)\n\nprint(f\"Mean: {data.mean():.3f}\")\nprint(f\"Standard deviation: {data.std():.3f}\")\nprint(f\"Min: {data.min():.3f}\")\nprint(f\"Max: {data.max():.3f}\")\n\n# Reshaping\nreshaped = data.view(2, 10)  # Reshape to 2x10\nflattened = data.flatten()   # Flatten to 1D\n\nprint(f\"Original shape: {data.shape}\")\nprint(f\"Reshaped: {reshaped.shape}\")\nprint(f\"Flattened: {flattened.shape}\")\n\n\nMean: 0.419\nStandard deviation: 1.160\nMin: -1.793\nMax: 2.824\nOriginal shape: torch.Size([4, 5])\nReshaped: torch.Size([2, 10])\nFlattened: torch.Size([20])"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#memory-management",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#memory-management",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Memory Management",
    "text": "Memory Management\n\nGPU memory monitoring\n\n\nCode\nif torch.cuda.is_available():\n    # Check current memory usage\n    allocated = torch.cuda.memory_allocated() / (1024**2)  # Convert to MB\n    reserved = torch.cuda.memory_reserved() / (1024**2)\n    \n    print(f\"GPU memory allocated: {allocated:.1f} MB\")\n    print(f\"GPU memory reserved: {reserved:.1f} MB\")\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    print(\"GPU cache cleared\")\nelse:\n    print(\"GPU memory monitoring not available (running on CPU)\")\n\n\nGPU memory monitoring not available (running on CPU)\n\n\n\n\nBest practices\n\n\nCode\n# Use context managers for temporary operations\ndef memory_efficient_operation():\n    with torch.no_grad():  # Disable gradient computation\n        large_tensor = torch.randn(1000, 1000)\n        result = large_tensor.mean()\n        return result\n\nresult = memory_efficient_operation()\nprint(f\"Result: {result:.3f}\")\n\n# The large_tensor is automatically garbage collected\n\n\nResult: -0.000"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#converting-between-formats",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#converting-between-formats",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Converting Between Formats",
    "text": "Converting Between Formats\n\nPyTorch ↔︎ NumPy\n\n\nCode\n# PyTorch to NumPy\ntorch_tensor = torch.randn(2, 3)\nnumpy_array = torch_tensor.numpy()  # Only works on CPU tensors\n\nprint(f\"PyTorch tensor: {torch_tensor}\")\nprint(f\"NumPy array: {numpy_array}\")\n\n# NumPy to PyTorch\nnew_torch = torch.from_numpy(numpy_array)\nprint(f\"Back to PyTorch: {new_torch}\")\n\n\nPyTorch tensor: tensor([[-1.4137, -0.5492,  1.2005],\n        [-0.6122,  0.3187, -0.8853]])\nNumPy array: [[-1.4137164  -0.54921     1.2005402 ]\n [-0.6122091   0.31872866 -0.8852541 ]]\nBack to PyTorch: tensor([[-1.4137, -0.5492,  1.2005],\n        [-0.6122,  0.3187, -0.8853]])\n\n\n\n\nHandling GPU tensors\n\n\nCode\nif torch.cuda.is_available():\n    # GPU tensor must be moved to CPU first\n    gpu_tensor = torch.randn(2, 3).cuda()\n    cpu_numpy = gpu_tensor.cpu().numpy()\n    print(f\"GPU tensor converted to NumPy: {cpu_numpy.shape}\")\nelse:\n    print(\"GPU conversion example not available (running on CPU)\")\n\n\nGPU conversion example not available (running on CPU)"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#introduction",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#introduction",
    "title": "EDS 217 - Lecture",
    "section": "Introduction",
    "text": "Introduction\n\nData cleaning is crucial in data analysis\nMissing data is a common challenge\nTwo main approaches:\n\nDropping missing data\nImputation\n\nUnderstanding the nature of missingness is key"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#types-of-missing-data",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#types-of-missing-data",
    "title": "EDS 217 - Lecture",
    "section": "Types of Missing Data",
    "text": "Types of Missing Data\n\n\nMissing Completely at Random (MCAR)\nMissing at Random (MAR)\nMissing Not at Random (MNAR)"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#missing-completely-at-random-mcar",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#missing-completely-at-random-mcar",
    "title": "EDS 217 - Lecture",
    "section": "Missing Completely at Random (MCAR)",
    "text": "Missing Completely at Random (MCAR)\n\nNo relationship between missingness and any values\nExample: Survey responses lost due to a computer glitch\nLeast problematic type of missing data\nDropping MCAR data is generally safe but reduces sample size"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#mcar-example-assigning-nan-randomly",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#mcar-example-assigning-nan-randomly",
    "title": "EDS 217 - Lecture",
    "section": "MCAR Example (Assigning nan randomly)",
    "text": "MCAR Example (Assigning nan randomly)\n\nimport pandas as pd\nimport numpy as np\n\n# Create sample data with MCAR\nnp.random.seed(42)\ndf = pd.DataFrame({'A': np.random.rand(100), 'B': np.random.rand(100)})\ndf.loc[np.random.choice(df.index, 10, replace=False), 'A'] = np.nan\nprint(df.isnull().sum())\n\nA    10\nB     0\ndtype: int64"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#missing-at-random-mar",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#missing-at-random-mar",
    "title": "EDS 217 - Lecture",
    "section": "Missing at Random (MAR)",
    "text": "Missing at Random (MAR)\n\nMissingness is related to other observed variables\nExample: Older participants more likely to skip income questions\nMore common in real-world datasets\nDropping MAR data can introduce bias"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#mar-example-assigning-nan-randomly-filtered-on-column-value",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#mar-example-assigning-nan-randomly-filtered-on-column-value",
    "title": "EDS 217 - Lecture",
    "section": "MAR Example (Assigning nan randomly, filtered on column value)",
    "text": "MAR Example (Assigning nan randomly, filtered on column value)\n\n# Create sample data with MAR\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'Age': np.random.randint(18, 80, 100),\n    'Income': np.random.randint(20000, 100000, 100)\n})\ndf.loc[df['Age'] &gt; 60, 'Income'] = np.where(\n    np.random.rand(len(df[df['Age'] &gt; 60])) &lt; 0.3, \n    np.nan, \n    df.loc[df['Age'] &gt; 60, 'Income']\n)\nprint(df[df['Age'] &gt; 60]['Income'].isnull().sum() / len(df[df['Age'] &gt; 60]))\n\n0.2972972972972973"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#missing-not-at-random-mnar",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#missing-not-at-random-mnar",
    "title": "EDS 217 - Lecture",
    "section": "Missing Not at Random (MNAR)",
    "text": "Missing Not at Random (MNAR)\n\nMissingness is related to the missing values themselves\nExample: People with high incomes more likely to skip income questions\nMost problematic type of missing data\nNeither dropping nor simple imputation may be appropriate"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#dropping-missing-data",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#dropping-missing-data",
    "title": "EDS 217 - Lecture",
    "section": "Dropping Missing Data",
    "text": "Dropping Missing Data\nPros:\n\n\nSimple and quick\nMaintains the distribution of complete cases\nAppropriate for MCAR data\n\n\nCons:\n\n\nReduces sample size\nCan introduce bias for MAR or MNAR data\nMay lose important information"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#drop-example",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#drop-example",
    "title": "EDS 217 - Lecture",
    "section": "Drop Example",
    "text": "Drop Example\n\n# Dropping missing data\ndf_dropped = df.dropna()\nprint(f\"Original shape: {df.shape}, After dropping: {df_dropped.shape}\")\n\nOriginal shape: (100, 2), After dropping: (89, 2)"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#imputation",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#imputation",
    "title": "EDS 217 - Lecture",
    "section": "Imputation",
    "text": "Imputation\nPros:\n\n\nPreserves sample size\nCan reduce bias for MAR data\nAllows use of all available information\n\n\nCons:\n\n\nCan introduce bias if done incorrectly\nMay underestimate variability\nCan be computationally intensive for complex methods"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#imputation-example",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#imputation-example",
    "title": "EDS 217 - Lecture",
    "section": "Imputation Example",
    "text": "Imputation Example\n\n# Simple mean imputation\ndf_imputed = df.fillna(df.mean())\nprint(f\"Original missing: {df['Income'].isnull().sum()}, After imputation: {df_imputed['Income'].isnull().sum()}\")\n\nOriginal missing: 11, After imputation: 0"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#imputation-methods",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#imputation-methods",
    "title": "EDS 217 - Lecture",
    "section": "Imputation Methods",
    "text": "Imputation Methods\n\nSimple imputation:\n\nMean, median, mode\nLast observation carried forward (LOCF)\n\nAdvanced imputation:\n\nMultiple Imputation\nK-Nearest Neighbors (KNN)\nRegression imputation"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#best-practices",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#best-practices",
    "title": "EDS 217 - Lecture",
    "section": "Best Practices",
    "text": "Best Practices\n\n\nUnderstand your data and the missingness mechanism\nVisualize patterns of missingness\nConsider the impact on your analysis\nUse appropriate methods based on the type of missingness\nConduct sensitivity analyses\nDocument your approach and assumptions"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#conclusion",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#conclusion",
    "title": "EDS 217 - Lecture",
    "section": "Conclusion",
    "text": "Conclusion\n\nUnderstanding the nature of missingness is crucial\nBoth dropping and imputation have pros and cons\nChoose the appropriate method based on:\n\nType of missingness (MCAR, MAR, MNAR)\nSample size\nAnalysis goals\n\nAlways document your approach and conduct sensitivity analyses"
  },
  {
    "objectID": "example_course/course-materials/lectures/05_Drop_or_Impute.html#questions",
    "href": "example_course/course-materials/lectures/05_Drop_or_Impute.html#questions",
    "title": "EDS 217 - Lecture",
    "section": "Questions?",
    "text": "Questions?\nThank you for your attention!"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html",
    "href": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "",
    "text": "Before we begin, here are quick links to our course cheatsheets. These may be helpful during the exercise:\n\nPython Basics Cheatsheet\nList Cheatsheet\nDictionaries Cheatsheet\nSets Cheatsheet\n\nFeel free to refer to these cheatsheets throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#quick-references",
    "href": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#quick-references",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "",
    "text": "Before we begin, here are quick links to our course cheatsheets. These may be helpful during the exercise:\n\nPython Basics Cheatsheet\nList Cheatsheet\nDictionaries Cheatsheet\nSets Cheatsheet\n\nFeel free to refer to these cheatsheets throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#introduction-to-paired-programming-5-minutes",
    "href": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#introduction-to-paired-programming-5-minutes",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "Introduction to Paired Programming (5 minutes)",
    "text": "Introduction to Paired Programming (5 minutes)\nWelcome to today’s Coding Colab! In this session, you’ll be working in pairs to explore and reinforce your understanding of lists and dictionaries, while also discovering the unique features of sets.\n\nBenefits of Paired Programming\n\nKnowledge sharing: Learn from each other’s experiences and approaches.\nImproved code quality: Catch errors earlier with two sets of eyes.\nEnhanced problem-solving: Discuss ideas for more creative solutions.\nSkill development: Improve communication and teamwork skills.\n\n\n\nHow to Make the Most of Paired Programming\n\nAssign roles: One person is the “driver” (typing), the other is the “navigator” (reviewing).\nSwitch roles regularly: Swap every 10-15 minutes to stay engaged.\nCommunicate clearly: Explain your thought process and ask questions.\nBe open to ideas: Listen to your partner’s suggestions.\nStay focused: Keep the conversation relevant to the task."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#exercise-overview",
    "href": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#exercise-overview",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "Exercise Overview",
    "text": "Exercise Overview\nThis Coding Colab will reinforce your understanding of lists and dictionaries while introducing you to sets. You’ll work through a series of tasks, discussing and implementing solutions together."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#part-1-lists-and-dictionaries-review-15-minutes",
    "href": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#part-1-lists-and-dictionaries-review-15-minutes",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "Part 1: Lists and Dictionaries Review (15 minutes)",
    "text": "Part 1: Lists and Dictionaries Review (15 minutes)\n\nTask 1: List Operations\nCreate a list of your favorite fruits and perform the following operations:\n\nCreate a list called fruits with at least 3 fruit names.\nAdd a new fruit to the end of the list.\nRemove the second fruit from the list.\nPrint the final list.\n\n\n\nTask 2: Dictionary Operations\nCreate a dictionary representing a simple inventory system:\n\nCreate a dictionary called inventory with at least 3 items and their quantities.\nAdd a new item to the inventory.\nUpdate the quantity of an existing item.\nPrint the final inventory."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#part-2-introducing-sets-15-minutes",
    "href": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#part-2-introducing-sets-15-minutes",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "Part 2: Introducing Sets (15 minutes)",
    "text": "Part 2: Introducing Sets (15 minutes)\nHere’s a Sets Cheatsheet. Sets are a lot like lists, so take a look at the cheatsheet to see how they are created and manipulated!\n\nTask 3: Creating and Manipulating Sets\n\nCreate two sets: evens with even numbers from 2 to 10, and odds with odd numbers from 1 to 9.\nPrint both sets.\nFind and print the union of the two sets.\nFind and print the intersection of the two sets.\nAdd a new element to the evens set.\n\n\n\nTask 4: Combining Set Operations and List Operations\nUsing a set is a great way to remove duplicates in a list.\n\nCreate a list with some duplicates: numbers = [1, 2, 2, 3, 3, 3, 4, 4, 5]\nUse a set to remove duplicates.\nCreate a new list from the set.\nPrint the new list without duplicates"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#conclusion-and-discussion-10-minutes",
    "href": "example_course/course-materials/coding-colabs/2c_lists_dictionaries_sets.html#conclusion-and-discussion-10-minutes",
    "title": "Day 2: 🙌 Coding Colab",
    "section": "Conclusion and Discussion (10 minutes)",
    "text": "Conclusion and Discussion (10 minutes)\nAs a pair, discuss the following questions:\n\nWhat are the main differences between lists, dictionaries, and sets?\nIn what situations would you prefer to use a set over a list or dictionary?\nHow did working in pairs help you understand these concepts better?"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html",
    "href": "course-materials/cheatsheets/plotting_satellite.html",
    "title": "Plotting Satellite Imagery",
    "section": "",
    "text": "Effective visualization is crucial for understanding satellite imagery and geospatial data. This cheatsheet covers essential plotting techniques.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#introduction",
    "href": "course-materials/cheatsheets/plotting_satellite.html#introduction",
    "title": "Plotting Satellite Imagery",
    "section": "",
    "text": "Effective visualization is crucial for understanding satellite imagery and geospatial data. This cheatsheet covers essential plotting techniques.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#creating-sample-satellite-data",
    "href": "course-materials/cheatsheets/plotting_satellite.html#creating-sample-satellite-data",
    "title": "Plotting Satellite Imagery",
    "section": "Creating Sample Satellite Data",
    "text": "Creating Sample Satellite Data\n\n\nCode\ndef create_sample_satellite_scene(size=256):\n    \"\"\"Create a realistic-looking satellite scene\"\"\"\n    \n    # Create coordinate grids\n    x = np.linspace(0, 10, size)\n    y = np.linspace(0, 10, size)\n    X, Y = np.meshgrid(x, y)\n    \n    # Simulate different land cover types\n    # Water bodies (low values)\n    water = np.exp(-((X - 2)**2 + (Y - 7)**2) / 2) * 0.3\n    water += np.exp(-((X - 8)**2 + (Y - 3)**2) / 4) * 0.25\n    \n    # Forest/vegetation (medium-high values)\n    forest = np.exp(-((X - 6)**2 + (Y - 8)**2) / 8) * 0.7\n    forest += np.exp(-((X - 3)**2 + (Y - 2)**2) / 6) * 0.6\n    \n    # Urban areas (varied values)\n    urban = np.exp(-((X - 7)**2 + (Y - 5)**2) / 3) * 0.8\n    \n    # Combine and add noise\n    scene = water + forest + urban\n    noise = np.random.normal(0, 0.05, scene.shape)\n    scene = np.clip(scene + noise, 0, 1)\n    \n    # Scale to typical satellite data range\n    return (scene * 255).astype(np.uint8)\n\n# Create sample scenes for different bands\nred_band = create_sample_satellite_scene()\ngreen_band = create_sample_satellite_scene() * 1.1\nblue_band = create_sample_satellite_scene() * 0.8\nnir_band = create_sample_satellite_scene() * 1.3\n\nprint(f\"Band shapes: {red_band.shape}\")\nprint(f\"Data ranges - Red: {red_band.min()}-{red_band.max()}\")\n\n\nBand shapes: (256, 256)\nData ranges - Red: 0-255"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#single-band-visualization",
    "href": "course-materials/cheatsheets/plotting_satellite.html#single-band-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Single Band Visualization",
    "text": "Single Band Visualization\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Different colormaps for single bands\ncmaps = ['gray', 'viridis', 'plasma', 'RdYlBu_r']\nband_names = ['Grayscale', 'Viridis', 'Plasma', 'Red-Yellow-Blue']\n\nfor i, (cmap, name) in enumerate(zip(cmaps, band_names)):\n    ax = axes[i//2, i%2]\n    im = ax.imshow(red_band, cmap=cmap, interpolation='bilinear')\n    ax.set_title(f'{name} Colormap')\n    ax.axis('off')\n    plt.colorbar(im, ax=ax, shrink=0.8)\n\nplt.suptitle('Single Band Visualization with Different Colormaps', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#rgb-composite-images",
    "href": "course-materials/cheatsheets/plotting_satellite.html#rgb-composite-images",
    "title": "Plotting Satellite Imagery",
    "section": "RGB Composite Images",
    "text": "RGB Composite Images\n\n\nCode\ndef create_rgb_composite(r, g, b, enhance=True):\n    \"\"\"Create RGB composite with optional enhancement\"\"\"\n    \n    # Stack bands\n    rgb = np.stack([r, g, b], axis=-1)\n    \n    if enhance:\n        # Simple linear stretch enhancement\n        for i in range(3):\n            band = rgb[:,:,i].astype(float)\n            # Stretch to 2-98 percentile\n            p2, p98 = np.percentile(band, (2, 98))\n            band = np.clip((band - p2) / (p98 - p2), 0, 1)\n            rgb[:,:,i] = (band * 255).astype(np.uint8)\n    \n    return rgb\n\n# Create different composites\ntrue_color = create_rgb_composite(red_band, green_band, blue_band)\nfalse_color = create_rgb_composite(nir_band, red_band, green_band)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\naxes[0].imshow(true_color)\naxes[0].set_title('True Color Composite (RGB)')\naxes[0].axis('off')\n\naxes[1].imshow(false_color)\naxes[1].set_title('False Color Composite (NIR-Red-Green)')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0]."
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#spectral-indices-visualization",
    "href": "course-materials/cheatsheets/plotting_satellite.html#spectral-indices-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Spectral Indices Visualization",
    "text": "Spectral Indices Visualization\n\n\nCode\n# Calculate NDVI\ndef calculate_ndvi(nir, red):\n    \"\"\"Calculate Normalized Difference Vegetation Index\"\"\"\n    nir_f = nir.astype(float)\n    red_f = red.astype(float)\n    return (nir_f - red_f) / (nir_f + red_f + 1e-8)\n\n# Calculate other indices\nndvi = calculate_ndvi(nir_band, red_band)\n\n# Water index (using blue and NIR)\nndwi = calculate_ndvi(green_band, nir_band)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# NDVI\nim1 = axes[0].imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1, interpolation='bilinear')\naxes[0].set_title('NDVI (Vegetation Index)')\naxes[0].axis('off')\nplt.colorbar(im1, ax=axes[0], shrink=0.8)\n\n# NDWI  \nim2 = axes[1].imshow(ndwi, cmap='RdYlBu', vmin=-1, vmax=1, interpolation='bilinear')\naxes[1].set_title('NDWI (Water Index)')\naxes[1].axis('off')\nplt.colorbar(im2, ax=axes[1], shrink=0.8)\n\n# Histogram of NDVI values\naxes[2].hist(ndvi.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\naxes[2].set_xlabel('NDVI Value')\naxes[2].set_ylabel('Frequency')\naxes[2].set_title('NDVI Distribution')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {ndvi.mean():.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI range: -1.000 to 1.000\nNDVI mean: 0.122"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#custom-colormaps-and-classification",
    "href": "course-materials/cheatsheets/plotting_satellite.html#custom-colormaps-and-classification",
    "title": "Plotting Satellite Imagery",
    "section": "Custom Colormaps and Classification",
    "text": "Custom Colormaps and Classification\n\n\nCode\n# Create land cover classification\ndef classify_land_cover(ndvi, ndwi):\n    \"\"\"Simple land cover classification\"\"\"\n    classification = np.zeros_like(ndvi, dtype=int)\n    \n    # Water (high NDWI, low NDVI)\n    water_mask = (ndwi &gt; 0.3) & (ndvi &lt; 0.1)\n    classification[water_mask] = 1\n    \n    # Vegetation (high NDVI)\n    veg_mask = ndvi &gt; 0.4\n    classification[veg_mask] = 2\n    \n    # Bare soil/urban (low NDVI, low NDWI)  \n    bare_mask = (ndvi &lt; 0.2) & (ndwi &lt; 0.1)\n    classification[bare_mask] = 3\n    \n    return classification\n\n# Classify the scene\nland_cover = classify_land_cover(ndvi, ndwi)\n\n# Create custom colormap\ncolors = ['black', 'blue', 'green', 'brown', 'gray']\nn_classes = len(np.unique(land_cover))\ncustom_cmap = ListedColormap(colors[:n_classes])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Show classification\nim1 = axes[0].imshow(land_cover, cmap=custom_cmap, interpolation='nearest')\naxes[0].set_title('Land Cover Classification')\naxes[0].axis('off')\n\n# Custom legend\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='black', label='Unclassified'),\n    Patch(facecolor='blue', label='Water'),\n    Patch(facecolor='green', label='Vegetation'), \n    Patch(facecolor='brown', label='Bare Soil/Urban')\n]\naxes[0].legend(handles=legend_elements, loc='upper right')\n\n# Classification statistics\nunique_classes, counts = np.unique(land_cover, return_counts=True)\npercentages = counts / counts.sum() * 100\n\naxes[1].bar(['Unclassified', 'Water', 'Vegetation', 'Bare/Urban'][:len(unique_classes)], \n           percentages, color=['black', 'blue', 'green', 'brown'][:len(unique_classes)])\naxes[1].set_ylabel('Percentage of Pixels')\naxes[1].set_title('Land Cover Distribution')\naxes[1].grid(True, alpha=0.3)\n\nfor i, (cls, pct) in enumerate(zip(unique_classes, percentages)):\n    axes[1].text(i, pct + 1, f'{pct:.1f}%', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#advanced-visualization-techniques",
    "href": "course-materials/cheatsheets/plotting_satellite.html#advanced-visualization-techniques",
    "title": "Plotting Satellite Imagery",
    "section": "Advanced Visualization Techniques",
    "text": "Advanced Visualization Techniques\n\n\nCode\n# Multi-scale visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Full scene\naxes[0, 0].imshow(true_color)\naxes[0, 0].set_title('Full Scene')\naxes[0, 0].axis('off')\n\n# Add rectangle showing zoom area\nzoom_area = Rectangle((100, 100), 80, 80, linewidth=2, \n                     edgecolor='red', facecolor='none')\naxes[0, 0].add_patch(zoom_area)\n\n# Zoomed areas with different processing\nzoom_slice = (slice(100, 180), slice(100, 180))\n\n# Zoomed true color\naxes[0, 1].imshow(true_color[zoom_slice])\naxes[0, 1].set_title('Zoomed True Color')\naxes[0, 1].axis('off')\n\n# Zoomed false color\naxes[0, 2].imshow(false_color[zoom_slice])\naxes[0, 2].set_title('Zoomed False Color')\naxes[0, 2].axis('off')\n\n# Zoomed NDVI\nim1 = axes[1, 0].imshow(ndvi[zoom_slice], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[1, 0].set_title('Zoomed NDVI')\naxes[1, 0].axis('off')\nplt.colorbar(im1, ax=axes[1, 0], shrink=0.8)\n\n# Zoomed classification\naxes[1, 1].imshow(land_cover[zoom_slice], cmap=custom_cmap, interpolation='nearest')\naxes[1, 1].set_title('Zoomed Classification')\naxes[1, 1].axis('off')\n\n# Profile plot\ncenter_row = land_cover.shape[0] // 2\nprofile_ndvi = ndvi[center_row, :]\nprofile_x = np.arange(len(profile_ndvi))\n\naxes[1, 2].plot(profile_x, profile_ndvi, 'g-', linewidth=2)\naxes[1, 2].axhline(y=0, color='k', linestyle='--', alpha=0.5)\naxes[1, 2].axhline(y=0.4, color='r', linestyle='--', alpha=0.5, label='Vegetation threshold')\naxes[1, 2].set_xlabel('Pixel Position')\naxes[1, 2].set_ylabel('NDVI Value')\naxes[1, 2].set_title('NDVI Profile (Center Row)')\naxes[1, 2].grid(True, alpha=0.3)\naxes[1, 2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [37.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [37.0..255.0]."
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#time-series-visualization",
    "href": "course-materials/cheatsheets/plotting_satellite.html#time-series-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Time Series Visualization",
    "text": "Time Series Visualization\n\n\nCode\n# Simulate time series of NDVI\ndef create_ndvi_time_series(n_dates=12):\n    \"\"\"Create sample NDVI time series\"\"\"\n    dates = []\n    ndvi_values = []\n    \n    # Simulate seasonal pattern\n    base_ndvi = 0.4\n    seasonal_amplitude = 0.3\n    \n    for i in range(n_dates):\n        # Simulate monthly data\n        month_angle = (i / 12) * 2 * np.pi\n        seasonal_component = seasonal_amplitude * np.sin(month_angle + np.pi/2)  # Peak in summer\n        noise = np.random.normal(0, 0.05)\n        \n        ndvi_val = base_ndvi + seasonal_component + noise\n        ndvi_values.append(ndvi_val)\n        dates.append(f'Month {i+1}')\n    \n    return dates, ndvi_values\n\n# Create sample time series for different land cover types\ndates, forest_ndvi = create_ndvi_time_series()\n_, cropland_ndvi = create_ndvi_time_series()  \n_, urban_ndvi = create_ndvi_time_series()\n\n# Adjust for different land cover characteristics\ncropland_ndvi = [v * 0.8 + 0.1 for v in cropland_ndvi]  # Lower peak, higher base\nurban_ndvi = [v * 0.3 + 0.15 for v in urban_ndvi]  # Much lower overall\n\nplt.figure(figsize=(12, 6))\n\nplt.plot(dates, forest_ndvi, 'g-o', label='Forest', linewidth=2, markersize=6)\nplt.plot(dates, cropland_ndvi, 'orange', marker='s', label='Cropland', linewidth=2, markersize=6)  \nplt.plot(dates, urban_ndvi, 'gray', marker='^', label='Urban', linewidth=2, markersize=6)\n\nplt.xlabel('Time Period')\nplt.ylabel('NDVI Value')\nplt.title('NDVI Time Series by Land Cover Type')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.ylim(-0.1, 0.8)\n\n# Add shaded regions for seasons\nsummer_months = [4, 5, 6, 7]  # Months 5-8\nplt.axvspan(summer_months[0]-0.5, summer_months[-1]+0.5, alpha=0.2, color='yellow', label='Summer')\n\nplt.tight_layout()\nplt.show()\n\n# Print some statistics\nprint(\"NDVI Statistics by Land Cover:\")\nprint(f\"Forest - Mean: {np.mean(forest_ndvi):.3f}, Std: {np.std(forest_ndvi):.3f}\")\nprint(f\"Cropland - Mean: {np.mean(cropland_ndvi):.3f}, Std: {np.std(cropland_ndvi):.3f}\")\nprint(f\"Urban - Mean: {np.mean(urban_ndvi):.3f}, Std: {np.std(urban_ndvi):.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI Statistics by Land Cover:\nForest - Mean: 0.407, Std: 0.202\nCropland - Mean: 0.420, Std: 0.166\nUrban - Mean: 0.271, Std: 0.067"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#summary",
    "href": "course-materials/cheatsheets/plotting_satellite.html#summary",
    "title": "Plotting Satellite Imagery",
    "section": "Summary",
    "text": "Summary\nKey visualization techniques covered:\n\nSingle band visualization with different colormaps\nRGB composite creation and enhancement\nSpectral indices (NDVI, NDWI) visualization\n\nCustom colormaps and classification maps\nMulti-scale visualization with zoom and profiles\nTime series plotting for temporal analysis\n\nThese techniques form the foundation for effective satellite imagery analysis and presentation."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help.html",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help.html",
    "title": "Interactive Session 2A",
    "section": "",
    "text": "Objective: Learn how to get help, work with variables, and explore methods available for different Python objects in Jupyter Notebooks.\nEstimated Time: 45-60 minutes"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help.html#getting-help-in-python",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help.html#getting-help-in-python",
    "title": "Interactive Session 2A",
    "section": "1. Getting Help in Python",
    "text": "1. Getting Help in Python\n\nUsing help()\n\nIn a Jupyter Notebook cell, type:\n\n\nCode\n  #| echo: false\n\nhelp(len)\n\n\nHelp on built-in function len in module builtins:\n\nlen(obj, /)\n    Return the number of items in a container.\n\n\n\nRun the cell to see detailed information about the len() function.\n\n\n\nTrying help() Yourself\n\nUse the help() function on other built-in functions like print or sum.\n\n\n\nUsing ? and ??\n\nType:\nRun the cell to see quick documentation.\nTry:\nThis gives more detailed information, including source code (if available)."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help.html#working-with-variables",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help.html#working-with-variables",
    "title": "Interactive Session 2A",
    "section": "2. Working with Variables",
    "text": "2. Working with Variables\n\nCreating Variables\n\nIn a new cell, create a few variables:\nUse type() to check the data type of each variable:\n\n\nCode\ntype(a)\ntype(b)\ntype(c)\n\n\nstr\n\n\n\n\n\nExploring Variables\n\nExperiment with creating your own variables and checking their types.\nChange the values and data types and see what happens."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help.html#exploring-methods-available-for-objects",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help.html#exploring-methods-available-for-objects",
    "title": "Interactive Session 2A",
    "section": "3. Exploring Methods Available for Objects",
    "text": "3. Exploring Methods Available for Objects\n\nUsing dir()\n\nUse dir() to explore available methods for objects:\n\n\nCode\ndir(a)\ndir(b)\ndir(c)\n\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getnewargs__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rmod__',\n '__rmul__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'capitalize',\n 'casefold',\n 'center',\n 'count',\n 'encode',\n 'endswith',\n 'expandtabs',\n 'find',\n 'format',\n 'format_map',\n 'index',\n 'isalnum',\n 'isalpha',\n 'isascii',\n 'isdecimal',\n 'isdigit',\n 'isidentifier',\n 'islower',\n 'isnumeric',\n 'isprintable',\n 'isspace',\n 'istitle',\n 'isupper',\n 'join',\n 'ljust',\n 'lower',\n 'lstrip',\n 'maketrans',\n 'partition',\n 'removeprefix',\n 'removesuffix',\n 'replace',\n 'rfind',\n 'rindex',\n 'rjust',\n 'rpartition',\n 'rsplit',\n 'rstrip',\n 'split',\n 'splitlines',\n 'startswith',\n 'strip',\n 'swapcase',\n 'title',\n 'translate',\n 'upper',\n 'zfill']\n\n\n\n\n\nUsing help() with Methods\n\nPick a method from the list returned by dir() and use help() to learn more about it:\n\n\nCode\nhelp(c.upper)\n\n\nHelp on built-in function upper:\n\nupper() method of builtins.str instance\n    Return a copy of the string converted to uppercase.\n\n\n\n\n\n\nExploring Methods\n\nTry calling a method on your variables:\n\n\n'HELLO, WORLD!'\n\n\n\n\n\n::: {.center-text .body-text-xl .teal-text}\nEnd interactive session 2A\n:::\n\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[Interactive Session 2A]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-sidebar-title\"}\n[GEOG 288KC]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar-title\"}\n[🏠 home]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🏠 home\"}\n[/index.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/index.html\"}\n[📋 syllabus]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:📋 syllabus\"}\n[/Syllabus.md]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/Syllabus.md\"}\n[🗓️ weekly materials]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🗓️ weekly materials\"}\n[1️⃣ Week 1 - Introduction to GFMs]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:1️⃣ Week 1 - Introduction to GFMs\"}\n[/course-materials/week1.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/week1.html\"}\n[2️⃣ Week 2 - Working with Geospatial Data]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:2️⃣ Week 2 - Working with Geospatial Data\"}\n[/course-materials/week2.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/week2.html\"}\n[3️⃣ Week 3 - Loading & Using Foundation Models]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:3️⃣ Week 3 - Loading & Using Foundation Models\"}\n[/course-materials/week3.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/week3.html\"}\n[4️⃣ Week 4 - Multi-modal & Generative Models]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:4️⃣ Week 4 - Multi-modal & Generative Models\"}\n[/course-materials/week4.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/week4.html\"}\n[5️⃣ Week 5 - Model Adaptation & Evaluation]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:5️⃣ Week 5 - Model Adaptation & Evaluation\"}\n[/course-materials/week5.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/week5.html\"}\n[6️⃣ Week 6 - Project Initiation & Planning]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:6️⃣ Week 6 - Project Initiation & Planning\"}\n[/course-materials/week6.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/week6.html\"}\n[7️⃣ Week 7 - Core Development & Implementation]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:7️⃣ Week 7 - Core Development & Implementation\"}\n[/course-materials/week7.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/week7.html\"}\n[8️⃣ Week 8 - Advanced Development & Optimization]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:8️⃣ Week 8 - Advanced Development & Optimization\"}\n[/course-materials/week8.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/week8.html\"}\n[9️⃣ Week 9 - Project Workshops & Synthesis]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:9️⃣ Week 9 - Project Workshops & Synthesis\"}\n[/course-materials/week9.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/week9.html\"}\n[🎯 Week 10 - Final Presentations]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🎯 Week 10 - Final Presentations\"}\n[/course-materials/week10.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/week10.html\"}\n[💻 interactive sessions]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:💻 interactive sessions\"}\n[Session 1 - 🔥 PyTorch & TorchGeo Fundamentals]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Session 1 - 🔥 PyTorch & TorchGeo Fundamentals\"}\n[/course-materials/interactive-sessions/session1_pytorch_torchgeo.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/interactive-sessions/session1_pytorch_torchgeo.html\"}\n[Session 2 - 🤗 Loading & Using Foundation Models]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Session 2 - 🤗 Loading & Using Foundation Models\"}\n[/course-materials/interactive-sessions/session2_foundation_models.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/interactive-sessions/session2_foundation_models.html\"}\n[Session 3 - 📐 Multi-modal & Sensor Fusion]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Session 3 - 📐 Multi-modal & Sensor Fusion\"}\n[/course-materials/interactive-sessions/session3_multimodal.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/interactive-sessions/session3_multimodal.html\"}\n[Session 4 - 🔧 Model Adaptation & Evaluation]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Session 4 - 🔧 Model Adaptation & Evaluation\"}\n[/course-materials/interactive-sessions/session4_adaptation.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/interactive-sessions/session4_adaptation.html\"}\n[Session 5 - ☁️ Scalable Analysis & Deployment]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Session 5 - ☁️ Scalable Analysis & Deployment\"}\n[/course-materials/interactive-sessions/session5_deployment.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/interactive-sessions/session5_deployment.html\"}\n[🎓 lectures]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🎓 lectures\"}\n[Lecture 1 - 🧠 Foundation Model Architectures]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Lecture 1 - 🧠 Foundation Model Architectures\"}\n[/course-materials/lectures/lecture1_architectures.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/lectures/lecture1_architectures.html\"}\n[Lecture 2 - 🌍 Geospatial Data & Remote Sensing]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Lecture 2 - 🌍 Geospatial Data & Remote Sensing\"}\n[/course-materials/lectures/lecture2_geospatial_data.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/lectures/lecture2_geospatial_data.html\"}\n[Lecture 3 - 🔧 Fine-tuning Strategies]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Lecture 3 - 🔧 Fine-tuning Strategies\"}\n[/course-materials/lectures/lecture3_finetuning.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/lectures/lecture3_finetuning.html\"}\n[Lecture 4 - 📐 Multi-modal Learning]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Lecture 4 - 📐 Multi-modal Learning\"}\n[/course-materials/lectures/lecture4_multimodal.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/lectures/lecture4_multimodal.html\"}\n[Lecture 5 - 🔍 Model Evaluation & Validation]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Lecture 5 - 🔍 Model Evaluation & Validation\"}\n[/course-materials/lectures/lecture5_evaluation.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/lectures/lecture5_evaluation.html\"}\n[Lecture 6 - ☁️ Cloud & Scalable Computing]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:Lecture 6 - ☁️ Cloud & Scalable Computing\"}\n[/course-materials/lectures/lecture6_cloud_computing.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/lectures/lecture6_cloud_computing.html\"}\n[📁 projects]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:📁 projects\"}\n[📝 Project Application Template]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:📝 Project Application Template\"}\n[/course-materials/projects/project-application-template.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/projects/project-application-template.html\"}\n[📋 Project Proposal Template]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:📋 Project Proposal Template\"}\n[/course-materials/projects/project-proposal-template.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/projects/project-proposal-template.html\"}\n[🚀 MVP Presentation Template]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🚀 MVP Presentation Template\"}\n[/course-materials/projects/mvp-template.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/course-materials/projects/mvp-template.html\"}\n[👀 cheatsheets]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:👀 cheatsheets\"}\n[/cheatsheets.html]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:/cheatsheets.html\"}\n[📚 resources]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:📚 resources\"}\n[🌍 Earth Engine Documentation]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🌍 Earth Engine Documentation\"}\n[https://developers.google.com/earth-engine]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://developers.google.com/earth-engine\"}\n[🔥 PyTorch Documentation]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🔥 PyTorch Documentation\"}\n[https://pytorch.org/docs/]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://pytorch.org/docs/\"}\n[🗺️ TorchGeo Documentation]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🗺️ TorchGeo Documentation\"}\n[https://torchgeo.readthedocs.io/]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://torchgeo.readthedocs.io/\"}\n[🤗 HuggingFace Documentation]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🤗 HuggingFace Documentation\"}\n[https://huggingface.co/docs]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://huggingface.co/docs\"}\n[🌐 Prithvi Foundation Model]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🌐 Prithvi Foundation Model\"}\n[https://github.com/NASA-IMPACT/Prithvi-100M]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://github.com/NASA-IMPACT/Prithvi-100M\"}\n[🛰️ SatMAE Model]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🛰️ SatMAE Model\"}\n[https://github.com/microsoft/SatMAE]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://github.com/microsoft/SatMAE\"}\n[🧠 UCSB AI Sandbox]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:🧠 UCSB AI Sandbox\"}\n[https://ai-sandbox.ucsb.edu]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://ai-sandbox.ucsb.edu\"}\n[📊 MLflow Documentation]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:📊 MLflow Documentation\"}\n[https://mlflow.org/docs/]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://mlflow.org/docs/\"}\n[⚡ FastAPI Documentation]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:⚡ FastAPI Documentation\"}\n[https://fastapi.tiangolo.com/]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://fastapi.tiangolo.com/\"}\n[📈 Weights & Biases]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:📈 Weights & Biases\"}\n[https://docs.wandb.ai/]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://docs.wandb.ai/\"}\n[https://github.com/kellycaylor/geoAI]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-int-navbar:https://github.com/kellycaylor/geoAI\"}\n\n:::{.hidden .quarto-markdown-envelope-contents render-id=\"footer-left\"}\n&lt;img src=\"/images/anna.png\" alt=\"Course instructors\" width=\"250\"/&gt;\n\n:::\n\n\n:::{.hidden .quarto-markdown-envelope-contents render-id=\"footer-right\"}\nThis website is built with [](https://github.com/kellycaylor/geoAI) and [Quarto](https://quarto.org/)\n:::\n\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[Interactive Session 2A]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-metatitle\"}\n[Interactive Session 2A]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-twittercardtitle\"}\n[Interactive Session 2A]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-ogcardtitle\"}\n[⚒️ Getting `help()` in Jupyter Notebooks]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-twittercarddesc\"}\n[⚒️ Getting `help()` in Jupyter Notebooks]{.hidden .quarto-markdown-envelope-contents render-id=\"quarto-ogcardddesc\"}\n:::\n\n\n\n\n&lt;!-- --&gt;\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n---\ntitle: \"Interactive Session 2A\"\nsubtitle: \"⚒️ Getting `help()` in Jupyter Notebooks\"\neditor_options: \n  chunk_output_type: console\njupyter: eds217_2025\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: show\n---\n\n**Objective:** Learn how to get help, work with variables, and explore methods available for different Python objects in Jupyter Notebooks.\n\n**Estimated Time:** 45-60 minutes\n\n---\n\n## 1. Getting Help in Python\n\n### Using `help()`\n- In a Jupyter Notebook cell, type:\n  ```{python}\n    #| echo: false\n\n  help(len)\n\nRun the cell to see detailed information about the len() function.\n\n\n\nTrying help() Yourself\n\nUse the help() function on other built-in functions like print or sum.\n\n\n\nUsing ? and ??\n\nType:\n#| echo: false\nlen?\nRun the cell to see quick documentation.\nTry:\n#| echo: false\nlen??\nThis gives more detailed information, including source code (if available)."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help.html#working-with-variables-1",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help.html#working-with-variables-1",
    "title": "Interactive Session 2A",
    "section": "2. Working with Variables",
    "text": "2. Working with Variables\n\nCreating Variables\n\nIn a new cell, create a few variables:\n#| echo: false\na = 10\nb = 5.5\nc = \"Hello, world!\"\nUse type() to check the data type of each variable:\ntype(a)\ntype(b)\ntype(c)\n\n\n\nExploring Variables\n\nExperiment with creating your own variables and checking their types.\nChange the values and data types and see what happens."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help.html#exploring-methods-available-for-objects-1",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help.html#exploring-methods-available-for-objects-1",
    "title": "Interactive Session 2A",
    "section": "3. Exploring Methods Available for Objects",
    "text": "3. Exploring Methods Available for Objects\n\nUsing dir()\n\nUse dir() to explore available methods for objects:\ndir(a)\ndir(b)\ndir(c)\n\n\n\nUsing help() with Methods\n\nPick a method from the list returned by dir() and use help() to learn more about it:\nhelp(c.upper)\n\n\n\nExploring Methods\n\nTry calling a method on your variables:\n#| echo: false\nc.upper()\n\n\n\n::: {.center-text .body-text-xl .teal-text}\nEnd interactive session 2A\n:::\n:::"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html",
    "href": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "",
    "text": "In this coding colab, you’ll analyze global temperature anomalies and CO2 concentration data. You’ll practice data manipulation, joining datasets, time series analysis, and visualization techniques."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#introduction",
    "href": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#introduction",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "",
    "text": "In this coding colab, you’ll analyze global temperature anomalies and CO2 concentration data. You’ll practice data manipulation, joining datasets, time series analysis, and visualization techniques."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#learning-objectives",
    "href": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#learning-objectives",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this colab, you will be able to:\n\nLoad and preprocess time series data\nJoin datasets based on datetime indices\nPerform basic time series analysis and resampling\nApply data manipulation techniques to extract insights from environmental datasets"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#setup",
    "href": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#setup",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Setup",
    "text": "Setup\nLet’s start by importing necessary libraries and loading our datasets:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the temperature anomaly dataset\ntemp_url = \"https://bit.ly/monthly_temp\"\ntemp_df = pd.read_csv(temp_url, parse_dates=['Date'])\n\n# Load the CO2 concentration dataset\nco2_url = \"https://bit.ly/monthly_CO2\"\nco2_df = pd.read_csv(co2_url, parse_dates=['Date'])\n\nprint(\"Temperature data:\")\nprint(temp_df.head())\nprint(\"\\nCO2 data:\")\nprint(co2_df.head())\n\n\nTemperature data:\n        Date  MonthlyAnomaly\n0 1880-01-01           -0.20\n1 1880-02-01           -0.25\n2 1880-03-01           -0.09\n3 1880-04-01           -0.16\n4 1880-05-01           -0.09\n\nCO2 data:\n        Date  CO2Concentration\n0 1958-04-01            317.45\n1 1958-05-01            317.51\n2 1958-06-01            317.27\n3 1958-07-01            315.87\n4 1958-08-01            314.93"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-1-data-preparation",
    "href": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-1-data-preparation",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Task 1: Data Preparation",
    "text": "Task 1: Data Preparation\n\nSet the ‘Date’ column as the index for both dataframes.\nEnsure that there are no missing values in either dataset."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-2-joining-datasets",
    "href": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-2-joining-datasets",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Task 2: Joining Datasets",
    "text": "Task 2: Joining Datasets\n\nMerge the temperature and CO2 datasets based on their date index.\nHandle any missing values that may have been introduced by the merge.\nCreate some plots showing temperature anomalies and CO2 concentrations over time using pandas built-in plotting functions."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-3-time-series-analysis",
    "href": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-3-time-series-analysis",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Task 3: Time Series Analysis",
    "text": "Task 3: Time Series Analysis\n\nResample the data to annual averages.\nCalculate the year-over-year change in temperature anomalies and CO2 concentrations.\nCreate a scatter plot (use the plt.scatter() function) of annual temperature anomalies vs CO2 concentrations."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-4-seasonal-analysis",
    "href": "example_course/course-materials/coding-colabs/6b_advanced_data_manipulation.html#task-4-seasonal-analysis",
    "title": "Day 6: 🙌 Coding Colab",
    "section": "Task 4: Seasonal Analysis",
    "text": "Task 4: Seasonal Analysis\n\nCreate a function to extract the season from a given date (hint: use the date.month attribute and if-elif-else to assign the season in your function).\nUse the function to create a new column called Season\nCalculate the average temperature anomaly and CO2 concentration for each season.\nCreate a box plot (use sns.boxplot) showing the distribution of temperature anomalies for each season."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#getting-started",
    "title": "Interactive Session 2B",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_2B_Dictionaries.ipynb\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day 2: Session B - Dictionaries\n\n[Link to session webpage](https://eds-217-essential-python.github.io/course-materials/interactive-sessions/2b_dictionaries.html)\n\nDate: 09/04/2024\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#part-1-basic-concepts-with-species-lookup-table",
    "href": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#part-1-basic-concepts-with-species-lookup-table",
    "title": "Interactive Session 2B",
    "section": "Part 1: Basic Concepts with Species Lookup Table",
    "text": "Part 1: Basic Concepts with Species Lookup Table\n\nIntroduction to Dictionaries\nDictionaries in Python are collections of key-value pairs that allow for efficient data storage and retrieval. Each key maps to a specific value, making dictionaries ideal for representing real-world data in a structured format.\nProbably the easiest mental model for thinking about structured data is a spreadsheet. You are all familiar with Excel spreadsheets, with their numbered rows and lettered columns. In the spreadsheet, data is often “structured” so that each row is an entry, and each column is perhaps a variable recorded for that entry.\n\n\n\nstructured-data"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#instructions",
    "href": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#instructions",
    "title": "Interactive Session 2B",
    "section": "Instructions",
    "text": "Instructions\nWe will work through this material together, writing a new notebook as we go.\n\n\n\n\n\n\nNote\n\n\n\n🐍     This symbol designates an important note about Python structure, syntax, or another quirk.\n\n\n\n✏️   This symbol designates code you should add to your notebook and run."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#dictionaries",
    "href": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#dictionaries",
    "title": "Interactive Session 2B",
    "section": "Dictionaries",
    "text": "Dictionaries\n\nTLDR: Dictionaries are a very common collection type that allows data to be organized using a key:value framework. Because of the similarity between key:value pairs and many data structures (e.g. “lookup tables”), you will see Dictionaries quite a bit when working in python\n\nThe first collection we will look at today is the dictionary, or dict. This is one of the most powerful data structures in python. It is a mutable, unordered collection, which means that it can be altered, but elements within the structure cannot be referenced by their position and they cannot be sorted.\nYou can create a dictionary using the {}, providing both a key and a value, which are separated by a :."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#creating-manipulating-dictionaries",
    "href": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#creating-manipulating-dictionaries",
    "title": "Interactive Session 2B",
    "section": "Creating & Manipulating Dictionaries",
    "text": "Creating & Manipulating Dictionaries\nWe’ll start by creating a dictionary to store the common name of various species found in California’s coastal tidepools.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Define a dictionary with species data containing latin names and corresponding common names.\nspecies_dict = {\n    \"P ochraceus\": \"Ochre sea star\",\n    \"M californianus\": \"California mussel\",\n    \"H rufescens\": \"Red abalone\"\n}\n\n\n\n\n\n\n\n\nNote\n\n\n\n🐍 &lt;b&gt;Note.&lt;/b&gt; The use of whitespace and indentation is important in python. In the example above, the dictionary entries are indented relative to the brackets &lt;code&gt;{&lt;/code&gt; and &lt;code&gt;}&lt;/code&gt;. In addition, there is no space between the &lt;code&gt;'key'&lt;/code&gt;, the &lt;code&gt;:&lt;/code&gt;, and the &lt;code&gt;'value'&lt;/code&gt; for each entry. Finally, notice that there is a &lt;code&gt;,&lt;/code&gt; following each dictionary entry. This pattern is the same as all of the other &lt;i&gt;collection&lt;/i&gt; data types we've seen so far, including &lt;b&gt;list&lt;/b&gt;, &lt;b&gt;set&lt;/b&gt;, and &lt;b&gt;tuple&lt;/b&gt;.\n\n\n\nAccessing elements in a dictionary\nAccessing an element in a dictionary is easy if you know what you are looking for.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nspecies_dict['M californianus']\n\n\n'California mussel'\n\n\n\n\nAdding a New Species\nBecause dictionaries are mutable, it is easy to add additional entries and doing so is straightforward. You specify the key and the value it maps to.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Adding a new entry for Leather star\nspecies_dict[\"D imbricata\"] = \"Leather star\"\n\n\n\n\nAccessing and Modifying Data\nAccessing data in a dictionary can be done directly by the key, and modifications are just as direct.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Accessing a species by its latin name\nprint(\"Common name for P ochraceus:\", species_dict[\"P ochraceus\"])\n\n\nCommon name for P ochraceus: Ochre sea star\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Updating the common name for Ochre Sea Star abalone\nspecies_dict[\"P ochraceus\"] = \"Purple Starfish\"\nprint(\"Updated data for Pisaster ochraceus:\", species_dict[\"P ochraceus\"])\n\n\nUpdated data for Pisaster ochraceus: Purple Starfish\n\n\n\n\nRemoving a Dictionary Element\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Removing \"P ochraceus\"\ndel species_dict[\"P ochraceus\"]\nprint(f\"Deleted data for Pisaster ochraceus, new dictionary: {species_dict}\")\n\n\nDeleted data for Pisaster ochraceus, new dictionary: {'M californianus': 'California mussel', 'H rufescens': 'Red abalone', 'D imbricata': 'Leather star'}"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#accessing-dictionary-keys-and-values",
    "href": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#accessing-dictionary-keys-and-values",
    "title": "Interactive Session 2B",
    "section": "Accessing dictionary keys and values",
    "text": "Accessing dictionary keys and values\nEvery dictionary has builtin methods to retrieve its keys and values. These functions are called, appropriately, keys() and values()\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Accessing the dictionary keys:\nlatin_names = species_dict.keys()\nprint(f\"Dictionary keys (latin names): {latin_names}\")\n\n\nDictionary keys (latin names): dict_keys(['M californianus', 'H rufescens', 'D imbricata'])\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Accessing the dictionary values\ncommon_names = species_dict.values()\nprint(f\"Dictionary values (common names): {common_names}\")\n\n\nDictionary values (common names): dict_values(['California mussel', 'Red abalone', 'Leather star'])\n\n\n\n\n\n\n\n\nNote\n\n\n\n🐍 Note. The keys() and values() functions return a dict_key object and dict_values object, respectively. Each of these objects contains a list of either the keys or values. You can force the result of the keys() or values() function into a list by wrapping either one in a list() command."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#looping-through-dictionaries",
    "href": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#looping-through-dictionaries",
    "title": "Interactive Session 2B",
    "section": "Looping through Dictionaries ",
    "text": "Looping through Dictionaries \nPython has an efficient way to loop through all the keys and values of a dictionary at the same time. The items() method returns a tuple containing a (key, value) for each element in a dictionary. In practice this means that we can loop through a dictionary in the following way:\n\n\nCode\nmy_dict = {'name': 'Homer Simpson',\n           'occupation': 'nuclear engineer',\n           'address': '742 Evergreen Terrace',\n           'city': 'Springfield',\n           'state': ' ? '\n          }\n\nfor key, value in my_dict.items():\n    print(f\"{key.capitalize()}: {value}.\")\n\n\nName: Homer Simpson.\nOccupation: nuclear engineer.\nAddress: 742 Evergreen Terrace.\nCity: Springfield.\nState:  ? .\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\nAdd a new code cell and code to loop through the species_dict dictionary and print out a sentence providing the common name of each species (e.g. “The common name of M californianus” is…“)."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#accessing-un-assigned-elements-in-dictionaries",
    "href": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#accessing-un-assigned-elements-in-dictionaries",
    "title": "Interactive Session 2B",
    "section": "Accessing un-assigned elements in Dictionaries",
    "text": "Accessing un-assigned elements in Dictionaries\nAttempting to retrieve an element of a dictionary that doesn’t exist is the same as requesting an index of a list that doesn’t exist - Python will raise an Exception. For example, if you attempt to retrieve the definition of a field that hasn’t been defined, then you get an error:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\nspecies_dict[\"E dofleini\"]\nYou should get a KeyError exception:\nKeyError: ‘E dofleini’\nTo avoid getting an error when requesting an element from a dict, you can use the get() function. The get() function will return None if the element doesn’t exist:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nspecies_description = species_dict.get(\"E dofleini\")\nprint(\"Accessing non-existent latin name, E dofleini:\\n\", species_description)\n\n\nAccessing non-existent latin name, E dofleini:\n None\n\n\nYou can also provide an argument to python to return if the item isn’t found:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nspecies_description = species_dict.get(\"E dofleini\", \"Species not found in dictionary\")\nprint(\"Accessing non-existent latin name, E dofleini:\\n\", species_description)\n\n\nAccessing non-existent latin name, E dofleini:\n Species not found in dictionary"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#summary-and-additional-resources",
    "href": "example_course/course-materials/interactive-sessions/2b_dictionaries.html#summary-and-additional-resources",
    "title": "Interactive Session 2B",
    "section": "Summary and Additional Resources",
    "text": "Summary and Additional Resources\nWe’ve explored the creation, modification, and application of dictionaries in Python, highlighting their utility in storing structured data. As you progress in Python, you’ll find dictionaries indispensable across various applications, from data analysis to machine learning.\nFor further study, consult the following resources: - Python’s Official Documentation on Dictionaries - Our class Dictionary Cheatsheet\n\n\nEnd interactive session 2B"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/7c_visualizations.html",
    "href": "example_course/course-materials/coding-colabs/7c_visualizations.html",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, you’ll work with a partner to explore a dataset using the seaborn library. You’ll focus on a workflow that includes:\n\nExploring distributions with histograms\nExamining correlations among variables\nInvestigating relationships more closely with regression plots and joint distribution plots\n\nWe’ll be using the Palmer Penguins dataset, which contains information about different penguin species, their physical characteristics, and the islands they inhabit."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/7c_visualizations.html#introduction",
    "href": "example_course/course-materials/coding-colabs/7c_visualizations.html#introduction",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, you’ll work with a partner to explore a dataset using the seaborn library. You’ll focus on a workflow that includes:\n\nExploring distributions with histograms\nExamining correlations among variables\nInvestigating relationships more closely with regression plots and joint distribution plots\n\nWe’ll be using the Palmer Penguins dataset, which contains information about different penguin species, their physical characteristics, and the islands they inhabit."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/7c_visualizations.html#setup",
    "href": "example_course/course-materials/coding-colabs/7c_visualizations.html#setup",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries and load our dataset.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set the style for better-looking plots\nsns.set_style(\"whitegrid\")\n\n# Load the Palmer Penguins dataset\npenguins = sns.load_dataset(\"penguins\")\n\n# Display the first few rows and basic information about the dataset\nprint(penguins.head())\nprint(penguins.info())\n\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen            39.1           18.7              181.0   \n1  Adelie  Torgersen            39.5           17.4              186.0   \n2  Adelie  Torgersen            40.3           18.0              195.0   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen            36.7           19.3              193.0   \n\n   body_mass_g     sex  \n0       3750.0    Male  \n1       3800.0  Female  \n2       3250.0  Female  \n3          NaN     NaN  \n4       3450.0  Female  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\nNone"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/7c_visualizations.html#task-1-exploring-distributions-with-histograms",
    "href": "example_course/course-materials/coding-colabs/7c_visualizations.html#task-1-exploring-distributions-with-histograms",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Task 1: Exploring Distributions with Histograms",
    "text": "Task 1: Exploring Distributions with Histograms\nLet’s start by exploring the distributions of various numerical variables in our dataset using histograms.\n\nCreate histograms for ‘bill_length_mm’, ‘bill_depth_mm’, ‘flipper_length_mm’, and ‘body_mass_g’.\nExperiment with different numbers of bins to see how it affects the visualization.\nTry using sns.histplot() with the ‘kde’ parameter set to True to overlay a kernel density estimate."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/7c_visualizations.html#task-2-examining-correlations",
    "href": "example_course/course-materials/coding-colabs/7c_visualizations.html#task-2-examining-correlations",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Task 2: Examining Correlations",
    "text": "Task 2: Examining Correlations\nNow, let’s look at the correlations between the numerical variables in our dataset using Seaborn’s built-in correlation plot.\n\nUse sns.pairplot() to create a grid of scatter plots for all numeric variables.\nModify the pairplot to show the species information using different colors.\nInterpret the pairplot: which variables seem to be most strongly correlated? Do you notice any patterns related to species?"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/7c_visualizations.html#task-3-investigating-relationships-with-regression-plots",
    "href": "example_course/course-materials/coding-colabs/7c_visualizations.html#task-3-investigating-relationships-with-regression-plots",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Task 3: Investigating Relationships with Regression Plots",
    "text": "Task 3: Investigating Relationships with Regression Plots\nLet’s dig deeper into the relationships between variables using regression plots.\n\nCreate a regression plot (sns.regplot) showing the relationship between ‘flipper_length_mm’ and ‘body_mass_g’.\nCreate another regplot showing the relationship between ‘bill_length_mm’ and ‘bill_depth_mm’.\nTry adding the ‘species’ information to one of these plots using different colors. Hint: You might want to use sns.lmplot for this."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/7c_visualizations.html#task-4-joint-distribution-plots",
    "href": "example_course/course-materials/coding-colabs/7c_visualizations.html#task-4-joint-distribution-plots",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Task 4: Joint Distribution Plots",
    "text": "Task 4: Joint Distribution Plots\nFinally, let’s use joint distribution plots to examine both the relationship between two variables and their individual distributions.\n\nCreate a joint plot for ‘flipper_length_mm’ and ‘body_mass_g’.\nExperiment with different kind parameters in the joint plot (e.g., ‘scatter’, ‘kde’, ‘hex’).\nCreate another joint plot, this time for ‘bill_length_mm’ and ‘bill_depth_mm’, colored by species."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/7c_visualizations.html#bonus-challenge",
    "href": "example_course/course-materials/coding-colabs/7c_visualizations.html#bonus-challenge",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Bonus Challenge",
    "text": "Bonus Challenge\nIf you finish early, try this bonus challenge:\nCreate a correlation matrix heatmap using Seaborn’s sns.heatmap() function. This will provide a different view of the correlations between variables compared to the pairplot.\n\nCreate a correlation matrix using the numerical columns in the dataset.\n\n\n\n\n\n\n\nCreating correlation matricies in pandas\n\n\n\nPandas dataframes include two built-in methods that can be combined to quickly create a correlation matrix between all the numerical data in a dataframe.\n\n.select_dtypes() is a method that selects only the columns of a dataframe that match a type of data. Running the .select_dtypes(include=np.number) method on a dataframe will return a new dataframe that contains only the columns that have a numeric datatype.\n.corr() is a method that creates a correlation matrix between every column in a dataframe. For it to work, you need to make sure you only have numeric data in your dataframe, so chaining this method after the .select_dtypes() method will get you a complete correlation matrix in a single line of code!\n\n\n\n\nVisualize this correlation matrix using sns.heatmap().\nCustomize the heatmap by adding annotations and adjusting the colormap.\nCompare the insights from this heatmap with those from the pairplot. What additional information does each visualization provide?"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/7c_visualizations.html#conclusion",
    "href": "example_course/course-materials/coding-colabs/7c_visualizations.html#conclusion",
    "title": "Day 7: 🙌 Coding Colab",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve practiced using seaborn to explore a dataset through various visualization techniques. Often these visualizations can be very helpful at the start of a data exploration activity as they are fundamental to exploratory data analysis in Python. As such, they will be valuable as you continue to work with more complex datasets.\n\nEnd Coding Colab Session (Day 7)"
  },
  {
    "objectID": "example_course/course-materials/lectures/seaborn.html#philosophy-of-seaborn",
    "href": "example_course/course-materials/lectures/seaborn.html#philosophy-of-seaborn",
    "title": "Introduction to Seaborn",
    "section": "Philosophy of Seaborn",
    "text": "Philosophy of Seaborn\nSeaborn aims to make visualization a central part of exploring and understanding data.\nIts dataset-oriented plotting functions operate on dataframes and arrays containing whole datasets.\nIt tries to automatically perform semantic mapping and statistical aggregation to produce informative plots."
  },
  {
    "objectID": "example_course/course-materials/lectures/seaborn.html#main-ideas-in-seaborn",
    "href": "example_course/course-materials/lectures/seaborn.html#main-ideas-in-seaborn",
    "title": "Introduction to Seaborn",
    "section": "Main Ideas in Seaborn",
    "text": "Main Ideas in Seaborn\n\nIntegration with Pandas: Works well with Pandas data structures.\nBuilt-in Themes: Provides built-in themes for styling matplotlib graphics.\nColor Palettes: Offers a variety of color palettes to reveal patterns in the data.\nStatistical Estimation: Seaborn includes functions to fit and visualize linear regression models."
  },
  {
    "objectID": "example_course/course-materials/lectures/seaborn.html#major-features-of-seaborn",
    "href": "example_course/course-materials/lectures/seaborn.html#major-features-of-seaborn",
    "title": "Introduction to Seaborn",
    "section": "Major Features of Seaborn",
    "text": "Major Features of Seaborn\nSeaborn simplifies many aspects of creating complex visualizations in Python. Some of its major features include:\n\nFacetGrids and PairGrids: For plotting conditional relationships.\nFactorplot: For categorical variables.\nJointplot: For joint distributions.\nTime Series functionality: Through functions like tsplot."
  },
  {
    "objectID": "example_course/course-materials/lectures/seaborn.html#using-seaborn",
    "href": "example_course/course-materials/lectures/seaborn.html#using-seaborn",
    "title": "Introduction to Seaborn",
    "section": "Using seaborn",
    "text": "Using seaborn\nimport seaborn as sns\nWhy sns?"
  },
  {
    "objectID": "example_course/course-materials/lectures/seaborn.html#theme-options",
    "href": "example_course/course-materials/lectures/seaborn.html#theme-options",
    "title": "Introduction to Seaborn",
    "section": "Theme Options",
    "text": "Theme Options\n# Set the theme to whitegrid\nsns.set_theme(style=\"whitegrid\")\n\ndarkgrid: The default theme. Background is a dark gray grid (not to be confused with a solid gray).\nwhitegrid: Similar to darkgrid but with a lighter background. This theme is particularly useful for plots with dense data points."
  },
  {
    "objectID": "example_course/course-materials/lectures/seaborn.html#themes-continued",
    "href": "example_course/course-materials/lectures/seaborn.html#themes-continued",
    "title": "Introduction to Seaborn",
    "section": "Themes (continued)",
    "text": "Themes (continued)\n\ndark: This theme provides a dark background without any grid lines. It’s suitable for presentations or where visuals are prioritized.\nwhite: Offers a clean, white background without grid lines. This is well in situations where the data and annotations need to stand out without any additional distraction.\nticks: This theme is similar to the white theme but adds ticks on the axes, which enhances the precision of interpreting the data."
  },
  {
    "objectID": "example_course/course-materials/lectures/seaborn.html#getting-ready-to-seaborn",
    "href": "example_course/course-materials/lectures/seaborn.html#getting-ready-to-seaborn",
    "title": "Introduction to Seaborn",
    "section": "Getting ready to Seaborn",
    "text": "Getting ready to Seaborn\nImport the library and set a style\n\nimport seaborn as sns # (but now you know it should have been ssn 🤓)\nsns.set(style=\"darkgrid\") # This is the default, so skip it if wanted"
  },
  {
    "objectID": "example_course/course-materials/lectures/seaborn.html#conclusion",
    "href": "example_course/course-materials/lectures/seaborn.html#conclusion",
    "title": "Introduction to Seaborn",
    "section": "Conclusion",
    "text": "Conclusion\nSeaborn is a versatile and powerful tool for statistical data visualization in Python. Whether you need to visualize the distribution of a dataset, the relationship between multiple variables, or the dependencies between categorical data, Seaborn has a plot type to make your analysis more intuitive and insightful."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3d_pandas_series.html",
    "href": "example_course/course-materials/coding-colabs/3d_pandas_series.html",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, we’ll explore Pandas Series, a fundamental data structure in the Pandas library. You’ll work together to create, manipulate, and analyze Series objects."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3d_pandas_series.html#introduction",
    "href": "example_course/course-materials/coding-colabs/3d_pandas_series.html#introduction",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "",
    "text": "In this collaborative coding exercise, we’ll explore Pandas Series, a fundamental data structure in the Pandas library. You’ll work together to create, manipulate, and analyze Series objects."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3d_pandas_series.html#resources",
    "href": "example_course/course-materials/coding-colabs/3d_pandas_series.html#resources",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Resources",
    "text": "Resources\nHere’s our course cheatsheet on pandas Series:\n\nPandas Series Cheatsheet\n\nFeel free to refer to this cheatsheet throughout the exercise if you need a quick reminder about syntax or functionality."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3d_pandas_series.html#setup",
    "href": "example_course/course-materials/coding-colabs/3d_pandas_series.html#setup",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries and create a sample Series.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Create a sample Series\nfruits = pd.Series(['apple', 'banana', 'cherry', 'date', 'elderberry'], name='Fruits')\nprint(fruits)\n\n\n0         apple\n1        banana\n2        cherry\n3          date\n4    elderberry\nName: Fruits, dtype: object"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3d_pandas_series.html#exercise-1-creating-a-series",
    "href": "example_course/course-materials/coding-colabs/3d_pandas_series.html#exercise-1-creating-a-series",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Exercise 1: Creating a Series",
    "text": "Exercise 1: Creating a Series\nWork together to create a Series representing the prices of the fruits in our fruits Series.\n\n\nCode\n# Your code here\n# Create a Series called 'prices' with the same index as 'fruits'\n# Use these prices: apple: $0.5, banana: $0.3, cherry: $1.0, date: $1.5, elderberry: $2.0"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3d_pandas_series.html#exercise-2-series-operations",
    "href": "example_course/course-materials/coding-colabs/3d_pandas_series.html#exercise-2-series-operations",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Exercise 2: Series Operations",
    "text": "Exercise 2: Series Operations\nCollaborate to perform the following operations:\n\nCalculate the total price of all fruits.\nFind the most expensive fruit.\nApply a 10% discount to all fruits priced over $1.0.\n\n\n\nCode\n# Your code here\n# 1. Calculate the total price of all fruits\n# 2. Find the most expensive fruit\n# 3. Apply a 10% discount to all fruits priced over $1.0"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3d_pandas_series.html#exercise-3-series-analysis",
    "href": "example_course/course-materials/coding-colabs/3d_pandas_series.html#exercise-3-series-analysis",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Exercise 3: Series Analysis",
    "text": "Exercise 3: Series Analysis\nWork as a team to answer the following questions:\n\nWhat is the average price of the fruits?\nHow many fruits cost less than $1.0?\nWhat is the price range (difference between max and min prices)?\n\n\n\nCode\n# Your code here\n# 1. Calculate the average price of the fruits\n# 2. Count how many fruits cost less than $1.0\n# 3. Calculate the price range (difference between max and min prices)"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3d_pandas_series.html#exercise-4-series-manipulation",
    "href": "example_course/course-materials/coding-colabs/3d_pandas_series.html#exercise-4-series-manipulation",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Exercise 4: Series Manipulation",
    "text": "Exercise 4: Series Manipulation\nCollaborate to perform these manipulations on the fruits and prices Series:\n\nAdd a new fruit ‘fig’ with a price of $1.2 to both Series using pd.concat\nRemove ‘banana’ from both Series.\nSort both Series by fruit name (alphabetically).\n\n\n\nCode\n# Your code here\n# 1. Add 'fig' to both Series (price: $1.2)\n# 2. Remove 'banana' from both Series\n# 3. Sort both Series alphabetically by fruit name"
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3d_pandas_series.html#conclusion",
    "href": "example_course/course-materials/coding-colabs/3d_pandas_series.html#conclusion",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Conclusion",
    "text": "Conclusion\nIn this collaborative exercise, you’ve practiced creating, manipulating, and analyzing Pandas Series. You’ve learned how to perform basic operations, apply conditions, and modify Series objects. These skills will be valuable as you work with more complex datasets in the future."
  },
  {
    "objectID": "example_course/course-materials/coding-colabs/3d_pandas_series.html#discussion-questions",
    "href": "example_course/course-materials/coding-colabs/3d_pandas_series.html#discussion-questions",
    "title": "Day 3: 🙌 Coding Colab",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\nWhat advantages does using a Pandas Series offer compared to using a Python list or dictionary?\nCan you think of a real-world scenario where you might use a Pandas Series instead of a DataFrame?\nWhat challenges did you face while working with Series in this exercise, and how did you overcome them?\n\nDiscuss these questions with your team and share your insights.\n\nEnd Coding Colab Session (Day 4)"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/comprehensions.html",
    "href": "example_course/course-materials/cheatsheets/comprehensions.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This cheatsheet provides a quick reference for using comprehensions in Python, including list comprehensions, dictionary comprehensions, and how to incorporate conditional logic. Use this as a guide during your master’s program to write more concise and readable code."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/comprehensions.html#list-comprehensions",
    "href": "example_course/course-materials/cheatsheets/comprehensions.html#list-comprehensions",
    "title": "EDS 217 Cheatsheet",
    "section": "List Comprehensions",
    "text": "List Comprehensions\n\nBasic Syntax\nA list comprehension provides a concise way to create lists. The basic syntax is:\n\n\nCode\n# [expression for item in iterable]\nsquares = [i ** 2 for i in range(1, 6)]\nprint(squares)\n\n\n[1, 4, 9, 16, 25]\n\n\n\n\nWith Conditional Logic\nYou can add a condition to include only certain items in the new list:\n\n\nCode\n# [expression for item in iterable if condition]\neven_squares = [i ** 2 for i in range(1, 6) if i % 2 == 0]\nprint(even_squares)\n\n\n[4, 16]\n\n\n\n\nNested List Comprehensions\nList comprehensions can be nested to handle more complex data structures:\n\n\nCode\n# [(expression1, expression2) for item1 in iterable1 for item2 in iterable2]\npairs = [(i, j) for i in range(1, 4) for j in range(1, 3)]\nprint(pairs)\n\n\n[(1, 1), (1, 2), (2, 1), (2, 2), (3, 1), (3, 2)]\n\n\n\n\nEvaluating Functions in a List Comprehension\nYou can use list comprehensions to apply a function to each item in an iterable:\n\n\nCode\n# Function to evaluate\ndef square(x):\n    return x ** 2\n\n# List comprehension applying the function\nsquares = [square(i) for i in range(1, 6)]\nprint(squares)\n\n\n[1, 4, 9, 16, 25]"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/comprehensions.html#dictionary-comprehensions",
    "href": "example_course/course-materials/cheatsheets/comprehensions.html#dictionary-comprehensions",
    "title": "EDS 217 Cheatsheet",
    "section": "Dictionary Comprehensions",
    "text": "Dictionary Comprehensions\n\nBasic Syntax\nDictionary comprehensions provide a concise way to create dictionaries. The basic syntax is:\n\n\nCode\n# {key_expression: value_expression for item in iterable}\n# Example: Mapping fruit names to their lengths\nfruits = ['apple', 'banana', 'cherry']\nfruit_lengths = {fruit: len(fruit) for fruit in fruits}\nprint(fruit_lengths)\n\n\n{'apple': 5, 'banana': 6, 'cherry': 6}\n\n\n\n\nWithout zip\nYou can create a dictionary without using zip by leveraging the index:\n\n\nCode\n# {key_expression: value_expression for index in range(len(list))}\n# Example: Mapping employee IDs to names\nemployee_ids = [101, 102, 103]\nemployee_names = ['Alice', 'Bob', 'Charlie']\nid_to_name = {employee_ids[i]: employee_names[i] for i in range(len(employee_ids))}\nprint(id_to_name)\n\n\n{101: 'Alice', 102: 'Bob', 103: 'Charlie'}\n\n\n\n\nWith Conditional Logic\nYou can include conditions to filter out key-value pairs:\n\n\nCode\n# {key_expression: value_expression for item in iterable if condition}\n# Example: Filtering students who passed\nstudents = ['Alice', 'Bob', 'Charlie']\nscores = [85, 62, 90]\npassing_students = {students[i]: scores[i] for i in range(len(students)) if scores[i] &gt;= 70}\nprint(passing_students)\n\n\n{'Alice': 85, 'Charlie': 90}\n\n\n\n\nEvaluating Functions in a Dictionary Comprehension\nYou can use dictionary comprehensions to apply a function to values in an iterable:\n\n\nCode\n# Function to evaluate\ndef capitalize_name(name):\n    return name.upper()\n\n# Example: Mapping student names to capitalized names\nstudents = ['alice', 'bob', 'charlie']\ncapitalized_names = {name: capitalize_name(name) for name in students}\nprint(capitalized_names)\n\n\n{'alice': 'ALICE', 'bob': 'BOB', 'charlie': 'CHARLIE'}"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/comprehensions.html#best-practices-for-using-comprehensions",
    "href": "example_course/course-materials/cheatsheets/comprehensions.html#best-practices-for-using-comprehensions",
    "title": "EDS 217 Cheatsheet",
    "section": "Best Practices for Using Comprehensions",
    "text": "Best Practices for Using Comprehensions\n\nKeep It Simple: Use comprehensions for simple transformations and filtering. For complex logic, consider using traditional loops for better readability.\nNested Comprehensions: While powerful, nested comprehensions can be hard to read. Use them sparingly and consider breaking down the logic into multiple steps if needed.\nReadability: Always prioritize code readability. If a comprehension is difficult to understand, it might be better to use a loop."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/comprehensions.html#additional-resources",
    "href": "example_course/course-materials/cheatsheets/comprehensions.html#additional-resources",
    "title": "EDS 217 Cheatsheet",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nOfficial Python Documentation: List Comprehensions\nPython Dictionary Comprehensions: Dictionary Comprehensions\nPEP 202: PEP 202 - List Comprehensions"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/8a_github.html",
    "href": "example_course/course-materials/interactive-sessions/8a_github.html",
    "title": "Interactive Session",
    "section": "",
    "text": "This session contains detailed instructions for creating a new GitHub repository and pushing your EDS 217 course work to it. It also covers how to clean your Jupyter notebooks before committing them to ensure your repository is clean and professional.\n\n\n\n\n\n\nWarning\n\n\n\nJupyter notebooks files can be hard to use in github because they contain information about code execution order and output in the file. For this reason, you should clean notebooks before pushing them to a github repo. We will clean them using the nbstripout python package"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/8a_github.html#introduction",
    "href": "example_course/course-materials/interactive-sessions/8a_github.html#introduction",
    "title": "Interactive Session",
    "section": "",
    "text": "This session contains detailed instructions for creating a new GitHub repository and pushing your EDS 217 course work to it. It also covers how to clean your Jupyter notebooks before committing them to ensure your repository is clean and professional.\n\n\n\n\n\n\nWarning\n\n\n\nJupyter notebooks files can be hard to use in github because they contain information about code execution order and output in the file. For this reason, you should clean notebooks before pushing them to a github repo. We will clean them using the nbstripout python package"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/8a_github.html#steps-to-setting-up-a-github-repo-for-your-coursework",
    "href": "example_course/course-materials/interactive-sessions/8a_github.html#steps-to-setting-up-a-github-repo-for-your-coursework",
    "title": "Interactive Session",
    "section": "Steps to Setting up a GitHub repo for your coursework:",
    "text": "Steps to Setting up a GitHub repo for your coursework:\n\nCreate a new repository on GitHub\nInitialize a local Git repository\nClean Jupyter notebooks of output and execution data\nAdd, commit, and push files to a GitHub repository"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/8a_github.html#creating-a-new-github-repository",
    "href": "example_course/course-materials/interactive-sessions/8a_github.html#creating-a-new-github-repository",
    "title": "Interactive Session",
    "section": "Creating a New GitHub Repository",
    "text": "Creating a New GitHub Repository\nLet’s start by creating a new repository on GitHub:\n\nLog in to your GitHub account\nClick the ‘+’ icon in the top-right corner and select ‘New repository’\nName your repository (e.g., “EDS-217-Course-Work”)\nAdd a description (optional)\nChoose to make the repository public or private\nDon’t initialize the repository with a README, .gitignore, or license\nClick ‘Create repository’\n\nAfter creating the repository, you’ll see a page with instructions. We’ll use these in the next steps."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/8a_github.html#initializing-a-local-git-repository",
    "href": "example_course/course-materials/interactive-sessions/8a_github.html#initializing-a-local-git-repository",
    "title": "Interactive Session",
    "section": "Initializing a Local Git Repository",
    "text": "Initializing a Local Git Repository\nNow, let’s set up your local directory as a Git repository:\n\nOpen a terminal on the class workbench server\nNavigate to your course work directory:\n\n\n\nCode\ncd path/to/your/EDS-217\n\n\n\nInitialize the repository:\n\n\n\nCode\ngit init\n\n\n\nAdd your GitHub repository as the remote origin:\n\n\n\nCode\ngit remote add origin https://github.com/your-username/EDS-217-Course-Work.git\n\n\nReplace your-username with your actual GitHub username."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/8a_github.html#cleaning-jupyter-notebooks",
    "href": "example_course/course-materials/interactive-sessions/8a_github.html#cleaning-jupyter-notebooks",
    "title": "Interactive Session",
    "section": "Cleaning Jupyter Notebooks",
    "text": "Cleaning Jupyter Notebooks\nBefore we commit our notebooks, let’s clean them to remove output cells and execution data:\n\nInstall the nbstripout tool if you haven’t already:\n\n\n\nCode\npip install nbstripout\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBy default, the server won’t add a new python package to the main package repository on workbench. For this reason, you will see a warning when running the pip command that looks something like this:\nWARNING: The script nbstripout is installed in '/Users/[your user id]/.local/bin' which is not on PATH\n\n\nTherefore, we need to access the nbsripout command by specifying it’s location in your local user folder:\n\nConfigure nbstripout for your repository:\n\n\n\nCode\n~/.local/bin/nbstripout --install --attributes .gitattributes\n\n\nThis sets up nbstripout to automatically clean your notebooks when you commit them."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/8a_github.html#adding-committing-and-pushing-files",
    "href": "example_course/course-materials/interactive-sessions/8a_github.html#adding-committing-and-pushing-files",
    "title": "Interactive Session",
    "section": "Adding, Committing, and Pushing Files",
    "text": "Adding, Committing, and Pushing Files\nNow we’re ready to add our files to the repository:\n\nAdd all files in the directory:\n\n\n\nCode\ngit add .\n\n\n\nCommit the files:\n\n\n\nCode\ngit commit -m \"Initial commit: Adding EDS 217 course work\"\n\n\n\nPush the files to GitHub:\n\n\n\nCode\ngit push -u origin main\n\n\nNote: If your default branch is named “master” instead of “main”, use git push -u origin master."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/8a_github.html#verifying-your-repository",
    "href": "example_course/course-materials/interactive-sessions/8a_github.html#verifying-your-repository",
    "title": "Interactive Session",
    "section": "Verifying Your Repository",
    "text": "Verifying Your Repository\n\nGo to your GitHub repository page in your web browser\nRefresh the page\nYou should now see all your course files listed in the repository"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/8a_github.html#conclusion",
    "href": "example_course/course-materials/interactive-sessions/8a_github.html#conclusion",
    "title": "Interactive Session",
    "section": "Conclusion",
    "text": "Conclusion\nCongratulations! You’ve successfully created a GitHub repository for your EDS 217 course work, cleaned your Jupyter notebooks, and pushed your files to GitHub. This process helps you maintain a clean, professional repository of your work that you can easily share or refer back to in the future."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/8a_github.html#additional-resources",
    "href": "example_course/course-materials/interactive-sessions/8a_github.html#additional-resources",
    "title": "Interactive Session",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGitHub Docs: Creating a new repository\nGit Documentation\nnbstripout Documentation"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1c_variables_strings.html",
    "href": "example_course/course-materials/interactive-sessions/1c_variables_strings.html",
    "title": "Interactive Session 1C",
    "section": "",
    "text": "Understanding the types of variables and the methods available for different objects is crucial for effective programming in Python. This guide will walk you through how to determine the type of a variable and explore the methods you can use with various objects."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#getting-started",
    "title": "Interactive Session 1C",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab (see the note below if you have trouble!)\nSelect “Rename”\nName your notebook with the format: Session_1C_Variables_and_Strings.ipynb\n\n\n\n\n\n\n\n\nRight-clicking in JupyterLab\n\n\n\nSome browsers and operating system combinations will not conceded right-clicking to the JupyterLab interface and will show a system menu when you try to right click. In those cases, usually CTRL-Right Click or OPTION-Right Click will bring up the Jupyter menu.\n\n\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day 1: Session C - Variables & Strings\n\n[Session Webpage](https://eds-217-essential-python.github.io/course-materials/interactive-sessions/1c_variables_strings.html)\n\nDate: 09/03/2024\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#variable-types",
    "href": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#variable-types",
    "title": "Interactive Session 1C",
    "section": "Variable Types",
    "text": "Variable Types\nIn Python, everything is an object. Each object has a specific type, and knowing the type of a variable helps you understand what operations and methods you can perform on it.\n\nDetermining Variable Type\nTo find out the type of a variable, you can use the type() function. This function returns the type of the object passed to it.\n\n\nCode\n# Define some variables\nnumber = 42\ntext = \"Hello, World!\"\npi = 3.14159\ndata = [1, 2, 3, 4, 5]\n\n# Determine the type of each variable\nprint(type(number))\nprint(type(text))\nprint(type(pi))\nprint(type(data))\n\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n&lt;class 'float'&gt;\n&lt;class 'list'&gt;\n\n\n\n\nBuilt-in Types\nHere are some common built-in types in Python:\n\nint: Represents integers.\nfloat: Represents floating-point numbers.\nstr: Represents strings.\nlist: Represents lists, which are ordered collections of items."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#exploring-methods",
    "href": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#exploring-methods",
    "title": "Interactive Session 1C",
    "section": "Exploring Methods",
    "text": "Exploring Methods\nOnce you know the type of an object, you can discover the methods available for that object. Methods are functions that belong to an object and can be used to perform operations on the data contained within the object.\n\nDiscovering Methods\nYou can use the dir() function to list all the attributes and methods available for an object. This function returns a list of the object’s attributes and methods, including special methods (also known as “dunder” methods) that begin and end with double underscores.\n\n\nCode\n# List all methods and attributes of a string object\nstring_methods = dir(text)\nprint(string_methods)\n\n\n['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n\n\n\n\nFiltering User-Facing Methods\nWhile dir() lists all methods, you often only need user-facing methods. You can filter out special methods by ignoring those with double underscores.\n\n\nCode\n# Filter out special methods\nuser_methods = [method for method in dir(text) if not method.startswith('__')]\nprint(user_methods)\n\n\n['capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#using-methods",
    "href": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#using-methods",
    "title": "Interactive Session 1C",
    "section": "Using Methods",
    "text": "Using Methods\nNow that you’ve discovered the methods available for an object, let’s see how to use them. Here are some examples with strings and lists.\n\nString Methods\nStrings in Python have various methods for text manipulation. Let’s look at a few commonly used methods.\n\nExample: upper(), lower(), and replace(), strip()\n\n\nCode\ntext = \" Hello, World!  \"\n\n# Convert to uppercase\nprint(text.upper())\n\n# Convert to lowercase\nprint(text.lower())\n\n# Replace a substring\nprint(text.replace(\"World\", \"Python\"))\n\n# Remove leading and trailing whitespace, including tabs and newlines\nprint(text.strip())\n\n\n HELLO, WORLD!  \n hello, world!  \n Hello, Python!  \nHello, World!\n\n\n\n\nAdvanced Example: String Normalization\nIn this section, we’ll explore how to normalize strings using built-in Python string methods. String normalization is a common task in data processing and can involve operations like converting text to lowercase, removing extra whitespace, and replacing spaces with other characters. For example, in R, the janitor package is often used to normalize dataframe columns names.\n\nBasic String Normalization\nLet’s start with a simple example of string normalization:\n\n\nCode\n# A string with mixed case and extra spaces\noriginal = \"  Hello, World!  \"\n\n# Normalize the string using the .strip() method to remove whitespace \n# and the .lower() method to convert to lowercase\nnormalized = original.strip().lower()\n\nprint(f\"Original: '{original}'\")\nprint(f\"Normalized: '{normalized}'\")\n\n\nOriginal: '  Hello, World!  '\nNormalized: 'hello, world!'\n\n\nIn this example, we used two string methods: - strip(): Removes leading and trailing whitespace - lower(): Converts the string to lowercase\n\n\n\nAdvanced String Normalization\nNow, let’s look at a more comprehensive approach to string normalization:\n\n\nCode\n# Examples of strings to normalize\nexamples = [\n    \"Hello World\",\n    \"  Python Programming  \",\n    \"STRING NORMALIZATION\\n\",\n    \"  Trim  Spaces  \",\n    \"\\tTabs and Newlines\\n\"\n]\n\nprint(\"String Normalization Examples:\")\nprint(\"==============================\")\n\nfor original in examples:\n    # Normalize the string:\n    # 1. Remove leading/trailing whitespace and newlines\n    # 2. Convert to lowercase\n    # 3. Replace internal whitespace with underscores\n    normalized = original.strip().lower().replace(\" \", \"_\")\n    \n    print(f\"Original: '{original}'\")\n    print(f\"Normalized: '{normalized}'\")\n    print()\n\n\nString Normalization Examples:\n==============================\nOriginal: 'Hello World'\nNormalized: 'hello_world'\n\nOriginal: '  Python Programming  '\nNormalized: 'python_programming'\n\nOriginal: 'STRING NORMALIZATION\n'\nNormalized: 'string_normalization'\n\nOriginal: '  Trim  Spaces  '\nNormalized: 'trim__spaces'\n\nOriginal: ' Tabs and Newlines\n'\nNormalized: 'tabs_and_newlines'\n\n\n\nIn this advanced example, we:\n\nStart with strip() to remove all leading and trailing whitespace, including spaces, tabs, and newlines.\nUse lower() to convert the string to lowercase.\nFinally, apply replace(\" \", \"_\") to substitute internal spaces with underscores.\n\n\n\nExercise\nNow it’s your turn to practice! Modify the code below to normalize the given strings according to these rules:\n\nRemove leading and trailing whitespace\nConvert to uppercase (instead of lowercase)\nReplace spaces with hyphens\n\n\n\nCode\nstrings_to_normalize = [\n    \"data science\",\n    \"  MACHINE learning \",\n    \"Artificial Intelligence\\t\",\n    \" Natural\\nLanguage Processing \"\n]\n\nprint(\"Your Normalized Strings:\")\nprint(\"========================\")\n\nfor string in strings_to_normalize:\n    # Your normalization code here\n    normalized = string  # Replace this line with your normalization steps!\n    \n    print(f\"Original: '{string}'\")\n    print(f\"Normalized: '{normalized}'\")\n    print()\n\n\nYour Normalized Strings:\n========================\nOriginal: 'data science'\nNormalized: 'data science'\n\nOriginal: '  MACHINE learning '\nNormalized: '  MACHINE learning '\n\nOriginal: 'Artificial Intelligence  '\nNormalized: 'Artificial Intelligence    '\n\nOriginal: ' Natural\nLanguage Processing '\nNormalized: ' Natural\nLanguage Processing '\n\n\n\nTry experimenting with different normalization techniques or adding your own strings to the list!\n\n\n\nList Methods\nLists also have several useful methods for data manipulation. Here are some examples.\n\nExample: append(), remove(), and sort()\n\n\nCode\nnumbers = [3, 1, 4, 1, 5, 9]\n\n# Append an item\nnumbers.append(2)\nprint(numbers)\n\n# Remove an item\nnumbers.remove(1)  # Removes the first occurrence of 1\nprint(numbers)\n\n# Sort the list\nnumbers.sort()\nprint(numbers)\n\n\n[3, 1, 4, 1, 5, 9, 2]\n[3, 4, 1, 5, 9, 2]\n[1, 2, 3, 4, 5, 9]"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#special-methods",
    "href": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#special-methods",
    "title": "Interactive Session 1C",
    "section": "Special Methods",
    "text": "Special Methods\nSpecial methods, also known as “dunder” (double underscore) methods, allow you to define the behavior of your objects for built-in operations. For example, __init__ is used for initializing objects, and __str__ defines the string representation.\nWhile important, these are generally more advanced and used in object-oriented programming, so we will not focus on them in this introductory course."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#conclusion",
    "href": "example_course/course-materials/interactive-sessions/1c_variables_strings.html#conclusion",
    "title": "Interactive Session 1C",
    "section": "Conclusion",
    "text": "Conclusion\nThis guide has introduced you to determining variable types and exploring the methods available for different objects in Python. By understanding how to discover and use methods, you’ll be better equipped to manipulate data and build powerful programs.\nFeel free to experiment with the code examples interactively in your Jupyter notebook to deepen your understanding.\n\nEnd interactive session 1C"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_grouping.html",
    "href": "example_course/course-materials/cheatsheets/data_grouping.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Grouping data allows you to split your DataFrame into groups based on one or more columns.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'B', 'A'],\n    'value': [1, 2, 3, 4, 5]\n})\nprint(df)\n\n\n  category  value\n0        A      1\n1        B      2\n2        A      3\n3        B      4\n4        A      5\n\n\n\n\n\n\nCode\n# Group by 'category'\ngrouped = df.groupby('category')"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_grouping.html#grouping-data",
    "href": "example_course/course-materials/cheatsheets/data_grouping.html#grouping-data",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Grouping data allows you to split your DataFrame into groups based on one or more columns.\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'B', 'A'],\n    'value': [1, 2, 3, 4, 5]\n})\nprint(df)\n\n\n  category  value\n0        A      1\n1        B      2\n2        A      3\n3        B      4\n4        A      5\n\n\n\n\n\n\nCode\n# Group by 'category'\ngrouped = df.groupby('category')"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_grouping.html#aggregating-data",
    "href": "example_course/course-materials/cheatsheets/data_grouping.html#aggregating-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Aggregating Data",
    "text": "Aggregating Data\nAfter grouping, you can apply various aggregation functions to summarize the data within each group.\n\nBasic aggregation\n\n\nCode\n# Basic aggregations\nprint(grouped['value'].mean())\nprint(grouped['value'].sum())\n\n\ncategory\nA    3.0\nB    3.0\nName: value, dtype: float64\ncategory\nA    9\nB    6\nName: value, dtype: int64\n\n\n\n\nDoing multiple aggregations at the same time using agg()\n\n\nCode\n# Multiple aggregations\nprint(grouped['value'].agg(['mean', 'sum', 'count']))\n\n\n          mean  sum  count\ncategory                  \nA          3.0    9      3\nB          3.0    6      2\n\n\n\n\nAggregation using a custom function\n\n\nCode\n# Custom aggregation function\ndef range_func(x):\n    return x.max() - x.min()\n\nprint(grouped['value'].agg(range_func))\n\n\ncategory\nA    4\nB    2\nName: value, dtype: int64"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_grouping.html#common-aggregation-functions",
    "href": "example_course/course-materials/cheatsheets/data_grouping.html#common-aggregation-functions",
    "title": "EDS 217 Cheatsheet",
    "section": "Common Aggregation Functions",
    "text": "Common Aggregation Functions\n\nmean(): Average\nsum(): Sum of values\ncount(): Count of non-null values\nmin(), max(): Minimum and maximum values\nmedian(): Median value\nstd(), var(): Standard deviation and variance\nfirst(), last(): First and last values in the group"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_grouping.html#grouped-operations",
    "href": "example_course/course-materials/cheatsheets/data_grouping.html#grouped-operations",
    "title": "EDS 217 Cheatsheet",
    "section": "Grouped Operations",
    "text": "Grouped Operations\nYou can apply operations to each group separately using transform() or apply().\n\nUsing transform() to alter each group in a group by object\n\n\nCode\n# Transform: apply function to each group, return same-sized DataFrame\ndef normalize(x):\n    return (x - x.mean()) / x.std()\n\ndf['value_normalized'] = grouped['value'].transform(normalize)\n\n\n\n\nUsing apply() to alter each group in a group by object\n\n\nCode\n# Apply: apply function to each group, return a DataFrame or Series\ndef group_range(x):\n    return x['value'].max() - x['value'].min()\n\nresult = grouped.apply(group_range)\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_37424/114114075.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  result = grouped.apply(group_range)"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_grouping.html#pivot-tables",
    "href": "example_course/course-materials/cheatsheets/data_grouping.html#pivot-tables",
    "title": "EDS 217 Cheatsheet",
    "section": "Pivot Tables",
    "text": "Pivot Tables\nPivot tables are a powerful tool for reorganizing and summarizing data. They allow you to transform your data from a long format to a wide format, making it easier to analyze and visualize patterns.\n\nWorking with Pivot Tables\n\n\nCode\n# Sample DataFrame\ndf = pd.DataFrame({\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'product': ['A', 'B', 'A', 'B'],\n    'sales': [100, 150, 120, 180]\n})\nprint(df)\n\n\n         date product  sales\n0  2023-01-01       A    100\n1  2023-01-01       B    150\n2  2023-01-02       A    120\n3  2023-01-02       B    180\n\n\n\nPivot tables with a single aggregation function\n\n\nCode\n# Create a pivot table\npivot_table = pd.pivot_table(df, values='sales', index='date', columns='product', aggfunc='sum')\nprint(pivot_table)\n\n\nproduct       A    B\ndate                \n2023-01-01  100  150\n2023-01-02  120  180\n\n\n\n\nPivot tables with multiple aggregation\n\n\nCode\n# Pivot table with multiple aggregation functions\npivot_multi = pd.pivot_table(df, values='sales', index='date', columns='product', \n                             aggfunc=[np.sum, np.mean])\nprint(pivot_multi)\n\n\n            sum        mean       \nproduct       A    B      A      B\ndate                              \n2023-01-01  100  150  100.0  150.0\n2023-01-02  120  180  120.0  180.0\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_37424/1326309547.py:2: FutureWarning: The provided callable &lt;function sum at 0x113214cc0&gt; is currently using DataFrameGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n  pivot_multi = pd.pivot_table(df, values='sales', index='date', columns='product',\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_37424/1326309547.py:2: FutureWarning: The provided callable &lt;function mean at 0x1132160c0&gt; is currently using DataFrameGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n  pivot_multi = pd.pivot_table(df, values='sales', index='date', columns='product',"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_grouping.html#key-pivot-table-parameters",
    "href": "example_course/course-materials/cheatsheets/data_grouping.html#key-pivot-table-parameters",
    "title": "EDS 217 Cheatsheet",
    "section": "Key Pivot Table Parameters",
    "text": "Key Pivot Table Parameters\n\nvalues: Column(s) to aggregate\nindex: Column(s) to use as row labels\ncolumns: Column(s) to use as column labels\naggfunc: Function(s) to use for aggregation (default is mean)\nfill_value: Value to use for missing data\nmargins: Add row/column with subtotals (default is False)\n\nFor more detailed information on grouping, aggregating, and pivot tables in Pandas, refer to the official Pandas documentation."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day3.html#introduction",
    "href": "example_course/course-materials/eod-practice/eod-day3.html#introduction",
    "title": "Day 3: Tasks & activities",
    "section": "Introduction",
    "text": "Introduction\nIn this end-of-day activity, we’ll practice using Pandas Series for data analysis and learn how to use NumPy’s random number generator. We’ll create a series of test scores using random numbers and explore how to make our random number generation reproducible."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day3.html#setup",
    "href": "example_course/course-materials/eod-practice/eod-day3.html#setup",
    "title": "Day 3: Tasks & activities",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries and set up our environment.\n\n\nCode\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day3.html#understanding-numpys-random-number-generator",
    "href": "example_course/course-materials/eod-practice/eod-day3.html#understanding-numpys-random-number-generator",
    "title": "Day 3: Tasks & activities",
    "section": "Understanding NumPy’s Random Number Generator",
    "text": "Understanding NumPy’s Random Number Generator\nNumPy provides a powerful random number generation tool called Generator. Let’s explore how to use it and why it’s important in data science.\n\nCreating a Random Number Generator\nWe can create a random number generator object like this:\n\n\nCode\nrng = np.random.default_rng()\n\n\nThis creates a generator with a random seed. Each time you run your code, you’ll get different random numbers.\n\n\nUsing a Seed for Reproducibility\nIn data science, it’s often crucial to be able to reproduce our results. We can do this by setting a seed for our random number generator. Here’s how:\n\n\nCode\nrng = np.random.default_rng(seed=42)\n\n\nNow, every time we use this rng object to generate random numbers, we’ll get the same sequence of “random” numbers. This is extremely useful for debugging, sharing results, and ensuring consistency in our analyses."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day3.html#creating-the-test-scores-series",
    "href": "example_course/course-materials/eod-practice/eod-day3.html#creating-the-test-scores-series",
    "title": "Day 3: Tasks & activities",
    "section": "Creating the Test Scores Series",
    "text": "Creating the Test Scores Series\n\nCreate a series called scores that contains 10 elements representing monthly test scores. We’ll use random integers between 70 and 100 to generate the monthly scores, and set the index to be the month names from September to June:\n\nmonths = ['Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day3.html#analyzing-the-test-scores",
    "href": "example_course/course-materials/eod-practice/eod-day3.html#analyzing-the-test-scores",
    "title": "Day 3: Tasks & activities",
    "section": "Analyzing the Test Scores",
    "text": "Analyzing the Test Scores\nNow that we have our test scores series, let’s analyze the data by answering the following questions:\n\n1. What is the student’s average test score for the entire year?\nCalculate the mean of all scores in the series.\n\n\n2. What is the student’s average test score during the first half of the year?\nCalculate the mean of the first five months’ scores.\n\n\n3. What is the student’s average test score during the second half of the year?\nCalculate the mean of the last five months’ scores.\n\n\n4. Did the student improve their performance in the second half? If so, by how much?\nCompare the average scores from the first and second half of the year."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day3.html#exploring-reproducibility",
    "href": "example_course/course-materials/eod-practice/eod-day3.html#exploring-reproducibility",
    "title": "Day 3: Tasks & activities",
    "section": "Exploring Reproducibility",
    "text": "Exploring Reproducibility\nTo demonstrate the importance of seeding, try creating two series with different random number generators:\n\n\nCode\nrng1 = np.random.default_rng(seed=42)\nrng2 = np.random.default_rng(seed=42)\n\nseries1 = pd.Series(rng1.integers(70, 101, size=10), index=months)\nseries2 = pd.Series(rng2.integers(70, 101, size=10), index=months)\n\nprint(series1.equals(series2))  # This should return True\n\n\nTrue\n\n\nNow try creating two series with random number generators that have different seeds:\n\n\nCode\nrng3 = np.random.default_rng(seed=42)\nrng4 = np.random.default_rng(seed=123)\n\nseries3 = pd.Series(rng3.integers(70, 101, size=10), index=months)\nseries4 = pd.Series(rng4.integers(70, 101, size=10), index=months)\n\nprint(series3.equals(series4))  # This should return False\n\n\nFalse"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day3.html#conclusion",
    "href": "example_course/course-materials/eod-practice/eod-day3.html#conclusion",
    "title": "Day 3: Tasks & activities",
    "section": "Conclusion",
    "text": "Conclusion\nIn this activity, you practiced creating and analyzing a Pandas Series representing test scores. You also learned about NumPy’s random number generator and the importance of seeding for reproducibility in data science. These skills are fundamental in data analysis and will be useful in more complex data science workflows."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day3.html#additional-resources",
    "href": "example_course/course-materials/eod-practice/eod-day3.html#additional-resources",
    "title": "Day 3: Tasks & activities",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nNumPy Random Generator\nPandas Series Documentation\nEDS 217 Cheatsheet: Pandas Series Basics\n\nRemember to document your code and results clearly in your Jupyter Notebook. Good luck!\n\nEnd Activity Session (Day 3)"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/read_csv.html",
    "href": "example_course/course-materials/cheatsheets/read_csv.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom io import StringIO\n\n# Create a simple CSV string\ncsv_data = \"\"\"\nname,age,city\nAlice,28,New York\nBob,35,San Francisco\nCharlie,42,Chicago\n\"\"\"\n\n# Read the CSV data\ndf = pd.read_csv(StringIO(csv_data))\nprint(df)\n\n\n      name  age           city\n0    Alice   28       New York\n1      Bob   35  San Francisco\n2  Charlie   42        Chicago"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/read_csv.html#basic-usage-of-pd.read_csv",
    "href": "example_course/course-materials/cheatsheets/read_csv.html#basic-usage-of-pd.read_csv",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom io import StringIO\n\n# Create a simple CSV string\ncsv_data = \"\"\"\nname,age,city\nAlice,28,New York\nBob,35,San Francisco\nCharlie,42,Chicago\n\"\"\"\n\n# Read the CSV data\ndf = pd.read_csv(StringIO(csv_data))\nprint(df)\n\n\n      name  age           city\n0    Alice   28       New York\n1      Bob   35  San Francisco\n2  Charlie   42        Chicago"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/read_csv.html#selecting-specific-columns",
    "href": "example_course/course-materials/cheatsheets/read_csv.html#selecting-specific-columns",
    "title": "EDS 217 Cheatsheet",
    "section": "Selecting Specific Columns",
    "text": "Selecting Specific Columns\n\nUsing the usecols Parameter\n\n\nCode\n# Read only specific columns\ndf = pd.read_csv(StringIO(csv_data), usecols=['name', 'city'])\nprint(df)\n\n\n      name           city\n0    Alice       New York\n1      Bob  San Francisco\n2  Charlie        Chicago"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/read_csv.html#naming-columns",
    "href": "example_course/course-materials/cheatsheets/read_csv.html#naming-columns",
    "title": "EDS 217 Cheatsheet",
    "section": "Naming Columns",
    "text": "Naming Columns\n\nUsing the names Parameter\n\n\nCode\n# Rename columns while reading\ndf = pd.read_csv(StringIO(csv_data), names=['full_name', 'years', 'location'])\nprint(df)\n\n\n  full_name years       location\n0      name   age           city\n1     Alice    28       New York\n2       Bob    35  San Francisco\n3   Charlie    42        Chicago"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/read_csv.html#specifying-an-index",
    "href": "example_course/course-materials/cheatsheets/read_csv.html#specifying-an-index",
    "title": "EDS 217 Cheatsheet",
    "section": "Specifying an Index",
    "text": "Specifying an Index\n\nUsing the index_col Parameter\n\n\nCode\n# Set 'name' column as index\ndf = pd.read_csv(StringIO(csv_data), index_col='name')\nprint(df)\n\n\n         age           city\nname                       \nAlice     28       New York\nBob       35  San Francisco\nCharlie   42        Chicago"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/read_csv.html#parsing-dates",
    "href": "example_course/course-materials/cheatsheets/read_csv.html#parsing-dates",
    "title": "EDS 217 Cheatsheet",
    "section": "Parsing Dates",
    "text": "Parsing Dates\n\nAutomatic Date Parsing\n\n\nCode\ncsv_data_with_dates = \"\"\"\ndate,event\n2023-01-15,Conference\n2023-02-28,Workshop\n2023-03-10,Seminar\n\"\"\"\n\ndf = pd.read_csv(StringIO(csv_data_with_dates), parse_dates=['date'])\nprint(df.dtypes)\nprint(df)\n\n\ndate     datetime64[ns]\nevent            object\ndtype: object\n        date       event\n0 2023-01-15  Conference\n1 2023-02-28    Workshop\n2 2023-03-10     Seminar\n\n\n\n\nCustom Date Parsing\n\n\nCode\ncsv_data_custom_dates = \"\"\"\ndate,event\n15/01/2023,Conference\n28/02/2023,Workshop\n10/03/2023,Seminar\n\"\"\"\n\ndf = pd.read_csv(StringIO(csv_data_custom_dates), parse_dates=['date'], date_format='%d/%m/%Y')\nprint(df.dtypes)\nprint(df)\n\n\ndate     datetime64[ns]\nevent            object\ndtype: object\n        date       event\n0 2023-01-15  Conference\n1 2023-02-28    Workshop\n2 2023-03-10     Seminar"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/read_csv.html#handling-headers",
    "href": "example_course/course-materials/cheatsheets/read_csv.html#handling-headers",
    "title": "EDS 217 Cheatsheet",
    "section": "Handling Headers",
    "text": "Handling Headers\n\nCSV with Multi-line Header\n\n\nCode\ncsv_data_with_header = \"\"\"\n# This file contains employee data\n# Created by: HR Department, Last updated: 2023-08-23\nEmployee ID,Name,Department,Salary\n101,John Doe,Marketing,50000\n102,Jane Smith,Engineering,60000\n103,Mike Johnson,Sales,55000\n\"\"\"\n\n# Read CSV ignoring the first two lines\ndf = pd.read_csv(StringIO(csv_data_with_header), header=2)\nprint(df)\n\n# Read CSV treating the first row as header and skipping the next two\ndf_alt = pd.read_csv(StringIO(csv_data_with_header), header=0, skiprows=2)\nprint(\"\\nAlternative method:\")\nprint(df_alt)\n\n\n   Employee ID          Name   Department  Salary\n0          101      John Doe    Marketing   50000\n1          102    Jane Smith  Engineering   60000\n2          103  Mike Johnson        Sales   55000\n\nAlternative method:\n                         # Created by: HR Department  Last updated: 2023-08-23\nEmployee ID Name                          Department                    Salary\n101         John Doe                       Marketing                     50000\n102         Jane Smith                   Engineering                     60000\n103         Mike Johnson                       Sales                     55000\n\n\n\n\nCSV with No Header\n\n\nCode\ncsv_data_no_header = \"\"\"\nAlice,28,New York\nBob,35,San Francisco\nCharlie,42,Chicago\n\"\"\"\n\ndf = pd.read_csv(StringIO(csv_data_no_header), header=None, names=['name', 'age', 'city'])\nprint(df)\n\n\n      name  age           city\n0    Alice   28       New York\n1      Bob   35  San Francisco\n2  Charlie   42        Chicago"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/read_csv.html#dealing-with-missing-data",
    "href": "example_course/course-materials/cheatsheets/read_csv.html#dealing-with-missing-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Dealing with Missing Data",
    "text": "Dealing with Missing Data\n\nCustomizing NA Values\n\n\nCode\ncsv_data_missing = \"\"\"\nname,age,city\nAlice,28,New York\nBob,N/A,San Francisco\nCharlie,42,Unknown\n\"\"\"\n\ndf = pd.read_csv(StringIO(csv_data_missing), na_values=['N/A', 'Unknown'])\nprint(df)\n\n\n      name   age           city\n0    Alice  28.0       New York\n1      Bob   NaN  San Francisco\n2  Charlie  42.0            NaN"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/read_csv.html#coercing-columns-to-specific-data-types",
    "href": "example_course/course-materials/cheatsheets/read_csv.html#coercing-columns-to-specific-data-types",
    "title": "EDS 217 Cheatsheet",
    "section": "Coercing Columns to Specific Data Types",
    "text": "Coercing Columns to Specific Data Types\n\nUsing the dtype Parameter\n\n\nCode\ncsv_data_types = \"\"\"\nid,name,score\n1,Alice,85.5\n2,Bob,92.0\n3,Charlie,78.5\n\"\"\"\n\ndf = pd.read_csv(StringIO(csv_data_types), dtype={'id': int, 'name': str, 'score': float})\nprint(df.dtypes)\nprint(df)\n\n\nid         int64\nname      object\nscore    float64\ndtype: object\n   id     name  score\n0   1    Alice   85.5\n1   2      Bob   92.0\n2   3  Charlie   78.5"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/read_csv.html#reading-large-csv-files",
    "href": "example_course/course-materials/cheatsheets/read_csv.html#reading-large-csv-files",
    "title": "EDS 217 Cheatsheet",
    "section": "Reading Large CSV Files",
    "text": "Reading Large CSV Files\n\nUsing chunksize for Memory Efficiency\n\n\nCode\nimport numpy as np\n\n# Generate a large CSV string (100,000 rows)\nnp.random.seed(0)\nlarge_csv_data = \"id,value\\n\" + \"\\n\".join([f\"{i},{np.random.rand()}\" for i in range(100000)])\n\n# Read the large CSV in chunks\nchunk_size = 20000\nchunks = pd.read_csv(StringIO(large_csv_data), chunksize=chunk_size)\n\n# Process each chunk\nfor i, chunk in enumerate(chunks):\n    print(f\"Chunk {i+1}:\")\n    print(chunk.head())\n    print(f\"Chunk shape: {chunk.shape}\")\n    print(\"\\n\")\n\n\nChunk 1:\n   id     value\n0   0  0.548814\n1   1  0.715189\n2   2  0.602763\n3   3  0.544883\n4   4  0.423655\nChunk shape: (20000, 2)\n\n\nChunk 2:\n          id     value\n20000  20000  0.392173\n20001  20001  0.041157\n20002  20002  0.923301\n20003  20003  0.406235\n20004  20004  0.944282\nChunk shape: (20000, 2)\n\n\nChunk 3:\n          id     value\n40000  40000  0.369256\n40001  40001  0.211326\n40002  40002  0.476905\n40003  40003  0.082234\n40004  40004  0.237659\nChunk shape: (20000, 2)\n\n\nChunk 4:\n          id     value\n60000  60000  0.927955\n60001  60001  0.902937\n60002  60002  0.427617\n60003  60003  0.510806\n60004  60004  0.583200\nChunk shape: (20000, 2)\n\n\nChunk 5:\n          id     value\n80000  80000  0.011097\n80001  80001  0.001770\n80002  80002  0.155055\n80003  80003  0.316761\n80004  80004  0.651845\nChunk shape: (20000, 2)\n\n\n\n\nRemember, these examples use StringIO to simulate reading from a file. When working with actual CSV files, you would replace StringIO(csv_data) with the file path or URL."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day2.html#introduction",
    "href": "example_course/course-materials/eod-practice/eod-day2.html#introduction",
    "title": "Day 2: Tasks & Activities",
    "section": "Introduction",
    "text": "Introduction\nWelcome to the end-of-day exercise for Day 2! Today, we’ll be putting into practice what you’ve learned about Python data structures, particularly lists and dictionaries. This exercise allows you to work with real data from your classmates while reinforcing key concepts."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day2.html#learning-objectives",
    "href": "example_course/course-materials/eod-practice/eod-day2.html#learning-objectives",
    "title": "Day 2: Tasks & Activities",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this exercise, you should be able to:\n\nCreate and manipulate lists and dictionaries in Python\nUse list and dictionary methods effectively\nIterate through data structures using loops\nApply basic data analysis techniques using Python data structures"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day2.html#setup",
    "href": "example_course/course-materials/eod-practice/eod-day2.html#setup",
    "title": "Day 2: Tasks & Activities",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries:\n\n\nCode\n# We won't use the random library until the end of this exercise, \n# but it's always good to put imported libraries at the top of your notebook.\nimport random"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day2.html#part-1-data-collection",
    "href": "example_course/course-materials/eod-practice/eod-day2.html#part-1-data-collection",
    "title": "Day 2: Tasks & Activities",
    "section": "Part 1: Data Collection",
    "text": "Part 1: Data Collection\nIn this first part, we’ll create data structures based on information from your classmates.\n\nTask 1: Create a List of Classmates\nCreate a list containing the names of at least 4 of your classmates in this course.\n\n\nCode\n# Your code here\n\n\n\n\nTask 2: Create a Dictionary of Classmate Information\nNow, let’s create a dictionary where the keys are your classmates’ names, and the values are another dictionary containing information about them. For each classmate, include the following information:\n\nFavorite color (favorite_color)\nNumber of pets (number_of_pets)\nPreferred study snack (preferred_study_snack)\n\n\n\nCode\n# Your code here"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day2.html#part-2-data-structure-manipulation",
    "href": "example_course/course-materials/eod-practice/eod-day2.html#part-2-data-structure-manipulation",
    "title": "Day 2: Tasks & Activities",
    "section": "Part 2: Data Structure Manipulation",
    "text": "Part 2: Data Structure Manipulation\nNow that we have our data structures, let’s practice manipulating them.\n\nTask 3: List Operations\nUsing the list of classmate names you created in Task 1, perform the following operations:\n\nAdd a new classmate to the end of the list\nRemove the second classmate from the list\nSort the list alphabetically\nFind and print the index of a specific classmate\n\n\n\nCode\n# Your code here\n\n\n\n\nTask 4: Dictionary Operations\nUsing the dictionary of classmate information from Task 2, perform the following operations:\n\nAdd a new key-value pair for each classmate: “favorite_study_spot”\nUpdate the “number of pets” for one classmate\nCreate a list of all the favorite colors your classmates mentioned\n\n\n\nCode\n# Your code here"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day2.html#part-3-fun-with-random-selections",
    "href": "example_course/course-materials/eod-practice/eod-day2.html#part-3-fun-with-random-selections",
    "title": "Day 2: Tasks & Activities",
    "section": "Part 3: Fun with Random Selections",
    "text": "Part 3: Fun with Random Selections\nLet’s add a fun element to our exercise using the random module. Before we dive into the main task, let’s look at how we can use the random library to select random items from a dictionary.\n\nExample: Random Selection from a Dictionary\nHere’s a simple example of how to select random items from a dictionary:\n\n\nCode\nimport random\n\n# Sample dictionary\nfruit_colors = {\n    \"apple\": \"red\",\n    \"banana\": \"yellow\",\n    \"grape\": \"purple\",\n    \"kiwi\": \"brown\",\n    \"orange\": \"orange\"\n}\n\n# Select a single random key-value pair\nrandom_fruit, random_color = random.choice(list(fruit_colors.items()))\nprint(f\"Randomly selected fruit: {random_fruit}\")\nprint(f\"Its color: {random_color}\")\n\n# To get just a random key:\nrandom_fruit = random.choice(list(fruit_colors.keys()))\nprint(f\"Another randomly selected fruit: {random_fruit}\")\n\n# To select multiple random items:\nnum_selections = 3\nrandom_fruits = random.sample(list(fruit_colors.keys()), num_selections)\nprint(f\"Randomly selected {num_selections} fruits: {random_fruits}\")\n\n\nRandomly selected fruit: kiwi\nIts color: brown\nAnother randomly selected fruit: kiwi\nRandomly selected 3 fruits: ['kiwi', 'grape', 'apple']\n\n\nThis example demonstrates how to:\n\nConvert a dictionary to a list of key-value pairs or keys\nUse random.choice() to select a single random item from a list\nUse random.sample() to select multiple unique random items from a list\n\nNote: random.choice() selects a single item, while random.sample() can select multiple unique items. For our snack-sharing task below, random.sample() might be more useful!\n\n\nTask 5: Random Snack Sharing\nCreate a function that randomly selects a classmate to share their snack with another random classmate. Print out the results as “Name will share [snack] with Name”.\n#| echo: true\ndef assign_random_snacks(classmate_info):\n    # Your code here\n    print(f\"{sharer} will share {snack} with {receiver}\")\n\n# Test your function\nassign_random_snacks(classmate_info)"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day2.html#conclusion",
    "href": "example_course/course-materials/eod-practice/eod-day2.html#conclusion",
    "title": "Day 2: Tasks & Activities",
    "section": "Conclusion",
    "text": "Conclusion\nGreat job completing this exercise! You’ve practiced creating and manipulating lists and dictionaries, performed basic data analysis, and even created a fun random snack-sharing function. These skills will be invaluable as you continue your journey in Python programming and data science.\nRemember, the key to mastering these concepts is practice. Feel free to modify this exercise with your own data or ideas to further reinforce your learning."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day2.html#additional-resources",
    "href": "example_course/course-materials/eod-practice/eod-day2.html#additional-resources",
    "title": "Day 2: Tasks & Activities",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nPython Lists (Python.org)\nPython Dictionaries (Python.org)\nRandom Module (Python.org)\n\nDon’t forget to check out our course cheatsheets for quick reference on lists and dictionaries!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html",
    "title": "Interactive Session",
    "section": "",
    "text": "In your final class activity next week, you will work in small groups (2-3) to develop a example data science workflow. The workflow will follow these 9 steps (although not all steps are used in every workflow):\n\nImport Data\nExplore Data\nClean Data\nFilter Data\nSort Data\nTransform Data\nGroup Data\nAggregate Data\nVisualize Data\n\n\n\n\n\n\n\nNote\n\n\n\nAlmost all pandas functions and dataframe methods can be classified into one or more of these 9 categories. For example, here is an cheatsheet that maps many of the most common pandas functions into our nine-step workflow."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#steps-for-python-data-science-workflows",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#steps-for-python-data-science-workflows",
    "title": "Interactive Session",
    "section": "",
    "text": "In your final class activity next week, you will work in small groups (2-3) to develop a example data science workflow. The workflow will follow these 9 steps (although not all steps are used in every workflow):\n\nImport Data\nExplore Data\nClean Data\nFilter Data\nSort Data\nTransform Data\nGroup Data\nAggregate Data\nVisualize Data\n\n\n\n\n\n\n\nNote\n\n\n\nAlmost all pandas functions and dataframe methods can be classified into one or more of these 9 categories. For example, here is an cheatsheet that maps many of the most common pandas functions into our nine-step workflow."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#building-an-example-workflow.",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#building-an-example-workflow.",
    "title": "Interactive Session",
    "section": "Building an example workflow.",
    "text": "Building an example workflow.\nIn this interactive session, we will use a sample dataset to go through the 9 steps and see how each one is used to move us forward in the data analysis pipeline.\nFor each step, we’ll focus on the most common and essential commands used in data science, providing detailed explanations and hints at more advanced topics we’ll cover in future sessions or in later courses.\nLet’s begin by setting up our environment and creating our dataset."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#setting-up-our-environment",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#setting-up-our-environment",
    "title": "Interactive Session",
    "section": "Setting up our environment",
    "text": "Setting up our environment\nFirst, let’s import the necessary libraries:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nNow, let’s create a sample dataset about ocean temperatures:\n\n\nCode\n#| echo: true\n#| echo: true\n\n# Create sample data\n# Create a random number generator object\n# Create a random number generator object\nrng = np.random.default_rng(42)  # 42 is the seed for reproducibility\n\n# Generate date range (3 years of data)\ndates = pd.date_range(start='2020-01-01', end='2022-12-31', freq='D')\nprint(f\"Number of days: {len(dates)}\")\n\n# Define locations (5 oceans)\nlocations = ['Pacific', 'Atlantic', 'Indian', 'Southern', 'Arctic']\nprint(f\"Number of locations: {len(locations)}\")\n\n# Calculate total number of rows (one observation for each day, for each location)\ntotal_rows = len(dates) * len(locations)\nprint(f\"Total number of rows: {total_rows}\")\n\n# Generate 'date' column\n# np.tile repeats the entire dates array for each location\ndate_column = np.tile(dates, len(locations))\nprint(\"Date column shape:\", date_column.shape)\n\n# Generate 'location' column\n# np.repeat repeats each location for all dates before moving to the next location\nlocation_column = np.repeat(locations, len(dates))\nprint(\"Location column shape:\", location_column.shape)\n\n# Generate 'temperature' column\n# Using normal distribution: mean=20, std_dev=5\ntemperature_column = rng.normal(20, 5, total_rows)\nprint(\"Temperature column shape:\", temperature_column.shape)\n\n# Generate 'salinity' column\n# Using normal distribution: mean=35, std_dev=1\nsalinity_column = rng.normal(35, 1, total_rows)\nprint(\"Salinity column shape:\", salinity_column.shape)\n\n# Generate 'depth' column\n# Using choice to randomly select from given depths\ndepth_options = [0, 50, 100, 200, 500, 1000]\ndepth_column = rng.choice(depth_options, total_rows)\nprint(\"Depth column shape:\", depth_column.shape)\n\n# Create DataFrame\ndf = pd.DataFrame({\n    'date': date_column,\n    'location': location_column,\n    'temperature': temperature_column,\n    'salinity': salinity_column,\n    'depth': depth_column\n})\n\n# Introduce missing values (NaN) to temperature and salinity columns\n# We'll use 5% as the probability of a value being NaN\n\n# For temperature\ntemp_mask = rng.choice([True, False], total_rows, p=[0.05, 0.95])\ndf.loc[temp_mask, 'temperature'] = np.nan\n\n# For salinity\nsal_mask = rng.choice([True, False], total_rows, p=[0.05, 0.95])\ndf.loc[sal_mask, 'salinity'] = np.nan\n\n# Display info about the resulting DataFrame\nprint(\"\\nDataFrame Info:\")\ndf.info()\n\n# Save as CSV\ndf.to_csv('ocean_temperatures.csv', index=False)\nprint(\"\\nDataset created and saved as 'ocean_temperatures.csv'\")\n\n\nNumber of days: 1096\nNumber of locations: 5\nTotal number of rows: 5480\nDate column shape: (5480,)\nLocation column shape: (5480,)\nTemperature column shape: (5480,)\nSalinity column shape: (5480,)\nDepth column shape: (5480,)\n\nDataFrame Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5480 entries, 0 to 5479\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   date         5480 non-null   datetime64[ns]\n 1   location     5480 non-null   object        \n 2   temperature  5197 non-null   float64       \n 3   salinity     5202 non-null   float64       \n 4   depth        5480 non-null   int64         \ndtypes: datetime64[ns](1), float64(2), int64(1), object(1)\nmemory usage: 214.2+ KB\n\nDataset created and saved as 'ocean_temperatures.csv'"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#import-data",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#import-data",
    "title": "Interactive Session",
    "section": "1. Import Data",
    "text": "1. Import Data\nThe read_csv() function is one of the most commonly used methods for importing data in pandas. It reads a comma-separated values (CSV) file into a DataFrame.\n\n\nCode\n# Read the CSV file\ndf = pd.read_csv('ocean_temperatures.csv')\nprint(\"Data imported successfully.\")\n\n\nData imported successfully.\n\n\n\n\n\n\n\n\nPython Data Science 101\n\n\n\n\npd.read_csv() is versatile and can handle various file formats and options.\nIt automatically infers data types for each column.\nThe resulting object is a DataFrame, pandas’ primary data structure.\n\n\n\nFuture topics:\n\nReading other file formats (Excel, JSON, SQL databases)\nHandling large datasets with chunksize parameter\nCustom parsers for non-standard file formats"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#explore-data",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#explore-data",
    "title": "Interactive Session",
    "section": "2. Explore Data",
    "text": "2. Explore Data\nExploration is crucial for understanding your dataset. We’ll use several key functions for this purpose.\n\n\nCode\nprint(\"First few rows:\")\nprint(df.head())\n\nprint(\"\\nDataFrame info:\")\ndf.info()\n\nprint(\"\\nSummary statistics:\")\nprint(df.describe())\n\nprint(\"\\nMissing values:\")\nprint(df.isna().sum())\n\n\nFirst few rows:\n         date location  temperature   salinity  depth\n0  2020-01-01  Pacific    21.523585        NaN    200\n1  2020-01-02  Pacific    14.800079  34.467264    100\n2  2020-01-03  Pacific    23.752256  35.016505    100\n3  2020-01-04  Pacific    24.702824  36.416944    200\n4  2020-01-05  Pacific    10.244824  35.807487   1000\n\nDataFrame info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5480 entries, 0 to 5479\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   date         5480 non-null   object \n 1   location     5480 non-null   object \n 2   temperature  5197 non-null   float64\n 3   salinity     5202 non-null   float64\n 4   depth        5480 non-null   int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 214.2+ KB\n\nSummary statistics:\n       temperature     salinity        depth\ncount  5197.000000  5202.000000  5480.000000\nmean     19.909088    34.993770   306.925182\nstd       4.979046     1.009512   349.828443\nmin       1.757936    30.610885     0.000000\n25%      16.532469    34.340138    50.000000\n50%      19.961369    34.974046   100.000000\n75%      23.156441    35.668759   500.000000\nmax      37.270232    39.025824  1000.000000\n\nMissing values:\ndate             0\nlocation         0\ntemperature    283\nsalinity       278\ndepth            0\ndtype: int64\n\n\n\n\n\n\n\n\nPython Data Science 101\n\n\n\n\nhead(): Shows the first few rows (default is 5) of the DataFrame.\ninfo(): Provides a concise summary of the DataFrame, including column names, non-null counts, and data types.\ndescribe(): Generates descriptive statistics for numerical columns.\nisna().sum(): Counts missing values in each column.\n\n\n\nFuture topics:\n\nCustom descriptive statistics\nVisualization techniques for data exploration (histograms, box plots)\nCorrelation analysis between variables"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#clean-data",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#clean-data",
    "title": "Interactive Session",
    "section": "3. Clean Data",
    "text": "3. Clean Data\nData cleaning is often one of the most time-consuming parts of data analysis. Here, we’ll focus on handling missing values.\n\n\nCode\n# Remove rows with NaN values.\n# Use .copy() to make a new dataframe instead of a view.\ndf_cleaned = df.dropna().copy()\n\nprint(f\"Rows with missing values removed: {len(df) - len(df_cleaned)}\")\nprint(f\"Remaining rows: {len(df_cleaned)}\")\n\n\nRows with missing values removed: 541\nRemaining rows: 4939\n\n\n\n\n\n\n\n\nPython Data Science 101\n\n\n\n\ndropna(): Removes rows containing any null values.\nThis method can be customized to drop based on specific columns or thresholds.\n\n\n\nFuture topics:\n\nImputation techniques for missing data\nHandling outliers and anomalies\nData type conversions and consistency checks"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#filter-data",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#filter-data",
    "title": "Interactive Session",
    "section": "4. Filter Data",
    "text": "4. Filter Data\nFiltering allows us to focus on specific subsets of our data based on conditions.\n\n\nCode\n# Filter for Pacific Ocean data in summer months (June, July, August) of 2021\npacific_summer = df_cleaned[(df_cleaned['location'] == 'Pacific') & \n                            (df_cleaned['date'].between('2021-06-01', '2021-08-31'))]\n\nprint(\"Pacific Ocean data for Summer 2021:\")\nprint(pacific_summer.head())\nprint(f\"\\nNumber of records: {len(pacific_summer)}\")\n\n\nPacific Ocean data for Summer 2021:\n           date location  temperature   salinity  depth\n517  2021-06-01  Pacific    16.558139  36.613441    500\n519  2021-06-03  Pacific    28.144685  33.617238   1000\n520  2021-06-04  Pacific    15.149252  35.225117    200\n521  2021-06-05  Pacific    15.561522  34.946657    200\n522  2021-06-06  Pacific    26.678922  35.864044    100\n\nNumber of records: 83\n\n\n\n\n\n\n\n\nPython Data Science 101\n\n\n\n\nBoolean indexing is used to filter data based on conditions.\n& operator combines multiple conditions (logical AND).\nbetween() is a convenient method for range comparisons.\n\n\n\nFuture topics:\n\nComplex filtering with multiple conditions\nUsing query() method for string expressions\nFiltering with regular expressions"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#sort-data",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#sort-data",
    "title": "Interactive Session",
    "section": "5. Sort Data",
    "text": "5. Sort Data\nSorting helps in understanding the distribution and extremes of our data.\n\n\nCode\n# Sort by temperature (descending) and then by date\nsorted_df = df_cleaned.sort_values(['temperature', 'date'], ascending=[False, True])\n\nprint(\"Top 10 warmest temperature readings:\")\nprint(sorted_df[['date', 'location', 'temperature']].head(10))\n\n\nTop 10 warmest temperature readings:\n            date  location  temperature\n4141  2022-05-03  Southern    37.270232\n4222  2022-07-23  Southern    36.355131\n846   2022-04-26   Pacific    35.894268\n3304  2020-01-17  Southern    35.442561\n2863  2021-11-02    Indian    35.297656\n3141  2022-08-07    Indian    35.158812\n4319  2022-10-28  Southern    35.118907\n5183  2022-03-10    Arctic    34.981195\n1616  2021-06-04  Atlantic    34.571223\n139   2020-05-19   Pacific    34.569312\n\n\n\n\n\n\n\n\nPython Data Science 101\n\n\n\n\nsort_values(): Sorts the DataFrame by one or more columns.\nascending parameter controls the sort order for each column.\n\n\n\nFuture topics:\n\nSorting by a computed metric\nHierarchical sorting with multi-index DataFrames\nImplementing custom sorting algorithms"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#transform-data",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#transform-data",
    "title": "Interactive Session",
    "section": "6. Transform Data",
    "text": "6. Transform Data\nData transformation involves creating new features or modifying existing ones. We’ll look at different types of transformations.\n\n\nCode\n# Single column transformation\ndf_cleaned['temperature_f'] = (df_cleaned['temperature'] * 9/5) + 32\n\n# Multi-column transformation\ndf_cleaned['temp_sal_ratio'] = df_cleaned['temperature'] / df_cleaned['salinity']\n\n# Using apply() for more complex transformations\ndef depth_category(depth):\n    if depth &lt;= 50:\n        return 'Shallow'\n    elif depth &lt;= 200:\n        return 'Medium'\n    else:\n        return 'Deep'\n\ndf_cleaned['depth_category'] = df_cleaned['depth'].apply(depth_category)\n\nprint(\"DataFrame with new columns:\")\nprint(df_cleaned[['temperature', 'temperature_f', 'temp_sal_ratio', 'depth', 'depth_category']].head())\n\n\nDataFrame with new columns:\n   temperature  temperature_f  temp_sal_ratio  depth depth_category\n1    14.800079      58.640143        0.429395    100         Medium\n2    23.752256      74.754061        0.678316    100         Medium\n3    24.702824      76.465082        0.678333    200         Medium\n4    10.244824      50.440683        0.286108   1000           Deep\n5    13.489102      56.280384        0.376834      0        Shallow\n\n\n\n\n\n\n\n\nPython Data Science 101\n\n\n\n\nDirect column operations for simple transformations.\nCreating new columns based on calculations from existing columns.\napply() function for applying custom functions to columns."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#group-data",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#group-data",
    "title": "Interactive Session",
    "section": "7. Group Data",
    "text": "7. Group Data\nGrouping allows us to split the data into subsets based on some criteria.\n\n\nCode\n# Group by location and depth category\ngrouped = df_cleaned.groupby(['location', 'depth_category'])\n\n# Display the group sizes\nprint(\"Group sizes:\")\nprint(grouped.size())\n\n\nGroup sizes:\nlocation  depth_category\nArctic    Deep              316\n          Medium            338\n          Shallow           334\nAtlantic  Deep              327\n          Medium            349\n          Shallow           316\nIndian    Deep              318\n          Medium            336\n          Shallow           340\nPacific   Deep              344\n          Medium            313\n          Shallow           333\nSouthern  Deep              318\n          Medium            319\n          Shallow           338\ndtype: int64\n\n\n\n\n\n\n\n\nPython Data Science 101\n\n\n\n\ngroupby(): Creates a GroupBy object, which doesn’t compute anything until an aggregation method is called.\nMultiple columns can be used for grouping, creating hierarchical groups."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#aggregate-data",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#aggregate-data",
    "title": "Interactive Session",
    "section": "8. Aggregate Data",
    "text": "8. Aggregate Data\nAggregation computes summary statistics for each group.\n\n\nCode\n# Calculate mean and standard deviation of temperature and salinity for each group\nagg_data = grouped[['temperature', 'salinity']].agg(['mean', 'std'])\n\nprint(\"Aggregated data:\")\nprint(agg_data)\n\n\nAggregated data:\n                        temperature             salinity          \n                               mean       std       mean       std\nlocation depth_category                                           \nArctic   Deep             19.899673  4.919967  35.024201  1.044496\n         Medium           20.278376  5.027461  34.996241  0.953111\n         Shallow          19.473178  5.022971  34.950225  1.017149\nAtlantic Deep             19.080289  4.784324  34.989573  0.945400\n         Medium           19.707030  5.222012  34.977164  1.033639\n         Shallow          20.109729  4.984291  34.977224  0.973582\nIndian   Deep             19.764921  4.999262  34.974590  0.939823\n         Medium           20.522776  4.980354  35.031657  1.025202\n         Shallow          20.233734  5.051566  35.038030  0.969988\nPacific  Deep             19.761942  4.611211  35.082996  1.035482\n         Medium           20.203291  4.773300  35.021385  1.059246\n         Shallow          19.968260  5.225368  35.068652  1.016507\nSouthern Deep             20.761787  4.922963  34.903479  1.084392\n         Medium           19.348108  4.878151  34.984482  0.980553\n         Shallow          20.004547  4.853461  34.934175  1.063821\n\n\n\n\n\n\n\n\nPython Data Science 101\n\n\n\n\nagg(): Applies multiple aggregation functions at once.\nResults in a multi-index DataFrame where the first level of columns represents the original columns and the second level represents the applied functions."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#visualize-data",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#visualize-data",
    "title": "Interactive Session",
    "section": "9. Visualize Data",
    "text": "9. Visualize Data\nVisualization is key for understanding patterns and communicating results.\n\n\nCode\n# Plot average temperatures by location, sorted from highest to lowest. Use parentheses to allow for multi-line commands.\navg_temps = (df_cleaned\n    .groupby('location')['temperature']\n    .mean()\n    .sort_values(ascending=False)\n)\n\nplt.figure(figsize=(10, 6))\navg_temps.plot(kind='bar')\nplt.title('Average Ocean Temperatures by Location')\nplt.xlabel('Location')\nplt.ylabel('Average Temperature (°C)')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#conclusion",
    "href": "example_course/course-materials/interactive-sessions/4c_dataframe_workflows.html#conclusion",
    "title": "Interactive Session",
    "section": "Conclusion",
    "text": "Conclusion\nThis comprehensive walkthrough of a 9-step data science workflow with pandas DataFrames has introduced you to the most common and essential commands used in each step of the process. We’ve covered importing data, exploration, cleaning, filtering, sorting, transformation, grouping, aggregation, and visualization.\nRemember, this is just the beginning. Each of these steps has much more depth that we’ll explore in future sessions. The pandas library is incredibly powerful and flexible, offering numerous ways to manipulate and analyze data efficiently.\nAs you continue your journey in data science, you’ll find yourself repeatedly using these core concepts, building upon them to tackle more complex problems and datasets. When you meet new pandas commands or packages used in data science, it will be helpful to map these functions to one or more of the nine areas of data science that they would be most useful for.\n\nEnd interactive session 4C"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_selection.html",
    "href": "example_course/course-materials/cheatsheets/data_selection.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "While filtering and selection are related concepts in data manipulation, they have distinct differences:\nSelection:\n\nDefinition: Selection refers to choosing specific columns or rows from a DataFrame based on their labels or positions.\nPurpose: It’s used to extract a subset of data you’re interested in, without necessarily applying any conditions.\nMethods: In pandas, selection is typically done using methods like .loc[], .iloc[], or square brackets df[] for column selection.\nExample: Selecting specific columns like df[['name', 'age']] or rows df.loc[0:5].\n\nFiltering:\n\nDefinition: Filtering involves choosing rows that meet specific conditions based on the values in one or more columns.\nPurpose: It’s used to extract data that satisfies certain criteria or conditions.\nMethods: In pandas, filtering is often done using boolean indexing or the .query() method.\nExample: Filtering rows where age is greater than 30: df[df['age'] &gt; 30].\n\nKey differences:\n\nScope:\n\nSelection typically deals with choosing columns or rows based on their labels or positions.\nFiltering typically deals with choosing rows based on conditions applied to the data values.\n\nCondition-based:\n\nSelection doesn’t necessarily involve conditions (though it can with .loc)\nFiltering always involves a condition or criteria.\n\nOutput:\n\nSelection can result in both a subset of columns and/or rows.\nFiltering typically results in a subset of rows (though the number of columns can be affected if combined with selection).\n\nUse cases:\n\nSelection is often used when you know exactly which columns or rows you want.\nFiltering is used when you want to find data that meets certain criteria.\n\n\nIt’s worth noting that in practice, these operations are often combined. For example:\n# This combines filtering (age &gt; 30) and selection (only 'name' and 'profession' columns)\nresult = df.loc[df['age'] &gt; 30, ['name', 'profession']]\nUnderstanding the distinction between filtering and selection helps in choosing the right methods for data manipulation tasks and in communicating clearly about data operations."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_selection.html#selection-vs.-filtering",
    "href": "example_course/course-materials/cheatsheets/data_selection.html#selection-vs.-filtering",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "While filtering and selection are related concepts in data manipulation, they have distinct differences:\nSelection:\n\nDefinition: Selection refers to choosing specific columns or rows from a DataFrame based on their labels or positions.\nPurpose: It’s used to extract a subset of data you’re interested in, without necessarily applying any conditions.\nMethods: In pandas, selection is typically done using methods like .loc[], .iloc[], or square brackets df[] for column selection.\nExample: Selecting specific columns like df[['name', 'age']] or rows df.loc[0:5].\n\nFiltering:\n\nDefinition: Filtering involves choosing rows that meet specific conditions based on the values in one or more columns.\nPurpose: It’s used to extract data that satisfies certain criteria or conditions.\nMethods: In pandas, filtering is often done using boolean indexing or the .query() method.\nExample: Filtering rows where age is greater than 30: df[df['age'] &gt; 30].\n\nKey differences:\n\nScope:\n\nSelection typically deals with choosing columns or rows based on their labels or positions.\nFiltering typically deals with choosing rows based on conditions applied to the data values.\n\nCondition-based:\n\nSelection doesn’t necessarily involve conditions (though it can with .loc)\nFiltering always involves a condition or criteria.\n\nOutput:\n\nSelection can result in both a subset of columns and/or rows.\nFiltering typically results in a subset of rows (though the number of columns can be affected if combined with selection).\n\nUse cases:\n\nSelection is often used when you know exactly which columns or rows you want.\nFiltering is used when you want to find data that meets certain criteria.\n\n\nIt’s worth noting that in practice, these operations are often combined. For example:\n# This combines filtering (age &gt; 30) and selection (only 'name' and 'profession' columns)\nresult = df.loc[df['age'] &gt; 30, ['name', 'profession']]\nUnderstanding the distinction between filtering and selection helps in choosing the right methods for data manipulation tasks and in communicating clearly about data operations."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_selection.html#setup",
    "href": "example_course/course-materials/cheatsheets/data_selection.html#setup",
    "title": "EDS 217 Cheatsheet",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import pandas and load our dataset.\n\n\nCode\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('https://bit.ly/eds217-studentdata')\n\n# Display the first few rows\nprint(df.head())\n\n\n   student_id  age   gpa             major\n0        1000   24  2.18       Mathematics\n1        1001   21  2.39           Physics\n2        1002   22  2.09           Physics\n3        1003   24  2.65  Computer Science\n4        1004   20  2.78         Chemistry"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_selection.html#basic-selection",
    "href": "example_course/course-materials/cheatsheets/data_selection.html#basic-selection",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Selection",
    "text": "Basic Selection\n\nSelect a Single Column\n\n\nCode\n# Using square brackets\nages = df['age']\n\n# Using dot notation (only works for valid Python identifiers)\nages = df.age\n\n\n\n\nSelect Multiple Columns\n\n\nCode\n# Select age and gpa columns\nage_gpa = df[['age', 'gpa']]\n\n\n\n\nSelect Rows by Index\n\n\nCode\n# Select first 5 rows\nfirst_five = df.iloc[0:5]\n\n# Select specific rows by index\nspecific_rows = df.iloc[[0, 2, 4]]\n\n\n\n\nSelect Rows and Columns\n\n\nCode\n# Select first 3 rows and 'age', 'gpa' columns\nsubset = df.loc[0:2, ['age', 'gpa']]"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_selection.html#filtering",
    "href": "example_course/course-materials/cheatsheets/data_selection.html#filtering",
    "title": "EDS 217 Cheatsheet",
    "section": "Filtering",
    "text": "Filtering\n\nFilter by a Single Condition\n\n\nCode\n# Students with age greater than 21\nolder_students = df[df['age'] &gt; 21]\n\n\n\n\nFilter by Multiple Conditions\n\n\nCode\n# Students with age &gt; 21 and gpa &gt; 3.5\nhigh_performing_older = df[(df['age'] &gt; 21) & (df['gpa'] &gt; 3.5)]\n\n\n\n\nFilter Using .isin()\n\n\nCode\n# Students majoring in Computer Science or Biology\ncs_bio_students = df[df['major'].isin(['Computer Science', 'Biology'])]\n\n\n\n\nFilter Using String Methods\n\n\nCode\n# Majors starting with 'E'\ne_majors = df[df['major'].str.startswith('E')]"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_selection.html#combining-selection-and-filtering",
    "href": "example_course/course-materials/cheatsheets/data_selection.html#combining-selection-and-filtering",
    "title": "EDS 217 Cheatsheet",
    "section": "Combining Selection and Filtering",
    "text": "Combining Selection and Filtering\n\n\nCode\n# Select 'age' and 'gpa' for students with gpa &gt; 3.5\nhigh_gpa_age = df.loc[df['gpa'] &gt; 3.5, ['age', 'gpa']]"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_selection.html#useful-methods",
    "href": "example_course/course-materials/cheatsheets/data_selection.html#useful-methods",
    "title": "EDS 217 Cheatsheet",
    "section": "Useful Methods",
    "text": "Useful Methods\n\n.loc[] vs .iloc[]\n\nUse .loc[] for label-based indexing\nUse .iloc[] for integer-based indexing\n\n\n\n.query() Method\n\n\nCode\n# Filter using query method\ncs_students = df.query(\"major == 'Computer Science'\")\n\n\n\n\n.where() Method\n\n\nCode\n# Replace values not meeting the condition with NaN\nhigh_gpa = df.where(df['gpa'] &gt; 3.5)"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/data_selection.html#tips-and-tricks",
    "href": "example_course/course-materials/cheatsheets/data_selection.html#tips-and-tricks",
    "title": "EDS 217 Cheatsheet",
    "section": "Tips and Tricks",
    "text": "Tips and Tricks\n\nChain methods for complex operations:\nresult = df[df['age'] &gt; 21].groupby('major')['gpa'].mean()\nUse & for AND, | for OR in multiple conditions:\ndf[(df['age'] &gt; 21) & (df['gpa'] &gt; 3.5) | (df['major'] == 'Computer Science')]\nReset index after filtering if needed:\nfiltered_df = df[df['age'] &gt; 21].reset_index(drop=True)\nUse ~ for negation:\nnot_cs = df[~(df['major'] == 'Computer Science')]\n\nRemember: Always chain indexers [] or use .loc[]/.iloc[] to avoid the SettingWithCopyWarning when modifying DataFrames. Alternatively, you can assign the output of a filtering or selection to the original dataframe if you want to alter the dataframe itself (and not make a copy or view)."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This cheatsheet provides a quick reference for common operations on Pandas DataFrames. It’s designed for beginning data science students who are just starting to work with Pandas."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#introduction",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#introduction",
    "title": "EDS 217 Cheatsheet",
    "section": "",
    "text": "This cheatsheet provides a quick reference for common operations on Pandas DataFrames. It’s designed for beginning data science students who are just starting to work with Pandas."
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#importing-pandas",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#importing-pandas",
    "title": "EDS 217 Cheatsheet",
    "section": "Importing Pandas",
    "text": "Importing Pandas\nAlways start by importing pandas:\n\n\nCode\nimport pandas as pd"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#creating-a-dataframe",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#creating-a-dataframe",
    "title": "EDS 217 Cheatsheet",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\n\nFrom a dictionary\n\n\nCode\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Age': [25, 30, 35],\n        'City': ['New York', 'San Francisco', 'Los Angeles']}\ndf = pd.DataFrame(data)\nprint(df)\n\n\n      Name  Age           City\n0    Alice   25       New York\n1      Bob   30  San Francisco\n2  Charlie   35    Los Angeles\n\n\n\n\nFrom a CSV file\n\n\nCode\n# Here's an example csv file we can use for read_csv:\nfrom io import StringIO\n# Create a CSV string\ncsv_data = \"\"\"\nName,Age,City\nAlice,25,New York\nBob,30,San Francisco\nCharlie,35,Los Angeles\n\"\"\"\n\n# Use StringIO to create a file-like object\ncsv_file = StringIO(csv_data.strip())\n\n# Read the CSV data into a DataFrame\ndf = pd.read_csv(csv_file)\nprint(df)\n\n\n      Name  Age           City\n0    Alice   25       New York\n1      Bob   30  San Francisco\n2  Charlie   35    Los Angeles"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#basic-dataframe-information",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#basic-dataframe-information",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic DataFrame Information",
    "text": "Basic DataFrame Information\n\n\nCode\n# Display the first few rows\nprint(df.head())\n\n# Get basic information about the DataFrame\nprint(df.info())\n\n# Get summary statistics\nprint(df.describe())\n\n# Get column names\nprint(df.columns)\n\n# Get dimensions (rows, columns)\nprint(df.shape)\n\n\n      Name  Age           City\n0    Alice   25       New York\n1      Bob   30  San Francisco\n2  Charlie   35    Los Angeles\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    3 non-null      object\n 1   Age     3 non-null      int64 \n 2   City    3 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 204.0+ bytes\nNone\n        Age\ncount   3.0\nmean   30.0\nstd     5.0\nmin    25.0\n25%    27.5\n50%    30.0\n75%    32.5\nmax    35.0\nIndex(['Name', 'Age', 'City'], dtype='object')\n(3, 3)"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#selecting-data",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#selecting-data",
    "title": "EDS 217 Cheatsheet",
    "section": "Selecting Data",
    "text": "Selecting Data\n\nSelecting columns\n\n\nCode\n# Select a single column\nage_column = df['Age']\n\n# Select multiple columns\nsubset = df[['Name', 'City']]\n\n\n\n\nSelecting rows\n\n\nCode\n# Select rows by index\nfirst_row = df.loc[0]\n\n# Select rows by condition\nadults = df[df['Age'] &gt;= 18]"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#basic-data-manipulation",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#basic-data-manipulation",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Data Manipulation",
    "text": "Basic Data Manipulation\n\nAdding a new column\n\n\nCode\ndf['Is Adult'] = df['Age'] &gt;= 18\n\n\n\n\nRenaming columns\n\n\nCode\ndf = df.rename(columns={'Name': 'Full Name'})\n\n\n\n\nHandling missing values\n\n\nCode\n# Drop rows with any missing values\ndf_cleaned = df.dropna()\n\n# Fill missing values\ndf_filled = df.fillna(0)  # Fills with 0"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#basic-calculations",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#basic-calculations",
    "title": "EDS 217 Cheatsheet",
    "section": "Basic Calculations",
    "text": "Basic Calculations\n\n\nCode\n# Calculate mean age\nmean_age = df['Age'].mean()\n\n# Count occurrences\ncity_counts = df['City'].value_counts()"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#grouping-and-aggregation",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#grouping-and-aggregation",
    "title": "EDS 217 Cheatsheet",
    "section": "Grouping and Aggregation",
    "text": "Grouping and Aggregation\n\n\nCode\n# Group by city and calculate mean age\ncity_age = df.groupby('City')['Age'].mean()"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#sorting",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#sorting",
    "title": "EDS 217 Cheatsheet",
    "section": "Sorting",
    "text": "Sorting\n\n\nCode\n# Sort by Age in descending order\ndf_sorted = df.sort_values('Age', ascending=False)"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#saving-a-dataframe",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#saving-a-dataframe",
    "title": "EDS 217 Cheatsheet",
    "section": "Saving a DataFrame",
    "text": "Saving a DataFrame\n\n\nCode\n# Save to CSV\ndf.to_csv('output.csv', index=False)"
  },
  {
    "objectID": "example_course/course-materials/cheatsheets/pandas_dataframes.html#further-learning",
    "href": "example_course/course-materials/cheatsheets/pandas_dataframes.html#further-learning",
    "title": "EDS 217 Cheatsheet",
    "section": "Further Learning",
    "text": "Further Learning\nFor more advanced operations and in-depth explanations, check out these resources:\n\nPandas Official Documentation\n10 Minutes to Pandas\nPython for Data Analysis by Wes McKinney\nPandas Cheat Sheet\n\nRemember, practice is key! Try these operations with different datasets to become more comfortable with Pandas DataFrames."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#getting-started",
    "title": "Interactive Session",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#introduction",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#introduction",
    "title": "Interactive Session",
    "section": "Introduction",
    "text": "Introduction\nIn this interactive session, we’ll explore the basics of working with pandas DataFrames using a dataset of world cities. We’ll cover importing data, basic DataFrame operations, and essential methods for data exploration and manipulation. This session will prepare you for more advanced data analysis tasks and upcoming collaborative coding exercises."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#learning-objectives",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#learning-objectives",
    "title": "Interactive Session",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nImport data into a pandas DataFrame\nExplore basic DataFrame properties and methods\nPerform simple data filtering and selection operations\nUse basic aggregation and grouping functions"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#setting-up",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#setting-up",
    "title": "Interactive Session",
    "section": "Setting Up",
    "text": "Setting Up\nLet’s start by importing the pandas library and loading our dataset.\n\n\nCode\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-data-importing",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-data-importing",
    "title": "Interactive Session",
    "section": "1. Basic Data Importing",
    "text": "1. Basic Data Importing\n\n\nCode\nurl = \"https://raw.githubusercontent.com/datasets/world-cities/master/data/world-cities.csv\"\ncities_df = pd.read_csv(url)"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-dataframe-exploration",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-dataframe-exploration",
    "title": "Interactive Session",
    "section": "2. Basic DataFrame Exploration",
    "text": "2. Basic DataFrame Exploration\n\nViewing the Data\nLet’s take a look at the first few rows of our DataFrame:\n\n\nCode\nprint(cities_df.head())\n\n\n                 name               country          subcountry  geonameid\n0        les Escaldes               Andorra  Escaldes-Engordany    3040051\n1    Andorra la Vella               Andorra    Andorra la Vella    3041563\n2             Warīsān  United Arab Emirates               Dubai     290503\n3          Umm Suqaym  United Arab Emirates               Dubai     290581\n4  Umm Al Quwain City  United Arab Emirates        UmmalQaywayn     290594\n\n\nTo see the last few rows, we can use:\n\n\nCode\nprint(cities_df.tail())\n\n\n                         name   country                   subcountry  \\\n31694                 Bindura  Zimbabwe          Mashonaland Central   \n31695              Beitbridge  Zimbabwe  Matabeleland South Province   \n31696                 Epworth  Zimbabwe                       Harare   \n31697             Chitungwiza  Zimbabwe                       Harare   \n31698  Harare Western Suburbs  Zimbabwe             Mashonaland West   \n\n       geonameid  \n31694     895061  \n31695     895269  \n31696    1085510  \n31697    1106542  \n31698   13132735  \n\n\n\n\nDataFrame Properties\nNow, let’s explore some basic properties of our DataFrame:\n\n\nCode\n# Number of rows and columns\nprint(\"Shape:\", cities_df.shape)\n\n# Column names\nprint(\"\\nColumns:\", cities_df.columns)\n\n# Data types of each column\nprint(\"\\nData types:\\n\", cities_df.dtypes)\n\n# Summary statistics of numeric columns (if any)\nprint(\"\\nSummary statistics:\\n\", cities_df.describe())\n\n\nShape: (31699, 4)\n\nColumns: Index(['name', 'country', 'subcountry', 'geonameid'], dtype='object')\n\nData types:\n name          object\ncountry       object\nsubcountry    object\ngeonameid      int64\ndtype: object\n\nSummary statistics:\n           geonameid\ncount  3.169900e+04\nmean   3.266489e+06\nstd    2.863419e+06\nmin    4.900000e+02\n25%    1.277083e+06\n50%    2.636503e+06\n75%    3.693436e+06\nmax    1.349420e+07\n\n\n\n\nChecking for Missing Values\nIt’s important to identify any missing data in your DataFrame:\n\n\nCode\nprint(cities_df.isnull().sum())\n\n\nname            0\ncountry         0\nsubcountry    118\ngeonameid       0\ndtype: int64"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#baisc-cleaning",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#baisc-cleaning",
    "title": "Interactive Session",
    "section": "3. Baisc Cleaning",
    "text": "3. Baisc Cleaning\nRemove rows with missing data in subcountry using dropna() and the subset argument.\n\n\nCode\ncities_df = cities_df.dropna(subset=['subcountry'])"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-data-selection-and-filtering",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-data-selection-and-filtering",
    "title": "Interactive Session",
    "section": "4. Basic Data Selection and Filtering",
    "text": "4. Basic Data Selection and Filtering\n\nSelecting Columns\nTo select specific columns:\n\n\nCode\n# Select a single column\nprint(cities_df['name'].head())\n\n# Select multiple columns\nprint(cities_df[['name', 'country', 'subcountry']].head())\n\n\n0          les Escaldes\n1      Andorra la Vella\n2               Warīsān\n3            Umm Suqaym\n4    Umm Al Quwain City\nName: name, dtype: object\n                 name               country          subcountry\n0        les Escaldes               Andorra  Escaldes-Engordany\n1    Andorra la Vella               Andorra    Andorra la Vella\n2             Warīsān  United Arab Emirates               Dubai\n3          Umm Suqaym  United Arab Emirates               Dubai\n4  Umm Al Quwain City  United Arab Emirates        UmmalQaywayn\n\n\n\n\nFiltering Rows\nWe can filter rows based on conditions:\n\n\nCode\n# Cities in the United States\nus_cities = cities_df[cities_df['country'] == 'United States']\nprint(us_cities[['name', 'country']].head())\n\n# Cities in California\ncalifornia_cities = cities_df[(cities_df['country'] == 'United States') & (cities_df['subcountry'] == 'California')]\nprint(california_cities[['name', 'country', 'subcountry']].head())\n\n\n             name        country\n27316   Fort Hunt  United States\n27317    Bessemer  United States\n27318     Paducah  United States\n27319  Birmingham  United States\n27320     Cordova  United States\n                name        country  subcountry\n29728       Fillmore  United States  California\n29777       Adelanto  United States  California\n29778         Agoura  United States  California\n29779   Agoura Hills  United States  California\n29780  Agua Caliente  United States  California\n\n\n\n\nCombining Conditions\nWe can use logical operators to combine multiple conditions:\n\n\nCode\n# Cities in Canada that start with the letter 'T'\ncanadian_t_cities = cities_df[(cities_df['country'] == 'Canada') & (cities_df['name'].str.startswith('T'))]\nprint(canadian_t_cities[['name', 'country', 'subcountry']])\n\n\n                        name country        subcountry\n3986  Tam O'Shanter-Sullivan  Canada           Ontario\n3987                Tecumseh  Canada           Ontario\n3988           Templeton-Est  Canada            Quebec\n3989                 Terrace  Canada  British Columbia\n3990              Terrebonne  Canada            Quebec\n3991             The Beaches  Canada           Ontario\n3992                 Thorold  Canada           Ontario\n3993             Thunder Bay  Canada           Ontario\n3994             Tillsonburg  Canada           Ontario\n3995                 Timmins  Canada           Ontario\n3996                 Toronto  Canada           Ontario\n3997          Trois-Rivières  Canada            Quebec\n3998              Tsawwassen  Canada  British Columbia\n4038          Thetford-Mines  Canada            Quebec\n4051       Trinity-Bellwoods  Canada           Ontario\n4080           Taylor-Massey  Canada           Ontario\n4094        Thorncliffe Park  Canada           Ontario"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-sorting-and-ranking",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-sorting-and-ranking",
    "title": "Interactive Session",
    "section": "5. Basic Sorting and Ranking",
    "text": "5. Basic Sorting and Ranking\nTo sort the DataFrame based on one or more columns:\n\n\nCode\n# Sort cities alphabetically\nsorted_cities = cities_df.sort_values('name')\nprint(sorted_cities[['name', 'country']].head())\n\n# Sort cities by country, then by name\nsorted_cities_by_country = cities_df.sort_values(['country', 'name'])\nprint(sorted_cities_by_country[['name', 'country']].head())\n\n\n                      name      country\n21637       's-Gravenzande  Netherlands\n21636     's-Hertogenbosch  Netherlands\n25121            'Ārdamatā        Sudan\n9057   6th of October City        Egypt\n9688              A Coruña        Spain\n         name      country\n112   Andkhōy  Afghanistan\n111  Asadābād  Afghanistan\n72      Aībak  Afghanistan\n108   Baghlān  Afghanistan\n107     Balkh  Afghanistan"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-transformations",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-transformations",
    "title": "Interactive Session",
    "section": "6. Basic Transformations",
    "text": "6. Basic Transformations\n\nCreating New Columns\nWe can create new columns based on existing data:\n\n\nCode\n# Create a column for city name length\ncities_df['name_length'] = cities_df['name'].str.len()\n\n# Display the top 5 cities with the longest names\nlong_named_cities = cities_df.nlargest(5, 'name_length')\nprint(long_named_cities[['name', 'country', 'name_length']])\n\n\n                                                    name        country  \\\n22968  Karachi University Employees Co-operative Hous...       Pakistan   \n30524      Diamond Head / Kapahulu / Saint Louis Heights  United States   \n8114             Universitäts- und Hansestadt Greifswald        Germany   \n30676             Aliamanu / Salt Lakes / Foster Village  United States   \n6569               Sandaoling Lutiankuang Wuqi Nongchang          China   \n\n       name_length  \n22968           57  \n30524           45  \n8114            39  \n30676           38  \n6569            37"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-grouping-and-aggregation",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#basic-grouping-and-aggregation",
    "title": "Interactive Session",
    "section": "7-8: Basic Grouping and Aggregation",
    "text": "7-8: Basic Grouping and Aggregation\nGrouping allows us to perform operations on subsets of the data:\n\n\nCode\n# Number of cities by country\ncities_per_country = cities_df.groupby('country')['name'].count().sort_values(ascending=False)\nprint(cities_per_country.head())\n\n# Number of subcountries (e.g., states, provinces) by country\nsubcountries_per_country = cities_df.groupby('country')['subcountry'].nunique().sort_values(ascending=False)\nprint(subcountries_per_country.head())\n\n\ncountry\nUnited States    3367\nIndia            3312\nBrazil           2111\nChina            1999\nJapan            1293\nName: name, dtype: int64\ncountry\nRussian Federation    83\nTürkiye               81\nThailand              75\nAlgeria               53\nUnited States         51\nName: subcountry, dtype: int64"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#conclusion",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#conclusion",
    "title": "Interactive Session",
    "section": "Conclusion",
    "text": "Conclusion\nIn this session, we’ve covered the basics of working with pandas DataFrames using a world cities dataset, including:\n\nImporting data\nExploring DataFrame properties\nSelecting and filtering data\nSorting and ranking\nGrouping and aggregation\nCreating new columns\n\nThese skills form the foundation of data analysis with pandas and will be essential for upcoming exercises and projects. Remember, pandas has many more functions and methods that we haven’t covered here. Don’t hesitate to explore the pandas documentation for more advanced features!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/4a_dataframes.html#resources",
    "href": "example_course/course-materials/interactive-sessions/4a_dataframes.html#resources",
    "title": "Interactive Session",
    "section": "Resources",
    "text": "Resources\n\nEDS 217 Pandas Cheatsheet\nPandas Workflows Functions\nPandas PDF Cheatsheet"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day5.html#introduction",
    "href": "example_course/course-materials/eod-practice/eod-day5.html#introduction",
    "title": "Day 5: Tasks & Activities",
    "section": "Introduction",
    "text": "Introduction\nIn this activity, you’ll explore the “Banana Index” dataset, which compares the environmental impact of various food products to that of a banana. These data were developed by the Economist magazine in 2023 and they posted their data to github for us to use. This exercise will help you practice working with pandas DataFrames, data manipulation, and visualization skills while learning about the environmental impacts of food production.\n\nReference:\nThe Economist and Solstad, S., 2023. The Economist’s Banana index. First published in the article “A different way to measure the climate impact of food”, The Economist, April 11, 2023."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day5.html#setup",
    "href": "example_course/course-materials/eod-practice/eod-day5.html#setup",
    "title": "Day 5: Tasks & Activities",
    "section": "Setup",
    "text": "Setup\nFirst, let’s import the necessary libraries and load the data:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\nurl = \"https://github.com/TheEconomist/banana-index-data/releases/download/1.0/bananaindex.csv\"\ndf = pd.read_csv(url)\n\n\n\n\nCode\n# Display the first few rows:\nprint(df.head())\n\n\n          entity  year  emissions_kg  emissions_1000kcal  \\\n0            Ale  2022      0.488690            0.317338   \n1  Almond butter  2022      0.387011            0.067265   \n2    Almond milk  2022      0.655888            2.222230   \n3        Almonds  2022      0.602368            0.105029   \n4    Apple juice  2022      0.458378            0.955184   \n\n   emissions_100g_protein  emissions_100g_fat  land_use_kg  land_use_1000kcal  \\\n0                0.878525            2.424209     0.811485           0.601152   \n1                0.207599            0.079103     7.683045           1.296870   \n2               13.595512            4.057470     1.370106           2.675063   \n3                0.328335            0.119361     8.230927           1.423376   \n4               29.152212           19.754980     0.660629           1.382839   \n\n   Land use per 100 grams of protein  Land use per 100 grams of fat  \\\n0                           1.577687                       3.065766   \n1                           3.608433                       1.495297   \n2                          12.687839                       4.600530   \n3                           4.261040                       1.610136   \n4                          43.232158                      26.246743   \n\n   Bananas index (kg)  Bananas index (1000 kcalories)  \\\n0            0.559558                        0.362340   \n1            0.443134                        0.076804   \n2            0.751002                        2.537364   \n3            0.689721                        0.119923   \n4            0.524851                        1.090638   \n\n   Bananas index (100g protein)  Chart?  type       Banana values  Unnamed: 16  \n0                      0.113771    True     1              Per KG     0.873350  \n1                      0.026885    True     1  Per 1000 kcalories     0.875803  \n2                      1.760651    True     1    Per 100g protein     7.721869  \n3                      0.042520    True     1                 NaN          NaN  \n4                      3.775280    True     1                 NaN          NaN  \n\n\n\n\nCode\n# Display the dataframe info:\nprint(df.info())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 160 entries, 0 to 159\nData columns (total 17 columns):\n #   Column                             Non-Null Count  Dtype  \n---  ------                             --------------  -----  \n 0   entity                             160 non-null    object \n 1   year                               160 non-null    int64  \n 2   emissions_kg                       160 non-null    float64\n 3   emissions_1000kcal                 160 non-null    float64\n 4   emissions_100g_protein             158 non-null    float64\n 5   emissions_100g_fat                 160 non-null    float64\n 6   land_use_kg                        160 non-null    float64\n 7   land_use_1000kcal                  160 non-null    float64\n 8   Land use per 100 grams of protein  158 non-null    float64\n 9   Land use per 100 grams of fat      160 non-null    float64\n 10  Bananas index (kg)                 160 non-null    float64\n 11  Bananas index (1000 kcalories)     160 non-null    float64\n 12  Bananas index (100g protein)       160 non-null    float64\n 13  Chart?                             160 non-null    bool   \n 14  type                               160 non-null    int64  \n 15  Banana values                      3 non-null      object \n 16  Unnamed: 16                        3 non-null      float64\ndtypes: bool(1), float64(12), int64(2), object(2)\nmemory usage: 20.3+ KB\nNone"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day5.html#tasks",
    "href": "example_course/course-materials/eod-practice/eod-day5.html#tasks",
    "title": "Day 5: Tasks & Activities",
    "section": "Tasks",
    "text": "Tasks\n\n1. Data Preparation\n\nSet the index of the DataFrame to be the ‘entity’ column.\nRemove the ‘year’, ‘Banana values’, ‘type’, ‘Unnamed: 16’, and ‘Chart?’ columns.\nDisplay the first few rows of the modified DataFrame.\n\n\n\n2. Exploring Banana Scores\n\nFor each of the pre-computed banana score columns (kg, calories, and protein), show the 10 highest-scoring food products.\nEdit the function below so that is returns the top 10 scores for a given column:\n\n\n\nCode\ndef return_top_ten(df, column):\n    \"\"\" Return the top 10 values of a column \"\"\"\n    pass\n\n\n\n\n\n\n\n\nreturn values from functions\n\n\n\nThe pass in our function is a temporary statement that allows the function to execute but not do anything. You need to remove the pass statement and add a return statement that provides the necessary functionality. For example, if the function was supposed to add 2 to every value of a column, you’d delete the pass statement and add return df[column] * 2\n\n\n\nUse your function to display the results for each of the three Banana index columns.\n\n\n\n3. Common High-Scoring Foods\nIdentify which foods, if any, appear in the top 10 for all three banana score lists (kg, calories, and protein).\n\n\n\n\n\n\nUnpacking iterables using the * operator\n\n\n\nPython sets allow you to quickly determine intersections: in_all_three = set.intersection(seta, setb, setc), or you can use the * operator to unpack a list of sets directly: in_all_three = set.intersection(*list_of_sets)\n\n\n\n\n4. Land Use Analysis\n\nCreate a new column named ‘Bananas index (land use 1000 kcal)’, calculating that food item’s use of land for every 1,000 kcal in comparison to a banana.\n\n\n\n\n\n\n\nTip\n\n\n\nThe data on land_use_1000kcal for bananas is found as the entry for this column in the Bananas row.\n\n\n\nDisplay the 10 foods with the highest land use score.\nCompare this list with the previous top 10 lists. Are there any common foods?\n\n\n\n5. Cheese Analysis\nIdentify the type of cheese with the highest banana score per 1,000 kcal. How does it compare to other cheeses in the dataset?"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day5.html#conclusion",
    "href": "example_course/course-materials/eod-practice/eod-day5.html#conclusion",
    "title": "Day 5: Tasks & Activities",
    "section": "Conclusion",
    "text": "Conclusion\nSummarize your findings from this analysis. What insights have you gained about the environmental impact of different foods? What aspects of Pandas do you want to practice more?\n\nEnd Activity Session (Day 5)"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day6.html",
    "href": "example_course/course-materials/eod-practice/eod-day6.html",
    "title": "Day 6: Tasks & Activities",
    "section": "",
    "text": "In this exercise, you’ll analyze Eurovision Song Contest data using pandas. You’ll practice various data manipulation techniques and explore trends in the contest’s history."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day6.html#setup",
    "href": "example_course/course-materials/eod-practice/eod-day6.html#setup",
    "title": "Day 6: Tasks & Activities",
    "section": "Setup",
    "text": "Setup\nFirst, import the necessary libraries and load the dataset:\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\nurl = \"https://github.com/Spijkervet/eurovision-dataset/releases/download/2020.0/contestants.csv\"\neurovision_df = pd.read_csv(url)"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day6.html#task-1-data-exploration-and-cleaning",
    "href": "example_course/course-materials/eod-practice/eod-day6.html#task-1-data-exploration-and-cleaning",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 1: Data Exploration and Cleaning",
    "text": "Task 1: Data Exploration and Cleaning\n\nDisplay the first few rows of the dataset.\nCheck the data types of each column.\nIdentify and handle any missing values.\nConvert the ‘year’ column to datetime type."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day6.html#task-2-filtering-and-transformation",
    "href": "example_course/course-materials/eod-practice/eod-day6.html#task-2-filtering-and-transformation",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 2: Filtering and Transformation",
    "text": "Task 2: Filtering and Transformation\n\nCreate a new dataframe containing only data from 1990 onwards\n\n\n\n\n\n\n\nImportant\n\n\n\nUse .copy() to make sure you create a new dataframe and not just a view.\n\n\n\nCalculate the difference between final points and semi-final points for each entry and make a histogram of these values using the builtin dataframe .hist() command."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day6.html#task-3-sorting-and-aggregation",
    "href": "example_course/course-materials/eod-practice/eod-day6.html#task-3-sorting-and-aggregation",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 3: Sorting and Aggregation",
    "text": "Task 3: Sorting and Aggregation\n\nFind the top 10 countries with the most Eurovision appearances (use the entire dataset for this calculation)\nCalculate the average final points for each country across all years. Make a simple bar plot of these data.\n\n\n\n\n\n\n\nNote\n\n\n\nUse value_counts() for counting appearances and groupby() for calculating averages."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day6.html#task-4-grouping-and-analysis",
    "href": "example_course/course-materials/eod-practice/eod-day6.html#task-4-grouping-and-analysis",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 4: Grouping and Analysis",
    "text": "Task 4: Grouping and Analysis\n\nDetermine the country with the highest average final points for each decade.\n\n\n\n\n\n\n\nHint: Grouping Years in Pandas\n\n\n\nWhen working with time series data, it’s often useful to group years into larger intervals like decades, 5-year periods, etc. Here’s a general approach using pandas:\n\nFor decades (10-year intervals):\ndf['decade'] = df['year'].dt.year // 10 * 10\nFor any N-year interval:\nN = 5  # Change this to your desired interval (e.g., 2, 5, 10, 20)\ndf['year_group'] = df['year'].dt.year // N * N\nFor more specific date ranges:\ndf['custom_group'] = pd.cut(df['year'], \n                            bins=[1990, 1995, 2000, 2005, 2010], \n                            labels=['1990-1994', '1995-1999', '2000-2004', '2005-2009'])\n\nRemember: - // is integer division (rounds down) - Multiplying by the interval after division ensures the start year of each group\nThese methods create a new column that you can use with groupby() for aggregations across your chosen time intervals."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day6.html#task-5-joining-data",
    "href": "example_course/course-materials/eod-practice/eod-day6.html#task-5-joining-data",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 5: Joining Data",
    "text": "Task 5: Joining Data\n\nRead in a new dataframe that contains population data stored at this url:\n\n\n\nCode\npopulation_url = 'https://bit.ly/euro_pop'\n\n\n\nJoin this data with the Eurovision dataframe.\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that country names match exactly between the two dataframes before joining.\n\n\n\nCalculate total entries per capita by country.\nSubsteps:\n3a. Create a new dataframe containing the counts of entries for each county (use value_counts)\n3b. Merge the dataframe of counts of entries for each country with the population dataframe.\n3c. Calculate entries per million population (using entries per million to make the numbers easier to work with)\n3d. Sort the results by entries per capita\n3e. Print the top 10 values"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day6.html#task-6-time-series-analysis",
    "href": "example_course/course-materials/eod-practice/eod-day6.html#task-6-time-series-analysis",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 6: Time Series Analysis",
    "text": "Task 6: Time Series Analysis\n\nPlot the trend of maximum final points awarded over the years.\nIdentify any significant changes in the scoring system based on this trend.\n\n(This step simply requires visual interpretation of the plot, but perhaps you could explore if there are actual rules changes underlying observed patterns using google)"
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day6.html#task-7-choose-your-own-analysis",
    "href": "example_course/course-materials/eod-practice/eod-day6.html#task-7-choose-your-own-analysis",
    "title": "Day 6: Tasks & Activities",
    "section": "Task 7: Choose your own analysis!",
    "text": "Task 7: Choose your own analysis!\nCome up with your own analysis of the Eurovision data that reveals some pattern across the data or through time. Feel free to discuss your ideas with others; often this leads to new ideas or refinement of ones you are already working on."
  },
  {
    "objectID": "example_course/course-materials/eod-practice/eod-day6.html#reflection",
    "href": "example_course/course-materials/eod-practice/eod-day6.html#reflection",
    "title": "Day 6: Tasks & Activities",
    "section": "Reflection",
    "text": "Reflection\nNow that you’ve completed the Eurovision data analysis exercise, it’s time to reflect on your experience. Add a new markdown cell to your notebook and answer the following questions:\n\nWhich tasks did you feel most comfortable with? Why do you think these were easier for you?\nWhich tasks did you find most challenging? What made these tasks difficult?\nAre there any pandas commands or concepts that you’d like to explore further? List a few and briefly explain why you’re interested in them.\nHow do you think the skills you practiced in this exercise could be applied to other datasets or real-world problems?\nWhat was the most interesting insight you gained about the Eurovision contest from this analysis?\n\n\n\n\n\n\n\nNote\n\n\n\nRemember, reflection is a crucial part of the learning process. It helps you identify areas for improvement and reinforces what you’ve learned.\n\n\nRemember to document your code, explain your reasoning, and interpret the results of your analysis throughout the exercise."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1d_operators_functions.html",
    "href": "example_course/course-materials/interactive-sessions/1d_operators_functions.html",
    "title": "Interactive Session 1D",
    "section": "",
    "text": "variables.jpg\nAll programming languages contain the same fundamental tools: variables, operators, and functions. This session covers a first introduction to each of these these basic elements of the Python language."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1d_operators_functions.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/1d_operators_functions.html#getting-started",
    "title": "Interactive Session 1D",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab (see the note below if you have trouble!)\nSelect “Rename”\nName your notebook with the format: Session_1D_Operators_and_Functions.ipynb (Replace X with the day number and Y with the session number)\n\n\n\n\n\n\n\n\nRight-clicking in JupyterLab\n\n\n\nSome browsers and operating system combinations will not conceded right-clicking to the JupyterLab interface and will show a system menu when you try to right click. In those cases, usually CTRL-Right Click or OPTION-Right Click will bring up the Jupyter menu.\n\n\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content:\n\n\n# Day 1: Session D - Operators & Functions\n\n[Session Webpage](https://eds-217-essential-python.github.io/course-materials/interactive-sessions/1d_operators_functions.html)\n\nDate: 09/03/2024\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1d_operators_functions.html#instructions",
    "href": "example_course/course-materials/interactive-sessions/1d_operators_functions.html#instructions",
    "title": "Interactive Session 1D",
    "section": "Instructions",
    "text": "Instructions\nWe will work through this material together, writing a new notebook as we go.\n\n\n\n\n\n\nNote\n\n\n\n🐍     This symbol designates an important note about Python structure, syntax, or another quirk.\n\n\n\n✏️     This symbol designates code you should add to your notebook and run."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1d_operators_functions.html#variables-operators",
    "href": "example_course/course-materials/interactive-sessions/1d_operators_functions.html#variables-operators",
    "title": "Interactive Session 1D",
    "section": "Variables + Operators",
    "text": "Variables + Operators\nVariables are used in Python to create references to an object (e.g. string, float, DataFrame, etc.). Variables are assigned in Python using =.\n\n\n\n\n\n\nNote\n\n\n\n🐍 Variable names should be chosen carefully and should indicate what the variable is used for. Python etiquette generally dictates using lowercase variable names. Underscores are common. Variable names cannot start with a number. Also, there are several names that cannot be used as variables, as they are reserved for built-in Python commands, functions, etc. We will see examples of these throughout this session.\n\n\n\n\n\nNumbers\nNumbers in Python can be either integers (whole numbers) or floats (floating-point decimal numbers).\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Define variables x and y as integers.\nx = 1\ny = 42\n\n\nThe following syntax is used to define a float:\na = 1.0\nb = 42.0\nc = 23.782043\n\n✏️ Try it. Create a new cell and define variables a, b, and c according to the values above.\n\n\n\nCode\n# Define variables a, b, and c as floats.\na = 1.0\nb = 42.0\nc = 23.782043\n\n\n\n\nArithmetic Operators\nJust like a calculator, basic arithmetic can be done on number variables. Python uses the following symbols:\n\n\n\nSymbol\nTask\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n%\nModulus\n\n\n//\nFloor division\n\n\n**\nPower\n\n\n\n\n✏️ Try it. Practice using arithmetic operations by running the code in a new cell. Feel free to add more to test the operators. Use the print() command to output your answers.\n\n\n\nCode\n# Do some math and print the results!\n\n\nNotice that the order of operations applies.\n\n\nCompound Assignment Operators\nCompound assignment operators combine an arithmetic or bitwise operation with assignment in a single statement. They provide a concise way to update a variable’s value based on its current value.\n\nExamples\n\n\nCode\n# Initialize a variable\ncount = 10\n\n# Decrement using compound assignment\ncount -= 1\nprint(f\"After count -= 1: {count}\")  # Output: 9\n\n# Increment using compound assignment\ncount += 2\nprint(f\"After count += 2: {count}\")  # Output: 11\n\n# Multiply using compound assignment\ncount *= 3\nprint(f\"After count *= 3: {count}\")  # Output: 33\n\n\nAfter count -= 1: 9\nAfter count += 2: 11\nAfter count *= 3: 33\n\n\n\n\nExercise\nComplete the following code to achieve the desired output:\n\n\nCode\n# Initialize the score\nscore = 100\n\n# TODO: Use compound assignment to decrease the score by 15\n# Your code here\n\n# TODO: Use compound assignment to double the score\n# Your code here\n\n# TODO: Use compound assignment to divide the score by 5\n# Your code here\n\nprint(f\"Final score: {score}\")  # Expected output: 34.0\n\n\nFinal score: 100\n\n\nTry modifying the initial score value or the operations to see how the result changes!\n\n\n\nBoolean Operators\nBoolean operators evaluate a condition between two operands, returning True if the condition is met and False otherwise. True and False are called booleans.\n\n\n\nSymbol\nTask\n\n\n\n\n==\nEquals\n\n\n!=\nDoes not equal\n\n\n&lt;\nLess than\n\n\n&gt;\nGreater than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;=\nGreater than or equal to\n\n\n\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(b &gt;= a)\nprint(87 &lt; -2)\nprint(c != 0)\nprint(y == x)\n\n\nTrue\nFalse\nTrue\nFalse\n\n\n\n Built-in Functions \n\nPython has a number of built-in functions. Here we will introduce a few of the useful built-in functions for numerical variables.\nThe type() function is used to check the data type of a variable. For numerical arguments, either float or int is returned.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(type(x))\nprint(type(c))\n\n\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n\n\nThe isinstance() function is used to determine whether an argument is in a certain class. It returns a boolean value. Multiple classes can be checked at once.\nisinstance(12, int)\n&gt;&gt;&gt; True\n\nisinstance(12.0, int)\n&gt;&gt;&gt; False\n\nisinstance(12.0, (int, float))\n&gt;&gt;&gt; True\nThe commands int() and float() are used to convert between data types.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(float(y))\nprint(int(c))\n\n\n42.0\n23\n\n\nNotice that when converting a float value to an integer, the int() command always rounds down to the nearest whole number.\nTo round a float to the nearest whole number, use the function round(). You can specify the number of decimal places by adding an integer as an argument to the round() function.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(round(c))\nprint(round(c, 3))\n\n\n24\n23.782\n\n\nTo return the absolute value of a number, use the abs() function.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(abs(c))\nprint(abs(-12))\n\n\n23.782043\n12\n\n\nThe pow() function is an alternative to the ** operator for raising a number to an exponent, i.e. \\(x^y\\).\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\npow(8, 2)\n\n\n64"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1d_operators_functions.html#strings",
    "href": "example_course/course-materials/interactive-sessions/1d_operators_functions.html#strings",
    "title": "Interactive Session 1D",
    "section": "Strings",
    "text": "Strings\nPieces of text in Python are referred to as strings. Strings are defined with either single or double quotes. The only difference between the two is that it is easier to use an apostrophe with double quotes.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nmytext = 'This is a string.'\nmytext2 = \"This is also a string.\"\n\n\nTo use an apostrophe or single quotes inside a string defined by single quotes (or to use double quotes), use a single backslash ( \\ ) referred to as an “escape” character.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nq1a = \"What is Newton's 1st law of motion?\"\nq1b = 'What is Newton\\'s 1st law of motion?'\n\n# Are q1a and q1b the same?\nq1a == q1b\n\n\nTrue\n\n\nPython has multi-line strings as well, which you can use when documenting your code or handling large quotes or chunks of text. Multi-line strings are started with three quotes (\"\"\") and terminated with three quotes (\"\"\"):\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\n# l(a or (A leaf falls on loneliness) by e.e. cummings\nla = \"\"\"\nl(a\nle\naf\nfa\nll\ns)\none\nl\niness\n\"\"\"\n\n\nMulti-line formatting is preserved in multi-line strings:\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(la)\n\n\n\nl(a\nle\naf\nfa\nll\ns)\none\nl\niness\n\n\n\n\nString Built-in Functions\nJust like the int() and float() commands, the str() command converts a number to a string.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nystr = str(y)\n\n\nThe + operator can be used to combine two or more strings.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\ns = 'isaac' + ' ' + 'newton'\n\n\nThe commands string.upper() and string.capitalize() can be used to convert all letters in the string to uppercase and capitalize the first letter in the string, respectively.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nprint(s.upper())\nprint(s.capitalize())\n\n\nISAAC NEWTON\nIsaac newton"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/1d_operators_functions.html#formatted-print-statements",
    "href": "example_course/course-materials/interactive-sessions/1d_operators_functions.html#formatted-print-statements",
    "title": "Interactive Session 1D",
    "section": "Formatted Print Statements",
    "text": "Formatted Print Statements\nPython’s f-string formatting provides an efficient and readable way to create formatted strings. This is useful for printing variables, formatting numerical output, and displaying messages.\n\nUsing f-strings\nTo use an f-string, place an f before the opening quotation mark of a string literal, and then include variables inside curly braces {}.\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nname = \"Alice\"\nage = 30\n\n# Example of an f-string\nprint(f\"My name is {name} and I am {age} years old.\")\n\n\nMy name is Alice and I am 30 years old.\n\n\n\n\nFormatting Numbers\nYou can also format numbers, especially floating-point numbers, within f-strings by specifying format specifiers inside the curly braces:\n\nFixed Point: Use .nf to display a float with n decimal places.\n\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\npi = 3.141592653589793\n\n# Format to 2 decimal places\nprint(f\"The value of pi is approximately {pi:.2f}.\")\n\n\nThe value of pi is approximately 3.14.\n\n\n\nWidth and Alignment: Use formatting options to align text or numbers.\n\n\n✏️ Try it. Enter and Run the cell below in your .ipynb file.\n\n\n\nCode\nvalue = 123.456\nprint(f\"Value aligned to 10 spaces: |{value:10.2f}|\")\n\n\nValue aligned to 10 spaces: |    123.46|\n\n\n\n\n\nEnd interactive session 1D"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6c_dates.html",
    "href": "example_course/course-materials/interactive-sessions/6c_dates.html",
    "title": "Interactive Session 6C",
    "section": "",
    "text": "Your turn: Try parsing the following dates: ‘2023-07-04 14:30:00’, ‘05/07/2023’, ‘June 6th, 2023’\nCode\n# add your code here!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6c_dates.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/6c_dates.html#getting-started",
    "title": "Interactive Session 6C",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6c_dates.html#introduction",
    "href": "example_course/course-materials/interactive-sessions/6c_dates.html#introduction",
    "title": "Interactive Session 6C",
    "section": "Introduction",
    "text": "Introduction\nIn this interactive session, we’ll explore in more detail to work with dates in pandas, which is a crucial skill for environmental data scientists. We’ll focus on:\n\nParsing dates\nUsing dates as an index for a DataFrame\nSelecting and filtering data based on date ranges\n\nHopefully, by the end of this session, you’ll be more comfortable manipulating time series data in pandas."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6c_dates.html#setting-up",
    "href": "example_course/course-materials/interactive-sessions/6c_dates.html#setting-up",
    "title": "Interactive Session 6C",
    "section": "Setting Up",
    "text": "Setting Up\nFirst, let’s import the necessary libraries and create a sample dataset.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Generate a sample dataset\ndate_rng = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\ntemperature = np.random.normal(loc=15, scale=5, size=len(date_rng))\nrainfall = np.random.exponential(scale=5, size=len(date_rng))\n\ndf = pd.DataFrame(data={'date': date_rng, 'temperature': temperature, 'rainfall': rainfall})\nprint(df.head())\n\n\n        date  temperature   rainfall\n0 2023-01-01    17.483571   1.265410\n1 2023-01-02    14.308678  16.514351\n2 2023-01-03    18.238443   0.061145\n3 2023-01-04    22.615149  17.512635\n4 2023-01-05    13.829233   0.220595"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6c_dates.html#parsing-dates",
    "href": "example_course/course-materials/interactive-sessions/6c_dates.html#parsing-dates",
    "title": "Interactive Session 6C",
    "section": "Parsing Dates",
    "text": "Parsing Dates\nPandas provides powerful tools for parsing dates. Let’s explore some common scenarios:\n\nUsing pd.to_datetime()\nThe pd.to_datetime() function is versatile and can handle various date formats.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are parsing strings with different date formats, use the format='mixed' keyword argument to pd.datetime()\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Parse dates in different formats\ndates = ['2023-07-01', '7/2/23', 'July 3, 2023']\nparsed_dates = pd.to_datetime(dates, format='mixed')\nprint(parsed_dates)\n\n\nDatetimeIndex(['2023-07-01', '2023-07-02', '2023-07-03'], dtype='datetime64[ns]', freq=None)\n\n\n\n\nSpecifying Date Format\nSometimes, you need to specify the format explicitly:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ncustom_dates = ['01-Jul-2023', '02-Jul-2023', '03-Jul-2023']\nparsed_custom_dates = pd.to_datetime(custom_dates, format='%d-%b-%Y')\nprint(parsed_custom_dates)\n\n\nDatetimeIndex(['2023-07-01', '2023-07-02', '2023-07-03'], dtype='datetime64[ns]', freq=None)"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6c_dates.html#date-format-strings-cheatsheet",
    "href": "example_course/course-materials/interactive-sessions/6c_dates.html#date-format-strings-cheatsheet",
    "title": "Interactive Session 6C",
    "section": "Date Format Strings Cheatsheet",
    "text": "Date Format Strings Cheatsheet\nCommon format codes for dates and times:\n\n%Y: Year with century as a decimal number (e.g., 2023)\n%y: Year without century as a zero-padded decimal number (e.g., 23)\n%m: Month as a zero-padded decimal number (01-12)\n%d: Day of the month as a zero-padded decimal number (01-31)\n%H: Hour (24-hour clock) as a zero-padded decimal number (00-23)\n%M: Minute as a zero-padded decimal number (00-59)\n%S: Second as a zero-padded decimal number (00-59)\n%f: Microsecond as a decimal number, zero-padded on the left (000000-999999)\n\nAdditional useful codes:\n\n%b: Month as locale’s abbreviated name (e.g., Jan, Feb)\n%B: Month as locale’s full name (e.g., January, February)\n%a: Weekday as locale’s abbreviated name (e.g., Sun, Mon)\n%A: Weekday as locale’s full name (e.g., Sunday, Monday)\n%j: Day of the year as a zero-padded decimal number (001-366)\n%U: Week number of the year (Sunday as the first day of the week)\n%W: Week number of the year (Monday as the first day of the week)\n\nCommon combinations:\n\n%Y-%m-%d: ISO date format (e.g., 2023-05-15)\n%d/%m/%Y: Common date format in some countries (e.g., 15/05/2023)\n%Y-%m-%d %H:%M:%S: ISO date and time format (e.g., 2023-05-15 14:30:00)\n\nRemember, when using these format strings with pandas, you typically use them with functions like pd.to_datetime() or when setting the date_format parameter in read_csv()."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6c_dates.html#using-dates-as-dataframe-index",
    "href": "example_course/course-materials/interactive-sessions/6c_dates.html#using-dates-as-dataframe-index",
    "title": "Interactive Session 6C",
    "section": "Using Dates as DataFrame Index",
    "text": "Using Dates as DataFrame Index\nSetting the date column as the index can make time-based operations more intuitive:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Set 'date' as the index\ndf = df.set_index('date')\nprint(df.head())\n\n\n            temperature   rainfall\ndate                              \n2023-01-01    17.483571   1.265410\n2023-01-02    14.308678  16.514351\n2023-01-03    18.238443   0.061145\n2023-01-04    22.615149  17.512635\n2023-01-05    13.829233   0.220595\n\n\nOnce the index is set to a datetime, you can use the .loc selector to locate specific data in the dataframe:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Accessing data for a specific date\nprint(df.loc['2023-01-15'])\n\n\ntemperature    6.375411\nrainfall       3.030639\nName: 2023-01-15 00:00:00, dtype: float64\n\n\n\nResampling Time Series Data\nWith dates as the index, we can easily resample our data using the resample command and a resampling interval.\n\n\n\n\n\n\nCommon Datetime Arguments for resample\n\n\n\n\n'D': Calendar day\n'W': Week (Sunday)\n'W-MON': Week (Monday)\n'ME': Month end\n'MS': Month start\n'QE': Quarter end\n'QS': Quarter start\n'YE' or 'AE': Year end\n'YS' or 'AS': Year start\n'H': Hourly\n'T' or 'min': Minutely\n'S': Secondly\n\nNote: You can also use multiples, e.g., 2D for every 2 days, 4H for every 4 hours.\n\n\n\n\n\n\n\n\nChanges in monthly resampling options\n\n\n\nThe M, ME, and MS options relate to monthly resampling:\n\nM (Deprecated): This used to represent the end of the month. It’s being phased out due to ambiguity.\nME (Month End): This is the direct replacement for M. It explicitly represents the last day of each month. When you resample with ME, the resulting timestamps will be on the last day of each month.\nMS (Month Start): This represents the first day of each month. When you resample with MS, the resulting timestamps will be on the first day of each month.\n\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Resample to monthly average\nmonthly_avg = df.resample('ME').mean()\nprint(monthly_avg.head())\n\n# Plot monthly average temperature\nplt.figure(figsize=(12, 6))\nmonthly_avg['temperature'].plot()\nplt.title('Monthly Average Temperature')\nplt.xlabel('Date')\nplt.ylabel('Temperature (°C)')\nplt.show()\n\n\n            temperature  rainfall\ndate                             \n2023-01-31    13.992562  6.531653\n2023-02-28    14.284158  7.272248\n2023-03-31    15.219692  6.984496\n2023-04-30    14.898742  4.785105\n2023-05-31    14.580927  3.228182\n\n\n\n\n\n\n\n\n\nYour turn: Try resampling the data to get weekly maximum rainfall. Plot the results."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6c_dates.html#selecting-and-filtering-by-date-ranges",
    "href": "example_course/course-materials/interactive-sessions/6c_dates.html#selecting-and-filtering-by-date-ranges",
    "title": "Interactive Session 6C",
    "section": "Selecting and Filtering by Date Ranges",
    "text": "Selecting and Filtering by Date Ranges\nPandas makes it easy to select data for specific date ranges. You can use list and array style slicing to get a range of dates from a datetime index.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Select data for the first quarter of 2023\nq1_data = df['2023-01-01':'2023-03-31']\nprint(q1_data.head())\n\n\n            temperature   rainfall\ndate                              \n2023-01-01    17.483571   1.265410\n2023-01-02    14.308678  16.514351\n2023-01-03    18.238443   0.061145\n2023-01-04    22.615149  17.512635\n2023-01-05    13.829233   0.220595\n\n\nDatetime objects contain a number of methods and attributes that provide information about them.\n\n\n\n\n\n\nDatetime Attributes:\n\n\n\n\n.year: Year of the datetime\n.month: Month of the datetime (1-12)\n.day: Day of the month\n.hour: Hour (0-23)\n.minute: Minute (0-59)\n.second: Second (0-59)\n.weekday(): Day of the week (0-6, where 0 is Monday)\n\n\n\nYou can use these attributes to easily filter datetime indicies:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Filter for summer months (June, July, August)\nsummer_data = df[(df.index.month &gt;= 6) & (df.index.month &lt;= 8)]\nprint(summer_data.head())\n\n\n            temperature  rainfall\ndate                             \n2023-06-01    16.732241  6.896786\n2023-06-02    11.599876  1.019898\n2023-06-03    16.161268  1.174495\n2023-06-04    16.465362  2.313926\n2023-06-05    11.428243  3.313313\n\n\n\nUseful Datetime Methods:\n\n.date(): Returns the date part of the datetime\n.time(): Returns the time part of the datetime\n.strftime(format): Converts datetime to string according to given format\n.isoformat(): Returns a string representation of the date in ISO 8601 format\n\n\n\nMore examples of datetime selections:\n\n\nCode\n# Select data for a specific year\ndf_2023 = df[df.index.year == 2023]\nprint(\"Shape of 2023 data:\", df_2023.shape)\n\n\nShape of 2023 data: (365, 2)\n\n\n\n\nCode\n# Select data for a specific month\ndf_june = df[df.index.month == 6]\nprint(\"Shape of June data:\", df_june.shape)\n\n\nShape of June data: (30, 2)\n\n\n\n\nCode\n# Select data for weekdays only (weekdays are zero-indexed!)\ndf_weekdays = df[df.index.weekday &lt; 5]\nprint(\"Shape of weekday data:\", df_weekdays.shape)\n\n\nShape of weekday data: (260, 2)\n\n\n\n\nCode\n# Select data for a specific date range\ndf_q2 = df['2023-04-01':'2023-06-30']\nprint(\"Shape of Q2 data:\", df_q2.shape)\n\n\nShape of Q2 data: (91, 2)\n\n\n\n\nCode\n# Resample to monthly frequency\ndf_monthly = df.resample('MS').mean()\nprint(\"Shape of monthly data:\", df_monthly.shape)\n\n\nShape of monthly data: (12, 2)\n\n\n\n\nCode\n# Demonstrate some useful datetime methods\nprint(\"\\nFirst date in ISO format:\", df.index[0].isoformat())\n\nprint(\"Last date formatted:\", df.index[-1].strftime('%B %d, %Y'))\n\n\n\nFirst date in ISO format: 2023-01-01T00:00:00\nLast date formatted: December 31, 2023\n\n\nYour turn: Select data for all Mondays in the dataset. Calculate the average temperature for Mondays.\n\n✏️ Try it. Add the cell below to your notebook and write some code to solve the problem above.\n\n\n\nCode\n# Add your code here!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6c_dates.html#key-points",
    "href": "example_course/course-materials/interactive-sessions/6c_dates.html#key-points",
    "title": "Interactive Session 6C",
    "section": "Key Points",
    "text": "Key Points\n\nUse pd.to_datetime() to parse dates in various formats.\nSetting dates as the DataFrame index enables powerful time-based operations.\nResampling allows you to change the frequency of your time series data.\nPandas provides flexible ways to select and filter data based on date ranges."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6c_dates.html#resources",
    "href": "example_course/course-materials/interactive-sessions/6c_dates.html#resources",
    "title": "Interactive Session 6C",
    "section": "Resources",
    "text": "Resources\n\nPandas Time Series documentation\nDatetime documentation\n[EDS 217 Cheatsheet: Working with Dates in Pandas]\n\nRemember, working with dates and time series is a fundamental skill in environmental data science. Practice these concepts with your own datasets to become proficient! ::: {.center-text .body-text-xl .teal-text} End interactive session 6C :::"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#getting-started",
    "title": "Interactive Session 5B",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#introduction-to-data-cleaning",
    "href": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#introduction-to-data-cleaning",
    "title": "Interactive Session 5B",
    "section": "Introduction to Data Cleaning",
    "text": "Introduction to Data Cleaning\nData cleaning is a crucial step in the data science workflow. It involves identifying and correcting errors, inconsistencies, and inaccuracies in datasets to ensure the quality and reliability of your analysis.\nIn this session, we’ll explore common issues in dataframes and learn how to address them using pandas."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#instructions",
    "href": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#instructions",
    "title": "Interactive Session 5B",
    "section": "Instructions",
    "text": "Instructions\nWe will work through this material together, writing a new notebook as we go.\n\n✏️     This symbol designates code you should add to your notebook and run.\n\n🤓 Where useful, this session contains links to Pandas Tutor, which helps you to visualize the chained functions in the accompanying code block.\n\n\nLet’s start by importing pandas and creating a sample dataframe with some issues we’ll need to clean:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Create a sample dataframe with issues\ndata = {\n    'species': ['Oak', 'Pine', 'Maple', 'Oak', 'Pine', None],\n    'height_m': [5.2, 12.0, '7.5', 5.2, 15.0, 8.1],\n    'diameter_cm': [20, 35, 25, 20, 40, np.nan],\n    'location': ['Park A', 'Park B', 'Park A', 'Park A', 'Park B', 'Park C '],\n    'date_planted': ['2020-01-15', '2019-05-20', '2020-03-10', '2020-01-15', '2018-11-30', '2021-07-05']\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n\n  species height_m  diameter_cm location date_planted\n0     Oak      5.2         20.0   Park A   2020-01-15\n1    Pine     12.0         35.0   Park B   2019-05-20\n2   Maple      7.5         25.0   Park A   2020-03-10\n3     Oak      5.2         20.0   Park A   2020-01-15\n4    Pine     15.0         40.0   Park B   2018-11-30\n5    None      8.1          NaN  Park C    2021-07-05"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#handling-missing-values",
    "href": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#handling-missing-values",
    "title": "Interactive Session 5B",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\n\nIdentifying Missing Values\nFirst, let’s check for missing values in our dataframe. For this we use the isnull() method on the dataframe.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.isnull())\n\n\n   species  height_m  diameter_cm  location  date_planted\n0    False     False        False     False         False\n1    False     False        False     False         False\n2    False     False        False     False         False\n3    False     False        False     False         False\n4    False     False        False     False         False\n5     True     False         True     False         False\n\n\nYou can see that the isnull() command returns a Booelan (True or False) value for each item in the dataframe. If the location (row, column) is empty, then the isnull() command will return True, otherwise it returns False.\nWe can apply the sum() method to the result of df.isnull() to see what columns have empty values in them.\n\n\n\n\n\n\nImportant\n\n\n\nThe axis argument is often used in pandas and numpy to indicate how an aggregation (e.g. sum()) should be applied. You should read this argument as an answer to the question:\n\nWhat should I apply this aggregation across, rows (axis 0) or columns (axis 1)?\n\ndf.sum(axis=0) adds up all the rows and returns a single sum for each column.\ndf.sum(axis=1) adds up all the columns and returns a single sum for each row.\nGenerally, aggregations over all rows are more useful than aggregations across all columns, so the default for pandas and numpy aggregations is to apply aggregations and dataframe operations assuming axis=0. However, as we’ll see, other commands default to axis=1.\nSome commands allow you to use alias string arguments (rows and columns), but this isn’t universal across the libarary.\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Use the axis argument as an integer (0)\nnull_values = df.isnull()\nprint(\"Using `axis=0`:\\n\",\n        null_values.sum(axis=0))\n\n# Show that this is the same as using the `axis='rows'` argument:\nprint(\"\\nUsing `axis='rows':\\n\",\n        null_values.sum(axis='rows'))\n\n# And that this is the same as the default behavior:\nprint(\"\\nUsing default arguments:\\n\",\n        null_values.sum())\n\n\nUsing `axis=0`:\n species         1\nheight_m        0\ndiameter_cm     1\nlocation        0\ndate_planted    0\ndtype: int64\n\nUsing `axis='rows':\n species         1\nheight_m        0\ndiameter_cm     1\nlocation        0\ndate_planted    0\ndtype: int64\n\nUsing default arguments:\n species         1\nheight_m        0\ndiameter_cm     1\nlocation        0\ndate_planted    0\ndtype: int64\n\n\nAs we requested, this command sums up all the rows in each column of null_values. Any False is a 0 and any True is a 1, so the result is the number of null values in each column of the dataframe.\n\nMethod chaining allows us to do both the finding of null values and the summing of values for all rows in each column with a single line of code\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Use method chaining to make our code more concise.\ndf.isnull().sum(axis='rows')\n\n\nspecies         1\nheight_m        0\ndiameter_cm     1\nlocation        0\ndate_planted    0\ndtype: int64\n\n\n🤓 Pandas Tutor\n\n\nDropping Missing Values\nWe can drop rows with missing values using the dropna() function:\n\n\nCode\ndf_dropped = df.dropna()\nprint(df_dropped)\n\n\n  species height_m  diameter_cm location date_planted\n0     Oak      5.2         20.0   Park A   2020-01-15\n1    Pine     12.0         35.0   Park B   2019-05-20\n2   Maple      7.5         25.0   Park A   2020-03-10\n3     Oak      5.2         20.0   Park A   2020-01-15\n4    Pine     15.0         40.0   Park B   2018-11-30\n\n\nNotice how we didn’t need to specify an axis - by default, dropna() operates on each row and removes and rows that are any missing values (the default is axis='rows').\n\n\nFilling Missing Values (Imputation)\nWhen you drop data (using methods like dropna() or drop()), you’re permanently removing information from your dataset.\nThis can potentially lead to:\n\nLoss of important insights\nBiased results\nReduced statistical power\nSmaller sample size, which can affect the reliability of your analysis\n\nImputation is the process of replacing missing values with substituted values. Instead of dropping rows or columns with missing data, you fill in the gaps.\n\nCommon imputation techniques include:\n\nMean/median/mode imputation\nForward fill or backward fill\nInterpolation\nUsing machine learning models to predict missing values\n\n\n\nOther techniques:\n\nCreating a “missing” category for categorical variables\nUsing algorithms that can handle missing data (like some decision tree-based methods)\nMultiple imputation for more rigorous statistical analysis\n\nWhen to consider alternatives:\n\nWhen missing data is not completely at random (MCAR)\nWhen you have a small dataset and can’t afford to lose samples\nWhen the missing data might contain important information about your problem\n\nYou can use any of the pandas Series aggregation commands to fill missing values instead of dropping the data.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Fill missing values with a specific value\ndf['species'] = df['species'].fillna('unknown')\n\nprint(df)\n\n\n   species height_m  diameter_cm location date_planted\n0      Oak      5.2         20.0   Park A   2020-01-15\n1     Pine     12.0         35.0   Park B   2019-05-20\n2    Maple      7.5         25.0   Park A   2020-03-10\n3      Oak      5.2         20.0   Park A   2020-01-15\n4     Pine     15.0         40.0   Park B   2018-11-30\n5  unknown      8.1          NaN  Park C    2021-07-05\n\n\nWe can even use aggregations to fill with values derived from our dataframe.\nFor example, let’s replace missing values of diameter_cm with the average value across all the rows.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Fill missing numeric values with the mean of the column\ndf['diameter_cm'] = df['diameter_cm'].fillna(df['diameter_cm'].mean())\n\n\n🤓 Pandas Tutor"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#dealing-with-duplicates",
    "href": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#dealing-with-duplicates",
    "title": "Interactive Session 5B",
    "section": "Dealing with Duplicates",
    "text": "Dealing with Duplicates\n\nIdentifying and Removing Duplicate Rows\nLet’s check for and remove duplicate rows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Check for duplicates\nprint(df.duplicated())\n\n\n0    False\n1    False\n2    False\n3     True\n4    False\n5    False\ndtype: bool\n\n\nIt looks like row 3 is a duplicate (it is the same as row 0). As before, we can see how many rows are duplicated by applying the sum command to the result of df.duplicated()\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndf.duplicated().sum()\n\n\nnp.int64(1)\n\n\n🤓 Pandas Tutor\nThe drop_duplicates() method returns a new dataframe that only contains the first row of any duplicated rows.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndf_no_duplicates = df.drop_duplicates()\nprint(df_no_duplicates)\n\n\n   species height_m  diameter_cm location date_planted\n0      Oak      5.2         20.0   Park A   2020-01-15\n1     Pine     12.0         35.0   Park B   2019-05-20\n2    Maple      7.5         25.0   Park A   2020-03-10\n4     Pine     15.0         40.0   Park B   2018-11-30\n5  unknown      8.1         28.0  Park C    2021-07-05\n\n\nThe extra entry for Oak no longer appears in df_no_duplicates.\n\nWhat if we wanted to simply get rid of the duplicates in our original df without having to make an entirely new dataframe? the inplace option allows for this with many pandas methods:\ndf.drop_duplicates(inplace=True)\n\n\n\n\n\n\nImportant\n\n\n\nWhile inplace=True can be useful when making changes to a dataframe without having to worry about creating a copy, you can’t do method chaining when using this argument.\n\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Make a copy of our dataframe\ndf2 = df.copy()\n# Remove the duplicates from df2 without making a new dataframe (save results back into df2)\ndf2.drop_duplicates(inplace=True)\nprint(df2)\n\n\n   species height_m  diameter_cm location date_planted\n0      Oak      5.2         20.0   Park A   2020-01-15\n1     Pine     12.0         35.0   Park B   2019-05-20\n2    Maple      7.5         25.0   Park A   2020-03-10\n4     Pine     15.0         40.0   Park B   2018-11-30\n5  unknown      8.1         28.0  Park C    2021-07-05\n\n\n\n\nHandling Duplicates Based on Specific Columns\nWe can also remove duplicates based on specific columns, in this case removing any rows that share the same species and location.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ndf_unique_species = df.drop_duplicates(subset=['species', 'location'])\nprint(df_unique_species)\n\n\n   species height_m  diameter_cm location date_planted\n0      Oak      5.2         20.0   Park A   2020-01-15\n1     Pine     12.0         35.0   Park B   2019-05-20\n2    Maple      7.5         25.0   Park A   2020-03-10\n5  unknown      8.1         28.0  Park C    2021-07-05\n\n\nAlthough our two Pines weren’t duplciates (their height_m, diameter_cm, and date_planted were different), we still dropped them from the dataframe based on the subset of columns (species and location)\n🤓 Pandas Tutor"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#data-type-conversion-and-consistency",
    "href": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#data-type-conversion-and-consistency",
    "title": "Interactive Session 5B",
    "section": "Data Type Conversion and Consistency",
    "text": "Data Type Conversion and Consistency\n\nChecking and Changing Data Types\nOften datasets - especially those collected by surveys or forms - contain a mixture of data types (i.e. some strings mixed in with mostly numbers)\nLet’s check the data types of our columns and convert them as needed:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(df.dtypes)\n\n\nspecies          object\nheight_m         object\ndiameter_cm     float64\nlocation         object\ndate_planted     object\ndtype: object\n\n\nThe object datatype is a generic term meaning “something, I don’t know what” in python (remember, in python everything is an object).\nGenerally, we want our data types to be something more specific, like a floating point number, an integer, a string, or a date. We can use the astype() method to coerce our data into a specific kind of thing.\n\nIn older versions of pandas, string columns were always still listed as type object. They are functionally str objects, but pandas isn’t storing them in any special “pandas” way, so they are just generic python objects. Newer versions of pandas allow you to create string (note: not the same as str) data types. They are optimized for use in pandas, although you will rarely see any difference in performance, it’s good practice to use them when you can.\n\nLet’s convert height_m to float.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Convert 'height_m' to float\ndf['height_m'] = df['height_m'].astype(float)\nprint(df.dtypes)\n\n\nspecies          object\nheight_m        float64\ndiameter_cm     float64\nlocation         object\ndate_planted     object\ndtype: object\n\n\nConverting generic objects to datetime is more complicated. In fact, we’ll have an entire session later this class on working with dates. Pandas has a helper function - pd.to_datetime() - that tries to infer dates from values in columns.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Convert 'date_planted' to datetime\ndf['date_planted'] = pd.to_datetime(df['date_planted'])\n\nprint(df.dtypes)\n\n\nspecies                 object\nheight_m               float64\ndiameter_cm            float64\nlocation                object\ndate_planted    datetime64[ns]\ndtype: object"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#string-manipulation-and-formatting",
    "href": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#string-manipulation-and-formatting",
    "title": "Interactive Session 5B",
    "section": "String Manipulation and Formatting",
    "text": "String Manipulation and Formatting\nWe can use string methods to clean text data. We access these methods using the .str attribute that is part of every pandas Series.\n\n\n\n\n\n\nNote\n\n\n\nRemember, every column in a DataFrame is a Series\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(\"'unkown' should be capitalized\")\nprint(df)\n\n# Capitalize species names (unknown -&gt; Unknown)\ndf['species'] = df['species'].str.capitalize()\n\nprint(\"\\nFixed it!\")\nprint(df)\n\n\n'unkown' should be capitalized\n   species  height_m  diameter_cm location date_planted\n0      Oak       5.2         20.0   Park A   2020-01-15\n1     Pine      12.0         35.0   Park B   2019-05-20\n2    Maple       7.5         25.0   Park A   2020-03-10\n3      Oak       5.2         20.0   Park A   2020-01-15\n4     Pine      15.0         40.0   Park B   2018-11-30\n5  unknown       8.1         28.0  Park C    2021-07-05\n\nFixed it!\n   species  height_m  diameter_cm location date_planted\n0      Oak       5.2         20.0   Park A   2020-01-15\n1     Pine      12.0         35.0   Park B   2019-05-20\n2    Maple       7.5         25.0   Park A   2020-03-10\n3      Oak       5.2         20.0   Park A   2020-01-15\n4     Pine      15.0         40.0   Park B   2018-11-30\n5  Unknown       8.1         28.0  Park C    2021-07-05\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nprint(\"'Park C ' should be 'Park C'\")\nprint(df)\n\n# Remove leading/trailing whitespace from location\n# \"Park C \" -&gt; \"Park C\"\ndf['location'] = df['location'].str.strip()\n\nprint(\"\\nFixed it!\")\nprint(df)\n\n\n'Park C ' should be 'Park C'\n   species  height_m  diameter_cm location date_planted\n0      Oak       5.2         20.0   Park A   2020-01-15\n1     Pine      12.0         35.0   Park B   2019-05-20\n2    Maple       7.5         25.0   Park A   2020-03-10\n3      Oak       5.2         20.0   Park A   2020-01-15\n4     Pine      15.0         40.0   Park B   2018-11-30\n5  Unknown       8.1         28.0  Park C    2021-07-05\n\nFixed it!\n   species  height_m  diameter_cm location date_planted\n0      Oak       5.2         20.0   Park A   2020-01-15\n1     Pine      12.0         35.0   Park B   2019-05-20\n2    Maple       7.5         25.0   Park A   2020-03-10\n3      Oak       5.2         20.0   Park A   2020-01-15\n4     Pine      15.0         40.0   Park B   2018-11-30\n5  Unknown       8.1         28.0   Park C   2021-07-05"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#wrap-up-and-best-practices",
    "href": "example_course/course-materials/interactive-sessions/5b_cleaning_data.html#wrap-up-and-best-practices",
    "title": "Interactive Session 5B",
    "section": "Wrap-up and Best Practices",
    "text": "Wrap-up and Best Practices\nIn this session, we’ve covered essential techniques for cleaning dataframes in pandas: - Handling missing values - Dealing with duplicates - Converting data types - String manipulation and formatting\nRemember these best practices:\n\nAlways examine your data before and after cleaning steps.\nRemember that the default for most operations is to act across all rows (axis=0).\nDocument your cleaning steps for reproducibility.\nBe cautious when dropping data - sometimes imputation or other techniques might be more appropriate.\n\nFor more advanced cleaning techniques and in-depth explanations, refer to the pandas documentation Pandas Documentation, the master Pandas Cheat Sheet, or our class Cleaning Data Cheatsheet\n\nEnd interactive session 5B"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_lists.html",
    "href": "example_course/course-materials/interactive-sessions/2a_lists.html",
    "title": "Interactive Session 2A",
    "section": "",
    "text": "Unsplash list image\nPython has four collection data types, the most common of which is the list. This session introduces lists and a few of the important list operations. We will also cover indexing, a key feature of programming."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_lists.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/2a_lists.html#getting-started",
    "title": "Interactive Session 2A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_2A_Lists.ipynb\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day 2: Session A - Lists\n\n[Link to session webpage](https://eds-217-essential-python.github.io/course-materials/interactive-sessions/2a_lists.html)\n\nDate: 09/04/2024\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_lists.html#session-1.2-topics",
    "href": "example_course/course-materials/interactive-sessions/2a_lists.html#session-1.2-topics",
    "title": "Interactive Session 2A",
    "section": "Session 1.2 Topics",
    "text": "Session 1.2 Topics"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_lists.html#instructions",
    "href": "example_course/course-materials/interactive-sessions/2a_lists.html#instructions",
    "title": "Interactive Session 2A",
    "section": "Instructions",
    "text": "Instructions\n\n\n\n\n\n\nNote\n\n\n\n🐍     This symbol designates an important note about Python structure, syntax, or another quirk.\n\n\n\n✏️     This symbol designates code you should add to your notebook and run."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_lists.html#lists",
    "href": "example_course/course-materials/interactive-sessions/2a_lists.html#lists",
    "title": "Interactive Session 2A",
    "section": "Lists",
    "text": "Lists\nA list is a Python object used to contain multiple values. Lists are ordered and changeable. They are defined as follows:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Define list variables\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\nstr_list = ['energy', 'water', 'carbon']\n\n\nWhile you can create lists containing mixed data types, this is not usually recommended.\nThe len() command returns the length of the list.\n\n\nCode\nlen(str_list)\n\n\n3\n\n\nThe min() and max() commands are used to find the minimum and maximum values in a list. For a list of strings, this corresponds to the alphabetically first and last elements.\n\n\nCode\nmin(str_list)\n\n\n'carbon'\n\n\n\n\nCode\nmax(str_list)\n\n\n'water'\n\n\n\n✏️ Try it. Use the len(), min(), and max() commands to find the length, minimum, and maximum of num_list.\n\n\nIndexing\nThe index is used to reference a value in an iterable object by its position. To access an element in a list by its index, use square brackets [].\n\n\n\n\n\n\nNote\n\n\n\n🐍 Python is zero-indexed. This means that the first element in the list is 0, the second is 1, and so on. The last element in a list with \\(n\\) elements is \\(n - 1\\).\n\n\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\n\n\nCode\nnum_list[2]\n\n\n654\n\n\nYou can also access an element based on its position from the end of the list.\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\n\n\nCode\nnum_list[-2]\n\n\n-12\n\n\n\n✏️ Try it. Find the 2nd element in str_list in two different ways. Remember that Python is zero-indexed!\n\n\n\nSlicing\nAccessing a range of values in a list is called slicing. A slice specifies a start and an endpoint, generating a new list based on the indices. The indices are separated by a :.\n\n\n\n\n\n\nNote\n\n\n\n🐍 The endpoint index in the slice is exclusive. To slice to the end of the list, omit an endpoint.\n\n\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\nThis code returns the 2nd and 3rd elements of the num_list\n\n\nCode\nnum_list[2:4]\n\n\n[654, 2]\n\n\nThis code would return everything from the 4th element to the end of the list:\n\n\nCode\nnum_list[3:]\n\n\n[2, 0, -12, 4391]\n\n\n\n✏️ Try it. Before running each of the following commands in a new cell in your notebook, try to determine what output you expect\n\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\n\n\nCode\nnum_list[2:6]\n\n\n\n\nCode\nnum_list[0:4]   \n\n\n\n\nCode\nnum_list[:4]    \n\n\n\n\nCode\nnum_list[-6:-1] \n\n\nAlthough less common, it is also possible to specify a step size, i.e. [start:stop:step]. A step size of 1 would select every element, 2 would select every other element, etc…\n\n\nCode\nnum_list[0:4:2]  \n\n\n[4, 654]\n\n\n\n\nCode\nnum_list[::2]\n\n\n[4, 654, 0, 4391]\n\n\nA step of -1 returns the list in reverse.\n\n\nCode\nnum_list[::-1]\n\n\n[4391, -12, 0, 2, 654, 23, 4]\n\n\n\n\nString indicies\nLike lists, strings can also be indexed using the same notation. This can be useful for many applications, such as selecting files in a certain folder for import based on their names or extension.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nword_str = 'antidisestablishmentarianism'\n\n\n\n\nCode\nword_str[14]\n\n\n's'\n\n\n\n\nCode\nword_str[::3]\n\n\n'aistlhnrnm'\n\n\n\n\nList Operations\nElements can be added to a list using the command list.append().\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\ncolors = ['red', 'blue', 'green', 'black', 'white', 'pink']\n\n\nYou can add an element to a list in a specific position using the command list.insert(i, x) where i is the position where the element x should be added.\n\n✏️  Try it.  Add ‘purple’ to the list colors between ‘green’ and ‘black’.\n\nThere are multiple ways to remove elements from a list. The commands list.pop() and del remove elements based on indices.\ncolors.pop()       # removes the last element\ncolors.pop(2)      # removes the third element\ndel colors[2]      # removes the third element\ndel colors[2:4]    # removes the third and fourth elements\nThe command list.remove() removes an element based on its value.\ncolors.remove('red')\nprint(colors)\n&gt;&gt;&gt; ['blue', 'green', 'black', 'purple', 'white', 'pink']\n\n\nLet’s reset our color list:\n\n\nCode\ncolors = ['red', 'blue', 'green', 'purple', 'black', 'white', 'pink']\n\n\n\n✏️  Try it.  Remove pink and purple from colors, using del for one of the strings and list.remove() for the other.\n\n\n\nSorting Lists\nYou can sort the elements in a list (numerically or alphabetically) in two ways. The first uses the command list.sort().\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nrand_list = [5.1 , 3.42 , 3.333 , 100.4 , 0.5 , 26.0 , 7.44 , 5.8 , 39.0]\nrand_list.sort()\nprint(rand_list)\n\n\n[0.5, 3.333, 3.42, 5.1, 5.8, 7.44, 26.0, 39.0, 100.4]\n\n\nSetting reverse=True within this command sorts the list in reverse order:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nrand_list = [5.1 , 3.42 , 3.333 , 100.4 , 0.5 , 26.0 , 7.44 , 5.8 , 39.0]\nrand_list.sort(reverse=True)\nprint(rand_list)\n\n\n[100.4, 39.0, 26.0, 7.44, 5.8, 5.1, 3.42, 3.333, 0.5]\n\n\nSo far, all of the list commands we’ve used have been in-place operators. This means that they perform the operation to the variable in place without requiring a new variable to be assigned. By contrast, standard operators do not change the original list variable. A new variable must be set in order to retain the operation.\n\n✏️  Try it.  Verify that rand_list was, in fact, sorted in place by using the min() and max() functions to determine the minmum and maximum values in the list and printing the first and last values in the list.\n\n\n\nCode\n# Step 1: Use f-strings to print the min and max values in rand_list.\n\n# Step 2: Use f-strings to print the first and last values in rand_list.\n\n\nThe other method of sorting a list is to use the sorted() command, which does not change the original list. Instead, the sorted list must be assigned to a new variable.\n\n\nCode\nrand_list = [5.1 , 3.42 , 3.333 , 100.4 , 0.5 , 26.0 , 7.44 , 5.8 , 39.0]\nsorted_list = sorted(rand_list)\nprint(rand_list[0])\nprint(sorted_list[0])\n\n\n5.1\n0.5\n\n\nTo avoid changing the original variable when using an in-place operator, it is wise to create a copy. There are multiple ways to create copies of lists, but it is important to know the difference between a true copy and a view.\n\n\n\n\n\n\nImportant\n\n\n\nThe difference between a copy and a view is a critical topic in Python and something we will come back to later in the class when working with DataFrames. Python only makes a copy of data when it is necessary, so make sure you understand the difference!\n\n\nA view of a list can be created as follows:\n\n\nCode\nstr_list = ['energy', 'water', 'carbon']\nstr_list_view = str_list\n\n\nAny in-place operation performed on str_list_view will also be applied to str_list. For example, look what happens when you run this code:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Sorting the str_list_view...\nstr_list_view.sort()\n\n# Actually sorts the original str_list!\nprint(str_list)\n\n\n['carbon', 'energy', 'water']\n\n\n\n\n\n\n\n\nImportant\n\n\n\nstr_list_view was just a “view” into to the str_list variable, but a copy!\n\n\nTo avoid this, create a copy of str_list using any of the following methods:\n\n\nCode\nstr_list_copy = str_list.copy()\n# or\nstr_list_copy = str_list[:]\n# or\nstr_list_copy = list(str_list)\n\n\nIn addition to adding single elements to a list using list.append() or list.insert(), multiple elements can be added to a list at the same time by adding multiple lists together.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nrainbow  = ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet']\nshades = ['coral', 'chartreuse', 'cyan', 'navy']\nprint( rainbow + shades )\n\n\n['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'coral', 'chartreuse', 'cyan', 'navy']\n\n\nSingle lists can be repeated by multiplying by an integer.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nstr_list2 = str_list * 2\nnum_list4 = num_list * 4\nprint( str_list2 )\nprint( num_list4 )\n\n\n['carbon', 'energy', 'water', 'carbon', 'energy', 'water']\n[4, 23, 654, 2, 0, -12, 4391, 4, 23, 654, 2, 0, -12, 4391, 4, 23, 654, 2, 0, -12, 4391, 4, 23, 654, 2, 0, -12, 4391]\n\n\n\n\nGenerating sequential lists\nSequential lists are valuable tools, particularly for iteration, which we will explore in later sessions. The range() function is used to create an iterable object based on the size of an integer argument.\nrange(4)\n&gt;&gt;&gt; range(0, 4)\nTo construct a sequential list from the range() object, use the list() function.\nlist(range(4))\n&gt;&gt;&gt; [0, 1, 2, 3]\nUsing multiple integer arguments, the range() function can be used to generate sequential lists between two bounds: range(start, stop [, step]).\n\n\n\n\n\n\nNote\n\n\n\n🐍 Like indexing, all Python functions using  start  and  stop  arguments, the  stop  value is  exclusive .\n\n\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nrange_10 = list(range(1,11))\nodds_10 = list(range(1,11,2))\nprint(range_10)\nprint(odds_10)\n\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n[1, 3, 5, 7, 9]\n\n\n\nEnd interactive session 2A"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#getting-started",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#getting-started",
    "title": "Interactive Session 6A",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.11.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#learning-objectives",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#learning-objectives",
    "title": "Interactive Session 6A",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe goals of this session are to:\n\nBe able to use basic sorting methods to organize dataframes\nUnderstand the concept of grouping data in Pandas\nUse the groupby() function to create groups\nApply aggregate functions to grouped data\nPerform multi-level grouping\nReshape data using pivot tables"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#first-steps",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#first-steps",
    "title": "Interactive Session 6A",
    "section": "First Steps",
    "text": "First Steps\nAs always, let’s start our notebook by loading our necessary libaries.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#basic-sorting-in-pandas",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#basic-sorting-in-pandas",
    "title": "Interactive Session 6A",
    "section": "Basic Sorting in Pandas",
    "text": "Basic Sorting in Pandas\nBefore we dive into grouping and aggregation, let’s cover some fundamental sorting operations in Pandas. Sorting your data can provide valuable insights and is often a precursor to more complex data manipulations.\n\nSorting by a Single Column\nTo sort a DataFrame by a single column, we use the sort_values() method:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a sample DataFrame\ndf = pd.DataFrame({\n    'date': pd.date_range(start='2023-01-01', periods=5),\n    'temperature': [20, 18, 22, 19, 21],\n    'precipitation': [0, 5, 2, 8, 3]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n\nOriginal DataFrame:\n        date  temperature  precipitation\n0 2023-01-01           20              0\n1 2023-01-02           18              5\n2 2023-01-03           22              2\n3 2023-01-04           19              8\n4 2023-01-05           21              3\n\n\n\nSorting by temperature (default is in ascending order)\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Sort by temperature\ndf_sorted = df.sort_values('temperature')\nprint(\"\\nSorted by temperature (ascending):\")\nprint(df_sorted)\n\n\n\nSorted by temperature (ascending):\n        date  temperature  precipitation\n1 2023-01-02           18              5\n3 2023-01-04           19              8\n0 2023-01-01           20              0\n4 2023-01-05           21              3\n2 2023-01-03           22              2\n\n\nBy default, sort_values() sorts in ascending order. To sort in descending order, use the ascending parameter:\n\n\nSorting in descending order\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Sort by temperature in descending order\ndf_sorted_desc = df.sort_values('temperature', ascending=False)\nprint(\"Sorted by temperature (descending):\")\nprint(df_sorted_desc)\n\n\nSorted by temperature (descending):\n        date  temperature  precipitation\n2 2023-01-03           22              2\n4 2023-01-05           21              3\n0 2023-01-01           20              0\n3 2023-01-04           19              8\n1 2023-01-02           18              5\n\n\n\n\n\nSorting by Multiple Columns\nYou can sort by multiple columns by passing a list of column names:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Sort by precipitation (ascending) and then temperature (descending)\ndf_multi_sorted = df.sort_values(['precipitation', 'temperature'], ascending=[True, False])\nprint(\"Sorted by precipitation (asc) and temperature (desc):\")\nprint(df_multi_sorted)\n\n\nSorted by precipitation (asc) and temperature (desc):\n        date  temperature  precipitation\n0 2023-01-01           20              0\n2 2023-01-03           22              2\n4 2023-01-05           21              3\n1 2023-01-02           18              5\n3 2023-01-04           19              8\n\n\nIn this example, we first sort by precipitation in ascending order, and then by temperature in descending order for any rows with the same precipitation value.\n\n\nSorting Index\nIf you want to sort by the index instead of a column, use sort_index():\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Sort by index\ndf_index_sorted = df.sort_index(ascending=False)\nprint(\"Sorted by index (descending):\")\nprint(df_index_sorted)\n\n\nSorted by index (descending):\n        date  temperature  precipitation\n4 2023-01-05           21              3\n3 2023-01-04           19              8\n2 2023-01-03           22              2\n1 2023-01-02           18              5\n0 2023-01-01           20              0\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember, by default sorting operations return a new DataFrame and don’t modify the original unless you use inplace=True."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#introduction-to-grouping",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#introduction-to-grouping",
    "title": "Interactive Session 6A",
    "section": "Introduction to Grouping",
    "text": "Introduction to Grouping\nGrouping data is a powerful technique in data analysis that allows us to split a DataFrame into groups based on some criteria, apply a function to each group independently, and combine the results. This is particularly useful when we want to calculate summary statistics for different categories within our data.\nLet’s start with a simple example using a dataset of environmental measurements across different locations and times.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n# Create a sample DataFrame\ndata = {\n    'location': ['A', 'B', 'A', 'B', 'A', 'B'],\n    'date': pd.date_range(start='2023-01-01', periods=6),\n    'temperature': [20, 22, 19, 24, 21, 23],\n    'humidity': [50, 48, 52, 45, 49, 47]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n\n  location       date  temperature  humidity\n0        A 2023-01-01           20        50\n1        B 2023-01-02           22        48\n2        A 2023-01-03           19        52\n3        B 2023-01-04           24        45\n4        A 2023-01-05           21        49\n5        B 2023-01-06           23        47"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#using-groupby",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#using-groupby",
    "title": "Interactive Session 6A",
    "section": "Using groupby()",
    "text": "Using groupby()\nThe groupby() function is the core of grouping operations in Pandas. It allows us to split the data into groups based on one or more columns.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Group by location\ngrouped = df.groupby('location')\n\n# Calculate mean for each group\nprint(grouped.mean())\n\n\n               date  temperature   humidity\nlocation                                   \nA        2023-01-03         20.0  50.333333\nB        2023-01-04         23.0  46.666667"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#aggregating-data",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#aggregating-data",
    "title": "Interactive Session 6A",
    "section": "Aggregating Data",
    "text": "Aggregating Data\nWe can apply various aggregation functions to our grouped data. Some common ones include mean(), sum(), count(), min(), max(), etc.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Multiple aggregations\nprint(grouped.agg(['mean', 'min', 'max']))\n\n\n               date                       temperature           humidity      \\\n               mean        min        max        mean min max       mean min   \nlocation                                                                       \nA        2023-01-03 2023-01-01 2023-01-05        20.0  19  21  50.333333  49   \nB        2023-01-04 2023-01-02 2023-01-06        23.0  22  24  46.666667  45   \n\n              \n         max  \nlocation      \nA         52  \nB         48  \n\n\nNow it’s your turn! Try to calculate the standard deviation of temperature and humidity for each location.\n\n✏️ Try it. Add the cell below to your notebook and then provide your code.\n\n\n\nCode\n# Your code here"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#multi-level-grouping",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#multi-level-grouping",
    "title": "Interactive Session 6A",
    "section": "Multi-level Grouping",
    "text": "Multi-level Grouping\nWe can group by multiple columns to create a hierarchical index.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Add a 'year' column\ndf['year'] = df['date'].dt.year\n\n# Group by location and year\nmulti_grouped = df.groupby(['location', 'year'])\n\nprint(multi_grouped.mean())\n\n\n                    date  temperature   humidity\nlocation year                                   \nA        2023 2023-01-03         20.0  50.333333\nB        2023 2023-01-04         23.0  46.666667\n\n\n\nUnderstanding Groupby Objects\nAfter using the groupby() function, it’s important to understand what kind of object we’re working with and how it differs from a regular DataFrame. Let’s explore this in more detail.\n\nThe Groupby Object\nWhen you apply the groupby() function to a DataFrame, the result is a DataFrameGroupBy object. This object is not a DataFrame itself, but rather a special object that contains information about the groups.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a groupby object\ngrouped = df.groupby('location')\n\n# Check the type of the grouped object\nprint(type(grouped))\n\n# Try to view the grouped object\nprint(grouped)\n\n\n&lt;class 'pandas.core.groupby.generic.DataFrameGroupBy'&gt;\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x110be2b10&gt;\n\n\nAs you can see, simply printing the groupby object doesn’t show us the data. Instead, it gives us information about the groupby operation.\n\n\n\nAccessing Group Data\nTo actually see the data in each group, we need to iterate over the groups or use aggregation functions.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Iterate over groups\nfor name, group in grouped:\n    print(f\"Group: {name}\")\n    print(group)\n    print()\n\n# Using an aggregation function\nprint(grouped.mean())\n\n\nGroup: A\n  location       date  temperature  humidity  year\n0        A 2023-01-01           20        50  2023\n2        A 2023-01-03           19        52  2023\n4        A 2023-01-05           21        49  2023\n\nGroup: B\n  location       date  temperature  humidity  year\n1        B 2023-01-02           22        48  2023\n3        B 2023-01-04           24        45  2023\n5        B 2023-01-06           23        47  2023\n\n               date  temperature   humidity    year\nlocation                                           \nA        2023-01-03         20.0  50.333333  2023.0\nB        2023-01-04         23.0  46.666667  2023.0\n\n\n\n\nKey Differences from DataFrames\n\nStructure: A groupby object is not a table-like structure like a DataFrame. It’s more like a container of groups.\nDirect Access: You can’t access columns or rows of a groupby object directly like you can with a DataFrame.\nLazy Evaluation: Groupby operations are lazy - they don’t actually compute anything until you call an aggregation function or iterate over the groups.\nAggregation Required: To get meaningful results from a groupby object, you typically need to apply some kind of aggregation function (like mean(), sum(), count(), etc.).\n\n\n\nConverting Groupby Results to DataFrame\nAfter applying an aggregation function to a groupby object, the result is typically a DataFrame:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Result of aggregation is a DataFrame\nresult = grouped.mean()\nprint(type(result))\n\n# We can now use DataFrame methods on this result\nprint(result.reset_index())\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n  location       date  temperature   humidity    year\n0        A 2023-01-03         20.0  50.333333  2023.0\n1        B 2023-01-04         23.0  46.666667  2023.0\n\n\n\nPractice\nTry grouping the data by both ‘location’ and ‘year’, then calculate the maximum temperature for each group. What type of object do you get? How can you reset the index to make it a regular DataFrame?\n\n✏️ Try it. Add the cell below to your notebook and then provide your code.\n\n\n\nCode\n# Your code here\n\n\n\n\nKey Groupby Points\n\nA groupby object is not a DataFrame, but a special object containing group information.\nTo view data in a groupby object, you need to iterate over it or apply aggregation functions.\nGroupby operations are lazy and require aggregation to produce results.\nThe result of aggregating a groupby object is typically a DataFrame or Series, which you can then manipulate further."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#reshaping-dataframes-with-pivot-tables",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#reshaping-dataframes-with-pivot-tables",
    "title": "Interactive Session 6A",
    "section": "Reshaping DataFrames with Pivot Tables",
    "text": "Reshaping DataFrames with Pivot Tables\nPivot tables provide a way to reshape data and calculate aggregations in one step.\n\nHow Pivot Tables Work\n\nReshaping Data: Pivot tables reshape data by turning unique values from one column into multiple columns.\nAggregation: They perform aggregations on a specified value column for each unique group created by the new columns.\nIndex and Columns: You specify which column to use as the new index, which to use as new columns, and which to aggregate.\n\nThe idea is very similar to the df.pivot command: \nThe main difference between df.pivot and df.pivot_table is that df.pivot_table includes aggregation.\nLet’s see an example:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a sample DataFrame\ndata = {\n    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02'],\n    'city': ['New York', 'Los Angeles', 'New York', 'Los Angeles'],\n    'temperature': [32, 68, 28, 72]\n}\ndf = pd.DataFrame(data)\nprint(\"Original DataFrame:\")\nprint(df)\n\n\nOriginal DataFrame:\n         date         city  temperature\n0  2023-01-01     New York           32\n1  2023-01-01  Los Angeles           68\n2  2023-01-02     New York           28\n3  2023-01-02  Los Angeles           72\n\n\n\nUsing df.pivot to rotate the dataframe:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\npivot = pd.pivot(df, values='temperature', index='date', columns='city')\nprint(\"\\nPivot:\")\nprint(pivot)\n\n\n\nPivot:\ncity        Los Angeles  New York\ndate                             \n2023-01-01           68        32\n2023-01-02           72        28\n\n\n\n\nUsing df.pivot_table to create a pivot table:\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a pivot table\npivot_table = df.pivot_table(values='temperature', index='date', columns='city', aggfunc='mean')\nprint(\"\\nPivot Table:\")\nprint(pivot_table)\n\n\n\nPivot Table:\ncity        Los Angeles  New York\ndate                             \n2023-01-01         68.0      32.0\n2023-01-02         72.0      28.0\n\n\nIn this example: - ‘date’ becomes the index - ‘city’ values become new columns - ‘temperature’ values are aggregated (mean) for each date-city combination\n\n\n\n\n\n\nNote\n\n\n\nIn this example, the result of our pivot and pivot_table commands are essentially the same. Why is that the case? When would we expect different results from these two commands?\n\n\n\n\n\nKey Features of Pivot Tables\n\nHandling Duplicates: If there are multiple values for a given index-column combination, an aggregation function (like mean, sum, count) must be specified.\nMissing Data: Pivot tables can reveal missing data, often filling these gaps with NaN.\nMulti-level Index: You can create multi-level indexes and columns for more complex reorganizations.\nFlexibility: You can pivot on multiple columns and use multiple value columns.\n\nPivot tables are especially useful for: - Summarizing large datasets - Creating cross-tabulations - Preparing data for visualization - Identifying patterns or trends across categories\nRemember, while pivot tables are powerful, they work best with well-structured data and clear categorical variables.\n\n✏️ Try it. Add the cell below to your notebook and run it.\n\n\n\nCode\n# Create a pivot table\npivot = df.pivot_table(values='temperature', index='city', columns='date', aggfunc='mean')\nprint(pivot)\n\n\ndate         2023-01-01  2023-01-02\ncity                               \nLos Angeles        68.0        72.0\nNew York           32.0        28.0\n\n\nTry creating a pivot table that shows the maximum humidity for each city and date.\n\n✏️ Try it. Add the cell below to your notebook and then provide your code.\n\n\n\nCode\n# Your code here"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#key-points",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#key-points",
    "title": "Interactive Session 6A",
    "section": "Key Points",
    "text": "Key Points\n\nGrouping allows us to split data based on categories and perform operations on each group.\nThe groupby() function is the primary tool for grouping in Pandas.\nWe can apply various aggregation functions to grouped data.\nMulti-level grouping creates a hierarchical index.\nPivot tables offer a way to reshape and aggregate data simultaneously."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#resources",
    "href": "example_course/course-materials/interactive-sessions/6a_grouping_joining_sorting_1_old.html#resources",
    "title": "Interactive Session 6A",
    "section": "Resources",
    "text": "Resources\n\nPandas Groupby Documentation\nPandas Pivot Table Documentation\n\nDon’t forget to check out our EDS 217 Cheatsheet on Grouping and Aggregating for quick reference!\n\nEnd interactive session 6A"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Geospatial Foundation Models and Applications",
    "section": "Course Description",
    "text": "Course Description\nThis project-driven seminar introduces students to state-of-the-art geospatial foundation models (GFMs) for remote sensing and environmental monitoring. Through a mix of lecture, discussion, and hands-on labs, students will explore frontier techniques in geospatial AI—including self-supervised learning, masked autoencoders, multimodal sensor fusion, and scalable inference pipelines—while developing their own applied project.\nThe goal of GEOG 288KC (Geospatial Foundation Models and Applications) is to equip students with cutting-edge skills in applying foundation models to environmental challenges. By the end of the course, students should be able to:\n\nUnderstand and apply state-of-the-art geospatial foundation models for environmental monitoring\nFine-tune and adapt large-scale models for specific remote sensing applications\nBuild scalable processing pipelines using Earth Engine, TorchGeo, and cloud computing platforms\nCreate interactive applications and APIs for model deployment and visualization\nEvaluate and interpret foundation model outputs using robust validation frameworks\nCollaborate on applied research projects and communicate results to diverse audiences"
  },
  {
    "objectID": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "href": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "title": "Geospatial Foundation Models and Applications",
    "section": "Getting Started with the UCSB AI Sandbox",
    "text": "Getting Started with the UCSB AI Sandbox\nHere are detailed instructions for setting up your environment on the UCSB AI Sandbox, including foundation model installation and GPU optimization."
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Geospatial Foundation Models and Applications",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\nKelly Caylor\nEmail: caylor@ucsb.edu\nLearn more: Bren profile\n\n\n\n\nTA\n\n\n\n\n\n\n\nAnna Boser\nEmail: anaboser@ucsb.edu\nLearn more: Bren profile"
  },
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#geospatial-data-fundamentals",
    "href": "cheatsheets.html#geospatial-data-fundamentals",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#pytorch-for-geospatial-ai",
    "href": "cheatsheets.html#pytorch-for-geospatial-ai",
    "title": "Cheatsheets",
    "section": "🔥 PyTorch for Geospatial AI",
    "text": "🔥 PyTorch for Geospatial AI\n\nPyTorch Tensors & GPU Operations\nTorchGeo Datasets & Transforms\nData Loading for Satellite Imagery"
  },
  {
    "objectID": "cheatsheets.html#foundation-models-huggingface",
    "href": "cheatsheets.html#foundation-models-huggingface",
    "title": "Cheatsheets",
    "section": "🤗 Foundation Models & HuggingFace",
    "text": "🤗 Foundation Models & HuggingFace\n\nLoading Pre-trained Models\nModel Inference & Feature Extraction\nFine-tuning Strategies"
  },
  {
    "objectID": "cheatsheets.html#visualization-analysis",
    "href": "cheatsheets.html#visualization-analysis",
    "title": "Cheatsheets",
    "section": "📊 Visualization & Analysis",
    "text": "📊 Visualization & Analysis\n\nPlotting Satellite Imagery\nInteractive Maps with Folium\nGeospatial Plotting with Matplotlib"
  },
  {
    "objectID": "test_fontawesome.html",
    "href": "test_fontawesome.html",
    "title": "FontAwesome Test",
    "section": "",
    "text": "Testing FontAwesome Icons\nHere are some test icons:\n\n Home\n GitHub\n\n Download\n Rocket\n Brain\n Earth"
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html",
    "href": "example_course/tasks/eod-alignment-action-plan.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html#objective",
    "href": "example_course/tasks/eod-alignment-action-plan.html#objective",
    "title": "",
    "section": "🎯 Objective",
    "text": "🎯 Objective\nSystematically review and fix misalignments between daily course content and end-of-day (EOD) practice activities to ensure students only encounter concepts and tools they have already learned."
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html#critical-issue-identified",
    "href": "example_course/tasks/eod-alignment-action-plan.html#critical-issue-identified",
    "title": "",
    "section": "🚨 Critical Issue Identified",
    "text": "🚨 Critical Issue Identified\n\nDay 1 EOD Practice - MAJOR MISALIGNMENT\n\nProblem: Uses advanced concepts (pandas DataFrames, read_csv(), matplotlib plotting, .head(), .isnull().sum()) before teaching them\nDay 1 actual coverage: Only covers JupyterLab basics, variables, strings, operators, and functions\nImpact: High frustration potential for students on their first day\nStatus: 🚨 IMMEDIATE ACTION REQUIRED"
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html#four-phase-action-plan",
    "href": "example_course/tasks/eod-alignment-action-plan.html#four-phase-action-plan",
    "title": "",
    "section": "📋 Four-Phase Action Plan",
    "text": "📋 Four-Phase Action Plan\n\nPhase 1: Immediate Fixes (Priority 1) - Complete by Course Start\n\n🔴 Task 1.1: Fix Day 1 EOD Activity ✅ COMPLETED\n\nCurrent file: course-materials/eod-practice/eod-day1.qmd\nDraft file: course-materials/eod-practice/eod-day1-draft.qmd ✅ CREATED\nAnswer key: course-materials/answer-keys/eod-day1-draft-key.qmd ✅ CREATED\nAction taken: Enhanced framing approach - kept all existing code but added comprehensive “preview” framing\nKey improvements:\n\nClear “Coming Attractions” messaging\n“Wonder moments” after each step explaining what happened\nLearning timeline showing when concepts will be taught\nEmphasis on copy/paste approach for Day 1\nProper expectation setting for beginners\n\nNext step: Test with stakeholders and replace original when approved\n\n\n\n🟡 Task 1.2: Review Day 4 EOD Activity\n\nCurrent file: course-materials/eod-practice/eod-day4.qmd\nIssue: Uses parse_dates parameter and potentially advanced visualizations\nAction needed:\n\nVerify parse_dates is covered in Day 4 sessions\nCheck if date handling concepts are taught before EOD\nIf not covered, either add to curriculum or simplify EOD\n\nEstimated time: 2-3 hours\n\n\n\n\nPhase 2: Comprehensive Content Audit (Priority 2)\n\nTask 2.1: Create Detailed Content Matrices\n\nDeliverable: Spreadsheet mapping concepts taught vs. concepts used in EOD\nFor each day:\n\nList all pandas methods introduced\nTrack matplotlib/seaborn concepts\nDocument Python syntax/functions taught\nMap library imports and timing\n\nEstimated time: 8-10 hours\n\n\n\nTask 2.2: Examine All Interactive Sessions\n\nFiles to review: All files in course-materials/interactive-sessions/\nExtract: Specific concepts, methods, functions introduced each session\nCross-reference: Against corresponding EOD activities\nEstimated time: 12-15 hours\n\n\n\nTask 2.3: Review Coding Colabs and Live Coding\n\nDirectories: coding-colabs/ and live-coding/\nPurpose: Capture all concepts taught in hands-on sessions\nEstimated time: 6-8 hours\n\n\n\n\nPhase 3: Systematic Review Process (Priority 3)\n\nTask 3.1: Develop Content Alignment Checklist\n## EOD Activity Pre-Flight Checklist\nFor each day's EOD activity, verify:\n\n### Python Fundamentals\n☐ All data types used have been introduced\n☐ All Python syntax patterns are previously taught\n☐ All built-in functions have been covered\n\n### Libraries and Imports\n☐ All pandas methods used have been taught\n☐ All visualization libraries have been introduced  \n☐ All NumPy functions are previously covered\n☐ All external libraries are imported and explained\n\n### Concept Complexity\n☐ Data manipulation concepts match skill level\n☐ Dataset complexity is appropriate\n☐ Problem-solving approach aligns with taught methods\n\n### Learning Progression\n☐ Builds incrementally on previous days\n☐ No \"surprise\" advanced concepts\n☐ Matches expected difficulty curve\n\n\nTask 3.2: Create Review Workflow Documentation\n\nBefore each course offering: Run alignment checklist\nAfter content updates: Re-verify alignment when changing sessions\nStudent feedback integration: Add alignment questions to evaluations\nEstimated time: 3-4 hours\n\n\n\n\nPhase 4: Process Integration (Priority 4)\n\nTask 4.1: Update Course Development Documentation\n\nFile: tasks/2025-course-updates.md\nAdd new section: “End-of-Day Activity Alignment Review”\nInclude:\n\nAlignment verification requirements\nChecklist integration into workflow\nResponsibility assignments\n\n\n\n\nTask 4.2: Create EOD Activity Template\n## New EOD Activity Template\n### Prerequisites\n- List all concepts/libraries required\n- Reference specific day/session where taught\n\n### Concept Verification\n- [ ] All methods used are previously taught\n- [ ] Difficulty matches daily progression\n- [ ] No \"surprise\" advanced concepts\n\n### Skills Assessment\n- Primary skills practiced:\n- Secondary skills reinforced:\n- New combinations introduced:"
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html#day-by-day-status-and-actions",
    "href": "example_course/tasks/eod-alignment-action-plan.html#day-by-day-status-and-actions",
    "title": "",
    "section": "🎯 Day-by-Day Status and Actions",
    "text": "🎯 Day-by-Day Status and Actions\n\n\n\n\n\n\n\n\n\n\nDay\nCurrent Status\nSpecific Action Required\nPriority\nEst. Hours\n\n\n\n\nDay 1\n🚨 Critical\nComplete rewrite - remove pandas/matplotlib\nP1\n4-6\n\n\nDay 2\n✅ Good\nNo changes needed\n-\n0\n\n\nDay 3\n✅ Good\nNo changes needed\n-\n0\n\n\nDay 4\n🟡 Review\nVerify parse_dates timing\nP1\n2-3\n\n\nDay 5\n✅ Good\nNo changes needed\n-\n0\n\n\nDay 6\n✅ Good\nNo changes needed\n-\n0\n\n\nDay 7\n✅ Good\nNo changes needed\n-\n0"
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html#implementation-timeline",
    "href": "example_course/tasks/eod-alignment-action-plan.html#implementation-timeline",
    "title": "",
    "section": "🔄 Implementation Timeline",
    "text": "🔄 Implementation Timeline\n\nWeek 1: Critical Fixes\n\nDay 1 EOD enhanced framing (draft created with answer key)\nTest Day 1 EOD draft with stakeholders\nDay 4 EOD verification and fixes\nInitial checklist development\n\n\n\nWeek 2-3: Content Audit\n\nMap all interactive sessions content\nCreate concept progression matrices\nReview coding colabs and live sessions\n\n\n\nWeek 4: Process Development\n\nFinalize alignment checklist\nCreate review workflow documentation\nUpdate course development processes\n\n\n\nWeek 5: Integration and Testing\n\nIntegrate into course update documentation\nTest new Day 1 EOD with stakeholders\nFinal review and validation"
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html#success-metrics",
    "href": "example_course/tasks/eod-alignment-action-plan.html#success-metrics",
    "title": "",
    "section": "💡 Success Metrics",
    "text": "💡 Success Metrics\n\nImmediate Indicators\n\nNo student complaints about “concepts not yet covered”\nImproved Day 1 confidence and engagement\nSmoother progression through early course days\n\n\n\nLong-term Indicators\n\nHigher course satisfaction scores\nReduced instructor “concept review” time\nMore consistent skill building progression"
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html#risk-mitigation",
    "href": "example_course/tasks/eod-alignment-action-plan.html#risk-mitigation",
    "title": "",
    "section": "🚨 Risk Mitigation",
    "text": "🚨 Risk Mitigation\n\nPotential Issues:\n\nTime constraints: Prioritize Day 1 fix above all else\nContent gaps: Better to have simpler, aligned activities than complex misaligned ones\nInstructor resistance: Frame as student success improvement, not criticism\n\n\n\nContingency Plans:\n\nIf Day 1 rewrite delayed: Create simplified version removing only pandas/matplotlib\nIf comprehensive audit delayed: Focus on Days 1-4 first as highest impact\nIf process integration delayed: Implement manual checklist first"
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html#file-organization",
    "href": "example_course/tasks/eod-alignment-action-plan.html#file-organization",
    "title": "",
    "section": "📁 File Organization",
    "text": "📁 File Organization\n\nNew Files Created:\n\ncourse-materials/eod-practice/eod-day1-draft.qmd - ✅ COMPLETED Enhanced Day 1 activity with preview framing\ncourse-materials/answer-keys/eod-day1-draft-key.qmd - ✅ COMPLETED Corresponding answer key with teaching notes\n\n\n\nNew Files to Create:\n\ntasks/eod-content-matrices.xlsx - Detailed concept mapping\ntasks/eod-alignment-checklist.md - Review checklist template\n\n\n\nFiles to Modify:\n\ntasks/2025-course-updates.md - Add alignment review section\ncourse-materials/eod-practice/eod-day1.qmd - Replace with aligned content\ncourse-materials/eod-practice/eod-day4.qmd - Fix date parsing issues"
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html#student-impact-statement",
    "href": "example_course/tasks/eod-alignment-action-plan.html#student-impact-statement",
    "title": "",
    "section": "🎓 Student Impact Statement",
    "text": "🎓 Student Impact Statement\nThis action plan directly addresses student feedback about encountering unfamiliar concepts in practice activities. By ensuring proper alignment:\n\nDay 1 experience becomes confidence-building rather than overwhelming\nLearning progression follows logical, incremental skill building\nStudent success increases through appropriate challenge levels\nCourse satisfaction improves through better pedagogical alignment"
  },
  {
    "objectID": "example_course/tasks/eod-alignment-action-plan.html#completed-day-1-eod-draft-solution",
    "href": "example_course/tasks/eod-alignment-action-plan.html#completed-day-1-eod-draft-solution",
    "title": "",
    "section": "🎉 COMPLETED: Day 1 EOD Draft Solution",
    "text": "🎉 COMPLETED: Day 1 EOD Draft Solution\nFiles Created: - course-materials/eod-practice/eod-day1-draft.qmd - Enhanced student version - course-materials/answer-keys/eod-day1-draft-key.qmd - Instructor version with teaching notes\nSolution Summary: Instead of removing the advanced concepts, the draft version reframes the entire activity as a “preview of coming attractions.” Students still run the same powerful data science workflow, but with clear messaging that this is a sneak peek at their future skills.\nKey Features: - 🎬 “Coming Attractions” framing throughout - 🎭 “What just happened?” explanations after each step - 🗓️ Learning timeline showing when concepts are taught - 🚀 Proper expectation setting and encouragement - 💡 Maintains pedagogical value while addressing alignment concerns\nNext Steps: 1. Test the draft version with a few students or colleagues 2. Gather feedback on clarity and motivational impact 3. Replace original with draft version when approved 4. Proceed with Day 4 EOD review (lower priority)"
  },
  {
    "objectID": "example_course/extra_files/zen_of_python.html",
    "href": "example_course/extra_files/zen_of_python.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "example_course/extra_files/zen_of_python.html#beautiful-is-better-than-ugly.",
    "href": "example_course/extra_files/zen_of_python.html#beautiful-is-better-than-ugly.",
    "title": "",
    "section": "1. Beautiful is better than ugly.",
    "text": "1. Beautiful is better than ugly.\nThis principle encourages writing clean, aesthetically pleasing code.\n# Beautiful\ndef get_average(numbers):\n    return sum(numbers) / len(numbers)\n\n# Ugly\ndef get_average(numbers):\n    s = 0\n    l = 0\n    for i in numbers:\n        s += i\n        l += 1\n    return s / l"
  },
  {
    "objectID": "example_course/extra_files/zen_of_python.html#explicit-is-better-than-implicit.",
    "href": "example_course/extra_files/zen_of_python.html#explicit-is-better-than-implicit.",
    "title": "",
    "section": "2. Explicit is better than implicit.",
    "text": "2. Explicit is better than implicit.\nBe clear about what your code is doing, rather than relying on implicit behavior.\n# Explicit\ndef greet(name):\n    return f\"Hello, {name}!\"\n\n# Implicit (and error-prone)\ndef greet(name):\n    return f\"Hello, {n}!\"  # Relies on a global 'n' variable"
  },
  {
    "objectID": "example_course/extra_files/zen_of_python.html#simple-is-better-than-complex.",
    "href": "example_course/extra_files/zen_of_python.html#simple-is-better-than-complex.",
    "title": "",
    "section": "3. Simple is better than complex.",
    "text": "3. Simple is better than complex.\nOpt for straightforward solutions when possible.\n# Simple\ndef is_even(number):\n    return number % 2 == 0\n\n# Complex (unnecessarily)\ndef is_even(number):\n    return (number & 1) ^ 1"
  },
  {
    "objectID": "example_course/extra_files/zen_of_python.html#complex-is-better-than-complicated.",
    "href": "example_course/extra_files/zen_of_python.html#complex-is-better-than-complicated.",
    "title": "",
    "section": "4. Complex is better than complicated.",
    "text": "4. Complex is better than complicated.\nIf complexity is necessary, it’s better than a convoluted solution.\n# Complex but clear\ndef quick_sort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quick_sort(left) + middle + quick_sort(right)\n\n# Complicated (trying to do too much in one line)\ndef quick_sort(arr): return arr if len(arr) &lt;= 1 else quick_sort([x for x in arr[1:] if x &lt; arr[0]]) + [arr[0]] + quick_sort([x for x in arr[1:] if x &gt;= arr[0]])"
  },
  {
    "objectID": "example_course/extra_files/zen_of_python.html#flat-is-better-than-nested.",
    "href": "example_course/extra_files/zen_of_python.html#flat-is-better-than-nested.",
    "title": "",
    "section": "5. Flat is better than nested.",
    "text": "5. Flat is better than nested.\nAvoid deep nesting of code blocks when possible.\n# Flat\ndef process_data(data):\n    if not data:\n        return None\n    \n    result = []\n    for item in data:\n        if item.is_valid():\n            result.append(item.process())\n    return result\n\n# Nested (harder to read)\ndef process_data(data):\n    if data:\n        result = []\n        for item in data:\n            if item.is_valid():\n                result.append(item.process())\n        return result\n    else:\n        return None\nThese examples demonstrate how the Zen of Python principles can be applied in practical coding scenarios. Each principle aims to make Python code more readable, maintainable, and effective."
  },
  {
    "objectID": "example_course/course-materials/lectures/libraries.html",
    "href": "example_course/course-materials/lectures/libraries.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nData Science Libraries:\n\nScikit-learn:\n\nFunctionality: A comprehensive library for machine learning and data mining.\nTopics to explore: Basic classification (e.g., Decision Trees, k-NN), basic regression, data preprocessing (scaling, encoding).\n\nSeaborn:\n\nFunctionality: A statistical data visualization library based on Matplotlib.\nTopics to explore: Basic plots like histograms, bar plots, and box plots. Advanced topics can include pair plots and violin plots.\n\nStatsmodels:\n\nFunctionality: Provides classes and functions for statistical models estimation.\nTopics to explore: Basic linear regression.\n\nSciPy:\n\nFunctionality: Library for scientific computing in Python. Builds on Numpy.\nTopics to explore: Basic statistical functions, simple optimizations.\n\nGeopandas:\n\nFunctionality: Extends the datatypes used by pandas to allow spatial operations on geometric types.\nTopics to explore: Basic spatial operations, plotting.\n\nPlotly:\n\nFunctionality: Interactive graph plotting.\nTopics to explore: Basic interactive plots.\n\n\n\n\nAdvanced Pandas Topics:\n\nTime Series functionality:\n\nBasic operations like setting a DateTime index, filtering by date, etc.\n\nCategorical Data:\n\nUsing the Categorical data type, ordering, categorization.\n\nMultiIndex:\n\nBasic operations like setting a MultiIndex and filtering data using it.\n\n.merge() and .join() methods:\n\nBasic data joins and merges (left, right, inner).\n\n.pivot_table() and .melt():\n\nBasic reshaping and pivoting of DataFrame objects.\n\nStyling DataFrames:\n\nApplying simple conditional formatting and styles to DataFrames for better visualization.\n\nUsing .applymap(), .map(), and .apply():\n\nBasic operations for transforming data.\n\nWorking with Text Data:\n\nBasic string methods like .str.upper(), .str.contains(), etc.\n\nHandling Missing Data:\n\nBasic operations like .fillna() and .dropna().\n\nPerformance and Memory Usage:\n\nUnderstanding memory usage with .info().\n\n\n\n\nWorking with Excel files:\n\nPandas:\n\nFunctionality: Has built-in functions to read from and write to Excel files.\nTopics to explore:\n\nReading Excel files using read_excel().\nWriting to Excel files using to_excel().\nHandling multiple sheets.\nFormatting Excel output (e.g., setting column widths, header formats).\n\n\nopenpyxl:\n\nFunctionality: A Python library to read/write Excel 2010 xlsx/xlsm files.\nTopics to explore:\n\nWorking with Excel sheets, rows, and cells.\nStyling and formatting.\nCreating charts.\n\n\nxlrd & xlwt:\n\nFunctionality: Libraries to read and write Excel data respectively (mainly for older xls files).\nTopics to explore:\n\nReading data from Excel using xlrd.\nWriting data to Excel using xlwt.\nBasic formatting and styling.\n\n\n\n\n\nWorking with SQL Databases:\n\nSQLite:\n\nFunctionality: Serverless, self-contained SQL database engine.\nTopics to explore:\n\nBasic SQL operations: SELECT, INSERT, UPDATE, DELETE.\nCreating and managing tables.\nUsing Python’s built-in sqlite3 module.\n\n\nSQLAlchemy:\n\nFunctionality: SQL toolkit and Object-Relational Mapping (ORM) library.\nTopics to explore:\n\nConnecting to databases.\nExecuting raw SQL queries.\nUsing the ORM to define and manipulate database tables and records.\n\n\nPandas with SQL:\n\nFunctionality: Pandas can interface with SQL databases.\nTopics to explore:\n\nReading data from SQL databases using read_sql_query().\nWriting data to SQL databases using to_sql().\nCombining SQL operations with pandas data manipulations.\n\n\nMySQL & MySQL-connector-python:\n\nFunctionality: MySQL is a popular relational database, and the connector is a Python driver to connect to MySQL databases.\nTopics to explore:\n\nSetting up a MySQL database.\nUsing the connector to execute SQL operations.\nFetching and manipulating data.\n\n\npsycopg2:\n\nFunctionality: PostgreSQL adapter for Python.\nTopics to explore:\n\nConnecting to PostgreSQL databases.\nExecuting SQL operations.\nHandling transactions.\n\n\n\n\n\nTools and Libraries for Large Datasets:\n\nDask:\n\nFunctionality: Parallel computing and task scheduling.\nTopics to explore:\n\nDask Arrays: like Numpy arrays, but distributed.\nDask DataFrames: like Pandas DataFrames, but distributed.\nLazy evaluation and custom computation graphs.\nDistributed computing with Dask.\n\n\nVaex:\n\nFunctionality: Dataframe library for fast, out-of-core DataFrames.\nTopics to explore:\n\nEfficiently visualizing and exploring big datasets.\nFiltering and evaluating expressions just-in-time.\nVirtual columns.\n\n\nDatatable:\n\nFunctionality: Library for manipulation of large data sets.\nTopics to explore:\n\nReading large datasets efficiently.\nBasic operations: grouping, joining, filtering.\nAggregation and statistical operations.\n\n\nPyArrow & Apache Arrow:\n\nFunctionality: Cross-language platform for in-memory data.\nTopics to explore:\n\nArrow tables and record batches.\nEfficient data interchange between Arrow and Pandas.\nMemory-mapped Arrow tables.\n\n\nPySpark:\n\nFunctionality: Interface for Apache Spark in Python. Allows for distributed data processing.\nTopics to explore:\n\nResilient Distributed Datasets (RDDs).\nSpark DataFrames and SQL operations.\nDistributed machine learning with Spark MLlib.\n\n\nHDF5 & h5py:\n\nFunctionality: HDF5 is a data model for storing large datasets, and h5py is a Python interface to the HDF5 binary data format.\nTopics to explore:\n\nCreating and reading HDF5 datasets.\nGrouping datasets within an HDF5 file.\nReading subsets of data efficiently.\n\n\nFeather:\n\nFunctionality: Binary columnar serialization for data frames.\nTopics to explore:\n\nEfficiently reading and writing data frames.\nData interchange between Python and R.\nFast serialization and deserialization.\n\n\nApache Kafka:\n\nFunctionality: Distributed streaming platform.\nTopics to explore:\n\nProducing and consuming streams of records.\nStream processing.\nIntegrating with other big data tools.\n\n\nRAPIDS cuDF:\n\nFunctionality: GPU DataFrame library based on Apache Arrow.\nTopics to explore:\n\nGPU acceleration for dataframe operations.\nInteroperability with other GPU data science libraries.\nGPU-accelerated algorithms.\n\n\nModin:\n\nFunctionality: Speed up your Pandas workflows by changing a single line of code.\nTopics to explore:\n\nDistributed dataframes.\nBackend options (Dask, Ray)."
  },
  {
    "objectID": "example_course/KERNEL_FIX.html",
    "href": "example_course/KERNEL_FIX.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "example_course/KERNEL_FIX.html#quick-fix",
    "href": "example_course/KERNEL_FIX.html#quick-fix",
    "title": "",
    "section": "🔧 Quick Fix",
    "text": "🔧 Quick Fix\n\nStep 1: Ensure Environment is Active\nconda activate eds217_2025\n\n\nStep 2: Reinstall the Kernel\npython -m ipykernel install --user --name eds217_2025 --display-name \"Python 3.11 (EDS 217 2025)\" --force\n\n\nStep 3: Restart JupyterLab\n# If JupyterLab is running, stop it (Ctrl+C) and restart\njupyter lab\n\n\nStep 4: Change Kernel in Notebook\n\nOpen your notebook in JupyterLab\nClick on the kernel name in the top-right corner\nSelect “Python 3.11 (EDS 217 2025)” from the dropdown\nThe notebook will restart with the correct environment"
  },
  {
    "objectID": "example_course/KERNEL_FIX.html#verify-its-working",
    "href": "example_course/KERNEL_FIX.html#verify-its-working",
    "title": "",
    "section": "🔍 Verify It’s Working",
    "text": "🔍 Verify It’s Working\nRun this in a notebook cell to confirm:\nimport sys\nprint(f\"Python path: {sys.executable}\")\n\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nprint(f\"✅ All packages imported successfully!\")\nprint(f\"Seaborn version: {sns.__version__}\")"
  },
  {
    "objectID": "example_course/KERNEL_FIX.html#common-issues",
    "href": "example_course/KERNEL_FIX.html#common-issues",
    "title": "",
    "section": "🚨 Common Issues",
    "text": "🚨 Common Issues\n\nWrong kernel selected: Always check the kernel name in the top-right of your notebook\nOld kernel cache: The --force flag in Step 2 overwrites any existing kernel registration\nEnvironment not activated: Make sure you’re in the eds217_2025 environment when installing the kernel"
  },
  {
    "objectID": "example_course/KERNEL_FIX.html#alternative-recreate-environment",
    "href": "example_course/KERNEL_FIX.html#alternative-recreate-environment",
    "title": "",
    "section": "🔄 Alternative: Recreate Environment",
    "text": "🔄 Alternative: Recreate Environment\nIf issues persist, recreate the environment:\nconda env remove -n eds217_2025\nmamba env create -f environment-fast.yml\nconda activate eds217_2025\npython -m ipykernel install --user --name eds217_2025 --display-name \"Python 3.11 (EDS 217 2025)\""
  },
  {
    "objectID": "example_course/ENVIRONMENT_MIGRATION.html",
    "href": "example_course/ENVIRONMENT_MIGRATION.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "example_course/ENVIRONMENT_MIGRATION.html#step-1-export-current-environment-optional-backup",
    "href": "example_course/ENVIRONMENT_MIGRATION.html#step-1-export-current-environment-optional-backup",
    "title": "",
    "section": "Step 1: Export Current Environment (Optional Backup)",
    "text": "Step 1: Export Current Environment (Optional Backup)\nIf you want to keep a backup of your current environment packages:\n# Activate your current environment\nconda activate eds217_2024\n\n# Export the environment to a backup file\nconda env export &gt; eds217_2024_backup.yml"
  },
  {
    "objectID": "example_course/ENVIRONMENT_MIGRATION.html#step-2-create-new-python-3.11-environment",
    "href": "example_course/ENVIRONMENT_MIGRATION.html#step-2-create-new-python-3.11-environment",
    "title": "",
    "section": "Step 2: Create New Python 3.11 Environment",
    "text": "Step 2: Create New Python 3.11 Environment\n\nOption A: Using the provided environment.yml file\n# Create the new environment from the environment.yml file\nconda env create -f environment.yml\n\n# Activate the new environment\nconda activate eds217_2025\n\n\nOption B: Manual creation\n# Create new environment with Python 3.11\nconda create -n eds217_2025 python=3.11\n\n# Activate the new environment\nconda activate eds217_2025\n\n# Install core packages\nconda install numpy pandas matplotlib seaborn jupyter jupyterlab ipykernel scipy scikit-learn plotly requests beautifulsoup4 lxml openpyxl xlrd\n\n# Install additional packages via pip\npip install geopandas folium"
  },
  {
    "objectID": "example_course/ENVIRONMENT_MIGRATION.html#step-3-register-the-new-environment-with-jupyter",
    "href": "example_course/ENVIRONMENT_MIGRATION.html#step-3-register-the-new-environment-with-jupyter",
    "title": "",
    "section": "Step 3: Register the New Environment with Jupyter",
    "text": "Step 3: Register the New Environment with Jupyter\n# Make sure you're in the new environment\nconda activate eds217_2025\n\n# Register the environment as a Jupyter kernel\npython -m ipykernel install --user --name eds217_2025 --display-name \"Python 3.11 (EDS 217 2025)\""
  },
  {
    "objectID": "example_course/ENVIRONMENT_MIGRATION.html#step-4-verify-installation",
    "href": "example_course/ENVIRONMENT_MIGRATION.html#step-4-verify-installation",
    "title": "",
    "section": "Step 4: Verify Installation",
    "text": "Step 4: Verify Installation\n# Check Python version\npython --version\n# Should show: Python 3.11.x\n\n# Check installed packages\nconda list\n\n# Launch JupyterLab to test\njupyter lab\nIn JupyterLab, you should now see “Python 3.11 (EDS 217 2025)” as an available kernel option."
  },
  {
    "objectID": "example_course/ENVIRONMENT_MIGRATION.html#step-5-update-your-workflow",
    "href": "example_course/ENVIRONMENT_MIGRATION.html#step-5-update-your-workflow",
    "title": "",
    "section": "Step 5: Update Your Workflow",
    "text": "Step 5: Update Your Workflow\n\nWhen creating new notebooks, select “Python 3.11 (EDS 217 2025)” as your kernel\nFor existing notebooks, you can change the kernel via Kernel → Change Kernel in JupyterLab"
  },
  {
    "objectID": "example_course/ENVIRONMENT_MIGRATION.html#step-6-remove-old-environment-optional",
    "href": "example_course/ENVIRONMENT_MIGRATION.html#step-6-remove-old-environment-optional",
    "title": "",
    "section": "Step 6: Remove Old Environment (Optional)",
    "text": "Step 6: Remove Old Environment (Optional)\nOnce you’ve verified everything works with the new environment:\n# Remove the old environment\nconda env remove -n eds217_2024\n\n# Remove the old Jupyter kernel\njupyter kernelspec remove eds217_2024"
  },
  {
    "objectID": "example_course/ENVIRONMENT_MIGRATION.html#troubleshooting",
    "href": "example_course/ENVIRONMENT_MIGRATION.html#troubleshooting",
    "title": "",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nIf packages are missing:\nconda activate eds217_2025\nconda install [package-name]\n\n\nIf you need to add the environment to a specific Jupyter installation:\nconda activate eds217_2025\npython -m ipykernel install --user --name eds217_2025\n\n\nCheck available environments:\nconda env list\n\n\nCheck available Jupyter kernels:\njupyter kernelspec list"
  },
  {
    "objectID": "example_course/ENVIRONMENT_MIGRATION.html#key-benefits-of-python-3.11",
    "href": "example_course/ENVIRONMENT_MIGRATION.html#key-benefits-of-python-3.11",
    "title": "",
    "section": "Key Benefits of Python 3.11",
    "text": "Key Benefits of Python 3.11\n\nPerformance: 10-60% faster than Python 3.10\nBetter error messages: More helpful debugging information\nEnhanced features: Latest Python language improvements\nImproved compatibility: Better support for modern data science libraries"
  },
  {
    "objectID": "example_course/ENVIRONMENT_MIGRATION.html#package-versions-included",
    "href": "example_course/ENVIRONMENT_MIGRATION.html#package-versions-included",
    "title": "",
    "section": "Package Versions Included",
    "text": "Package Versions Included\nThe new environment includes: - Python 3.11.x - NumPy (latest) - Pandas (latest) - Matplotlib (latest) - Seaborn (latest) - JupyterLab (latest) - SciPy (latest) - Scikit-learn (latest) - And more…\nAll packages will be the latest compatible versions, ensuring you have the most up-to-date and secure data science stack."
  },
  {
    "objectID": "example_course/BUILD_DOCS.html",
    "href": "example_course/BUILD_DOCS.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#overview",
    "href": "example_course/BUILD_DOCS.html#overview",
    "title": "",
    "section": "Overview",
    "text": "Overview\nThe EDS 217 website is built using Quarto, which renders .qmd and .ipynb files into a static website deployed to the docs/ folder for GitHub Pages.\n🚀 Key Features: - Incremental builds by default: Only rebuilds files that have changed since your last git commit - Smart cleaning: Remove intermediate files (HTML, _files/, etc.) from course materials - Cross-platform: Works on Windows, macOS, and Linux - Integrated conda environment: Automatically activates the eds217_2025 environment"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#build-script-usage",
    "href": "example_course/BUILD_DOCS.html#build-script-usage",
    "title": "",
    "section": "Build Script Usage",
    "text": "Build Script Usage\n\nBasic Commands\n# Incremental build (default - only changed files)\npython build_docs.py\n# or\n./build_docs.py\n\n# Full rebuild (rebuild everything)\npython build_docs.py --full\n# or\npython build_docs.py -f\n\n# Incremental build and serve locally\npython build_docs.py --serve\n# or\npython build_docs.py -s\n\n# Full rebuild and serve locally\npython build_docs.py --full --serve\n\n# Clean intermediate files only (no build)\npython build_docs.py --clean\n# or\npython build_docs.py -c\n\n# Clean and then do full rebuild\npython build_docs.py --clean --full"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#command-options",
    "href": "example_course/BUILD_DOCS.html#command-options",
    "title": "",
    "section": "Command Options",
    "text": "Command Options\n\nBuild Options\n\n--full or -f: Force complete rebuild of all files (overrides incremental mode)\n--serve or -s: Build and automatically start a local server for preview\n\n\n\nCleaning Options\n\n--clean or -c: Clean intermediate files from course materials directories\n\n\n\nOption Combinations\n\nDefault behavior: Incremental build, show next steps\n--serve: Incremental build + start local server\n--full: Complete rebuild, show next steps\n\n--full --serve: Complete rebuild + start local server\n--clean: Clean intermediate files only (no build)\n--clean --full: Clean intermediate files + complete rebuild\n--clean --serve: Clean intermediate files + incremental build + serve"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#what-the-script-does",
    "href": "example_course/BUILD_DOCS.html#what-the-script-does",
    "title": "",
    "section": "What the Script Does",
    "text": "What the Script Does\n\nStandard Build Process\n\nActivate Environment: Automatically activate the eds217_2025 conda environment\nCheck Prerequisites: Verify that Quarto is installed and you’re in the correct directory\n🆕 Smart File Detection (incremental mode): Use git to detect which .qmd and .ipynb files have changed since the last commit\nClean Previous Build: Remove old files from the docs/ directory\nBuild Website: Run quarto render on all files (full build) or only changed files (incremental build) with progress indicator\nVerify Build: Check that the build completed successfully and required files exist\nReport Status: Show build summary and next steps\nServe Locally (optional): Start a local server to preview your changes\n\n\n\n🧹 Cleaning Process (with –clean flag)\nThe --clean option removes intermediate build files from the course-materials/ directories:\nFiles and directories removed: - *.html - Rendered HTML files - *_files/ - Quarto output directories\n- *.ipynb_checkpoints/ - Jupyter checkpoints - */.quarto/ - Quarto cache directories - LaTeX auxiliary files (*.aux, *.log, *.out, *.toc, etc.) - Other temporary build artifacts\nBenefits of cleaning: - ✅ Removes stale intermediate files that can cause build conflicts - ✅ Reduces repository size by eliminating generated files - ✅ Ensures fresh builds without cached artifacts - ✅ Helps troubleshoot build issues caused by outdated intermediate files"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#incremental-build-features",
    "href": "example_course/BUILD_DOCS.html#incremental-build-features",
    "title": "",
    "section": "🚀 Incremental Build Features",
    "text": "🚀 Incremental Build Features\nAutomatic Change Detection: The script uses git to automatically detect: - Modified files since the last commit (git diff --name-only HEAD) - Staged changes (git diff --name-only --cached) - New untracked files (git ls-files --others --exclude-standard)\nIntelligent Fallbacks: - If not in a git repository: Falls back to full build - If no previous commits: Falls back to full build - If no changes detected: Skips build entirely (with option to force full build)\nFile Type Filtering: Only processes .qmd and .ipynb files, respecting the exclusions in _quarto.yml\n\nProgress Indicators\nThe script shows detailed progress information during builds: - File-by-file progress: Shows each file being built with progress counter (e.g., “[5/23] Building: file.qmd”) - Timing information: Displays elapsed time and estimated time remaining (ETA) - Progress percentage: Shows completion percentage for the current build - Individual file timing: Shows how long each file takes to render - Build summary: Final statistics including total time, average per file, fastest/slowest files - Real-time feedback: Shows current file being processed and rendering status - Error handling: Shows ❌ if individual files fail with detailed error information\nExample output:\n🚀 Starting full build (23 files to process)\n\n[ 1/23] (  0.0%) Building: course-materials/day1.qmd\n        ⏱️  Elapsed: 0.5s\n        🔨 Rendering... ✅ Done (2.3s)\n\n[ 2/23] (  4.3%) Building: course-materials/day2.qmd\n        ⏱️  Elapsed: 2.8s (ETA: 48.3s)\n        🔨 Rendering... ✅ Done (1.8s)\n\n📊 Build completed!\n   ⏱️  Total time: 45.2s\n   📈 Average per file: 2.0s\n   📁 Files processed: 23\n   ⚡ Fastest file: 0.8s\n   🐌 Slowest file: 4.2s"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#prerequisites",
    "href": "example_course/BUILD_DOCS.html#prerequisites",
    "title": "",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nQuarto must be installed\nThe eds217_2025 conda environment must exist (see Environment Setup)\nRun the script from the project root directory (where _quarto.yml is located)\n🆕 For incremental builds: Git repository with at least one commit (automatic fallback to full build otherwise)\n\nNote: The script automatically activates the eds217_2025 environment before building."
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#typical-workflows",
    "href": "example_course/BUILD_DOCS.html#typical-workflows",
    "title": "",
    "section": "Typical Workflows",
    "text": "Typical Workflows\n\nOption 1: Fast Incremental Development (Recommended)\n\nMake changes to your .qmd files or course materials\nIncremental build and preview (builds only changed files):\npython build_docs.py --serve\nReview your changes in the browser (opens automatically)\nPress Ctrl+C to stop the local server\nCommit and push changes to GitHub:\ngit add .\ngit commit -m \"Update course content\"\ngit push origin main\n\n\n\nOption 2: Clean Development Workflow\n\nMake extensive changes to course materials\nClean intermediate files and rebuild:\npython build_docs.py --clean --full --serve\nReview the complete rebuilt site\nCommit and push changes to GitHub\n\n\n\nOption 3: Quick Cleanup\n\nClean intermediate files only (no rebuild):\npython build_docs.py --clean\nReview what was cleaned and commit if needed\n\n\n\nOption 4: Complete Rebuild (When Needed)\n\nMake extensive changes or troubleshoot build issues\nFull build and preview:\npython build_docs.py --full --serve\nReview the complete rebuilt site\nCommit and push changes to GitHub"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#when-to-use-each-option",
    "href": "example_course/BUILD_DOCS.html#when-to-use-each-option",
    "title": "",
    "section": "When to Use Each Option",
    "text": "When to Use Each Option\n\nUse Incremental Build (Default) When:\n\n✅ Making content updates to individual course materials\n✅ Fixing typos or small changes\n✅ Adding new course materials\n✅ Regular development workflow\n✅ Speed is important (builds 5-10x faster for small changes)\n\n\n\nUse Full Build (--full) When:\n\n🔄 Troubleshooting build issues\n🔄 Making changes to _quarto.yml configuration\n🔄 Updating global styles or templates\n🔄 After major git operations (rebasing, etc.)\n🔄 Publishing final version\n🔄 When incremental build doesn’t capture all dependencies\n\n\n\nUse Clean (--clean) When:\n\n🧹 Repository feels cluttered with intermediate files\n🧹 Experiencing unexplained build issues\n🧹 Before major releases or when switching branches\n🧹 Intermediate files are causing conflicts\n🧹 Want to reduce repository size"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#build-performance",
    "href": "example_course/BUILD_DOCS.html#build-performance",
    "title": "",
    "section": "Build Performance",
    "text": "Build Performance\n\nEnhanced Progress Tracking:\n\n📊 Real-time progress: See exactly which files are being processed and how many remain\n⏱️ Timing estimates: Get ETA based on average file processing times\n📈 Performance insights: Identify slow-building files for optimization\n🎯 Granular feedback: Individual file timing helps with debugging build issues\n\n\n\nIncremental Build Benefits:\n\n⚡ 5-10x faster for small changes (seconds instead of minutes)\n🎯 Targeted rebuilds - only processes changed files\n🔍 Clear feedback - shows exactly which files are being built with progress counters\n💚 Environmentally friendly - uses less CPU and energy\n\n\n\nExample Performance:\n\nFull build: ~2-5 minutes for entire course website (with detailed file-by-file progress)\nIncremental build: ~10-30 seconds for 1-3 changed files (with individual file timing)\nNo changes: ~2 seconds (skips build entirely)\nClean operation: ~5-10 seconds\n\n\n\nProgress Tracking Features:\n\nFile counters: [15/42] shows current progress through all files\nPercentage completion: Shows how much of the build is complete\nETA calculation: Estimates remaining time based on average file processing speed\nIndividual timing: See which files are slow to help optimize your content\nSummary statistics: Get insights into build performance with fastest/slowest file times"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#troubleshooting",
    "href": "example_course/BUILD_DOCS.html#troubleshooting",
    "title": "",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nGeneral Issues\n\n“Quarto not found”: Install Quarto from https://quarto.org/docs/get-started/\n“eds217_2025 environment not found”: Create the environment using one of the setup guides in the README\n**“_quarto.yml not found”**: Make sure you’re running the script from the project root directory\nPermission denied: Make sure the script is executable with chmod +x build_docs.py\n\n\n\nIncremental Build Issues\n\n“Not in a git repository”: Script automatically falls back to full build\nBuild seems incomplete: Use --full flag to rebuild everything\nChanges not detected: Ensure files are saved and try git status to verify changes\nDependencies not updated: Some changes (like _quarto.yml modifications) require --full rebuild\n\n\n\nCleaning Issues\n\n“No course-materials directory found”: Script will report this and continue\nPermission denied during cleaning: Ensure you have write permissions to the course-materials directory\nFiles won’t delete: Some files might be in use; close any open editors and try again\n\n\n\nWhen Incremental Builds Don’t Work\nIf you encounter issues with incremental builds: 1. Clean first: python build_docs.py --clean 2. Use full build: python build_docs.py --full 3. Check git status: git status to see what files are changed 4. Commit changes first: Sometimes uncommitted changes aren’t detected properly 5. Report the issue: If incremental builds consistently fail, please report it"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#manual-build",
    "href": "example_course/BUILD_DOCS.html#manual-build",
    "title": "",
    "section": "Manual Build",
    "text": "Manual Build\nIf you prefer to build manually:\n# Full build\nquarto render\n\n# Build specific files\nquarto render course-materials/day1.qmd\nquarto render course-materials/interactive-sessions/1a_iPython_JupyterLab.qmd\n\n# Clean manually (remove intermediate files)\nfind course-materials -name \"*.html\" -delete\nfind course-materials -name \"*_files\" -type d -exec rm -rf {} +"
  },
  {
    "objectID": "example_course/BUILD_DOCS.html#migration-notes-for-existing-users",
    "href": "example_course/BUILD_DOCS.html#migration-notes-for-existing-users",
    "title": "",
    "section": "Migration Notes for Existing Users",
    "text": "Migration Notes for Existing Users\nSimplified workflow: Previously there were both shell and Python scripts. Now there’s only the Python script for easier maintenance.\nNew features available: - --clean option: Remove intermediate files from course materials - Enhanced cross-platform compatibility: Works consistently across all operating systems - Same build behavior: All previous commands still work exactly the same\nCommand migration: - Previous: ./build_docs.sh → New: python build_docs.py - Previous: ./build_docs.sh --serve → New: python build_docs.py --serve - Previous: ./build_docs.sh --full → New: python build_docs.py --full"
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CLAUDE.html#repository-overview",
    "href": "CLAUDE.html#repository-overview",
    "title": "",
    "section": "Repository Overview",
    "text": "Repository Overview\nThis is a geoAI repository for developing GEOG 288KC: Geospatial Foundation Models and Applications (Fall 2025). The repository contains a template structure and build system for creating educational materials on state-of-the-art geospatial foundation models (GFMs) for remote sensing and environmental monitoring.\nRepository Structure: - example_course/: Complete template structure from EDS 217 showing how to organize course materials using Quarto - Target: Adapt this template to create GEOG 288KC course materials following the syllabus structure"
  },
  {
    "objectID": "CLAUDE.html#course-context-geog-288kc",
    "href": "CLAUDE.html#course-context-geog-288kc",
    "title": "",
    "section": "Course Context (GEOG 288KC)",
    "text": "Course Context (GEOG 288KC)\nSchedule: Fridays 9am-12pm + optional lab office hours Fridays 2pm-5pm Format: Project-driven seminar combining lectures, discussions, and hands-on labs Focus: Frontier techniques in geospatial AI including self-supervised learning, masked autoencoders, multimodal sensor fusion, and scalable inference pipelines\n\nCourse Timeline and Deliverables\n\nWeek 0: Setup for geospatial AI in UCSB AI Sandbox + Project Applications\nWeek 1: Introduction to geospatial foundation models\nWeek 2: Working with geospatial data and pretrained model outputs\nWeek 3: Fine-tuning foundation models + Project Proposals due\nWeek 4: Multi-modal and generative models for remote sensing\nWeek 5: Semi-independent project work and proposal workshops\nWeek 6: Model adaptation, efficient fine-tuning, and evaluation strategies\nWeek 7: Scalable analysis pipelines using Earth Engine, TorchGeo, cloud tools + Initial MVPs due\nWeek 8: Building lightweight APIs and applications for model inference\nWeek 9: Project workshops and synthesis\nWeek 10: Final project presentations"
  },
  {
    "objectID": "CLAUDE.html#key-commands",
    "href": "CLAUDE.html#key-commands",
    "title": "",
    "section": "Key Commands",
    "text": "Key Commands\n\nBuilding and Documentation\n# Build the course website (incremental build - only changed files)\npython example_course/build_docs.py\n\n# Full rebuild of all files\npython example_course/build_docs.py --full\n\n# Build and serve locally for preview\npython example_course/build_docs.py --serve\n\n# Clean intermediate files (HTML, _files/, etc.) from course materials\npython example_course/build_docs.py --clean\n\n# Clean and full rebuild\npython example_course/build_docs.py --clean --full\n\n\nEnvironment Management\n# Create the conda environment for the course\ncd example_course\nconda env create -f environment.yml\n\n# Or for faster installation with mamba\nmamba env create -f environment.yml\n\n# Activate the environment (build script does this automatically)\nconda activate eds217_2025\n\n\nManual Quarto Commands\ncd example_course\nquarto render                    # Build entire site\nquarto preview                   # Build and serve locally\nquarto render specific_file.qmd  # Build specific file"
  },
  {
    "objectID": "CLAUDE.html#project-architecture",
    "href": "CLAUDE.html#project-architecture",
    "title": "",
    "section": "Project Architecture",
    "text": "Project Architecture\n\nHigh-Level Structure\n\nexample_course/: Complete Python course materials built with Quarto\nRoot level: Container repository for geoAI educational content\n\n\n\nTemplate Structure (example_course/)\nThe template provides a complete organizational framework that should be adapted for GEOG 288KC:\nCurrent Template Structure: - course-materials/: All teaching content organized by type: - day1.qmd through day9.qmd: Daily lesson plans and overviews - interactive-sessions/: Live coding sessions and demonstrations\n- coding-colabs/: Collaborative coding exercises - lectures/: Formal lecture content (.ipynb and .qmd files) - live-coding/: Real-time coding demonstrations with student data - eod-practice/: End-of-day practice exercises - answer-keys/: Solution files for exercises - cheatsheets/: Quick reference materials - docs/: Generated static website (GitHub Pages output) - nbs/: Additional Jupyter notebooks (excluded from builds) - data/: Course datasets and data generation scripts - images/: Course imagery and visual assets\nAdaptation for GEOG 288KC: - Replace daily structure with 10 weekly modules (Week 0-10) - Focus content on geospatial foundation models instead of Python fundamentals - Maintain project-driven approach with deliverable tracking - Include resources for Earth Engine, TorchGeo, and UCSB AI Sandbox\n\n\nBuild System\nThe course uses a sophisticated Python-based build system (build_docs.py) with: - Incremental builds: Only rebuilds changed files using git diff - Smart environment handling: Automatically activates eds217_2025 conda environment - Progress tracking: Detailed build progress with timing and ETA - Cleaning capabilities: Removes intermediate files to prevent conflicts\n\n\nContent Types\n\n.qmd files: Quarto markdown for most course content\n.ipynb files: Jupyter notebooks for interactive lessons\nGenerated data: Python scripts create sample datasets for exercises\nStatic assets: Images, CSS, and other web resources"
  },
  {
    "objectID": "CLAUDE.html#development-workflow",
    "href": "CLAUDE.html#development-workflow",
    "title": "",
    "section": "Development Workflow",
    "text": "Development Workflow\n\nStandard Content Updates\n\nEdit .qmd or .ipynb files in example_course/course-materials/\nRun python build_docs.py --serve for quick preview\nReview changes in browser\nCommit and push changes\n\n\n\nMajor Changes or Troubleshooting\n\nRun python build_docs.py --clean --full for complete rebuild\nCheck docs/ directory for correct output\nVerify all course materials render properly\n\n\n\nEnvironment Requirements\n\nPython 3.11 with pandas, numpy, matplotlib, seaborn, jupyter, jupyterlab\nQuarto for static site generation\nGit for incremental build detection\nConda/Mamba for environment management"
  },
  {
    "objectID": "CLAUDE.html#prerequisites-and-requirements",
    "href": "CLAUDE.html#prerequisites-and-requirements",
    "title": "",
    "section": "Prerequisites and Requirements",
    "text": "Prerequisites and Requirements\nFor GEOG 288KC students should have: - Experience with remote sensing, geospatial data, or ML (Python, Earth Engine, PyTorch) - A defined project interest area for applying Geospatial Foundation Models - Access to UCSB AI Sandbox for computational resources\nTechnical Requirements: - Python 3.11 with pandas, numpy, matplotlib, seaborn, jupyter, jupyterlab (for example_course materials) - Quarto for static site generation (if building documentation) - Git for version control and incremental builds - Conda/Mamba for environment management - PyTorch, Earth Engine, TorchGeo likely needed for main course work"
  },
  {
    "objectID": "CLAUDE.html#development-approach",
    "href": "CLAUDE.html#development-approach",
    "title": "",
    "section": "Development Approach",
    "text": "Development Approach\nTemplate-Based Development: - Use example_course/ as the structural template for building GEOG 288KC materials - The sophisticated build system and Quarto setup can be directly reused - Adapt content organization from daily lessons to weekly modules - Maintain the same development workflow and build commands\nContent Development Strategy: - Week-by-week content development following the syllabus timeline - Project-centric approach with deliverable milestones (Proposals, MVPs, Final Presentations) - Integration with UCSB AI Sandbox computational environment - Focus on hands-on labs with geospatial foundation models"
  },
  {
    "objectID": "CLAUDE.html#important-notes",
    "href": "CLAUDE.html#important-notes",
    "title": "",
    "section": "Important Notes",
    "text": "Important Notes\n\nThe example_course/ serves as a complete template for organizing and building GEOG 288KC course materials\nThe build system (build_docs.py) can be reused as-is for the new course\nCourse content should focus on geospatial foundation models, Earth Engine, TorchGeo, and cloud-based analysis\nProjects are central - students work on applied GFM projects throughout the semester with specific deliverable deadlines\nCourse uses UCSB AI Sandbox for computational resources\nAssessment is pass/fail based on attendance, participation, and deliverable submission\nStudents may optionally submit projects to Hugging Face or GitHub for broader visibility"
  },
  {
    "objectID": "example_course/CONDA_ENVIRONMENT_SETUP.html",
    "href": "example_course/CONDA_ENVIRONMENT_SETUP.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "example_course/CONDA_ENVIRONMENT_SETUP.html#for-students-simple-setup",
    "href": "example_course/CONDA_ENVIRONMENT_SETUP.html#for-students-simple-setup",
    "title": "",
    "section": "For Students (Simple Setup)",
    "text": "For Students (Simple Setup)\n\nOption 1: Quick Setup (Base Environment)\nconda install python=3.11\nconda install jupyter jupyterlab numpy pandas matplotlib seaborn\n\n\nOption 2: Dedicated Environment (Recommended)\n# Create the EDS 217 environment\nconda env create -f environment.yml\n\n# Activate it\nconda activate eds217_2025\n\n# Register with Jupyter\npython -m ipykernel install --user --name eds217_2025 --display-name \"Python 3.11 (EDS 217 2025)\""
  },
  {
    "objectID": "example_course/CONDA_ENVIRONMENT_SETUP.html#for-instructors-migration-from-2024",
    "href": "example_course/CONDA_ENVIRONMENT_SETUP.html#for-instructors-migration-from-2024",
    "title": "",
    "section": "For Instructors (Migration from 2024)",
    "text": "For Instructors (Migration from 2024)\n\nIf you have an existing eds217_2024 environment:\n\nExport current environment (backup):\nconda activate eds217_2024\nconda env export &gt; eds217_2024_backup.yml\nCreate new environment:\nconda env create -f environment.yml\nconda activate eds217_2025\npython -m ipykernel install --user --name eds217_2025 --display-name \"Python 3.11 (EDS 217 2025)\"\nTest the environment:\npython --version  # Should show 3.11.x\njupyter lab       # Should show new kernel option\nRemove old environment (optional):\nconda env remove -n eds217_2024\njupyter kernelspec remove eds217_2024"
  },
  {
    "objectID": "example_course/CONDA_ENVIRONMENT_SETUP.html#verification",
    "href": "example_course/CONDA_ENVIRONMENT_SETUP.html#verification",
    "title": "",
    "section": "Verification",
    "text": "Verification\nIn JupyterLab, you should see “Python 3.11 (EDS 217 2025)” as a kernel option when creating new notebooks."
  },
  {
    "objectID": "example_course/CONDA_ENVIRONMENT_SETUP.html#package-list",
    "href": "example_course/CONDA_ENVIRONMENT_SETUP.html#package-list",
    "title": "",
    "section": "Package List",
    "text": "Package List\nThe environment includes: - Python 3.11 - NumPy, Pandas, Matplotlib, Seaborn - JupyterLab, IPython - SciPy, Scikit-learn - Plotly, Requests - GeoPandas, Folium - And more…"
  },
  {
    "objectID": "example_course/FAST_SETUP.html",
    "href": "example_course/FAST_SETUP.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "example_course/FAST_SETUP.html#option-1-use-mamba-fastest",
    "href": "example_course/FAST_SETUP.html#option-1-use-mamba-fastest",
    "title": "",
    "section": "Option 1: Use Mamba (Fastest!)",
    "text": "Option 1: Use Mamba (Fastest!)\nMamba is a drop-in replacement for conda that’s much faster:\n# Install mamba if you don't have it\nconda install mamba -n base -c conda-forge\n\n# Create environment with mamba (much faster!)\nmamba env create -f environment-fast.yml\n\n# Or create manually with mamba\nmamba create -n eds217_2025 python=3.11 numpy pandas matplotlib seaborn jupyter jupyterlab ipykernel -c conda-forge\n\n# Activate and register\nconda activate eds217_2025\npython -m ipykernel install --user --name eds217_2025 --display-name \"Python 3.11 (EDS 217 2025)\""
  },
  {
    "objectID": "example_course/FAST_SETUP.html#option-2-step-by-step-installation",
    "href": "example_course/FAST_SETUP.html#option-2-step-by-step-installation",
    "title": "",
    "section": "Option 2: Step-by-Step Installation",
    "text": "Option 2: Step-by-Step Installation\nCreate the environment first, then add packages:\n# Create basic environment (fast)\nconda create -n eds217_2025 python=3.11 -c conda-forge\n\n# Activate it\nconda activate eds217_2025\n\n# Install core packages in batches (faster than all at once)\nconda install numpy pandas -c conda-forge\nconda install matplotlib seaborn -c conda-forge  \nconda install jupyter jupyterlab ipykernel -c conda-forge\n\n# Register with Jupyter\npython -m ipykernel install --user --name eds217_2025 --display-name \"Python 3.11 (EDS 217 2025)\""
  },
  {
    "objectID": "example_course/FAST_SETUP.html#option-3-minimal-install-as-needed",
    "href": "example_course/FAST_SETUP.html#option-3-minimal-install-as-needed",
    "title": "",
    "section": "Option 3: Minimal + Install as Needed",
    "text": "Option 3: Minimal + Install as Needed\nStart with just the essentials and add packages when you need them:\n# Super minimal environment (very fast)\nconda create -n eds217_2025 python=3.11 pandas matplotlib jupyter jupyterlab -c conda-forge\n\n# Activate and register\nconda activate eds217_2025\npython -m ipykernel install --user --name eds217_2025 --display-name \"Python 3.11 (EDS 217 2025)\"\n\n# Add other packages as needed during the course\n# conda install seaborn scipy scikit-learn -c conda-forge"
  },
  {
    "objectID": "example_course/FAST_SETUP.html#option-4-if-all-else-fails---cancel-and-retry",
    "href": "example_course/FAST_SETUP.html#option-4-if-all-else-fails---cancel-and-retry",
    "title": "",
    "section": "Option 4: If All Else Fails - Cancel and Retry",
    "text": "Option 4: If All Else Fails - Cancel and Retry\nIf conda is stuck:\n# Cancel the current operation (Ctrl+C)\n# Then try with just conda-forge channel:\nconda create -n eds217_2025 python=3.11 -c conda-forge\nconda activate eds217_2025\nconda install numpy pandas matplotlib seaborn jupyter jupyterlab ipykernel -c conda-forge"
  },
  {
    "objectID": "example_course/FAST_SETUP.html#why-this-happens",
    "href": "example_course/FAST_SETUP.html#why-this-happens",
    "title": "",
    "section": "Why This Happens",
    "text": "Why This Happens\nConda gets slow because: - Multiple channels can create conflicts - Too many packages at once = exponential complexity - No version constraints = huge search space - Some packages have complex dependency trees"
  },
  {
    "objectID": "example_course/FAST_SETUP.html#recommended-approach",
    "href": "example_course/FAST_SETUP.html#recommended-approach",
    "title": "",
    "section": "Recommended Approach",
    "text": "Recommended Approach\n\nTry mamba first (if you can install it)\nUse conda-forge only (avoid defaults channel)\nInstall core packages first, add extras later\nSpecify versions if you know what works\n\nThe minimal environment with the core packages will work for 90% of the course. You can always add more packages later!"
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html",
    "title": "Interactive Session 2A (Instructor)",
    "section": "",
    "text": "Objective: Guide students through getting help, working with variables, and exploring methods available for different Python objects in Jupyter Notebooks.\nEstimated Time: 45-60 minutes\n\n\n\n\n\n\nInstructor Action: Demonstrate using help() with the len function:\nhelp(len)\n\nExplain how the output provides details on how to use the function.\n\nStudent Activity: Ask students to try help() on print or sum in their own notebooks.\nKey Point: Encourage discussion about what information was provided and how it could be useful.\n\n\n\n\n\nInstructor Action: Demonstrate using ? for quick documentation and ?? for more details:\nlen?\nlen??\nStudent Activity: Have students use ? and ?? with other functions like type or max.\nAdditional Notes: Explain that ? is great for quick reference, while ?? can show source code when available.\n\n\n\n\n\n\n\n\n\nInstructor Action: Create a few variables and demonstrate using type():\na = 10\nb = 5.5\nc = \"Hello, world!\"\n\nExplain the different data types (int, float, str).\n\nStudent Activity: Students create their own variables and use type() to explore their types.\nAdditional Notes: Discuss why understanding data types is crucial for avoiding errors.\n\n\n\n\n\nInstructor Action: Encourage experimentation with changing variable values and types.\nStudent Activity: Have students discuss the effects of changing types on variable behavior.\nKey Point: Highlight Python’s dynamic typing and how it influences variable usage.\n\n\n\n\n\n\n\n\n\nInstructor Action: Demonstrate using dir() on variables to explore available methods:\ndir(a)\ndir(b)\ndir(c)\n\nExplain how dir() lists all attributes and methods tied to an object.\n\nStudent Activity: Students use dir() on their own variables and explore the listed methods.\nAdditional Notes: Emphasize that understanding these methods can greatly enhance coding efficiency.\n\n\n\n\n\nInstructor Action: Show how to use help() with a method:\nhelp(c.upper)\n\nDiscuss what the output reveals about the method’s usage.\n\nStudent Activity: Students pick a method and use help() to learn more.\nKey Point: Encourage students to document what they find and how they might use these methods in practice.\n\n\n\n\n\nInstructor Action: Demonstrate calling a method on an object:\nc.upper()\n\nExplain how methods perform operations directly on the object.\n\nStudent Activity: Students experiment with calling different methods.\nAdditional Notes: Ask students to share interesting methods they discovered and their potential applications.\n\n\n\n\n\n\n\n\n\nInstructor Action: Guide students through using methods on lists (e.g., .append(), .pop(), .sort()).\nStudent Activity: Students follow along and replicate the process on their own data.\nAdditional Notes: The TA should circulate to provide help and answer individual questions.\n\n\n\n\n\nInstructor Action: Open the floor for questions about anything covered so far.\nKey Point: Use this time to address any misunderstandings or explore further examples.\n\n\n\n\n\n\n\n\n\nInstructor Action: Summarize the session, emphasizing the importance of help features, understanding variables, and exploring object methods.\nKey Point: Encourage students to use these tools regularly as they continue learning Python.\n\n\n\n\n\nStudent Activity: Students write a brief summary in a markdown cell about what they learned.\nInstructor Action: If time allows, ask a few students to share their reflections.\n\n\n\n\n\nInstructor Action: Provide a preview of the next session and how these foundational skills will be applied.\nKey Point: Reinforce the idea that mastering these basics will make more advanced topics easier to learn.\n\n\nInstructor Tip: Make sure to actively involve the TA throughout the session. The TA should help address individual questions, ensure that all students are following along, and provide additional explanations as needed."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html#getting-help-in-python",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html#getting-help-in-python",
    "title": "Interactive Session 2A (Instructor)",
    "section": "",
    "text": "Instructor Action: Demonstrate using help() with the len function:\nhelp(len)\n\nExplain how the output provides details on how to use the function.\n\nStudent Activity: Ask students to try help() on print or sum in their own notebooks.\nKey Point: Encourage discussion about what information was provided and how it could be useful.\n\n\n\n\n\nInstructor Action: Demonstrate using ? for quick documentation and ?? for more details:\nlen?\nlen??\nStudent Activity: Have students use ? and ?? with other functions like type or max.\nAdditional Notes: Explain that ? is great for quick reference, while ?? can show source code when available."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html#working-with-variables",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html#working-with-variables",
    "title": "Interactive Session 2A (Instructor)",
    "section": "",
    "text": "Instructor Action: Create a few variables and demonstrate using type():\na = 10\nb = 5.5\nc = \"Hello, world!\"\n\nExplain the different data types (int, float, str).\n\nStudent Activity: Students create their own variables and use type() to explore their types.\nAdditional Notes: Discuss why understanding data types is crucial for avoiding errors.\n\n\n\n\n\nInstructor Action: Encourage experimentation with changing variable values and types.\nStudent Activity: Have students discuss the effects of changing types on variable behavior.\nKey Point: Highlight Python’s dynamic typing and how it influences variable usage."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html#exploring-methods-available-for-objects",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html#exploring-methods-available-for-objects",
    "title": "Interactive Session 2A (Instructor)",
    "section": "",
    "text": "Instructor Action: Demonstrate using dir() on variables to explore available methods:\ndir(a)\ndir(b)\ndir(c)\n\nExplain how dir() lists all attributes and methods tied to an object.\n\nStudent Activity: Students use dir() on their own variables and explore the listed methods.\nAdditional Notes: Emphasize that understanding these methods can greatly enhance coding efficiency.\n\n\n\n\n\nInstructor Action: Show how to use help() with a method:\nhelp(c.upper)\n\nDiscuss what the output reveals about the method’s usage.\n\nStudent Activity: Students pick a method and use help() to learn more.\nKey Point: Encourage students to document what they find and how they might use these methods in practice.\n\n\n\n\n\nInstructor Action: Demonstrate calling a method on an object:\nc.upper()\n\nExplain how methods perform operations directly on the object.\n\nStudent Activity: Students experiment with calling different methods.\nAdditional Notes: Ask students to share interesting methods they discovered and their potential applications."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html#guided-practice-and-qa",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html#guided-practice-and-qa",
    "title": "Interactive Session 2A (Instructor)",
    "section": "",
    "text": "Instructor Action: Guide students through using methods on lists (e.g., .append(), .pop(), .sort()).\nStudent Activity: Students follow along and replicate the process on their own data.\nAdditional Notes: The TA should circulate to provide help and answer individual questions.\n\n\n\n\n\nInstructor Action: Open the floor for questions about anything covered so far.\nKey Point: Use this time to address any misunderstandings or explore further examples."
  },
  {
    "objectID": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html#wrap-up-and-reflection",
    "href": "example_course/course-materials/interactive-sessions/2a_getting_help-instructor.html#wrap-up-and-reflection",
    "title": "Interactive Session 2A (Instructor)",
    "section": "",
    "text": "Instructor Action: Summarize the session, emphasizing the importance of help features, understanding variables, and exploring object methods.\nKey Point: Encourage students to use these tools regularly as they continue learning Python.\n\n\n\n\n\nStudent Activity: Students write a brief summary in a markdown cell about what they learned.\nInstructor Action: If time allows, ask a few students to share their reflections.\n\n\n\n\n\nInstructor Action: Provide a preview of the next session and how these foundational skills will be applied.\nKey Point: Reinforce the idea that mastering these basics will make more advanced topics easier to learn.\n\n\nInstructor Tip: Make sure to actively involve the TA throughout the session. The TA should help address individual questions, ensure that all students are following along, and provide additional explanations as needed."
  },
  {
    "objectID": "example_course/extra_files/intro_block.html",
    "href": "example_course/extra_files/intro_block.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "example_course/extra_files/intro_block.html#getting-started",
    "href": "example_course/extra_files/intro_block.html#getting-started",
    "title": "",
    "section": "Getting Started",
    "text": "Getting Started\nBefore we begin our interactive session, please follow these steps to set up your Jupyter Notebook:\n\nOpen JupyterLab and create a new notebook:\n\nClick on the + button in the top left corner\nSelect Python 3.10.0 from the Notebook options\n\nRename your notebook:\n\nRight-click on the Untitled.ipynb tab\nSelect “Rename”\nName your notebook with the format: Session_XY_Topic.ipynb (Replace X with the day number and Y with the session number)\n\nAdd a title cell:\n\nIn the first cell of your notebook, change the cell type to “Markdown”\nAdd the following content (replace the placeholders with the actual information):\n\n\n# Day X: Session Y - [Session Topic]\n\n[Link to session webpage]\n\nDate: [Current Date]\n\nAdd a code cell:\n\nBelow the title cell, add a new cell\nEnsure it’s set as a “Code” cell\nThis will be where you start writing your Python code for the session\n\nThroughout the session:\n\nTake notes in Markdown cells\nCopy or write code in Code cells\nRun cells to test your code\nAsk questions if you need clarification\n\n\n\n\n\n\n\n\nCaution\n\n\n\nRemember to save your work frequently by clicking the save icon or using the keyboard shortcut (Ctrl+S or Cmd+S).\n\n\nLet’s begin our interactive session!"
  },
  {
    "objectID": "example_course/tasks/2025-course-updates.html",
    "href": "example_course/tasks/2025-course-updates.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "example_course/tasks/2025-course-updates.html#critical-updates-must-do-first",
    "href": "example_course/tasks/2025-course-updates.html#critical-updates-must-do-first",
    "title": "",
    "section": "Critical Updates (Must Do First)",
    "text": "Critical Updates (Must Do First)\n\nDates and Schedule\n\nUpdate all course dates from September 2024 to 2025 schedule in _quarto.yml navigation (lines 21-38)\nUpdate dates in individual day files (day1.qmd, day7.qmd, etc.) to match 2025 schedule\n\nChange index.qmd subtitle from “Summer 2024” to “Summer 2025”\n\n\n\nExternal Links and Data Sources\n\nCheck and update external links: syllabus Google Doc, NASA GISS data URLs, documentation links\nTest all data URLs in data/data_urls.py and update environmental datasets for currency\nVerify GitHub repository link in navigation points to correct location"
  },
  {
    "objectID": "example_course/tasks/2025-course-updates.html#important-content-technology-updates",
    "href": "example_course/tasks/2025-course-updates.html#important-content-technology-updates",
    "title": "",
    "section": "Important Content & Technology Updates",
    "text": "Important Content & Technology Updates\n\nPython Environment\n\nCOMPLETED: Updated from Python 3.10 to Python 3.11 for 2025 (all .qmd files updated)\nCOMPLETED: Created new eds217_2025 conda environment with Python 3.11\nCOMPLETED: Updated all course materials to reference eds217_2025 environment\nCOMPLETED: Created migration guide and environment.yml file\nReview and update core library versions (pandas, numpy, matplotlib, seaborn, jupyter)\nReview and update Python/Miniconda installation instructions for current best practices\n\n\n\nCourse Content\n\nDecide on commented-out sessions in _quarto.yml (lines 49, 53, 57, 77) - include or remove\nUpdate examples to use modern Python features (f-strings, type hints, current pandas syntax)\nConsider adding content on modern data science practices (type hints, conda-lock/poetry, etc.)"
  },
  {
    "objectID": "example_course/tasks/2025-course-updates.html#quality-enhancement-tasks",
    "href": "example_course/tasks/2025-course-updates.html#quality-enhancement-tasks",
    "title": "",
    "section": "Quality & Enhancement Tasks",
    "text": "Quality & Enhancement Tasks\n\nCourse Structure\n\nEvaluate day-by-day content distribution and interactive vs lecture balance\nPRIORITY: Complete End-of-Day Activity Alignment Review (see tasks/eod-alignment-action-plan.md)\nReview end-of-day practice distribution across all days\nUpdate data science workflow template to reflect current best practices\n\n\n\nAccessibility & Usability\n\nReview materials for accessibility (screen sizes, color schemes, interactive elements)\nEnsure all interactive elements work properly\nUpdate any references to software versions or capabilities\n\n\n\nModern Examples\n\nUpdate examples to reflect current environmental challenges\nConsider incorporating more recent data science techniques\nAdd more recent climate/environmental data examples"
  },
  {
    "objectID": "example_course/tasks/2025-course-updates.html#nice-to-have-improvements",
    "href": "example_course/tasks/2025-course-updates.html#nice-to-have-improvements",
    "title": "",
    "section": "Nice to Have Improvements",
    "text": "Nice to Have Improvements\n\nConsider adding modern visualization libraries (plotly, altair) content\nEvaluate JupyterLab 4.x features and VS Code integration recommendations\nReview conda vs. mamba recommendations for faster package installation\nAdd content on collaborative coding practices (GitHub workflows, etc.)\n\n\nNotes: - Priority should be given to Critical Updates first - Test all changes in a development environment before deploying - Consider student feedback from previous years when making content changes - Verify all external dependencies are stable for the duration of the 2025 course"
  }
]