[
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Building Geospatial Foundation Models",
    "section": "Course Description",
    "text": "Course Description\nThis project-driven seminar teaches students to build geospatial foundation models (GFMs) from scratch. Students implement every layer of the pipeline‚Äîfrom data pipelines and tokenization through attention mechanisms, full architectures, pretraining, evaluation, and deployment‚Äîculminating in a working end-to-end GFM tailored to a chosen geospatial application.\nBy the end of the course, students will be able to:\n\nDesign and implement geospatial data pipelines for multi-spectral, spatial, and temporal data\nBuild attention mechanisms and assemble transformer-based architectures for geospatial inputs\nPretrain using masked autoencoding and evaluate learned representations\nFine-tune models for specific Earth observation tasks\nDeploy models via APIs and interactive interfaces with honest performance analysis"
  },
  {
    "objectID": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "href": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "title": "Building Geospatial Foundation Models",
    "section": "Getting Started with the UCSB AI Sandbox",
    "text": "Getting Started with the UCSB AI Sandbox\nHere are detailed instructions for setting up your environment on the UCSB AI Sandbox, including foundation model installation and GPU optimization."
  },
  {
    "objectID": "index.html#course-structure-3-stages-9-steps",
    "href": "index.html#course-structure-3-stages-9-steps",
    "title": "Building Geospatial Foundation Models",
    "section": "Course Structure: 3 Stages, 9 Steps",
    "text": "Course Structure: 3 Stages, 9 Steps\n\nüèóÔ∏è Stage 1: Build GFM Architecture (Weeks 1-3)\n\nWeek 1: Geospatial Data Foundations (data pipelines, tokenization, loaders)\nWeek 2: Spatial-Temporal Attention Mechanisms (from-scratch implementation)\nWeek 3: Complete GFM Architecture (Vision Transformer for geospatial)\n\n\n\nüöÄ Stage 2: Train a Foundation Model (Weeks 4-7)\n\nWeek 4: Pretraining Implementation (masked autoencoder)\nWeek 5: Training Loop Optimization (stability, efficiency, mixed precision)\nWeek 6: Model Evaluation & Analysis (embeddings, probing, reconstructions)\nWeek 7: Integration with Existing Models (Prithvi, SatMAE)\n\n\n\nüéØ Stage 3: Apply & Deploy (Weeks 8-10)\n\nWeek 8: Task-Specific Fine-tuning (efficient strategies, few-shot)\nWeek 9: Model Implementation & Deployment (APIs, UI, benchmarking)\nWeek 10: Project Presentations & Synthesis\n\nQuick links: - Weekly materials: see navbar ‚Üí üóìÔ∏è weekly materials - Interactive sessions: see navbar ‚Üí üíª interactive sessions"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Building Geospatial Foundation Models",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\nKelly Caylor\nEmail: caylor@ucsb.edu\nLearn more: Bren profile\n\n\n\n\nTA\n\n\n\n\n\n\n\nAnna Boser\nEmail: anaboser@ucsb.edu\nLearn more: Bren profile"
  },
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#geospatial-data-fundamentals",
    "href": "cheatsheets.html#geospatial-data-fundamentals",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#pytorch-for-geospatial-ai",
    "href": "cheatsheets.html#pytorch-for-geospatial-ai",
    "title": "Cheatsheets",
    "section": "üî• PyTorch for Geospatial AI",
    "text": "üî• PyTorch for Geospatial AI\n\nPyTorch Tensors & GPU Operations\nTorchGeo Datasets & Transforms\nData Loading for Satellite Imagery"
  },
  {
    "objectID": "cheatsheets.html#foundation-models-huggingface",
    "href": "cheatsheets.html#foundation-models-huggingface",
    "title": "Cheatsheets",
    "section": "ü§ó Foundation Models & HuggingFace",
    "text": "ü§ó Foundation Models & HuggingFace\n\nLoading Pre-trained Models\nModel Inference & Feature Extraction\nFine-tuning Strategies"
  },
  {
    "objectID": "cheatsheets.html#visualization-analysis",
    "href": "cheatsheets.html#visualization-analysis",
    "title": "Cheatsheets",
    "section": "üìä Visualization & Analysis",
    "text": "üìä Visualization & Analysis\n\nPlotting Satellite Imagery\nInteractive Maps with Folium\nGeospatial Plotting with Matplotlib"
  },
  {
    "objectID": "course-materials/week8.html",
    "href": "course-materials/week8.html",
    "title": "Real-World Application",
    "section": "",
    "text": "This week adapts foundation models for specific tasks using efficient fine-tuning strategies.\n\n\n\nAdapt foundation models for specific tasks\nImplement efficient fine-tuning strategies\nHandle limited labeled data\nEvaluate task performance\n\n\n\n\n\nFull Fine-tuning vs.¬†Parameter-efficient: LoRA and adapter strategies\nFew-shot Learning: Working with limited labeled data\nTask-specific Data: Preparation and augmentation strategies\nMulti-task Learning: Joint training for multiple objectives\nFine-tuning Evaluation: Task-specific performance metrics\n\n\n\n\n\nImplement LoRA and adapter-based fine-tuning\nPrepare task-specific datasets\nCompare full vs.¬†parameter-efficient fine-tuning\nEvaluate few-shot learning capabilities\nOptimize for limited data scenarios\n\n\n\n\nSession 8: Task-Specific Fine-tuning - Efficient fine-tuning strategies and performance optimization\n\n\n\n\nOptimization and feature development work\nSystem testing and validation across scenarios\nDocumentation and user guide creation\nFinal debugging and system refinement\n\n\n\n\nFine-tuned model for chosen application with performance analysis and comparison\n\n\n\n\nComputational Efficiency: Algorithm optimization, parallel processing, memory management\nSystem Architecture: Modular design, component interaction, API development\nUser Experience: Interface improvements, workflow automation, result visualization\nDeployment Readiness: Configuration management, environment portability, monitoring\n\n\n\n\n\nDistributed computing and cloud deployment patterns\nModel serving and API development best practices\nAdvanced visualization and interactive dashboard creation\nSystem monitoring, logging, and error tracking implementation\n\n\n\n\n\nPerformance optimization guides and profiling tools\nAdvanced deployment patterns and containerization\nScalable architecture design principles\nSystem monitoring and observability frameworks\n\n\n\n\nWeek 9 will focus on final project polish, comprehensive documentation, and presentation preparation."
  },
  {
    "objectID": "course-materials/week8.html#week-8-overview",
    "href": "course-materials/week8.html#week-8-overview",
    "title": "Real-World Application",
    "section": "",
    "text": "This week adapts foundation models for specific tasks using efficient fine-tuning strategies.\n\n\n\nAdapt foundation models for specific tasks\nImplement efficient fine-tuning strategies\nHandle limited labeled data\nEvaluate task performance\n\n\n\n\n\nFull Fine-tuning vs.¬†Parameter-efficient: LoRA and adapter strategies\nFew-shot Learning: Working with limited labeled data\nTask-specific Data: Preparation and augmentation strategies\nMulti-task Learning: Joint training for multiple objectives\nFine-tuning Evaluation: Task-specific performance metrics\n\n\n\n\n\nImplement LoRA and adapter-based fine-tuning\nPrepare task-specific datasets\nCompare full vs.¬†parameter-efficient fine-tuning\nEvaluate few-shot learning capabilities\nOptimize for limited data scenarios\n\n\n\n\nSession 8: Task-Specific Fine-tuning - Efficient fine-tuning strategies and performance optimization\n\n\n\n\nOptimization and feature development work\nSystem testing and validation across scenarios\nDocumentation and user guide creation\nFinal debugging and system refinement\n\n\n\n\nFine-tuned model for chosen application with performance analysis and comparison\n\n\n\n\nComputational Efficiency: Algorithm optimization, parallel processing, memory management\nSystem Architecture: Modular design, component interaction, API development\nUser Experience: Interface improvements, workflow automation, result visualization\nDeployment Readiness: Configuration management, environment portability, monitoring\n\n\n\n\n\nDistributed computing and cloud deployment patterns\nModel serving and API development best practices\nAdvanced visualization and interactive dashboard creation\nSystem monitoring, logging, and error tracking implementation\n\n\n\n\n\nPerformance optimization guides and profiling tools\nAdvanced deployment patterns and containerization\nScalable architecture design principles\nSystem monitoring and observability frameworks\n\n\n\n\nWeek 9 will focus on final project polish, comprehensive documentation, and presentation preparation."
  },
  {
    "objectID": "course-materials/week9.html",
    "href": "course-materials/week9.html",
    "title": "Real-World Application",
    "section": "",
    "text": "This week deploys models for production use, optimizes for inference, and builds user-friendly interfaces.\n\n\n\nDeploy models for production use\nOptimize models for inference\nBuild user-friendly interfaces\nDocument model capabilities\n\n\n\n\n\nModel Optimization: Quantization and optimization for inference\nAPI Development: Building inference APIs with FastAPI\nUser Interface: Creation of interactive web interfaces\nModel Documentation: Model cards and capability documentation\nPerformance Benchmarking: Speed and accuracy analysis\n\n\n\n\n\nOptimize model for efficient inference\nBuild deployment API with documentation\nCreate user interface for model interaction\nWrite comprehensive model cards\nBenchmark performance and document capabilities\n\n\n\n\nDeployable model with documentation: API, user interface, model card, and performance benchmarks\n\n\n\nSession 9: Model Implementation & Deployment - Real-time deployment challenges with live APIs and immediate feedback\n\n\n\nExtended support for: - Final debugging and optimization - Advanced visualization and result analysis - Presentation rehearsals and feedback - Repository organization and documentation\n\n\n\n\nCode review partnerships for quality assurance\nPractice presentations with constructive feedback\nCross-project learning and knowledge sharing\nCollaborative problem-solving for technical challenges\n\n\n\n\n\nProject documentation templates\nPresentation guidelines and rubrics\nCode review checklists\nFinal project submission requirements\n\n\n\n\nWeek 10 will feature final project presentations and discussion of future directions in geospatial foundation models."
  },
  {
    "objectID": "course-materials/week9.html#week-9-overview",
    "href": "course-materials/week9.html#week-9-overview",
    "title": "Real-World Application",
    "section": "",
    "text": "This week deploys models for production use, optimizes for inference, and builds user-friendly interfaces.\n\n\n\nDeploy models for production use\nOptimize models for inference\nBuild user-friendly interfaces\nDocument model capabilities\n\n\n\n\n\nModel Optimization: Quantization and optimization for inference\nAPI Development: Building inference APIs with FastAPI\nUser Interface: Creation of interactive web interfaces\nModel Documentation: Model cards and capability documentation\nPerformance Benchmarking: Speed and accuracy analysis\n\n\n\n\n\nOptimize model for efficient inference\nBuild deployment API with documentation\nCreate user interface for model interaction\nWrite comprehensive model cards\nBenchmark performance and document capabilities\n\n\n\n\nDeployable model with documentation: API, user interface, model card, and performance benchmarks\n\n\n\nSession 9: Model Implementation & Deployment - Real-time deployment challenges with live APIs and immediate feedback\n\n\n\nExtended support for: - Final debugging and optimization - Advanced visualization and result analysis - Presentation rehearsals and feedback - Repository organization and documentation\n\n\n\n\nCode review partnerships for quality assurance\nPractice presentations with constructive feedback\nCross-project learning and knowledge sharing\nCollaborative problem-solving for technical challenges\n\n\n\n\n\nProject documentation templates\nPresentation guidelines and rubrics\nCode review checklists\nFinal project submission requirements\n\n\n\n\nWeek 10 will feature final project presentations and discussion of future directions in geospatial foundation models."
  },
  {
    "objectID": "course-materials/week1.html",
    "href": "course-materials/week1.html",
    "title": "Foundation Building",
    "section": "",
    "text": "This week establishes the foundation for building geospatial foundation models by mastering data preparation and preprocessing pipelines.\n\n\n\nUnderstand geospatial data as foundation model input\nImplement robust data preprocessing pipelines\nHandle missing data (clouds, gaps) effectively\nCreate temporal sequences from satellite imagery\n\n\n\n\n\nGeospatial Tokenization: Convert satellite imagery to model-ready tokens\nMulti-spectral Data: Properties and preprocessing of different bands\nCloud Masking: Strategies for handling missing data\nTemporal Sequences: Building time series from satellite imagery\nData Loaders: Efficient batch processing for training\n\n\n\n\n\nBuild complete geospatial tokenization pipeline\nImplement cloud masking and missing data strategies\nCreate temporal sequence datasets\nOptimize data loading for large-scale training\n\n\n\n\n\nMulti-spectral satellite data properties\nCloud masking algorithms and implementations\nPyTorch data loading best practices\n\n\n\n\nSession 1: Geospatial Data Foundations - Hands-on implementation of geospatial data preprocessing pipeline\n\n\n\nComplete geospatial data pipeline capable of processing satellite imagery for foundation model training\n\n\n\nWeek 2 will implement spatial-temporal attention mechanisms from scratch."
  },
  {
    "objectID": "course-materials/week1.html#week-1-overview",
    "href": "course-materials/week1.html#week-1-overview",
    "title": "Foundation Building",
    "section": "",
    "text": "This week establishes the foundation for building geospatial foundation models by mastering data preparation and preprocessing pipelines.\n\n\n\nUnderstand geospatial data as foundation model input\nImplement robust data preprocessing pipelines\nHandle missing data (clouds, gaps) effectively\nCreate temporal sequences from satellite imagery\n\n\n\n\n\nGeospatial Tokenization: Convert satellite imagery to model-ready tokens\nMulti-spectral Data: Properties and preprocessing of different bands\nCloud Masking: Strategies for handling missing data\nTemporal Sequences: Building time series from satellite imagery\nData Loaders: Efficient batch processing for training\n\n\n\n\n\nBuild complete geospatial tokenization pipeline\nImplement cloud masking and missing data strategies\nCreate temporal sequence datasets\nOptimize data loading for large-scale training\n\n\n\n\n\nMulti-spectral satellite data properties\nCloud masking algorithms and implementations\nPyTorch data loading best practices\n\n\n\n\nSession 1: Geospatial Data Foundations - Hands-on implementation of geospatial data preprocessing pipeline\n\n\n\nComplete geospatial data pipeline capable of processing satellite imagery for foundation model training\n\n\n\nWeek 2 will implement spatial-temporal attention mechanisms from scratch."
  },
  {
    "objectID": "course-materials/week0.html",
    "href": "course-materials/week0.html",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/week0.html#week-0-overview",
    "href": "course-materials/week0.html#week-0-overview",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/week2.html",
    "href": "course-materials/week2.html",
    "title": "Foundation Building",
    "section": "",
    "text": "This week implements attention mechanisms from scratch, adapting them specifically for spatial-temporal geospatial relationships.\n\n\n\nImplement self-attention from scratch\nAdapt attention for spatial relationships\nAdd temporal attention for time series\nUnderstand positional encoding for 2D/3D data\n\n\n\n\n\nMulti-head Self-Attention: Implementation from mathematical formulations\n2D Positional Encoding: Spatial position encoding for image patches\nTemporal Encoding: Time series attention for satellite sequences\nCross-attention: Multi-modal data fusion mechanisms\nAttention Visualization: Understanding learned attention patterns\n\n\n\n\n\nImplement multi-head attention from scratch\nBuild 2D positional encoding for spatial patches\nCreate temporal attention for time series data\nVisualize and interpret attention patterns\n\n\n\n\n\nMathematical implementation of attention mechanisms\nPyTorch tensor operations and gradient computation\nAttention pattern visualization\nDebugging dimension mismatches and training issues\n\n\n\n\nSession 2: Spatial-Temporal Attention Mechanisms - Live coding session: Building attention mechanisms with intentional errors and debugging\n\n\n\nCustom attention module for geospatial data with spatial and temporal components\n\n\n\n\nAttention mechanism mathematical foundations\nTransformer architecture deep dive\nSpatial attention for computer vision\n\n\n\n\nWeek 3 will assemble the complete GFM architecture using our custom attention modules."
  },
  {
    "objectID": "course-materials/week2.html#week-2-overview",
    "href": "course-materials/week2.html#week-2-overview",
    "title": "Foundation Building",
    "section": "",
    "text": "This week implements attention mechanisms from scratch, adapting them specifically for spatial-temporal geospatial relationships.\n\n\n\nImplement self-attention from scratch\nAdapt attention for spatial relationships\nAdd temporal attention for time series\nUnderstand positional encoding for 2D/3D data\n\n\n\n\n\nMulti-head Self-Attention: Implementation from mathematical formulations\n2D Positional Encoding: Spatial position encoding for image patches\nTemporal Encoding: Time series attention for satellite sequences\nCross-attention: Multi-modal data fusion mechanisms\nAttention Visualization: Understanding learned attention patterns\n\n\n\n\n\nImplement multi-head attention from scratch\nBuild 2D positional encoding for spatial patches\nCreate temporal attention for time series data\nVisualize and interpret attention patterns\n\n\n\n\n\nMathematical implementation of attention mechanisms\nPyTorch tensor operations and gradient computation\nAttention pattern visualization\nDebugging dimension mismatches and training issues\n\n\n\n\nSession 2: Spatial-Temporal Attention Mechanisms - Live coding session: Building attention mechanisms with intentional errors and debugging\n\n\n\nCustom attention module for geospatial data with spatial and temporal components\n\n\n\n\nAttention mechanism mathematical foundations\nTransformer architecture deep dive\nSpatial attention for computer vision\n\n\n\n\nWeek 3 will assemble the complete GFM architecture using our custom attention modules."
  },
  {
    "objectID": "course-materials/week3.html",
    "href": "course-materials/week3.html",
    "title": "Foundation Building",
    "section": "",
    "text": "This week assembles a complete Vision Transformer architecture adapted for geospatial foundation models, integrating data pipelines and attention mechanisms.\n\n\n\nAssemble complete Vision Transformer architecture\nHandle multi-spectral input processing\nImplement memory-efficient designs\nValidate architecture through testing\n\n\n\n\n\nTransformer Encoder Blocks: Layer normalization and residual connections\nMulti-spectral Input Embedding: Handling different numbers of spectral bands\nArchitecture Testing: Forward pass validation and gradient checking\nMemory Optimization: Efficient attention and activation checkpointing\nModel Scaling: Understanding parameter count and computational requirements\n\n\n\n\n\nBuild complete transformer encoder architecture\nImplement multi-spectral input processing\nTest architecture with sample geospatial data\nOptimize memory usage and computational efficiency\n\n\n\n\n\nPyTorch module composition and inheritance\nMemory profiling and optimization\nArchitecture validation and testing\nUnderstanding model complexity and scaling\n\n\n\n\nSession 3: Complete GFM Architecture - Collaborative architecture decisions: Debating design choices and testing empirically\n\n\n\nWorking GFM architecture (~10M parameters) capable of processing multi-spectral satellite imagery\n\n\n\n\nVision Transformer architecture details\nEfficient transformer implementations\nMemory optimization techniques for large models\n\n\n\n\nWeek 4 begins Stage 2 with masked autoencoder pretraining implementation."
  },
  {
    "objectID": "course-materials/week3.html#week-3-overview",
    "href": "course-materials/week3.html#week-3-overview",
    "title": "Foundation Building",
    "section": "",
    "text": "This week assembles a complete Vision Transformer architecture adapted for geospatial foundation models, integrating data pipelines and attention mechanisms.\n\n\n\nAssemble complete Vision Transformer architecture\nHandle multi-spectral input processing\nImplement memory-efficient designs\nValidate architecture through testing\n\n\n\n\n\nTransformer Encoder Blocks: Layer normalization and residual connections\nMulti-spectral Input Embedding: Handling different numbers of spectral bands\nArchitecture Testing: Forward pass validation and gradient checking\nMemory Optimization: Efficient attention and activation checkpointing\nModel Scaling: Understanding parameter count and computational requirements\n\n\n\n\n\nBuild complete transformer encoder architecture\nImplement multi-spectral input processing\nTest architecture with sample geospatial data\nOptimize memory usage and computational efficiency\n\n\n\n\n\nPyTorch module composition and inheritance\nMemory profiling and optimization\nArchitecture validation and testing\nUnderstanding model complexity and scaling\n\n\n\n\nSession 3: Complete GFM Architecture - Collaborative architecture decisions: Debating design choices and testing empirically\n\n\n\nWorking GFM architecture (~10M parameters) capable of processing multi-spectral satellite imagery\n\n\n\n\nVision Transformer architecture details\nEfficient transformer implementations\nMemory optimization techniques for large models\n\n\n\n\nWeek 4 begins Stage 2 with masked autoencoder pretraining implementation."
  },
  {
    "objectID": "course-materials/week7.html",
    "href": "course-materials/week7.html",
    "title": "System Training",
    "section": "",
    "text": "This week integrates with existing foundation models and compares custom implementations against state-of-the-art.\n\n\n\nLoad and use pretrained foundation models\nCompare custom vs.¬†state-of-the-art models\nUnderstand when to build vs.¬†use existing models\nImplement model ensembling\n\n\n\n\n\nLoading Prithvi and SatMAE: Working with pretrained weights\nArchitecture Compatibility: Adapting existing models to new data\nPerformance Comparison: Custom models vs.¬†state-of-the-art\nTransfer Learning: Strategies for model adaptation\nModel Selection: Criteria for choosing when to build vs.¬†use\n\n\n\n\n\nLoad and configure Prithvi and SatMAE models\nAdapt architectures for compatibility with course data\nCompare performance against custom models\nImplement transfer learning strategies\nCreate model selection framework\n\n\n\n\nSession 7: Integration with Existing Models - Working with Prithvi, SatMAE, and performance comparison\n\n\n\n\nIndependent implementation with instructor feedback\nSmall-group collaboration and knowledge sharing\nTroubleshooting sessions and milestone tracking\n\n\n\n\nIntegrated system using multiple models with performance comparison and model selection guidelines\n\n\n\n\nDevelopment Best Practices: Code organization, testing frameworks, documentation\nPerformance Monitoring: Computational efficiency, resource utilization, bottleneck identification\nVersion Control: Branching strategies, collaborative workflows, milestone tracking\nEnvironment Management: Reproducible setups, dependency management, deployment preparation\n\n\n\n\n\nStructured code review sessions with feedback protocols\nKnowledge sharing presentations on technical solutions\nCollaborative debugging and problem-solving workshops\nCross-project learning and technique exchange\n\n\n\n\n\nAdvanced development pattern guides\nModel optimization and debugging tools\nPerformance profiling and monitoring utilities\nCollaborative development workflows\n\n\n\n\nWeek 8 will focus on project refinement, advanced features, and scalability improvements."
  },
  {
    "objectID": "course-materials/week7.html#week-7-overview",
    "href": "course-materials/week7.html#week-7-overview",
    "title": "System Training",
    "section": "",
    "text": "This week integrates with existing foundation models and compares custom implementations against state-of-the-art.\n\n\n\nLoad and use pretrained foundation models\nCompare custom vs.¬†state-of-the-art models\nUnderstand when to build vs.¬†use existing models\nImplement model ensembling\n\n\n\n\n\nLoading Prithvi and SatMAE: Working with pretrained weights\nArchitecture Compatibility: Adapting existing models to new data\nPerformance Comparison: Custom models vs.¬†state-of-the-art\nTransfer Learning: Strategies for model adaptation\nModel Selection: Criteria for choosing when to build vs.¬†use\n\n\n\n\n\nLoad and configure Prithvi and SatMAE models\nAdapt architectures for compatibility with course data\nCompare performance against custom models\nImplement transfer learning strategies\nCreate model selection framework\n\n\n\n\nSession 7: Integration with Existing Models - Working with Prithvi, SatMAE, and performance comparison\n\n\n\n\nIndependent implementation with instructor feedback\nSmall-group collaboration and knowledge sharing\nTroubleshooting sessions and milestone tracking\n\n\n\n\nIntegrated system using multiple models with performance comparison and model selection guidelines\n\n\n\n\nDevelopment Best Practices: Code organization, testing frameworks, documentation\nPerformance Monitoring: Computational efficiency, resource utilization, bottleneck identification\nVersion Control: Branching strategies, collaborative workflows, milestone tracking\nEnvironment Management: Reproducible setups, dependency management, deployment preparation\n\n\n\n\n\nStructured code review sessions with feedback protocols\nKnowledge sharing presentations on technical solutions\nCollaborative debugging and problem-solving workshops\nCross-project learning and technique exchange\n\n\n\n\n\nAdvanced development pattern guides\nModel optimization and debugging tools\nPerformance profiling and monitoring utilities\nCollaborative development workflows\n\n\n\n\nWeek 8 will focus on project refinement, advanced features, and scalability improvements."
  },
  {
    "objectID": "course-materials/week6.html",
    "href": "course-materials/week6.html",
    "title": "System Training",
    "section": "",
    "text": "This week evaluates representation quality, assesses reconstruction performance, and compares against baselines.\n\n\n\nEvaluate representation quality\nAssess reconstruction performance\nCompare against baselines\nUnderstand learned features\n\n\n\n\n\nEmbedding Visualization: t-SNE, UMAP for learned representations\nReconstruction Quality: Assessment metrics and visual analysis\nLinear Probing: Evaluation of learned features for downstream tasks\nFeature Interpretation: Understanding what the model has learned\nAblation Studies: Component importance analysis\n\n\n\n\n\nGenerate and visualize learned embeddings\nEvaluate reconstruction quality on test data\nImplement linear probing for classification tasks\nAnalyze learned attention patterns and features\nCompare against random initialization baselines\n\n\n\n\nSession 6: Model Evaluation & Analysis - Comprehensive evaluation with embedding visualization and performance analysis\n\n\n\nComprehensive evaluation report with embedding visualizations, reconstruction analysis, and baseline comparisons\n\n\n\n\nProject repository structure and best practices\nReproducible environment setup (conda, Docker, requirements)\nData management and version control strategies\nComputational resource allocation and optimization\n\n\n\n\n\nProject proposal templates and examples\nRepository structure guidelines\nUCSB AI Sandbox resource allocation\nTechnical consultation scheduling system\n\n\n\n\nWeek 7 will focus on intensive development with regular progress check-ins and technical workshops."
  },
  {
    "objectID": "course-materials/week6.html#week-6-overview",
    "href": "course-materials/week6.html#week-6-overview",
    "title": "System Training",
    "section": "",
    "text": "This week evaluates representation quality, assesses reconstruction performance, and compares against baselines.\n\n\n\nEvaluate representation quality\nAssess reconstruction performance\nCompare against baselines\nUnderstand learned features\n\n\n\n\n\nEmbedding Visualization: t-SNE, UMAP for learned representations\nReconstruction Quality: Assessment metrics and visual analysis\nLinear Probing: Evaluation of learned features for downstream tasks\nFeature Interpretation: Understanding what the model has learned\nAblation Studies: Component importance analysis\n\n\n\n\n\nGenerate and visualize learned embeddings\nEvaluate reconstruction quality on test data\nImplement linear probing for classification tasks\nAnalyze learned attention patterns and features\nCompare against random initialization baselines\n\n\n\n\nSession 6: Model Evaluation & Analysis - Comprehensive evaluation with embedding visualization and performance analysis\n\n\n\nComprehensive evaluation report with embedding visualizations, reconstruction analysis, and baseline comparisons\n\n\n\n\nProject repository structure and best practices\nReproducible environment setup (conda, Docker, requirements)\nData management and version control strategies\nComputational resource allocation and optimization\n\n\n\n\n\nProject proposal templates and examples\nRepository structure guidelines\nUCSB AI Sandbox resource allocation\nTechnical consultation scheduling system\n\n\n\n\nWeek 7 will focus on intensive development with regular progress check-ins and technical workshops."
  },
  {
    "objectID": "course-materials/week4.html",
    "href": "course-materials/week4.html",
    "title": "System Training",
    "section": "",
    "text": "This week implements masked autoencoder pretraining for geospatial foundation models, setting up the complete training pipeline.\n\n\n\nImplement masked autoencoder objective\nSet up distributed training pipeline\nMonitor training progress effectively\nHandle large-scale geospatial datasets\n\n\n\n\n\nMasked Patch Reconstruction: Prithvi-style self-supervised learning\nTraining Data Preparation: Augmentation and batch creation\nLoss Functions: Reconstruction loss for masked patches\nDistributed Training: Multi-GPU setup and synchronization\nMonitoring and Logging: Training metrics and visualization\n\n\n\n\n\nImplement masked autoencoder training objective\nSet up distributed training infrastructure\nCreate training data augmentation pipeline\nBuild monitoring and logging system\n\n\n\n\n\nSelf-supervised learning implementation\nDistributed training with PyTorch Lightning\nTraining monitoring and debugging\nLarge-scale data handling\n\n\n\n\nSession 4: Pretraining Implementation - Launch first pretraining run with real satellite data\n\n\n\nActive pretraining pipeline with monitoring and checkpointing\n\n\n\n\nMasked Autoencoder (MAE) paper and implementation\nPrithvi training methodology\nDistributed training best practices\n\n\n\n\nWeek 5 will optimize the training loop for stability and efficiency."
  },
  {
    "objectID": "course-materials/week4.html#week-4-overview",
    "href": "course-materials/week4.html#week-4-overview",
    "title": "System Training",
    "section": "",
    "text": "This week implements masked autoencoder pretraining for geospatial foundation models, setting up the complete training pipeline.\n\n\n\nImplement masked autoencoder objective\nSet up distributed training pipeline\nMonitor training progress effectively\nHandle large-scale geospatial datasets\n\n\n\n\n\nMasked Patch Reconstruction: Prithvi-style self-supervised learning\nTraining Data Preparation: Augmentation and batch creation\nLoss Functions: Reconstruction loss for masked patches\nDistributed Training: Multi-GPU setup and synchronization\nMonitoring and Logging: Training metrics and visualization\n\n\n\n\n\nImplement masked autoencoder training objective\nSet up distributed training infrastructure\nCreate training data augmentation pipeline\nBuild monitoring and logging system\n\n\n\n\n\nSelf-supervised learning implementation\nDistributed training with PyTorch Lightning\nTraining monitoring and debugging\nLarge-scale data handling\n\n\n\n\nSession 4: Pretraining Implementation - Launch first pretraining run with real satellite data\n\n\n\nActive pretraining pipeline with monitoring and checkpointing\n\n\n\n\nMasked Autoencoder (MAE) paper and implementation\nPrithvi training methodology\nDistributed training best practices\n\n\n\n\nWeek 5 will optimize the training loop for stability and efficiency."
  },
  {
    "objectID": "course-materials/week5.html",
    "href": "course-materials/week5.html",
    "title": "System Training",
    "section": "",
    "text": "This week optimizes training for stability and efficiency, handling geospatial-specific training challenges.\n\n\n\nOptimize training for stability and efficiency\nHandle geospatial-specific training challenges\nImplement advanced optimization techniques\nDebug training issues\n\n\n\n\n\nLearning Rate Scheduling: Warm-up, cosine annealing, adaptive strategies\nGradient Management: Clipping, accumulation, normalization\nMixed Precision Training: Automatic mixed precision (AMP) implementation\nMissing Data Handling: Training with cloud-contaminated patches\nTraining Stability: Convergence analysis and debugging\n\n\n\n\n\nImplement advanced learning rate schedulers\nAdd gradient clipping and accumulation\nEnable mixed precision training\nDebug training instabilities and convergence issues\n\n\n\n\n\nAdvanced PyTorch optimization\nTraining stability analysis\nPerformance profiling and optimization\nSystematic debugging of training issues\n\n\n\n\nSession 5: Training Loop Optimization - Performance archaeology: Investigating training curves and optimization dynamics\n\n\n\nOptimized training loop with monitoring and stability guarantees\n\n\n\n\nTraining stability best practices\nMixed precision training guides\nOptimization theory for deep learning\n\n\n\n\nWeek 6 will evaluate model performance and analyze learned representations."
  },
  {
    "objectID": "course-materials/week5.html#week-5-overview",
    "href": "course-materials/week5.html#week-5-overview",
    "title": "System Training",
    "section": "",
    "text": "This week optimizes training for stability and efficiency, handling geospatial-specific training challenges.\n\n\n\nOptimize training for stability and efficiency\nHandle geospatial-specific training challenges\nImplement advanced optimization techniques\nDebug training issues\n\n\n\n\n\nLearning Rate Scheduling: Warm-up, cosine annealing, adaptive strategies\nGradient Management: Clipping, accumulation, normalization\nMixed Precision Training: Automatic mixed precision (AMP) implementation\nMissing Data Handling: Training with cloud-contaminated patches\nTraining Stability: Convergence analysis and debugging\n\n\n\n\n\nImplement advanced learning rate schedulers\nAdd gradient clipping and accumulation\nEnable mixed precision training\nDebug training instabilities and convergence issues\n\n\n\n\n\nAdvanced PyTorch optimization\nTraining stability analysis\nPerformance profiling and optimization\nSystematic debugging of training issues\n\n\n\n\nSession 5: Training Loop Optimization - Performance archaeology: Investigating training curves and optimization dynamics\n\n\n\nOptimized training loop with monitoring and stability guarantees\n\n\n\n\nTraining stability best practices\nMixed precision training guides\nOptimization theory for deep learning\n\n\n\n\nWeek 6 will evaluate model performance and analyze learned representations."
  },
  {
    "objectID": "course-materials/week10.html",
    "href": "course-materials/week10.html",
    "title": "Real-World Application",
    "section": "",
    "text": "This week presents complete model pipelines, synthesizes course learnings, and identifies future research directions.\n\n\n\nPresent complete model pipeline\nSynthesize course learnings\nIdentify future research directions\nPlan continued development\n\n\n\n\n\nFinal Project Presentations: Complete foundation model pipeline demonstrations\nPerformance Analysis: Comparison with state-of-the-art models\nScaling Strategies: Discussion of computational and data requirements\nFuture Work: Identification of research and development opportunities\nCourse Reflection: Synthesis of build-first learning approach\n\n\n\n\nComplete foundation model pipeline with: - Working architecture built from scratch - Trained model with evaluation results - Deployed system with API and interface - Comparison with existing foundation models - Documentation and presentation materials\n\n\n\nSession 10: Project Presentations & Synthesis - Present complete foundation model systems and synthesize course learnings\n\n\n\n\nPresent complete foundation model systems\nDemonstrate end-to-end pipelines with live data\nCompare performance against state-of-the-art\nDiscuss scaling and future development\nReflect on course transformation from consumers to creators\n\n\n\n\n\nEmerging Trends: Latest developments in foundation model architectures\nApplication Domains: Unexplored areas and promising applications\nTechnical Challenges: Scalability, interpretability, robustness\nSocietal Impact: Environmental monitoring, disaster response, climate science\nFuture Research: Open problems, collaboration opportunities\n\n\n\n\nHugging Face/GitHub Submission: - Students may optionally submit polished projects to public repositories - Opportunity for broader visibility and community engagement - Instructor support for preparing submission-quality documentation\nConference/Workshop Submissions: - Guidance on adapting projects for academic conferences - Identification of relevant venues and submission opportunities - Support for preparing extended abstracts or full papers\n\n\n\n\nProject Repository: Complete, documented codebase with examples\nFinal Report: Comprehensive technical document (5-10 pages)\nPresentation Materials: Slides and any supporting materials\nPeer Evaluations: Constructive feedback on all peer presentations\n\n\n\n\n\nIndividual reflection essays on key learnings and growth\nCourse feedback and suggestions for future iterations\nIdentification of personal next steps and continued learning goals\n\n\n\n\n\nPresentation templates and guidelines\nProject submission checklists\nConference and journal venue lists\nNetworking and collaboration opportunities\n\n\n\n\nRecognition of outstanding projects and contributions to course community!"
  },
  {
    "objectID": "course-materials/week10.html#week-10-overview",
    "href": "course-materials/week10.html#week-10-overview",
    "title": "Real-World Application",
    "section": "",
    "text": "This week presents complete model pipelines, synthesizes course learnings, and identifies future research directions.\n\n\n\nPresent complete model pipeline\nSynthesize course learnings\nIdentify future research directions\nPlan continued development\n\n\n\n\n\nFinal Project Presentations: Complete foundation model pipeline demonstrations\nPerformance Analysis: Comparison with state-of-the-art models\nScaling Strategies: Discussion of computational and data requirements\nFuture Work: Identification of research and development opportunities\nCourse Reflection: Synthesis of build-first learning approach\n\n\n\n\nComplete foundation model pipeline with: - Working architecture built from scratch - Trained model with evaluation results - Deployed system with API and interface - Comparison with existing foundation models - Documentation and presentation materials\n\n\n\nSession 10: Project Presentations & Synthesis - Present complete foundation model systems and synthesize course learnings\n\n\n\n\nPresent complete foundation model systems\nDemonstrate end-to-end pipelines with live data\nCompare performance against state-of-the-art\nDiscuss scaling and future development\nReflect on course transformation from consumers to creators\n\n\n\n\n\nEmerging Trends: Latest developments in foundation model architectures\nApplication Domains: Unexplored areas and promising applications\nTechnical Challenges: Scalability, interpretability, robustness\nSocietal Impact: Environmental monitoring, disaster response, climate science\nFuture Research: Open problems, collaboration opportunities\n\n\n\n\nHugging Face/GitHub Submission: - Students may optionally submit polished projects to public repositories - Opportunity for broader visibility and community engagement - Instructor support for preparing submission-quality documentation\nConference/Workshop Submissions: - Guidance on adapting projects for academic conferences - Identification of relevant venues and submission opportunities - Support for preparing extended abstracts or full papers\n\n\n\n\nProject Repository: Complete, documented codebase with examples\nFinal Report: Comprehensive technical document (5-10 pages)\nPresentation Materials: Slides and any supporting materials\nPeer Evaluations: Constructive feedback on all peer presentations\n\n\n\n\n\nIndividual reflection essays on key learnings and growth\nCourse feedback and suggestions for future iterations\nIdentification of personal next steps and continued learning goals\n\n\n\n\n\nPresentation templates and guidelines\nProject submission checklists\nConference and journal venue lists\nNetworking and collaboration opportunities\n\n\n\n\nRecognition of outstanding projects and contributions to course community!"
  },
  {
    "objectID": "course-materials/projects/project-application-template.html",
    "href": "course-materials/projects/project-application-template.html",
    "title": "Project Application Template",
    "section": "",
    "text": "Student Name: [Your Name]\nEmail: [Your Email]\nDate: [Submission Date]\n\n\nPlease describe your past experience with:\nRemote Sensing and Geospatial Data (2-3 sentences): - [Describe your experience with satellite imagery, GIS, remote sensing tools, etc.]\nMachine Learning and Deep Learning (2-3 sentences): - [Describe your experience with Python, PyTorch, model training, etc.]\nSpecific Tools and Platforms (1-2 sentences): - [Mention experience with Earth Engine, QGIS, ArcGIS, cloud platforms, etc.]\n\n\n\nApplication Domain (2-3 sentences): - [Describe the specific application area you‚Äôre interested in exploring] - [Examples: crop monitoring, deforestation detection, disaster response, urban planning, etc.]\nResearch Questions or Goals (2-3 sentences): - [What specific questions do you hope to answer or problems to solve?] - [What would successful outcomes look like for your project?]\n\n\n\nAvailable Data (1-2 sentences): - [Do you have access to specific datasets, ground truth data, or study areas?] - [If not, describe what types of data you would need to acquire]\nComputational Resources (1 sentence): - [Describe your current access to computational resources, cloud credits, etc.]\n\n\n\nInitial Ideas (2-3 sentences): - [Any preliminary ideas about which foundation models or techniques to explore?] - [This can be very preliminary - you‚Äôll refine throughout the course]\n\n\n\nWhat you hope to learn (2-3 sentences): - [What specific skills or knowledge do you want to gain from this course?] - [How does this fit into your broader academic or career goals?]"
  },
  {
    "objectID": "course-materials/projects/project-application-template.html#project-application-due-week-0",
    "href": "course-materials/projects/project-application-template.html#project-application-due-week-0",
    "title": "Project Application Template",
    "section": "",
    "text": "Student Name: [Your Name]\nEmail: [Your Email]\nDate: [Submission Date]\n\n\nPlease describe your past experience with:\nRemote Sensing and Geospatial Data (2-3 sentences): - [Describe your experience with satellite imagery, GIS, remote sensing tools, etc.]\nMachine Learning and Deep Learning (2-3 sentences): - [Describe your experience with Python, PyTorch, model training, etc.]\nSpecific Tools and Platforms (1-2 sentences): - [Mention experience with Earth Engine, QGIS, ArcGIS, cloud platforms, etc.]\n\n\n\nApplication Domain (2-3 sentences): - [Describe the specific application area you‚Äôre interested in exploring] - [Examples: crop monitoring, deforestation detection, disaster response, urban planning, etc.]\nResearch Questions or Goals (2-3 sentences): - [What specific questions do you hope to answer or problems to solve?] - [What would successful outcomes look like for your project?]\n\n\n\nAvailable Data (1-2 sentences): - [Do you have access to specific datasets, ground truth data, or study areas?] - [If not, describe what types of data you would need to acquire]\nComputational Resources (1 sentence): - [Describe your current access to computational resources, cloud credits, etc.]\n\n\n\nInitial Ideas (2-3 sentences): - [Any preliminary ideas about which foundation models or techniques to explore?] - [This can be very preliminary - you‚Äôll refine throughout the course]\n\n\n\nWhat you hope to learn (2-3 sentences): - [What specific skills or knowledge do you want to gain from this course?] - [How does this fit into your broader academic or career goals?]"
  },
  {
    "objectID": "course-materials/projects/project-proposal-template.html",
    "href": "course-materials/projects/project-proposal-template.html",
    "title": "Project Proposal Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Descriptive Project Title]\nDate: [Submission Date]\n\n\nProblem Description (3-4 sentences): - [Clearly describe the problem you‚Äôre addressing] - [Why is this problem important? What is the broader significance?] - [What are the current limitations or gaps in existing approaches?]\nResearch Objectives (2-3 bullet points): - [Specific, measurable goals for your project] - [What specific outcomes will you deliver?]\n\n\n\nPrimary Dataset(s): - Source: [e.g., Landsat, Sentinel, MODIS, custom dataset] - Spatial Coverage: [Geographic extent] - Temporal Coverage: [Time period] - Resolution: [Spatial and temporal resolution] - Size: [Approximate data volume]\nGround Truth/Labels (if applicable): - [Description of labeled data for training/validation] - [Source and quality of labels]\nData Availability Assessment: - [x] Data is readily accessible - [ ] Data requires special access/permissions - [ ] Data needs to be collected/generated - [ ] Alternative data sources identified\n\n\n\nFoundation Model Selection: - Primary Model: [e.g., Prithvi, SatMAE, custom architecture] - Justification: [Why is this model appropriate for your problem?] - Baseline Comparisons: [What will you compare against?]\nMethodology Overview (4-5 sentences): - [High-level description of your approach] - [Key technical components and workflow] - [How will you adapt/fine-tune the foundation model?]\nKey Technical Components: - [ ] Data preprocessing pipeline - [ ] Model fine-tuning/adaptation - [ ] Evaluation framework - [ ] Visualization/interpretation tools - [ ] Scalable inference pipeline\n\n\n\nWeek 4-5: [Specific goals and deliverables] Week 6: [Specific goals and deliverables]\nWeek 7 (MVP): [What will your MVP demonstration include?] Week 8-9: [Final implementation goals] Week 10: [Final presentation preparation]\n\n\n\nPerformance Metrics: - [How will you measure success? Specific metrics] - [Quantitative measures: accuracy, F1, IoU, etc.] - [Qualitative measures: interpretability, usability, etc.]\nValidation Approach: - [Cross-validation strategy] - [Train/validation/test splits] - [Spatial/temporal validation considerations]\n\n\n\nTechnical Risks and Mitigation: - Risk 1: [Description] ‚Üí Mitigation: [How you‚Äôll address it] - Risk 2: [Description] ‚Üí Mitigation: [How you‚Äôll address it]\nResource Requirements: - Computational: [GPU hours, memory requirements, storage] - Data: [Download time, storage space, preprocessing time] - Time: [Realistic assessment of scope given 10-week timeframe]\nBackup Plans: - [What will you do if primary approach doesn‚Äôt work?] - [How can you ensure you have a working system for final presentation?]\n\n\n\nPrimary Deliverables: - [What specific outputs will your project produce?] - [Code repository, trained models, analysis results, etc.]\nPotential Impact: - [How could this work be used in practice?] - [What are the broader implications of success?]\nFuture Work: - [How could this project be extended beyond the course?] - [What are the next logical steps?]\n\n\n\nKey Literature (3-5 papers): - [List relevant papers that inform your approach]\nSoftware/Tools: - [List key libraries, platforms, tools you‚Äôll use]\nAdditional Resources: - [Any other resources, datasets, collaborations, etc.]"
  },
  {
    "objectID": "course-materials/projects/project-proposal-template.html#project-proposal-due-week-3",
    "href": "course-materials/projects/project-proposal-template.html#project-proposal-due-week-3",
    "title": "Project Proposal Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Descriptive Project Title]\nDate: [Submission Date]\n\n\nProblem Description (3-4 sentences): - [Clearly describe the problem you‚Äôre addressing] - [Why is this problem important? What is the broader significance?] - [What are the current limitations or gaps in existing approaches?]\nResearch Objectives (2-3 bullet points): - [Specific, measurable goals for your project] - [What specific outcomes will you deliver?]\n\n\n\nPrimary Dataset(s): - Source: [e.g., Landsat, Sentinel, MODIS, custom dataset] - Spatial Coverage: [Geographic extent] - Temporal Coverage: [Time period] - Resolution: [Spatial and temporal resolution] - Size: [Approximate data volume]\nGround Truth/Labels (if applicable): - [Description of labeled data for training/validation] - [Source and quality of labels]\nData Availability Assessment: - [x] Data is readily accessible - [ ] Data requires special access/permissions - [ ] Data needs to be collected/generated - [ ] Alternative data sources identified\n\n\n\nFoundation Model Selection: - Primary Model: [e.g., Prithvi, SatMAE, custom architecture] - Justification: [Why is this model appropriate for your problem?] - Baseline Comparisons: [What will you compare against?]\nMethodology Overview (4-5 sentences): - [High-level description of your approach] - [Key technical components and workflow] - [How will you adapt/fine-tune the foundation model?]\nKey Technical Components: - [ ] Data preprocessing pipeline - [ ] Model fine-tuning/adaptation - [ ] Evaluation framework - [ ] Visualization/interpretation tools - [ ] Scalable inference pipeline\n\n\n\nWeek 4-5: [Specific goals and deliverables] Week 6: [Specific goals and deliverables]\nWeek 7 (MVP): [What will your MVP demonstration include?] Week 8-9: [Final implementation goals] Week 10: [Final presentation preparation]\n\n\n\nPerformance Metrics: - [How will you measure success? Specific metrics] - [Quantitative measures: accuracy, F1, IoU, etc.] - [Qualitative measures: interpretability, usability, etc.]\nValidation Approach: - [Cross-validation strategy] - [Train/validation/test splits] - [Spatial/temporal validation considerations]\n\n\n\nTechnical Risks and Mitigation: - Risk 1: [Description] ‚Üí Mitigation: [How you‚Äôll address it] - Risk 2: [Description] ‚Üí Mitigation: [How you‚Äôll address it]\nResource Requirements: - Computational: [GPU hours, memory requirements, storage] - Data: [Download time, storage space, preprocessing time] - Time: [Realistic assessment of scope given 10-week timeframe]\nBackup Plans: - [What will you do if primary approach doesn‚Äôt work?] - [How can you ensure you have a working system for final presentation?]\n\n\n\nPrimary Deliverables: - [What specific outputs will your project produce?] - [Code repository, trained models, analysis results, etc.]\nPotential Impact: - [How could this work be used in practice?] - [What are the broader implications of success?]\nFuture Work: - [How could this project be extended beyond the course?] - [What are the next logical steps?]\n\n\n\nKey Literature (3-5 papers): - [List relevant papers that inform your approach]\nSoftware/Tools: - [List key libraries, platforms, tools you‚Äôll use]\nAdditional Resources: - [Any other resources, datasets, collaborations, etc.]"
  },
  {
    "objectID": "course-materials/projects/mvp-template.html",
    "href": "course-materials/projects/mvp-template.html",
    "title": "Initial MVP Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Your Project Title]\nPresentation Date: [Date]\n\n\nProblem Recap (30 seconds): - [Brief reminder of the problem you‚Äôre solving]\nCurrent Status (1 minute): - [What have you accomplished so far?] - [What is working in your current implementation?]\n\n\n\nSystem Architecture:\n[Include a simple diagram or flowchart of your system]\nData Input ‚Üí Preprocessing ‚Üí Model ‚Üí Post-processing ‚Üí Output\nKey Components Implemented: - [x] Data loading and preprocessing pipeline - [x] Model setup and configuration\n- [x] Basic training/fine-tuning workflow - [ ] Advanced evaluation metrics - [ ] Visualization dashboard - [ ] Scalable deployment\nTechnology Stack: - Data Processing: [e.g., Earth Engine, rasterio, xarray] - Model Framework: [e.g., PyTorch, TorchGeo, HuggingFace] - Visualization: [e.g., Matplotlib, Folium, Plotly] - Infrastructure: [e.g., UCSB AI Sandbox, local development]\n\n\n\nDemo Script (prepare talking points for each step):\n\nData Loading Demo (1 minute):\n\n[Show loading and preprocessing of your specific dataset]\n[Highlight any data challenges you‚Äôve solved]\n\nModel Inference Demo (2 minutes):\n\n[Run your model on sample data]\n[Show inputs and outputs clearly]\n\nResults Visualization (1-2 minutes):\n\n[Display results in an intuitive format]\n[Compare with baselines or ground truth if available]\n\n\n\n\n\nQuantitative Results: - Dataset Size: [Number of samples, spatial/temporal coverage] - Model Performance: [Key metrics with numbers] - Processing Time: [Inference speed, training time]\nExample Results Table: | Metric | Baseline | Your Model | Notes | |‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî-| | Accuracy | XX% | YY% | [Context] | | F1-Score | XX | YY | [Context] | | Processing Speed | XX sec | YY sec | [Context] |\nQualitative Observations: - [What patterns or insights have you discovered?] - [What works well? What doesn‚Äôt work as expected?]\n\n\n\nTechnical Challenges Overcome: 1. Challenge: [Specific technical problem] Solution: [How you solved it]\n\nChallenge: [Another problem] Solution: [Your approach]\n\nCurrent Limitations: - [What are the current limitations of your system?] - [What assumptions are you making?]\n\n\n\nImmediate Goals (Week 8): - [ ] [Specific technical improvement] - [ ] [Additional evaluation or validation] - [ ] [User interface or API development]\nFinal Implementation Goals (Week 9-10): - [ ] [Advanced features or optimizations] - [ ] [Comprehensive evaluation and comparison] - [ ] [Documentation and reproducibility]\nRisks and Contingencies: - [What could go wrong? How will you adapt?]\n\n\n\nSpecific Areas Where You‚Äôd Like Input: 1. [Technical question about approach or implementation] 2. [Question about evaluation methodology]\n3. [Question about scope or next steps]\nOpen Discussion: - [Any other areas where peer feedback would be valuable]"
  },
  {
    "objectID": "course-materials/projects/mvp-template.html#initial-mvp-presentation-due-week-7",
    "href": "course-materials/projects/mvp-template.html#initial-mvp-presentation-due-week-7",
    "title": "Initial MVP Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Your Project Title]\nPresentation Date: [Date]\n\n\nProblem Recap (30 seconds): - [Brief reminder of the problem you‚Äôre solving]\nCurrent Status (1 minute): - [What have you accomplished so far?] - [What is working in your current implementation?]\n\n\n\nSystem Architecture:\n[Include a simple diagram or flowchart of your system]\nData Input ‚Üí Preprocessing ‚Üí Model ‚Üí Post-processing ‚Üí Output\nKey Components Implemented: - [x] Data loading and preprocessing pipeline - [x] Model setup and configuration\n- [x] Basic training/fine-tuning workflow - [ ] Advanced evaluation metrics - [ ] Visualization dashboard - [ ] Scalable deployment\nTechnology Stack: - Data Processing: [e.g., Earth Engine, rasterio, xarray] - Model Framework: [e.g., PyTorch, TorchGeo, HuggingFace] - Visualization: [e.g., Matplotlib, Folium, Plotly] - Infrastructure: [e.g., UCSB AI Sandbox, local development]\n\n\n\nDemo Script (prepare talking points for each step):\n\nData Loading Demo (1 minute):\n\n[Show loading and preprocessing of your specific dataset]\n[Highlight any data challenges you‚Äôve solved]\n\nModel Inference Demo (2 minutes):\n\n[Run your model on sample data]\n[Show inputs and outputs clearly]\n\nResults Visualization (1-2 minutes):\n\n[Display results in an intuitive format]\n[Compare with baselines or ground truth if available]\n\n\n\n\n\nQuantitative Results: - Dataset Size: [Number of samples, spatial/temporal coverage] - Model Performance: [Key metrics with numbers] - Processing Time: [Inference speed, training time]\nExample Results Table: | Metric | Baseline | Your Model | Notes | |‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî-| | Accuracy | XX% | YY% | [Context] | | F1-Score | XX | YY | [Context] | | Processing Speed | XX sec | YY sec | [Context] |\nQualitative Observations: - [What patterns or insights have you discovered?] - [What works well? What doesn‚Äôt work as expected?]\n\n\n\nTechnical Challenges Overcome: 1. Challenge: [Specific technical problem] Solution: [How you solved it]\n\nChallenge: [Another problem] Solution: [Your approach]\n\nCurrent Limitations: - [What are the current limitations of your system?] - [What assumptions are you making?]\n\n\n\nImmediate Goals (Week 8): - [ ] [Specific technical improvement] - [ ] [Additional evaluation or validation] - [ ] [User interface or API development]\nFinal Implementation Goals (Week 9-10): - [ ] [Advanced features or optimizations] - [ ] [Comprehensive evaluation and comparison] - [ ] [Documentation and reproducibility]\nRisks and Contingencies: - [What could go wrong? How will you adapt?]\n\n\n\nSpecific Areas Where You‚Äôd Like Input: 1. [Technical question about approach or implementation] 2. [Question about evaluation methodology]\n3. [Question about scope or next steps]\nOpen Discussion: - [Any other areas where peer feedback would be valuable]"
  },
  {
    "objectID": "course-materials/projects/mvp-template.html#mvp-demonstration-checklist",
    "href": "course-materials/projects/mvp-template.html#mvp-demonstration-checklist",
    "title": "Initial MVP Template",
    "section": "MVP Demonstration Checklist",
    "text": "MVP Demonstration Checklist\n\nBefore Your Presentation:\n\nTest all code and demos on presentation computer\nPrepare backup slides in case live demo fails\nTime your presentation (aim for 10-12 minutes + 3-5 for Q&A)\nHave clear talking points for each section\nSave sample outputs/screenshots as backup\n\n\n\nDuring Presentation:\n\nSpeak clearly and at appropriate pace\nShow your screen clearly for demos\nEngage with questions and feedback\nStay within time limits\nBe honest about current limitations\n\n\n\nAfter Presentation:\n\nTake notes on feedback received\nUpdate project plan based on suggestions\nSchedule follow-up consultations if needed"
  },
  {
    "objectID": "course-materials/resources/course_resources.html",
    "href": "course-materials/resources/course_resources.html",
    "title": "GeoAI Course Resources",
    "section": "",
    "text": "Handbook of Geospatial Artificial Intelligence (Gao, Hu, Li, 2023) ‚Äì Comprehensive reference with 30+ topics, code examples, datasets.\nDeep Learning for the Earth Sciences (Camps-Valls, Tuia, Zhu, 2021) ‚Äì Earth data meets deep learning; ideal for understanding EO models.\nGeoAI and Human Geography (Xiao Huang, 2025) ‚Äì Connects GeoAI with environmental and human systems.\n\n\n\n\n\nBoutayeb et al., 2024 ‚Äì A comprehensive survey of GeoAI methods and applications."
  },
  {
    "objectID": "course-materials/resources/course_resources.html#essential-background-and-foundational-resources",
    "href": "course-materials/resources/course_resources.html#essential-background-and-foundational-resources",
    "title": "GeoAI Course Resources",
    "section": "",
    "text": "Handbook of Geospatial Artificial Intelligence (Gao, Hu, Li, 2023) ‚Äì Comprehensive reference with 30+ topics, code examples, datasets.\nDeep Learning for the Earth Sciences (Camps-Valls, Tuia, Zhu, 2021) ‚Äì Earth data meets deep learning; ideal for understanding EO models.\nGeoAI and Human Geography (Xiao Huang, 2025) ‚Äì Connects GeoAI with environmental and human systems.\n\n\n\n\n\nBoutayeb et al., 2024 ‚Äì A comprehensive survey of GeoAI methods and applications."
  },
  {
    "objectID": "course-materials/resources/course_resources.html#week-by-week-resources",
    "href": "course-materials/resources/course_resources.html#week-by-week-resources",
    "title": "GeoAI Course Resources",
    "section": "Week-by-Week Resources",
    "text": "Week-by-Week Resources\n\nWeek 1: Introduction to Geospatial Foundation Models\n\nüåê Development Seed Blog (2024): Using Foundation Models for Earth Observation\nüöÄ NASA/IBM Release: Prithvi HLS Foundation Model\n‚òÅÔ∏è AWS ML Blog (2025): Deploying GeoFMs on AWS\n\n\n\nWeek 2: Working with Geospatial Data\n\nüìö Book: Learning Geospatial Analysis with Python (4th ed., 2023)\nüß∞ TorchGeo Docs: https://pytorch.org/geo\nüåç OpenGeoAI Toolkit: https://github.com/opengeos/geoai\n\n\n\nWeek 3: Loading & Using Foundation Models\n\nü§ó Hugging Face Prithvi Models: https://huggingface.co/ibm-nasa-geospatial\nüìì Demo Notebooks: Available on Hugging Face model cards.\nüß™ AWS GeoFM Deployment Examples: Code and pipelines via GitHub.\nüß† IEEE GRSS Blog: Self-Supervised Geospatial Backbones\n\n\n\nWeek 4: Multi-modal & Generative Models\n\nüõ∞Ô∏è Sensor Fusion via TorchGeo: Stack Sentinel-1/2 & others.\nüåê Presto, Clay, DOFA Models: Explained in DevSeed & AWS blogs.\nüß™ DiffusionSat Example: Medium Tutorial on Generating EO Images\n\n\n\nWeek 5: Model Adaptation & Evaluation\n\nüîå Adapters for GeoFM: Explained via Development Seed blog.\nüìä Evaluation Metrics: IoU, mAP, F1 from Boutayeb et al.¬†(2024) + SpaceNet.\nüìÅ Radiant Earth MLHub: https://mlhub.earth ‚Äì Public geospatial ML datasets and benchmarks."
  },
  {
    "objectID": "course-materials/resources/course_resources.html#interactive-sessions",
    "href": "course-materials/resources/course_resources.html#interactive-sessions",
    "title": "GeoAI Course Resources",
    "section": "Interactive Sessions",
    "text": "Interactive Sessions\n\n1Ô∏è‚É£ PyTorch & TorchGeo Fundamentals\n\nTorchGeo docs & sample loaders (CRS, tile sampling, multispectral).\n\n\n\n2Ô∏è‚É£ Loading & Using Foundation Models\n\nHugging Face GeoFM models (Prithvi, Clay) and notebook demos.\n\n\n\n3Ô∏è‚É£ Multi-modal & Sensor Fusion\n\nDOFA & Clay examples; stacking Sentinel data with TorchGeo.\n\n\n\n4Ô∏è‚É£ Model Adaptation & Evaluation\n\nFine-tuning using TerraTorch and evaluation on public datasets.\n\n\n\n5Ô∏è‚É£ Scalable Analysis & Deployment\n\nüìö Geospatial Data Analytics on AWS (2023)\n‚òÅÔ∏è AWS SageMaker + GeoFM repo (scalable inference)"
  },
  {
    "objectID": "course-materials/resources/course_resources.html#general-tools-repos",
    "href": "course-materials/resources/course_resources.html#general-tools-repos",
    "title": "GeoAI Course Resources",
    "section": "üì¶ General Tools & Repos",
    "text": "üì¶ General Tools & Repos\n\nüîß OpenGeoAI: https://github.com/opengeos/geoai\nüõ∞Ô∏è IBM/NASA TerraTorch: https://github.com/ibm/terrapytorch\nüìç Radiant MLHub Datasets: https://mlhub.earth\nüß™ SpaceNet Challenges: https://spacenet.ai/challenges/"
  },
  {
    "objectID": "course-materials/resources/course_resources.html#deployment-and-project-resources",
    "href": "course-materials/resources/course_resources.html#deployment-and-project-resources",
    "title": "GeoAI Course Resources",
    "section": "üß≠ Deployment and Project Resources",
    "text": "üß≠ Deployment and Project Resources\n\nüîß **Flask/Streamlit for D"
  },
  {
    "objectID": "course-materials/examples/text_encoder.html",
    "href": "course-materials/examples/text_encoder.html",
    "title": "Deep Learning Basics",
    "section": "",
    "text": "In this interactive session, we will explore how to build deep learning encoders and decoders from scratch. Our first example will use text encoders and a small corpus of text.\nThe test we will use is ‚ÄúThe Verdict‚Äù, which is used in Sebastian Raschka‚Äôs excellent book, ‚ÄúBuild a Large Language Model from Scratch.‚Äù\n\n\nCode\nimport urllib.request\n\n\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n       \"the-verdict.txt\")\n\nwith urllib.request.urlopen(url) as response:\n    raw_text = response.read().decode(\"utf-8\")\n\nprint(\"Total number of character:\", len(raw_text))\nprint(raw_text[:99])\n\n\nTotal number of character: 20479\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n\n\nThe first step is to tokenize this text. We can use regular expressions to split the text into words.\n\n\nCode\nimport re\ntext = \"Hello, world. Welcome to text encoding.\"\nresult = re.split(r'(\\s)', text)\n\nprint(result)\n\n\n['Hello,', ' ', 'world.', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding.']\n\n\nWe want to separate punctuation and preserve capitalization.\n\n\nCode\nresult = re.split(r'([,.]|\\s)', text)\nprint(result)\n\n\n['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding', '.', '']\n\n\nThe decision to remove or include whitespace characters is application-dependent. For example, in coding (especially Pyhon coding), whitespace is essential to the structure and function of text, so it should be included in training.\n\n\n\n\n\n\nTip\n\n\n\nMost tokenization schemes include whitespace, but for brevity, these next examples exclude it.\n\n\n\n\nCode\ntext = \"Hello, world. Is this-- a good decoder?\"\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\n\n\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'good', 'decoder', '?']\n\n\nWe can run this against our entire corpus of text.\n\n\nCode\npreprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(preprocessed[:20])\n\n\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n\n\n\n\nModels assign each token a unique ID using a dictionary. These dictionaries are built from the text corpus.\nWe create a set of unique tokens that define the vocabulary of the corpus, which determines the number of items in our dictionary.\n\n\nCode\nall_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\nprint(vocab_size)\n\n\n1130\n\n\nCreating a vocabulary requires assigning each unique ID to each token. We can do this using a dictionary comprehension.\n\n\nCode\nvocab = {token: idx for idx, token in enumerate(\n    all_words, start=0)}\n\n# Print a list of the first 10 tokens and their ids:\nprint(list(vocab.items())[:10])\n\n\n[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)]\n\n\nTo facilitate the conversion of tokens to ids and vice versa, we can create a tokenizer class with an encode and decode method. The encode method splits text into tokens and then maps these to ids using the vocabulary. The decode method does the reverse, converting ids back to tokens.\n\n\nCode\nclass Tokenizer:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.token_to_id = vocab\n        self.id_to_token = {idx: token for token, idx in vocab.items()}\n\n    def encode(self, text):\n        \"\"\"Encode a string into a list of token IDs.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = (\n            t.strip() for t in re.split(pattern, text) if t.strip()\n        )\n        return [self.token_to_id[t] for t in tokens]\n\n    def decode(self, ids):\n        \"\"\"Decode a list of token IDs into a string.\"\"\"\n        text = \" \".join(self.id_to_token[i] for i in ids)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n\n\nLet‚Äôs test our new Tokenizer class\n\n\nCode\ntext = \"\"\"\"It's the last he painted, you know,\" \n       Mrs. Gisburn said with pardonable pride.\"\"\"\ntokenizer = Tokenizer(vocab)\nids = tokenizer.encode(text)\n\nprint(f\"Encoded:\\n{ids}\")\nprint(f\"Decoded:\\n{tokenizer.decode(ids)}\")\n\n\nEncoded:\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\nDecoded:\n\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n\n\nThe vocabulary of our tokenizer is limited by the tokens that appear in the corpus. Any word that is not in the corpus cannot be encoded.\ntext = \"Hello, do you like tea?\"\nprint(tokenizer.encode(text))\nCauses a KeyError:\nKeyError: 'Hello'"
  },
  {
    "objectID": "course-materials/examples/text_encoder.html#introduction",
    "href": "course-materials/examples/text_encoder.html#introduction",
    "title": "Deep Learning Basics",
    "section": "",
    "text": "In this interactive session, we will explore how to build deep learning encoders and decoders from scratch. Our first example will use text encoders and a small corpus of text.\nThe test we will use is ‚ÄúThe Verdict‚Äù, which is used in Sebastian Raschka‚Äôs excellent book, ‚ÄúBuild a Large Language Model from Scratch.‚Äù\n\n\nCode\nimport urllib.request\n\n\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n       \"the-verdict.txt\")\n\nwith urllib.request.urlopen(url) as response:\n    raw_text = response.read().decode(\"utf-8\")\n\nprint(\"Total number of character:\", len(raw_text))\nprint(raw_text[:99])\n\n\nTotal number of character: 20479\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n\n\nThe first step is to tokenize this text. We can use regular expressions to split the text into words.\n\n\nCode\nimport re\ntext = \"Hello, world. Welcome to text encoding.\"\nresult = re.split(r'(\\s)', text)\n\nprint(result)\n\n\n['Hello,', ' ', 'world.', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding.']\n\n\nWe want to separate punctuation and preserve capitalization.\n\n\nCode\nresult = re.split(r'([,.]|\\s)', text)\nprint(result)\n\n\n['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding', '.', '']\n\n\nThe decision to remove or include whitespace characters is application-dependent. For example, in coding (especially Pyhon coding), whitespace is essential to the structure and function of text, so it should be included in training.\n\n\n\n\n\n\nTip\n\n\n\nMost tokenization schemes include whitespace, but for brevity, these next examples exclude it.\n\n\n\n\nCode\ntext = \"Hello, world. Is this-- a good decoder?\"\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\n\n\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'good', 'decoder', '?']\n\n\nWe can run this against our entire corpus of text.\n\n\nCode\npreprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(preprocessed[:20])\n\n\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n\n\n\n\nModels assign each token a unique ID using a dictionary. These dictionaries are built from the text corpus.\nWe create a set of unique tokens that define the vocabulary of the corpus, which determines the number of items in our dictionary.\n\n\nCode\nall_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\nprint(vocab_size)\n\n\n1130\n\n\nCreating a vocabulary requires assigning each unique ID to each token. We can do this using a dictionary comprehension.\n\n\nCode\nvocab = {token: idx for idx, token in enumerate(\n    all_words, start=0)}\n\n# Print a list of the first 10 tokens and their ids:\nprint(list(vocab.items())[:10])\n\n\n[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)]\n\n\nTo facilitate the conversion of tokens to ids and vice versa, we can create a tokenizer class with an encode and decode method. The encode method splits text into tokens and then maps these to ids using the vocabulary. The decode method does the reverse, converting ids back to tokens.\n\n\nCode\nclass Tokenizer:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.token_to_id = vocab\n        self.id_to_token = {idx: token for token, idx in vocab.items()}\n\n    def encode(self, text):\n        \"\"\"Encode a string into a list of token IDs.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = (\n            t.strip() for t in re.split(pattern, text) if t.strip()\n        )\n        return [self.token_to_id[t] for t in tokens]\n\n    def decode(self, ids):\n        \"\"\"Decode a list of token IDs into a string.\"\"\"\n        text = \" \".join(self.id_to_token[i] for i in ids)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n\n\nLet‚Äôs test our new Tokenizer class\n\n\nCode\ntext = \"\"\"\"It's the last he painted, you know,\" \n       Mrs. Gisburn said with pardonable pride.\"\"\"\ntokenizer = Tokenizer(vocab)\nids = tokenizer.encode(text)\n\nprint(f\"Encoded:\\n{ids}\")\nprint(f\"Decoded:\\n{tokenizer.decode(ids)}\")\n\n\nEncoded:\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\nDecoded:\n\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n\n\nThe vocabulary of our tokenizer is limited by the tokens that appear in the corpus. Any word that is not in the corpus cannot be encoded.\ntext = \"Hello, do you like tea?\"\nprint(tokenizer.encode(text))\nCauses a KeyError:\nKeyError: 'Hello'"
  },
  {
    "objectID": "course-materials/examples/text_encoder.html#handling-unknown-tokens-and-special-tokens",
    "href": "course-materials/examples/text_encoder.html#handling-unknown-tokens-and-special-tokens",
    "title": "Deep Learning Basics",
    "section": "Handling Unknown Tokens and Special Tokens",
    "text": "Handling Unknown Tokens and Special Tokens\nIn practical LLM applications, we need to handle words that don‚Äôt appear in our training vocabulary. This is where unknown tokens and other special tokens become essential.\n\nSpecial Tokens in LLMs\nModern tokenizers use several special tokens:\n\n&lt;|unk|&gt; or [UNK]: Unknown token for out-of-vocabulary words\n&lt;|endoftext|&gt; or [EOS]: End of sequence/text marker\n&lt;|startoftext|&gt; or [BOS]: Beginning of sequence marker (less common in GPT-style models)\n&lt;|pad|&gt;: Padding token for batch processing\n\nLet‚Äôs create an enhanced tokenizer that handles unknown tokens:\n\n\nCode\nclass EnhancedTokenizer:\n    def __init__(self, vocab):\n        # Add special tokens to vocabulary\n        self.special_tokens = {\n            \"&lt;|unk|&gt;\": len(vocab),\n            \"&lt;|endoftext|&gt;\": len(vocab) + 1\n        }\n        \n        # Combine original vocab with special tokens\n        self.full_vocab = {**vocab, **self.special_tokens}\n        self.token_to_id = self.full_vocab\n        self.id_to_token = {idx: token for token, idx in self.full_vocab.items()}\n        \n        # Store reverse mapping for special tokens\n        self.unk_token_id = self.special_tokens[\"&lt;|unk|&gt;\"]\n        self.endoftext_token_id = self.special_tokens[\"&lt;|endoftext|&gt;\"]\n\n    def encode(self, text, add_endoftext=True):\n        \"\"\"Encode text, handling unknown tokens.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = [t.strip() for t in re.split(pattern, text) if t.strip()]\n        \n        # Convert tokens to IDs, using &lt;|unk|&gt; for unknown tokens\n        ids = []\n        for token in tokens:\n            if token in self.token_to_id:\n                ids.append(self.token_to_id[token])\n            else:\n                ids.append(self.unk_token_id)  # Use unknown token\n        \n        # Optionally add end-of-text token\n        if add_endoftext:\n            ids.append(self.endoftext_token_id)\n            \n        return ids\n\n    def decode(self, ids):\n        \"\"\"Decode token IDs back to text.\"\"\"\n        tokens = []\n        for id in ids:\n            if id == self.endoftext_token_id:\n                break  # Stop at end-of-text token\n            elif id in self.id_to_token:\n                tokens.append(self.id_to_token[id])\n            else:\n                tokens.append(\"&lt;|unk|&gt;\")  # Fallback for invalid IDs\n                \n        text = \" \".join(tokens)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n    \n    def vocab_size(self):\n        \"\"\"Return the total vocabulary size including special tokens.\"\"\"\n        return len(self.full_vocab)\n\n\nLet‚Äôs test our enhanced tokenizer:\n\n\nCode\nenhanced_tokenizer = EnhancedTokenizer(vocab)\n\n# Test with unknown words\ntext_with_unknown = \"Hello, do you like tea? This is amazing!\"\nencoded = enhanced_tokenizer.encode(text_with_unknown)\n\nprint(f\"Original text: {text_with_unknown}\")\nprint(f\"Encoded IDs: {encoded}\")\nprint(f\"Decoded text: {enhanced_tokenizer.decode(encoded)}\")\nprint(f\"Vocabulary size: {enhanced_tokenizer.vocab_size()}\")\n\n\nOriginal text: Hello, do you like tea? This is amazing!\nEncoded IDs: [1130, 5, 355, 1126, 628, 975, 10, 97, 584, 1130, 0, 1131]\nDecoded text: &lt;|unk|&gt;, do you like tea? This is &lt;|unk|&gt;!\nVocabulary size: 1132\n\n\nNotice how unknown words like ‚ÄúHello‚Äù and ‚Äúamazing‚Äù are now handled gracefully using the &lt;|unk|&gt; token, and the sequence ends with an &lt;|endoftext|&gt; token.\n\n\nCode\n# Let's see what the special token IDs are\nprint(f\"Unknown token '&lt;|unk|&gt;' has ID: {enhanced_tokenizer.unk_token_id}\")\nprint(f\"End-of-text token '&lt;|endoftext|&gt;' has ID: {enhanced_tokenizer.endoftext_token_id}\")\n\n# Test decoding with end-of-text in the middle\ntest_ids = [100, 200, enhanced_tokenizer.endoftext_token_id, 300, 400]\nprint(f\"Decoding stops at &lt;|endoftext|&gt;: {enhanced_tokenizer.decode(test_ids)}\")\n\n\nUnknown token '&lt;|unk|&gt;' has ID: 1130\nEnd-of-text token '&lt;|endoftext|&gt;' has ID: 1131\nDecoding stops at &lt;|endoftext|&gt;: Thwing bean-stalk"
  },
  {
    "objectID": "course-materials/examples/text_encoder.html#byte-pair-encoding-bpe-with-tiktoken",
    "href": "course-materials/examples/text_encoder.html#byte-pair-encoding-bpe-with-tiktoken",
    "title": "Deep Learning Basics",
    "section": "Byte Pair Encoding (BPE) with Tiktoken",
    "text": "Byte Pair Encoding (BPE) with Tiktoken\nWhile our simple tokenizer is educational, production LLMs use more sophisticated tokenization schemes like Byte Pair Encoding (BPE). BPE creates subword tokens that balance vocabulary size with representation efficiency.\nThe tiktoken library provides access to the same tokenizers used by OpenAI‚Äôs GPT models.\n\n\nCode\nimport tiktoken\n\n# Load the GPT-2 tokenizer (used by GPT-2, GPT-3, and early GPT-4)\ngpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n\nprint(f\"GPT-2 vocabulary size: {gpt2_tokenizer.n_vocab:,}\")\n\n\nGPT-2 vocabulary size: 50,257\n\n\nLet‚Äôs compare our simple tokenizer with GPT-2‚Äôs BPE tokenizer:\n\n\nCode\ntest_text = \"Hello, world! This demonstrates byte pair encoding tokenization.\"\n\n# Our enhanced tokenizer\nour_tokens = enhanced_tokenizer.encode(test_text, add_endoftext=False)\nour_decoded = enhanced_tokenizer.decode(our_tokens)\n\n# GPT-2 BPE tokenizer  \ngpt2_tokens = gpt2_tokenizer.encode(test_text)\ngpt2_decoded = gpt2_tokenizer.decode(gpt2_tokens)\n\nprint(\"=== Comparison ===\")\nprint(f\"Original text: {test_text}\")\nprint()\nprint(f\"Our tokenizer:\")\nprint(f\"  Tokens: {our_tokens}\")\nprint(f\"  Count: {len(our_tokens)} tokens\")\nprint(f\"  Decoded: {our_decoded}\")\nprint()\nprint(f\"GPT-2 BPE tokenizer:\")\nprint(f\"  Tokens: {gpt2_tokens}\")\nprint(f\"  Count: {len(gpt2_tokens)} tokens\") \nprint(f\"  Decoded: {gpt2_decoded}\")\n\n\n=== Comparison ===\nOriginal text: Hello, world! This demonstrates byte pair encoding tokenization.\n\nOur tokenizer:\n  Tokens: [1130, 5, 1130, 0, 97, 1130, 1130, 1130, 1130, 1130, 7]\n  Count: 11 tokens\n  Decoded: &lt;|unk|&gt;, &lt;|unk|&gt;! This &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt;.\n\nGPT-2 BPE tokenizer:\n  Tokens: [15496, 11, 995, 0, 770, 15687, 18022, 5166, 21004, 11241, 1634, 13]\n  Count: 12 tokens\n  Decoded: Hello, world! This demonstrates byte pair encoding tokenization.\n\n\n\nUnderstanding BPE Token Breakdown\nBPE tokenizers split text into subword units. Let‚Äôs examine how GPT-2 breaks down different types of text:\n\n\nCode\ndef analyze_tokenization(text, tokenizer_name=\"gpt2\"):\n    \"\"\"Analyze how tiktoken breaks down text.\"\"\"\n    tokenizer = tiktoken.get_encoding(tokenizer_name)\n    tokens = tokenizer.encode(text)\n    \n    print(f\"Text: '{text}'\")\n    print(f\"Tokens: {tokens}\")\n    print(f\"Token count: {len(tokens)}\")\n    print(\"Token breakdown:\")\n    \n    for i, token_id in enumerate(tokens):\n        token_text = tokenizer.decode([token_id])\n        print(f\"  {i:2d}: {token_id:5d} -&gt; '{token_text}'\")\n    print()\n\n# Test different types of text\nanalyze_tokenization(\"Hello world\")\nanalyze_tokenization(\"Tokenization\")  \nanalyze_tokenization(\"supercalifragilisticexpialidocious\")\nanalyze_tokenization(\"AI/ML researcher\")\nanalyze_tokenization(\"The year 2024\")\n\n\nText: 'Hello world'\nTokens: [15496, 995]\nToken count: 2\nToken breakdown:\n   0: 15496 -&gt; 'Hello'\n   1:   995 -&gt; ' world'\n\nText: 'Tokenization'\nTokens: [30642, 1634]\nToken count: 2\nToken breakdown:\n   0: 30642 -&gt; 'Token'\n   1:  1634 -&gt; 'ization'\n\nText: 'supercalifragilisticexpialidocious'\nTokens: [16668, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\nToken count: 11\nToken breakdown:\n   0: 16668 -&gt; 'super'\n   1:  9948 -&gt; 'cal'\n   2:   361 -&gt; 'if'\n   3: 22562 -&gt; 'rag'\n   4:   346 -&gt; 'il'\n   5:   396 -&gt; 'ist'\n   6:   501 -&gt; 'ice'\n   7: 42372 -&gt; 'xp'\n   8:   498 -&gt; 'ial'\n   9:   312 -&gt; 'id'\n  10: 32346 -&gt; 'ocious'\n\nText: 'AI/ML researcher'\nTokens: [20185, 14, 5805, 13453]\nToken count: 4\nToken breakdown:\n   0: 20185 -&gt; 'AI'\n   1:    14 -&gt; '/'\n   2:  5805 -&gt; 'ML'\n   3: 13453 -&gt; ' researcher'\n\nText: 'The year 2024'\nTokens: [464, 614, 48609]\nToken count: 3\nToken breakdown:\n   0:   464 -&gt; 'The'\n   1:   614 -&gt; ' year'\n   2: 48609 -&gt; ' 2024'\n\n\n\n\n\nWorking with Different Tiktoken Encodings\nOpenAI uses different encodings for different models:\n\n\nCode\n# Available encodings\navailable_encodings = [\"gpt2\", \"p50k_base\", \"cl100k_base\"]\n\ntest_text = \"Geospatial foundation models revolutionize remote sensing!\"\n\nfor encoding_name in available_encodings:\n    try:\n        tokenizer = tiktoken.get_encoding(encoding_name)\n        tokens = tokenizer.encode(test_text)\n        \n        print(f\"{encoding_name:12s}: {len(tokens):2d} tokens, vocab size: {tokenizer.n_vocab:6,}\")\n        print(f\"              Tokens: {tokens}\")\n        print()\n    except Exception as e:\n        print(f\"{encoding_name}: Error - {e}\")\n\n\ngpt2        : 10 tokens, vocab size: 50,257\n              Tokens: [10082, 2117, 34961, 8489, 4981, 5854, 1096, 6569, 34244, 0]\n\np50k_base   : 10 tokens, vocab size: 50,281\n              Tokens: [10082, 2117, 34961, 8489, 4981, 5854, 1096, 6569, 34244, 0]\n\ncl100k_base : 10 tokens, vocab size: 100,277\n              Tokens: [9688, 437, 33514, 16665, 4211, 14110, 553, 8870, 60199, 0]\n\n\n\n\n\nBPE for Geospatial Text\nLet‚Äôs see how BPE handles domain-specific geospatial terminology:\n\n\nCode\ngeospatial_texts = [\n    \"NDVI vegetation index analysis\",\n    \"Landsat-8 multispectral imagery\", \n    \"Convolutional neural networks for land cover classification\",\n    \"Sentinel-2 satellite data preprocessing\",\n    \"Geospatial foundation models fine-tuning\"\n]\n\ngpt2_enc = tiktoken.get_encoding(\"gpt2\")\n\nprint(\"=== Geospatial Text Tokenization ===\")\nfor text in geospatial_texts:\n    tokens = gpt2_enc.encode(text)\n    print(f\"'{text}'\")\n    print(f\"  -&gt; {len(tokens)} tokens: {tokens}\")\n    print()\n\n\n=== Geospatial Text Tokenization ===\n'NDVI vegetation index analysis'\n  -&gt; 5 tokens: [8575, 12861, 28459, 6376, 3781]\n\n'Landsat-8 multispectral imagery'\n  -&gt; 10 tokens: [43, 1746, 265, 12, 23, 1963, 271, 806, 1373, 19506]\n\n'Convolutional neural networks for land cover classification'\n  -&gt; 10 tokens: [3103, 85, 2122, 282, 17019, 7686, 329, 1956, 3002, 17923]\n\n'Sentinel-2 satellite data preprocessing'\n  -&gt; 8 tokens: [31837, 20538, 12, 17, 11210, 1366, 662, 36948]\n\n'Geospatial foundation models fine-tuning'\n  -&gt; 9 tokens: [10082, 2117, 34961, 8489, 4981, 3734, 12, 28286, 278]\n\n\n\n\n\nUnderstanding Token Efficiency\nThe choice of tokenizer affects model efficiency. Let‚Äôs compare token counts for our text corpus:\n\n\nCode\n# Use a sample from our corpus\nsample_text = raw_text[:500]  # First 500 characters\n\nprint(\"=== Token Efficiency Comparison ===\")\nprint(f\"Text length: {len(sample_text)} characters\")\nprint()\n\n# Our simple tokenizer\nour_tokens = enhanced_tokenizer.encode(sample_text, add_endoftext=False)\nprint(f\"Simple tokenizer: {len(our_tokens)} tokens\")\n\n# GPT-2 BPE\ngpt2_tokens = gpt2_tokenizer.encode(sample_text)\nprint(f\"GPT-2 BPE:       {len(gpt2_tokens)} tokens\")\n\n# Calculate efficiency\nchars_per_token_simple = len(sample_text) / len(our_tokens)\nchars_per_token_bpe = len(sample_text) / len(gpt2_tokens)\n\nprint(f\"\\nCharacters per token:\")\nprint(f\"  Simple: {chars_per_token_simple:.2f}\")\nprint(f\"  BPE:    {chars_per_token_bpe:.2f}\")\nprint(f\"  BPE is {chars_per_token_bpe/chars_per_token_simple:.1f}x more efficient\")\n\n\n=== Token Efficiency Comparison ===\nText length: 500 characters\n\nSimple tokenizer: 110 tokens\nGPT-2 BPE:       123 tokens\n\nCharacters per token:\n  Simple: 4.55\n  BPE:    4.07\n  BPE is 0.9x more efficient"
  },
  {
    "objectID": "course-materials/examples/text_encoder.html#key-takeaways",
    "href": "course-materials/examples/text_encoder.html#key-takeaways",
    "title": "Deep Learning Basics",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSpecial Tokens: Essential for handling unknown words and sequence boundaries\nUnknown Token Handling: &lt;|unk|&gt; tokens allow models to gracefully handle out-of-vocabulary words\nEnd-of-Text Tokens: &lt;|endoftext|&gt; tokens help models understand sequence boundaries\nBPE Efficiency: Byte Pair Encoding creates a good balance between vocabulary size and representation efficiency\nDomain Adaptation: Different tokenizers may handle domain-specific text differently\n\nIn geospatial AI applications, choosing the right tokenization strategy affects both model performance and computational efficiency, especially when working with specialized terminology from remote sensing and Earth science domains."
  },
  {
    "objectID": "course-materials/examples/resnet.html",
    "href": "course-materials/examples/resnet.html",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "",
    "text": "This notebook demonstrates the core insight behind Residual Networks (ResNets) by comparing them to a traditional plain CNN of similar depth. You will:\n\nVisualize how residual connections transform intermediate representations\nCompare training loss and accuracy curves\nAnalyze gradient flow to see why ResNets enable deeper models to train effectively"
  },
  {
    "objectID": "course-materials/examples/resnet.html#overview",
    "href": "course-materials/examples/resnet.html#overview",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "",
    "text": "This notebook demonstrates the core insight behind Residual Networks (ResNets) by comparing them to a traditional plain CNN of similar depth. You will:\n\nVisualize how residual connections transform intermediate representations\nCompare training loss and accuracy curves\nAnalyze gradient flow to see why ResNets enable deeper models to train effectively"
  },
  {
    "objectID": "course-materials/examples/resnet.html#setup-and-imports",
    "href": "course-materials/examples/resnet.html#setup-and-imports",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üì¶ Setup and Imports",
    "text": "üì¶ Setup and Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm"
  },
  {
    "objectID": "course-materials/examples/resnet.html#define-plain-cnn-and-resnet-like-cnn",
    "href": "course-materials/examples/resnet.html#define-plain-cnn-and-resnet-like-cnn",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üìä Define Plain CNN and ResNet-like CNN",
    "text": "üìä Define Plain CNN and ResNet-like CNN\nclass PlainBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        return F.relu(out)\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(identity)\n        return F.relu(out)\n\nclass PlainCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = PlainBlock(3, 16)\n        self.block2 = PlainBlock(16, 32, stride=2)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.pool(x).view(x.size(0), -1)\n        return self.fc(x)\n\nclass ResNetMini(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = ResBlock(3, 16)\n        self.block2 = ResBlock(16, 32, stride=2)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.pool(x).view(x.size(0), -1)\n        return self.fc(x)"
  },
  {
    "objectID": "course-materials/examples/resnet.html#load-cifar-10-data",
    "href": "course-materials/examples/resnet.html#load-cifar-10-data",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üñºÔ∏è Load CIFAR-10 Data",
    "text": "üñºÔ∏è Load CIFAR-10 Data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=2)"
  },
  {
    "objectID": "course-materials/examples/resnet.html#training-loop-and-gradient-tracking",
    "href": "course-materials/examples/resnet.html#training-loop-and-gradient-tracking",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üöÄ Training Loop and Gradient Tracking",
    "text": "üöÄ Training Loop and Gradient Tracking\ndef train_model(model, name, epochs=10):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loss = []\n    train_gradients = []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0\n        grad_norm = 0\n        for inputs, targets in tqdm(train_loader, desc=f\"{name} Epoch {epoch+1}\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n\n            total_norm = 0\n            for p in model.parameters():\n                if p.grad is not None:\n                    param_norm = p.grad.data.norm(2)\n                    total_norm += param_norm.item() ** 2\n            grad_norm += total_norm ** 0.5\n\n            optimizer.step()\n            running_loss += loss.item()\n\n        train_loss.append(running_loss / len(train_loader))\n        train_gradients.append(grad_norm / len(train_loader))\n\n    return train_loss, train_gradients"
  },
  {
    "objectID": "course-materials/examples/resnet.html#train-and-compare",
    "href": "course-materials/examples/resnet.html#train-and-compare",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üìà Train and Compare",
    "text": "üìà Train and Compare\nplain_model = PlainCNN()\nresnet_model = ResNetMini()\n\nplain_loss, plain_grads = train_model(plain_model, \"PlainCNN\")\nresnet_loss, resnet_grads = train_model(resnet_model, \"ResNetMini\")"
  },
  {
    "objectID": "course-materials/examples/resnet.html#plot-training-loss-and-gradient-flow",
    "href": "course-materials/examples/resnet.html#plot-training-loss-and-gradient-flow",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "üìä Plot Training Loss and Gradient Flow",
    "text": "üìä Plot Training Loss and Gradient Flow\nplt.figure(figsize=(14,5))\n\nplt.subplot(1,2,1)\nplt.plot(plain_loss, label=\"PlainCNN\")\nplt.plot(resnet_loss, label=\"ResNetMini\")\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(plain_grads, label=\"PlainCNN\")\nplt.plot(resnet_grads, label=\"ResNetMini\")\nplt.title(\"Gradient Norm per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Average Gradient Norm\")\nplt.legend()\n\nplt.suptitle(\"Residual Connections Help Optimize Deeper Networks\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/examples/resnet.html#conclusion",
    "href": "course-materials/examples/resnet.html#conclusion",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "‚úÖ Conclusion",
    "text": "‚úÖ Conclusion\nThis notebook demonstrates that residual connections help preserve gradient flow, allowing deeper networks to train faster and more reliably. Even though our example was relatively shallow (2 blocks), the benefits in convergence and stability are clear."
  },
  {
    "objectID": "course-materials/cheatsheets/dataloader_satellite.html",
    "href": "course-materials/cheatsheets/dataloader_satellite.html",
    "title": "Data Loading for Satellite Imagery",
    "section": "",
    "text": "Cheatsheet content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html",
    "href": "course-materials/cheatsheets/rasterio_basics.html",
    "title": "Working with Rasterio",
    "section": "",
    "text": "Rasterio is the essential Python library for reading and writing geospatial raster data.\n\n\nCode\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\n\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#introduction-to-rasterio",
    "href": "course-materials/cheatsheets/rasterio_basics.html#introduction-to-rasterio",
    "title": "Working with Rasterio",
    "section": "",
    "text": "Rasterio is the essential Python library for reading and writing geospatial raster data.\n\n\nCode\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\n\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#creating-sample-data",
    "href": "course-materials/cheatsheets/rasterio_basics.html#creating-sample-data",
    "title": "Working with Rasterio",
    "section": "Creating Sample Data",
    "text": "Creating Sample Data\nSince we may not have actual satellite imagery files, let‚Äôs create some sample raster data:\n\n\nCode\n# Create sample raster data\ndef create_sample_raster():\n    # Create a simple 100x100 raster with some pattern\n    height, width = 100, 100\n    \n    # Create coordinate grids\n    x = np.linspace(-2, 2, width)\n    y = np.linspace(-2, 2, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Create a pattern (e.g., circular pattern)\n    data = np.sin(np.sqrt(X**2 + Y**2)) * 255\n    data = data.astype(np.uint8)\n    \n    return data\n\nsample_data = create_sample_raster()\nprint(f\"Sample data shape: {sample_data.shape}\")\nprint(f\"Data type: {sample_data.dtype}\")\nprint(f\"Data range: {sample_data.min()} to {sample_data.max()}\")\n\n\nSample data shape: (100, 100)\nData type: uint8\nData range: 7 to 254"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#working-with-raster-arrays",
    "href": "course-materials/cheatsheets/rasterio_basics.html#working-with-raster-arrays",
    "title": "Working with Rasterio",
    "section": "Working with Raster Arrays",
    "text": "Working with Raster Arrays\n\nBasic array operations\n\n\nCode\n# Basic statistics\nprint(f\"Mean: {np.mean(sample_data):.2f}\")\nprint(f\"Standard deviation: {np.std(sample_data):.2f}\")\nprint(f\"Unique values: {len(np.unique(sample_data))}\")\n\n# Histogram of values\nplt.figure(figsize=(8, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Sample Raster Data')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.hist(sample_data.flatten(), bins=50, alpha=0.7)\nplt.title('Histogram of Values')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\nMean: 215.01\nStandard deviation: 46.13\nUnique values: 182\n\n\n\n\n\n\n\n\n\n\n\nArray masking and filtering\n\n\nCode\n# Create masks\nhigh_values = sample_data &gt; 200\nlow_values = sample_data &lt; 50\n\nprint(f\"Pixels with high values (&gt;200): {np.sum(high_values)}\")\nprint(f\"Pixels with low values (&lt;50): {np.sum(low_values)}\")\n\n# Apply mask\nmasked_data = np.where(high_values, sample_data, 0)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Original Data')\n\nplt.subplot(1, 3, 2)\nplt.imshow(high_values, cmap='gray')\nplt.title('High Value Mask')\n\nplt.subplot(1, 3, 3)\nplt.imshow(masked_data, cmap='viridis')\nplt.title('Masked Data')\n\nplt.tight_layout()\nplt.show()\n\n\nPixels with high values (&gt;200): 7364\nPixels with low values (&lt;50): 76"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#raster-metadata-and-transforms",
    "href": "course-materials/cheatsheets/rasterio_basics.html#raster-metadata-and-transforms",
    "title": "Working with Rasterio",
    "section": "Raster Metadata and Transforms",
    "text": "Raster Metadata and Transforms\n\nUnderstanding geospatial transforms\n\n\nCode\nfrom rasterio.transform import from_bounds\n\n# Create a sample transform (geographic coordinates)\nwest, south, east, north = -120.0, 35.0, -115.0, 40.0  # Bounding box\ntransform = from_bounds(west, south, east, north, 100, 100)\n\nprint(f\"Transform: {transform}\")\nprint(f\"Pixel width: {transform[0]}\")\nprint(f\"Pixel height: {abs(transform[4])}\")\n\n# Convert pixel coordinates to geographic coordinates\ndef pixel_to_geo(row, col, transform):\n    x, y = rasterio.transform.xy(transform, row, col)\n    return x, y\n\n# Example: center pixel\ncenter_row, center_col = 50, 50\ngeo_x, geo_y = pixel_to_geo(center_row, center_col, transform)\nprint(f\"Center pixel ({center_row}, {center_col}) -&gt; Geographic: ({geo_x:.2f}, {geo_y:.2f})\")\n\n\nTransform: | 0.05, 0.00,-120.00|\n| 0.00,-0.05, 40.00|\n| 0.00, 0.00, 1.00|\nPixel width: 0.05\nPixel height: 0.05\nCenter pixel (50, 50) -&gt; Geographic: (-117.47, 37.48)\n\n\n\n\nCreating profiles for writing rasters\n\n\nCode\n# Create a profile for writing raster data\nprofile = {\n    'driver': 'GTiff',\n    'dtype': 'uint8',\n    'nodata': None,\n    'width': 100,\n    'height': 100,\n    'count': 1,\n    'crs': 'EPSG:4326',  # WGS84 lat/lon\n    'transform': transform\n}\n\nprint(\"Raster profile:\")\nfor key, value in profile.items():\n    print(f\"  {key}: {value}\")\n\n\nRaster profile:\n  driver: GTiff\n  dtype: uint8\n  nodata: None\n  width: 100\n  height: 100\n  count: 1\n  crs: EPSG:4326\n  transform: | 0.05, 0.00,-120.00|\n| 0.00,-0.05, 40.00|\n| 0.00, 0.00, 1.00|"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#multi-band-operations",
    "href": "course-materials/cheatsheets/rasterio_basics.html#multi-band-operations",
    "title": "Working with Rasterio",
    "section": "Multi-band Operations",
    "text": "Multi-band Operations\n\nWorking with multi-band data\n\n\nCode\n# Create multi-band sample data (RGB-like)\ndef create_multiband_sample():\n    height, width = 100, 100\n    bands = 3\n    \n    # Create different patterns for each band\n    x = np.linspace(-2, 2, width)\n    y = np.linspace(-2, 2, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Band 1: Circular pattern\n    band1 = (np.sin(np.sqrt(X**2 + Y**2)) * 127 + 127).astype(np.uint8)\n    \n    # Band 2: Linear gradient\n    band2 = (np.linspace(0, 255, width) * np.ones((height, 1))).astype(np.uint8)\n    \n    # Band 3: Checkerboard pattern\n    band3 = ((X + Y) &gt; 0).astype(np.uint8) * 255\n    \n    return np.stack([band1, band2, band3])\n\nmultiband_data = create_multiband_sample()\nprint(f\"Multiband shape: {multiband_data.shape}\")\nprint(f\"Shape format: (bands, height, width)\")\n\n\nMultiband shape: (3, 100, 100)\nShape format: (bands, height, width)\n\n\n\n\nVisualizing multi-band data\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\n# Individual bands\nfor i in range(3):\n    row, col = i // 2, i % 2\n    axes[row, col].imshow(multiband_data[i], cmap='gray')\n    axes[row, col].set_title(f'Band {i+1}')\n\n# RGB composite (transpose for matplotlib)\nrgb_composite = np.transpose(multiband_data, (1, 2, 0))\naxes[1, 1].imshow(rgb_composite)\naxes[1, 1].set_title('RGB Composite')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#band-math-and-indices",
    "href": "course-materials/cheatsheets/rasterio_basics.html#band-math-and-indices",
    "title": "Working with Rasterio",
    "section": "Band Math and Indices",
    "text": "Band Math and Indices\n\nCalculate vegetation index (NDVI-like)\n\n\nCode\n# Simulate NIR and Red bands\nnir_band = multiband_data[0].astype(np.float32)\nred_band = multiband_data[1].astype(np.float32)\n\n# Calculate NDVI-like index\n# NDVI = (NIR - Red) / (NIR + Red)\nndvi = (nir_band - red_band) / (nir_band + red_band + 1e-8)  # Small value to avoid division by zero\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(nir_band, cmap='RdYlBu_r')\nplt.title('NIR-like Band')\nplt.colorbar()\n\nplt.subplot(1, 3, 2)\nplt.imshow(red_band, cmap='Reds')\nplt.title('Red-like Band')\nplt.colorbar()\n\nplt.subplot(1, 3, 3)\nplt.imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.title('NDVI-like Index')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {np.mean(ndvi):.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI range: -0.211 to 1.000\nNDVI mean: 0.353"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#resampling-and-reprojection-concepts",
    "href": "course-materials/cheatsheets/rasterio_basics.html#resampling-and-reprojection-concepts",
    "title": "Working with Rasterio",
    "section": "Resampling and Reprojection Concepts",
    "text": "Resampling and Reprojection Concepts\n\nUnderstanding resampling methods\n\n\nCode\nfrom scipy import ndimage\n\n# Demonstrate different resampling methods\noriginal = sample_data\n\n# Downsample (reduce resolution)\ndownsampled = ndimage.zoom(original, 0.5, order=1)  # Linear interpolation\n\n# Upsample (increase resolution) \nupsampled = ndimage.zoom(original, 2.0, order=1)  # Linear interpolation\n\nplt.figure(figsize=(15, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(original, cmap='viridis')\nplt.title(f'Original ({original.shape[0]}x{original.shape[1]})')\n\nplt.subplot(1, 3, 2)\nplt.imshow(downsampled, cmap='viridis')\nplt.title(f'Downsampled ({downsampled.shape[0]}x{downsampled.shape[1]})')\n\nplt.subplot(1, 3, 3)\nplt.imshow(upsampled, cmap='viridis')\nplt.title(f'Upsampled ({upsampled.shape[0]}x{upsampled.shape[1]})')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Original size: {original.shape}\")\nprint(f\"Downsampled size: {downsampled.shape}\")  \nprint(f\"Upsampled size: {upsampled.shape}\")\n\n\n\n\n\n\n\n\n\nOriginal size: (100, 100)\nDownsampled size: (50, 50)\nUpsampled size: (200, 200)"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#common-rasterio-patterns",
    "href": "course-materials/cheatsheets/rasterio_basics.html#common-rasterio-patterns",
    "title": "Working with Rasterio",
    "section": "Common Rasterio Patterns",
    "text": "Common Rasterio Patterns\n\nWorking with windows and blocks\n\n\nCode\n# Simulate processing large rasters in blocks\ndef process_in_blocks(data, block_size=50):\n    \"\"\"Process data in blocks to simulate handling large rasters\"\"\"\n    height, width = data.shape\n    processed = np.zeros_like(data)\n    \n    for row in range(0, height, block_size):\n        for col in range(0, width, block_size):\n            # Define window bounds\n            row_end = min(row + block_size, height)\n            col_end = min(col + block_size, width)\n            \n            # Extract block\n            block = data[row:row_end, col:col_end]\n            \n            # Process block (example: enhance contrast)\n            processed_block = np.clip(block * 1.2, 0, 255)\n            \n            # Write back to result\n            processed[row:row_end, col:col_end] = processed_block\n            \n            print(f\"Processed block: ({row}:{row_end}, {col}:{col_end})\")\n    \n    return processed\n\n# Process sample data in blocks\nprocessed_data = process_in_blocks(sample_data, block_size=25)\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Original')\n\nplt.subplot(1, 2, 2)\nplt.imshow(processed_data, cmap='viridis')\nplt.title('Processed (Enhanced)')\n\nplt.tight_layout()\nplt.show()\n\n\nProcessed block: (0:25, 0:25)\nProcessed block: (0:25, 25:50)\nProcessed block: (0:25, 50:75)\nProcessed block: (0:25, 75:100)\nProcessed block: (25:50, 0:25)\nProcessed block: (25:50, 25:50)\nProcessed block: (25:50, 50:75)\nProcessed block: (25:50, 75:100)\nProcessed block: (50:75, 0:25)\nProcessed block: (50:75, 25:50)\nProcessed block: (50:75, 50:75)\nProcessed block: (50:75, 75:100)\nProcessed block: (75:100, 0:25)\nProcessed block: (75:100, 25:50)\nProcessed block: (75:100, 50:75)\nProcessed block: (75:100, 75:100)"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#summary",
    "href": "course-materials/cheatsheets/rasterio_basics.html#summary",
    "title": "Working with Rasterio",
    "section": "Summary",
    "text": "Summary\nKey Rasterio concepts covered: - Reading and understanding raster data structure - Working with single and multi-band imagery - Coordinate transforms and geospatial metadata\n- Band math and index calculations - Resampling and processing techniques - Block-based processing for large datasets"
  },
  {
    "objectID": "course-materials/cheatsheets/loading_models.html",
    "href": "course-materials/cheatsheets/loading_models.html",
    "title": "Loading Pre-trained Models",
    "section": "",
    "text": "Cheatsheet content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/cheatsheets/finetuning_basics.html",
    "href": "course-materials/cheatsheets/finetuning_basics.html",
    "title": "Fine-tuning Strategies",
    "section": "",
    "text": "Cheatsheet content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/cheatsheets/model_inference.html",
    "href": "course-materials/cheatsheets/model_inference.html",
    "title": "Model Inference & Feature Extraction",
    "section": "",
    "text": "Cheatsheet content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html",
    "href": "course-materials/cheatsheets/pytorch_tensors.html",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "",
    "text": "Code\nimport torch\nimport numpy as np\n\n# From Python lists\ntensor_from_list = torch.tensor([[1, 2], [3, 4]])\nprint(f\"From list: {tensor_from_list}\")\n\n# From NumPy arrays\nnumpy_array = np.array([[1, 2], [3, 4]])\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(f\"From numpy: {tensor_from_numpy}\")\n\n# Common initialization patterns\nzeros_tensor = torch.zeros(3, 3)\nones_tensor = torch.ones(2, 4)\nrandom_tensor = torch.randn(2, 3)  # Normal distribution\n\nprint(f\"Zeros: {zeros_tensor}\")\nprint(f\"Random: {random_tensor}\")\n\n\nFrom list: tensor([[1, 2],\n        [3, 4]])\nFrom numpy: tensor([[1, 2],\n        [3, 4]])\nZeros: tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nRandom: tensor([[-2.0007,  0.0227,  0.8911],\n        [-0.7456, -0.0902,  1.6010]])\n\n\n\n\n\n\n\nCode\n# Check tensor properties\nsample_tensor = torch.randn(3, 4, 5)\n\nprint(f\"Shape: {sample_tensor.shape}\")\nprint(f\"Size: {sample_tensor.size()}\")\nprint(f\"Data type: {sample_tensor.dtype}\")\nprint(f\"Device: {sample_tensor.device}\")\nprint(f\"Number of dimensions: {sample_tensor.ndim}\")\nprint(f\"Total elements: {sample_tensor.numel()}\")\n\n\nShape: torch.Size([3, 4, 5])\nSize: torch.Size([3, 4, 5])\nData type: torch.float32\nDevice: cpu\nNumber of dimensions: 3\nTotal elements: 60"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#creating-tensors",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#creating-tensors",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "",
    "text": "Code\nimport torch\nimport numpy as np\n\n# From Python lists\ntensor_from_list = torch.tensor([[1, 2], [3, 4]])\nprint(f\"From list: {tensor_from_list}\")\n\n# From NumPy arrays\nnumpy_array = np.array([[1, 2], [3, 4]])\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(f\"From numpy: {tensor_from_numpy}\")\n\n# Common initialization patterns\nzeros_tensor = torch.zeros(3, 3)\nones_tensor = torch.ones(2, 4)\nrandom_tensor = torch.randn(2, 3)  # Normal distribution\n\nprint(f\"Zeros: {zeros_tensor}\")\nprint(f\"Random: {random_tensor}\")\n\n\nFrom list: tensor([[1, 2],\n        [3, 4]])\nFrom numpy: tensor([[1, 2],\n        [3, 4]])\nZeros: tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nRandom: tensor([[-2.0007,  0.0227,  0.8911],\n        [-0.7456, -0.0902,  1.6010]])\n\n\n\n\n\n\n\nCode\n# Check tensor properties\nsample_tensor = torch.randn(3, 4, 5)\n\nprint(f\"Shape: {sample_tensor.shape}\")\nprint(f\"Size: {sample_tensor.size()}\")\nprint(f\"Data type: {sample_tensor.dtype}\")\nprint(f\"Device: {sample_tensor.device}\")\nprint(f\"Number of dimensions: {sample_tensor.ndim}\")\nprint(f\"Total elements: {sample_tensor.numel()}\")\n\n\nShape: torch.Size([3, 4, 5])\nSize: torch.Size([3, 4, 5])\nData type: torch.float32\nDevice: cpu\nNumber of dimensions: 3\nTotal elements: 60"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#gpu-operations",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#gpu-operations",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "GPU Operations",
    "text": "GPU Operations\n\nCheck GPU availability\n\n\nCode\n# Check if CUDA is available\ncuda_available = torch.cuda.is_available()\nprint(f\"CUDA available: {cuda_available}\")\n\nif cuda_available:\n    gpu_count = torch.cuda.device_count()\n    gpu_name = torch.cuda.get_device_name(0)\n    print(f\"Number of GPUs: {gpu_count}\")\n    print(f\"GPU name: {gpu_name}\")\nelse:\n    print(\"Running on CPU\")\n\n\nCUDA available: False\nRunning on CPU\n\n\n\n\nMoving tensors to GPU\n\n\nCode\n# Create a tensor\ncpu_tensor = torch.randn(3, 3)\nprint(f\"Original device: {cpu_tensor.device}\")\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpu_tensor = cpu_tensor.to(device)\nprint(f\"After moving to device: {gpu_tensor.device}\")\n\n# Alternative methods\nif torch.cuda.is_available():\n    # Method 2: .cuda() method\n    gpu_tensor2 = cpu_tensor.cuda()\n    print(f\"Using .cuda(): {gpu_tensor2.device}\")\n    \n    # Method 3: Create directly on GPU\n    direct_gpu = torch.randn(3, 3, device='cuda')\n    print(f\"Created on GPU: {direct_gpu.device}\")\n\n\nOriginal device: cpu\nAfter moving to device: cpu"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#basic-tensor-operations",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#basic-tensor-operations",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\n\nMathematical operations\n\n\nCode\n# Create sample tensors\na = torch.randn(3, 3)\nb = torch.randn(3, 3)\n\n# Element-wise operations\naddition = a + b\nsubtraction = a - b  \nmultiplication = a * b\ndivision = a / b\n\nprint(f\"Addition shape: {addition.shape}\")\nprint(f\"Subtraction mean: {subtraction.mean():.3f}\")\n\n# Matrix operations\nmatmul = torch.matmul(a, b)  # Matrix multiplication\ntranspose = a.t()  # Transpose\n\nprint(f\"Matrix multiplication shape: {matmul.shape}\")\nprint(f\"Transpose shape: {transpose.shape}\")\n\n\nAddition shape: torch.Size([3, 3])\nSubtraction mean: -0.084\nMatrix multiplication shape: torch.Size([3, 3])\nTranspose shape: torch.Size([3, 3])\n\n\n\n\nUseful tensor methods\n\n\nCode\n# Statistical operations\ndata = torch.randn(4, 5)\n\nprint(f\"Mean: {data.mean():.3f}\")\nprint(f\"Standard deviation: {data.std():.3f}\")\nprint(f\"Min: {data.min():.3f}\")\nprint(f\"Max: {data.max():.3f}\")\n\n# Reshaping\nreshaped = data.view(2, 10)  # Reshape to 2x10\nflattened = data.flatten()   # Flatten to 1D\n\nprint(f\"Original shape: {data.shape}\")\nprint(f\"Reshaped: {reshaped.shape}\")\nprint(f\"Flattened: {flattened.shape}\")\n\n\nMean: 0.162\nStandard deviation: 0.681\nMin: -1.106\nMax: 1.240\nOriginal shape: torch.Size([4, 5])\nReshaped: torch.Size([2, 10])\nFlattened: torch.Size([20])"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#memory-management",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#memory-management",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Memory Management",
    "text": "Memory Management\n\nGPU memory monitoring\n\n\nCode\nif torch.cuda.is_available():\n    # Check current memory usage\n    allocated = torch.cuda.memory_allocated() / (1024**2)  # Convert to MB\n    reserved = torch.cuda.memory_reserved() / (1024**2)\n    \n    print(f\"GPU memory allocated: {allocated:.1f} MB\")\n    print(f\"GPU memory reserved: {reserved:.1f} MB\")\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    print(\"GPU cache cleared\")\nelse:\n    print(\"GPU memory monitoring not available (running on CPU)\")\n\n\nGPU memory monitoring not available (running on CPU)\n\n\n\n\nBest practices\n\n\nCode\n# Use context managers for temporary operations\ndef memory_efficient_operation():\n    with torch.no_grad():  # Disable gradient computation\n        large_tensor = torch.randn(1000, 1000)\n        result = large_tensor.mean()\n        return result\n\nresult = memory_efficient_operation()\nprint(f\"Result: {result:.3f}\")\n\n# The large_tensor is automatically garbage collected\n\n\nResult: 0.001"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#converting-between-formats",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#converting-between-formats",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Converting Between Formats",
    "text": "Converting Between Formats\n\nPyTorch ‚ÜîÔ∏é NumPy\n\n\nCode\n# PyTorch to NumPy\ntorch_tensor = torch.randn(2, 3)\nnumpy_array = torch_tensor.numpy()  # Only works on CPU tensors\n\nprint(f\"PyTorch tensor: {torch_tensor}\")\nprint(f\"NumPy array: {numpy_array}\")\n\n# NumPy to PyTorch\nnew_torch = torch.from_numpy(numpy_array)\nprint(f\"Back to PyTorch: {new_torch}\")\n\n\nPyTorch tensor: tensor([[ 1.5828,  0.2641,  1.5333],\n        [-1.5289,  0.4187, -0.8595]])\nNumPy array: [[ 1.5827799   0.26410338  1.5333444 ]\n [-1.5288545   0.4186888  -0.85953194]]\nBack to PyTorch: tensor([[ 1.5828,  0.2641,  1.5333],\n        [-1.5289,  0.4187, -0.8595]])\n\n\n\n\nHandling GPU tensors\n\n\nCode\nif torch.cuda.is_available():\n    # GPU tensor must be moved to CPU first\n    gpu_tensor = torch.randn(2, 3).cuda()\n    cpu_numpy = gpu_tensor.cpu().numpy()\n    print(f\"GPU tensor converted to NumPy: {cpu_numpy.shape}\")\nelse:\n    print(\"GPU conversion example not available (running on CPU)\")\n\n\nGPU conversion example not available (running on CPU)"
  },
  {
    "objectID": "course-materials/cheatsheets/xarray_basics.html",
    "href": "course-materials/cheatsheets/xarray_basics.html",
    "title": "Xarray for Multi-dimensional Data",
    "section": "",
    "text": "Cheatsheet content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/cheatsheets/earth_engine_basics.html",
    "href": "course-materials/cheatsheets/earth_engine_basics.html",
    "title": "Earth Engine Basics",
    "section": "",
    "text": "Cheatsheet content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html",
    "href": "course-materials/cheatsheets/plotting_satellite.html",
    "title": "Plotting Satellite Imagery",
    "section": "",
    "text": "Effective visualization is crucial for understanding satellite imagery and geospatial data. This cheatsheet covers essential plotting techniques.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#introduction",
    "href": "course-materials/cheatsheets/plotting_satellite.html#introduction",
    "title": "Plotting Satellite Imagery",
    "section": "",
    "text": "Effective visualization is crucial for understanding satellite imagery and geospatial data. This cheatsheet covers essential plotting techniques.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#creating-sample-satellite-data",
    "href": "course-materials/cheatsheets/plotting_satellite.html#creating-sample-satellite-data",
    "title": "Plotting Satellite Imagery",
    "section": "Creating Sample Satellite Data",
    "text": "Creating Sample Satellite Data\n\n\nCode\ndef create_sample_satellite_scene(size=256):\n    \"\"\"Create a realistic-looking satellite scene\"\"\"\n    \n    # Create coordinate grids\n    x = np.linspace(0, 10, size)\n    y = np.linspace(0, 10, size)\n    X, Y = np.meshgrid(x, y)\n    \n    # Simulate different land cover types\n    # Water bodies (low values)\n    water = np.exp(-((X - 2)**2 + (Y - 7)**2) / 2) * 0.3\n    water += np.exp(-((X - 8)**2 + (Y - 3)**2) / 4) * 0.25\n    \n    # Forest/vegetation (medium-high values)\n    forest = np.exp(-((X - 6)**2 + (Y - 8)**2) / 8) * 0.7\n    forest += np.exp(-((X - 3)**2 + (Y - 2)**2) / 6) * 0.6\n    \n    # Urban areas (varied values)\n    urban = np.exp(-((X - 7)**2 + (Y - 5)**2) / 3) * 0.8\n    \n    # Combine and add noise\n    scene = water + forest + urban\n    noise = np.random.normal(0, 0.05, scene.shape)\n    scene = np.clip(scene + noise, 0, 1)\n    \n    # Scale to typical satellite data range\n    return (scene * 255).astype(np.uint8)\n\n# Create sample scenes for different bands\nred_band = create_sample_satellite_scene()\ngreen_band = create_sample_satellite_scene() * 1.1\nblue_band = create_sample_satellite_scene() * 0.8\nnir_band = create_sample_satellite_scene() * 1.3\n\nprint(f\"Band shapes: {red_band.shape}\")\nprint(f\"Data ranges - Red: {red_band.min()}-{red_band.max()}\")\n\n\nBand shapes: (256, 256)\nData ranges - Red: 0-255"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#single-band-visualization",
    "href": "course-materials/cheatsheets/plotting_satellite.html#single-band-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Single Band Visualization",
    "text": "Single Band Visualization\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Different colormaps for single bands\ncmaps = ['gray', 'viridis', 'plasma', 'RdYlBu_r']\nband_names = ['Grayscale', 'Viridis', 'Plasma', 'Red-Yellow-Blue']\n\nfor i, (cmap, name) in enumerate(zip(cmaps, band_names)):\n    ax = axes[i//2, i%2]\n    im = ax.imshow(red_band, cmap=cmap, interpolation='bilinear')\n    ax.set_title(f'{name} Colormap')\n    ax.axis('off')\n    plt.colorbar(im, ax=ax, shrink=0.8)\n\nplt.suptitle('Single Band Visualization with Different Colormaps', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#rgb-composite-images",
    "href": "course-materials/cheatsheets/plotting_satellite.html#rgb-composite-images",
    "title": "Plotting Satellite Imagery",
    "section": "RGB Composite Images",
    "text": "RGB Composite Images\n\n\nCode\ndef create_rgb_composite(r, g, b, enhance=True):\n    \"\"\"Create RGB composite with optional enhancement\"\"\"\n    \n    # Stack bands\n    rgb = np.stack([r, g, b], axis=-1)\n    \n    if enhance:\n        # Simple linear stretch enhancement\n        for i in range(3):\n            band = rgb[:,:,i].astype(float)\n            # Stretch to 2-98 percentile\n            p2, p98 = np.percentile(band, (2, 98))\n            band = np.clip((band - p2) / (p98 - p2), 0, 1)\n            rgb[:,:,i] = (band * 255).astype(np.uint8)\n    \n    return rgb\n\n# Create different composites\ntrue_color = create_rgb_composite(red_band, green_band, blue_band)\nfalse_color = create_rgb_composite(nir_band, red_band, green_band)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\naxes[0].imshow(true_color)\naxes[0].set_title('True Color Composite (RGB)')\naxes[0].axis('off')\n\naxes[1].imshow(false_color)\naxes[1].set_title('False Color Composite (NIR-Red-Green)')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0]."
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#spectral-indices-visualization",
    "href": "course-materials/cheatsheets/plotting_satellite.html#spectral-indices-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Spectral Indices Visualization",
    "text": "Spectral Indices Visualization\n\n\nCode\n# Calculate NDVI\ndef calculate_ndvi(nir, red):\n    \"\"\"Calculate Normalized Difference Vegetation Index\"\"\"\n    nir_f = nir.astype(float)\n    red_f = red.astype(float)\n    return (nir_f - red_f) / (nir_f + red_f + 1e-8)\n\n# Calculate other indices\nndvi = calculate_ndvi(nir_band, red_band)\n\n# Water index (using blue and NIR)\nndwi = calculate_ndvi(green_band, nir_band)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# NDVI\nim1 = axes[0].imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1, interpolation='bilinear')\naxes[0].set_title('NDVI (Vegetation Index)')\naxes[0].axis('off')\nplt.colorbar(im1, ax=axes[0], shrink=0.8)\n\n# NDWI  \nim2 = axes[1].imshow(ndwi, cmap='RdYlBu', vmin=-1, vmax=1, interpolation='bilinear')\naxes[1].set_title('NDWI (Water Index)')\naxes[1].axis('off')\nplt.colorbar(im2, ax=axes[1], shrink=0.8)\n\n# Histogram of NDVI values\naxes[2].hist(ndvi.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\naxes[2].set_xlabel('NDVI Value')\naxes[2].set_ylabel('Frequency')\naxes[2].set_title('NDVI Distribution')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {ndvi.mean():.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI range: -1.000 to 1.000\nNDVI mean: 0.120"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#custom-colormaps-and-classification",
    "href": "course-materials/cheatsheets/plotting_satellite.html#custom-colormaps-and-classification",
    "title": "Plotting Satellite Imagery",
    "section": "Custom Colormaps and Classification",
    "text": "Custom Colormaps and Classification\n\n\nCode\n# Create land cover classification\ndef classify_land_cover(ndvi, ndwi):\n    \"\"\"Simple land cover classification\"\"\"\n    classification = np.zeros_like(ndvi, dtype=int)\n    \n    # Water (high NDWI, low NDVI)\n    water_mask = (ndwi &gt; 0.3) & (ndvi &lt; 0.1)\n    classification[water_mask] = 1\n    \n    # Vegetation (high NDVI)\n    veg_mask = ndvi &gt; 0.4\n    classification[veg_mask] = 2\n    \n    # Bare soil/urban (low NDVI, low NDWI)  \n    bare_mask = (ndvi &lt; 0.2) & (ndwi &lt; 0.1)\n    classification[bare_mask] = 3\n    \n    return classification\n\n# Classify the scene\nland_cover = classify_land_cover(ndvi, ndwi)\n\n# Create custom colormap\ncolors = ['black', 'blue', 'green', 'brown', 'gray']\nn_classes = len(np.unique(land_cover))\ncustom_cmap = ListedColormap(colors[:n_classes])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Show classification\nim1 = axes[0].imshow(land_cover, cmap=custom_cmap, interpolation='nearest')\naxes[0].set_title('Land Cover Classification')\naxes[0].axis('off')\n\n# Custom legend\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='black', label='Unclassified'),\n    Patch(facecolor='blue', label='Water'),\n    Patch(facecolor='green', label='Vegetation'), \n    Patch(facecolor='brown', label='Bare Soil/Urban')\n]\naxes[0].legend(handles=legend_elements, loc='upper right')\n\n# Classification statistics\nunique_classes, counts = np.unique(land_cover, return_counts=True)\npercentages = counts / counts.sum() * 100\n\naxes[1].bar(['Unclassified', 'Water', 'Vegetation', 'Bare/Urban'][:len(unique_classes)], \n           percentages, color=['black', 'blue', 'green', 'brown'][:len(unique_classes)])\naxes[1].set_ylabel('Percentage of Pixels')\naxes[1].set_title('Land Cover Distribution')\naxes[1].grid(True, alpha=0.3)\n\nfor i, (cls, pct) in enumerate(zip(unique_classes, percentages)):\n    axes[1].text(i, pct + 1, f'{pct:.1f}%', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#advanced-visualization-techniques",
    "href": "course-materials/cheatsheets/plotting_satellite.html#advanced-visualization-techniques",
    "title": "Plotting Satellite Imagery",
    "section": "Advanced Visualization Techniques",
    "text": "Advanced Visualization Techniques\n\n\nCode\n# Multi-scale visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Full scene\naxes[0, 0].imshow(true_color)\naxes[0, 0].set_title('Full Scene')\naxes[0, 0].axis('off')\n\n# Add rectangle showing zoom area\nzoom_area = Rectangle((100, 100), 80, 80, linewidth=2, \n                     edgecolor='red', facecolor='none')\naxes[0, 0].add_patch(zoom_area)\n\n# Zoomed areas with different processing\nzoom_slice = (slice(100, 180), slice(100, 180))\n\n# Zoomed true color\naxes[0, 1].imshow(true_color[zoom_slice])\naxes[0, 1].set_title('Zoomed True Color')\naxes[0, 1].axis('off')\n\n# Zoomed false color\naxes[0, 2].imshow(false_color[zoom_slice])\naxes[0, 2].set_title('Zoomed False Color')\naxes[0, 2].axis('off')\n\n# Zoomed NDVI\nim1 = axes[1, 0].imshow(ndvi[zoom_slice], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[1, 0].set_title('Zoomed NDVI')\naxes[1, 0].axis('off')\nplt.colorbar(im1, ax=axes[1, 0], shrink=0.8)\n\n# Zoomed classification\naxes[1, 1].imshow(land_cover[zoom_slice], cmap=custom_cmap, interpolation='nearest')\naxes[1, 1].set_title('Zoomed Classification')\naxes[1, 1].axis('off')\n\n# Profile plot\ncenter_row = land_cover.shape[0] // 2\nprofile_ndvi = ndvi[center_row, :]\nprofile_x = np.arange(len(profile_ndvi))\n\naxes[1, 2].plot(profile_x, profile_ndvi, 'g-', linewidth=2)\naxes[1, 2].axhline(y=0, color='k', linestyle='--', alpha=0.5)\naxes[1, 2].axhline(y=0.4, color='r', linestyle='--', alpha=0.5, label='Vegetation threshold')\naxes[1, 2].set_xlabel('Pixel Position')\naxes[1, 2].set_ylabel('NDVI Value')\naxes[1, 2].set_title('NDVI Profile (Center Row)')\naxes[1, 2].grid(True, alpha=0.3)\naxes[1, 2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [40.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [40.0..255.0]."
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#time-series-visualization",
    "href": "course-materials/cheatsheets/plotting_satellite.html#time-series-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Time Series Visualization",
    "text": "Time Series Visualization\n\n\nCode\n# Simulate time series of NDVI\ndef create_ndvi_time_series(n_dates=12):\n    \"\"\"Create sample NDVI time series\"\"\"\n    dates = []\n    ndvi_values = []\n    \n    # Simulate seasonal pattern\n    base_ndvi = 0.4\n    seasonal_amplitude = 0.3\n    \n    for i in range(n_dates):\n        # Simulate monthly data\n        month_angle = (i / 12) * 2 * np.pi\n        seasonal_component = seasonal_amplitude * np.sin(month_angle + np.pi/2)  # Peak in summer\n        noise = np.random.normal(0, 0.05)\n        \n        ndvi_val = base_ndvi + seasonal_component + noise\n        ndvi_values.append(ndvi_val)\n        dates.append(f'Month {i+1}')\n    \n    return dates, ndvi_values\n\n# Create sample time series for different land cover types\ndates, forest_ndvi = create_ndvi_time_series()\n_, cropland_ndvi = create_ndvi_time_series()  \n_, urban_ndvi = create_ndvi_time_series()\n\n# Adjust for different land cover characteristics\ncropland_ndvi = [v * 0.8 + 0.1 for v in cropland_ndvi]  # Lower peak, higher base\nurban_ndvi = [v * 0.3 + 0.15 for v in urban_ndvi]  # Much lower overall\n\nplt.figure(figsize=(12, 6))\n\nplt.plot(dates, forest_ndvi, 'g-o', label='Forest', linewidth=2, markersize=6)\nplt.plot(dates, cropland_ndvi, 'orange', marker='s', label='Cropland', linewidth=2, markersize=6)  \nplt.plot(dates, urban_ndvi, 'gray', marker='^', label='Urban', linewidth=2, markersize=6)\n\nplt.xlabel('Time Period')\nplt.ylabel('NDVI Value')\nplt.title('NDVI Time Series by Land Cover Type')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.ylim(-0.1, 0.8)\n\n# Add shaded regions for seasons\nsummer_months = [4, 5, 6, 7]  # Months 5-8\nplt.axvspan(summer_months[0]-0.5, summer_months[-1]+0.5, alpha=0.2, color='yellow', label='Summer')\n\nplt.tight_layout()\nplt.show()\n\n# Print some statistics\nprint(\"NDVI Statistics by Land Cover:\")\nprint(f\"Forest - Mean: {np.mean(forest_ndvi):.3f}, Std: {np.std(forest_ndvi):.3f}\")\nprint(f\"Cropland - Mean: {np.mean(cropland_ndvi):.3f}, Std: {np.std(cropland_ndvi):.3f}\")\nprint(f\"Urban - Mean: {np.mean(urban_ndvi):.3f}, Std: {np.std(urban_ndvi):.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI Statistics by Land Cover:\nForest - Mean: 0.394, Std: 0.207\nCropland - Mean: 0.430, Std: 0.175\nUrban - Mean: 0.271, Std: 0.067"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#summary",
    "href": "course-materials/cheatsheets/plotting_satellite.html#summary",
    "title": "Plotting Satellite Imagery",
    "section": "Summary",
    "text": "Summary\nKey visualization techniques covered:\n\nSingle band visualization with different colormaps\nRGB composite creation and enhancement\nSpectral indices (NDVI, NDWI) visualization\n\nCustom colormaps and classification maps\nMulti-scale visualization with zoom and profiles\nTime series plotting for temporal analysis\n\nThese techniques form the foundation for effective satellite imagery analysis and presentation."
  },
  {
    "objectID": "course-materials/cheatsheets/torchgeo_basics.html",
    "href": "course-materials/cheatsheets/torchgeo_basics.html",
    "title": "TorchGeo Datasets & Transforms",
    "section": "",
    "text": "Cheatsheet content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/cheatsheets/folium_basics.html",
    "href": "course-materials/cheatsheets/folium_basics.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "Cheatsheet content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/cheatsheets/matplotlib_geospatial.html",
    "href": "course-materials/cheatsheets/matplotlib_geospatial.html",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "",
    "text": "Cheatsheet content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/lectures/lecture4_multimodal.html",
    "href": "course-materials/lectures/lecture4_multimodal.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/lectures/lecture2_geospatial_data.html",
    "href": "course-materials/lectures/lecture2_geospatial_data.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/lectures/lecture5_evaluation.html",
    "href": "course-materials/lectures/lecture5_evaluation.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/lectures/lecture1_architectures.html",
    "href": "course-materials/lectures/lecture1_architectures.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/lectures/lecture3_finetuning.html",
    "href": "course-materials/lectures/lecture3_finetuning.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/lectures/lecture6_cloud_computing.html",
    "href": "course-materials/lectures/lecture6_cloud_computing.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/interactive-sessions/session2_attention_mechanisms.html",
    "href": "course-materials/interactive-sessions/session2_attention_mechanisms.html",
    "title": "Week 2 Interactive Session: Spatial-Temporal Attention Mechanisms",
    "section": "",
    "text": "Interactive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/interactive-sessions/session3_architecture.html",
    "href": "course-materials/interactive-sessions/session3_architecture.html",
    "title": "Week 3 Interactive Session: Complete GFM Architecture",
    "section": "",
    "text": "Interactive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/interactive-sessions/session5_training_loop.html",
    "href": "course-materials/interactive-sessions/session5_training_loop.html",
    "title": "Week 5 Interactive Session: Training Loop Optimization",
    "section": "",
    "text": "Interactive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/interactive-sessions/session10_presentations.html",
    "href": "course-materials/interactive-sessions/session10_presentations.html",
    "title": "Week 10 Interactive Session: Project Presentations & Synthesis",
    "section": "",
    "text": "Interactive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/interactive-sessions/session7_pretrained_integration.html",
    "href": "course-materials/interactive-sessions/session7_pretrained_integration.html",
    "title": "Week 7 Interactive Session: Integration with Existing Models",
    "section": "",
    "text": "Interactive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/interactive-sessions/session4_pretraining.html",
    "href": "course-materials/interactive-sessions/session4_pretraining.html",
    "title": "Week 4 Interactive Session: Pretraining Implementation",
    "section": "",
    "text": "Interactive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/interactive-sessions/session9_deployment.html",
    "href": "course-materials/interactive-sessions/session9_deployment.html",
    "title": "Week 9 Interactive Session: Model Implementation & Deployment",
    "section": "",
    "text": "Interactive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/interactive-sessions/session8_finetuning.html",
    "href": "course-materials/interactive-sessions/session8_finetuning.html",
    "title": "Week 8 Interactive Session: Task-Specific Fine-tuning",
    "section": "",
    "text": "Interactive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/interactive-sessions/session6_evaluation.html",
    "href": "course-materials/interactive-sessions/session6_evaluation.html",
    "title": "Week 6 Interactive Session: Model Evaluation & Analysis",
    "section": "",
    "text": "Interactive session content coming soon‚Ä¶"
  },
  {
    "objectID": "course-materials/interactive-sessions/session1_data_foundations.html",
    "href": "course-materials/interactive-sessions/session1_data_foundations.html",
    "title": "Week 1 Interactive Session: Geospatial Data Foundations",
    "section": "",
    "text": "This session covers the fundamental data preparation steps for building geospatial foundation models. Just as text requires tokenization and careful data loading for LLMs, geospatial data requires specialized preprocessing, robust handling of missing data (clouds!), and spatial-temporal sequence creation for foundation models like Prithvi.\nWe‚Äôll build complete data pipelines that can handle real-world satellite imagery for foundation model training, including the 6-band HLS data used by NASA-IBM‚Äôs Prithvi foundation model.\n\n\n\nUnderstand geospatial data as foundation model input\nImplement robust data preprocessing pipelines\n\nHandle missing data (clouds, gaps) effectively\nCreate temporal sequences from satellite imagery\nBuild efficient PyTorch data loaders for training\nCompare text vs.¬†image tokenization approaches\n\n\n\n\nWe‚Äôll cover: 1. Understanding Geospatial Embeddings (vs.¬†text embeddings) 2. Tokenizing Satellite Imagery (vs.¬†text tokenization)\n3. Creating Spatial-Temporal Sequences (vs.¬†text sequences) 4. Building Data Loaders (adapted for geospatial data) 5. Preprocessing Pipelines (radiometric calibration, cloud masking)"
  },
  {
    "objectID": "course-materials/interactive-sessions/session1_data_foundations.html#introduction",
    "href": "course-materials/interactive-sessions/session1_data_foundations.html#introduction",
    "title": "Week 1 Interactive Session: Geospatial Data Foundations",
    "section": "",
    "text": "This session covers the fundamental data preparation steps for building geospatial foundation models. Just as text requires tokenization and careful data loading for LLMs, geospatial data requires specialized preprocessing, robust handling of missing data (clouds!), and spatial-temporal sequence creation for foundation models like Prithvi.\nWe‚Äôll build complete data pipelines that can handle real-world satellite imagery for foundation model training, including the 6-band HLS data used by NASA-IBM‚Äôs Prithvi foundation model.\n\n\n\nUnderstand geospatial data as foundation model input\nImplement robust data preprocessing pipelines\n\nHandle missing data (clouds, gaps) effectively\nCreate temporal sequences from satellite imagery\nBuild efficient PyTorch data loaders for training\nCompare text vs.¬†image tokenization approaches\n\n\n\n\nWe‚Äôll cover: 1. Understanding Geospatial Embeddings (vs.¬†text embeddings) 2. Tokenizing Satellite Imagery (vs.¬†text tokenization)\n3. Creating Spatial-Temporal Sequences (vs.¬†text sequences) 4. Building Data Loaders (adapted for geospatial data) 5. Preprocessing Pipelines (radiometric calibration, cloud masking)"
  },
  {
    "objectID": "course-materials/interactive-sessions/session1_data_foundations.html#understanding-geospatial-embeddings",
    "href": "course-materials/interactive-sessions/session1_data_foundations.html#understanding-geospatial-embeddings",
    "title": "Week 1 Interactive Session: Geospatial Data Foundations",
    "section": "1. Understanding Geospatial Embeddings",
    "text": "1. Understanding Geospatial Embeddings\nText models work with discrete tokens that map to embedding vectors. But geospatial data is fundamentally different - we‚Äôre working with continuous multi-spectral measurements that represent physical properties of Earth‚Äôs surface.\n\nText vs.¬†Geospatial Embeddings\nLet‚Äôs first understand how text embeddings work, then see how we adapt this for satellite imagery:\n\n\nCode\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Text approach\nprint(\"=== TEXT EMBEDDINGS ===\")\nvocab_size = 50257  # GPT-2 vocabulary\nembed_dim = 768     # GPT-2 embedding dimension\n\n# Discrete token IDs -&gt; embedding lookup\ntext_vocab = {\"the\": 1, \"cat\": 2, \"sat\": 3, \"on\": 4, \"mat\": 5}\ntoken_ids = torch.tensor([1, 2, 3, 4, 1, 5])  # \"the cat sat on the mat\"\n\ntext_embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\ntext_embeddings = text_embedding_layer(token_ids)\n\nprint(f\"Text tokens shape: {token_ids.shape}\")\nprint(f\"Text embeddings shape: {text_embeddings.shape}\")\nprint(f\"Process: Discrete tokens ‚Üí Lookup table ‚Üí Embeddings\")\n\nprint(\"\\n=== GEOSPATIAL EMBEDDINGS ===\")\n# Geospatial approach - continuous patch vectors\npatch_size = 16\nnum_bands = 6  # HLS bands: Blue, Green, Red, NIR, SWIR1, SWIR2\ninput_dim = patch_size * patch_size * num_bands  # 16 * 16 * 6 = 1536\n\n# Continuous patch values -&gt; learned projection\nnp.random.seed(42)\npatch_data = torch.randn(4, input_dim)  # 4 patches from satellite image\n\ngeo_projection_layer = torch.nn.Linear(input_dim, embed_dim)\ngeo_embeddings = geo_projection_layer(patch_data)\n\nprint(f\"Patch data shape: {patch_data.shape}\")\nprint(f\"Geo embeddings shape: {geo_embeddings.shape}\")\nprint(f\"Process: Continuous patches ‚Üí Linear projection ‚Üí Embeddings\")\n\nprint(f\"\\nKey difference:\")\nprint(f\"‚Ä¢ Text: {vocab_size:,} discrete symbols in vocabulary\")\nprint(f\"‚Ä¢ Geospatial: Infinite continuous values (no traditional vocabulary)\")\n\n\n=== TEXT EMBEDDINGS ===\nText tokens shape: torch.Size([6])\nText embeddings shape: torch.Size([6, 768])\nProcess: Discrete tokens ‚Üí Lookup table ‚Üí Embeddings\n\n=== GEOSPATIAL EMBEDDINGS ===\nPatch data shape: torch.Size([4, 1536])\nGeo embeddings shape: torch.Size([4, 768])\nProcess: Continuous patches ‚Üí Linear projection ‚Üí Embeddings\n\nKey difference:\n‚Ä¢ Text: 50,257 discrete symbols in vocabulary\n‚Ä¢ Geospatial: Infinite continuous values (no traditional vocabulary)\n\n\n\n\n\n\n\n\nFundamental Difference\n\n\n\nText models: Use discrete symbol lookup tables\nGeospatial models: Use continuous vector projections\nThis is why we can‚Äôt simply apply text tokenization to images!"
  },
  {
    "objectID": "course-materials/interactive-sessions/session1_data_foundations.html#downloading-and-loading-geospatial-data",
    "href": "course-materials/interactive-sessions/session1_data_foundations.html#downloading-and-loading-geospatial-data",
    "title": "Week 1 Interactive Session: Geospatial Data Foundations",
    "section": "2. Downloading and Loading Geospatial Data",
    "text": "2. Downloading and Loading Geospatial Data\nLet‚Äôs start by downloading sample satellite imagery:\n\n\nCode\nimport urllib.request\nimport os\nfrom pathlib import Path\nimport rasterio\nimport numpy as np\n\n# Use the course data directory\ndata_dir = Path(\"../../data\")\ndata_dir.mkdir(exist_ok=True)\n\n# Download sample satellite image (similar to Raschka's text download)\nurl = \"https://raw.githubusercontent.com/kellycaylor/geoAI/main/data/landcover_sample.tif\"\nsample_file = data_dir / \"landcover_sample.tif\"\n\nif not sample_file.exists():\n    print(\"Downloading sample satellite image...\")\n    urllib.request.urlretrieve(url, sample_file)\n    print(f\"Downloaded to: {sample_file}\")\nelse:\n    print(f\"Sample satellite image already exists at: {sample_file}\")\n\nprint(f\"File size: {sample_file.stat().st_size / 1024:.1f} KB\")\n\n# Load and examine the data\ntry:\n    with rasterio.open(sample_file) as src:\n        print(f\"Image dimensions: {src.width} x {src.height}\")\n        print(f\"Number of bands: {src.count}\")\n        print(f\"Data type: {src.dtypes[0]}\")\n        print(f\"Coordinate system: {src.crs}\")\n        print(f\"Bounds: {src.bounds}\")\n        \n        # Read first 3 bands for RGB visualization\n        rgb_data = src.read([1, 2, 3])  # Assuming RGB bands\n        \nexcept Exception as e:\n    print(f\"Could not read with rasterio: {e}\")\n    rgb_data = None\n\n\nSample satellite image already exists at: ../../data/landcover_sample.tif\nFile size: 12.6 KB\nImage dimensions: 64 x 64\nNumber of bands: 3\nData type: uint8\nCoordinate system: PROJCS[\"Projection: Transverse Mercator; Datum: WGS84; Ellipsoid: WGS84\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",19],PARAMETER[\"scale_factor\",0.9993],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",-5300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\nBounds: BoundingBox(left=280307.7499987148, bottom=394530.9999900842, right=280323.7499987148, top=394546.9999900842)\n\n\nLet‚Äôs also create synthetic HLS data to match what Prithvi uses:\n\n\nCode\ndef create_synthetic_hls_data(height=224, width=224):\n    \"\"\"\n    Create synthetic 6-band HLS-like data for demonstration\n    Bands: Blue, Green, Red, NIR, SWIR1, SWIR2\n    \"\"\"\n    np.random.seed(42)  # For reproducible results\n    \n    # Create realistic spectral signatures for different land cover types\n    x = np.linspace(0, 4*np.pi, width)\n    y = np.linspace(0, 4*np.pi, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Base elevation-like pattern\n    elevation = np.sin(X/3) * np.cos(Y/3) + 0.5 * np.sin(X) * np.sin(Y)\n    \n    # Simulate different spectral bands with realistic values\n    bands = {}\n    \n    # Visible bands (vegetation appears darker)\n    bands['blue'] = np.clip(0.1 + 0.05 * elevation + 0.02 * np.random.randn(height, width), 0, 1)\n    bands['green'] = np.clip(0.15 + 0.08 * elevation + 0.02 * np.random.randn(height, width), 0, 1)\n    bands['red'] = np.clip(0.12 + 0.06 * elevation + 0.02 * np.random.randn(height, width), 0, 1)\n    \n    # NIR band (vegetation appears bright)\n    vegetation_mask = elevation &gt; 0.2\n    bands['nir'] = np.clip(0.4 * vegetation_mask + 0.1 * (~vegetation_mask) + \n                   0.05 * elevation + 0.03 * np.random.randn(height, width), 0, 1)\n    \n    # SWIR bands (sensitive to moisture and soil)\n    bands['swir1'] = np.clip(0.2 + 0.1 * elevation + 0.03 * np.random.randn(height, width), 0, 1)\n    bands['swir2'] = np.clip(0.15 + 0.08 * elevation + 0.03 * np.random.randn(height, width), 0, 1)\n    \n    # Stack bands in HLS order\n    hls_data = np.stack([\n        bands['blue'], bands['green'], bands['red'],\n        bands['nir'], bands['swir1'], bands['swir2']\n    ])\n    \n    return hls_data, bands\n\n# Create our dataset\nhls_image, band_dict = create_synthetic_hls_data()\nprint(f\"Synthetic HLS data shape: {hls_image.shape}\")  # (6, 224, 224)\nprint(f\"Data type: {hls_image.dtype}\")\nprint(f\"Value range: [{hls_image.min():.3f}, {hls_image.max():.3f}]\")\n\n# Visualize the multi-spectral data\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nband_names = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2']\nfor i, band_name in enumerate(band_names):\n    im = axes[i].imshow(band_dict[band_name], cmap='viridis', vmin=0, vmax=1)\n    axes[i].set_title(f'{band_name.upper()} Band')\n    axes[i].axis('off')\n    plt.colorbar(im, ax=axes[i], fraction=0.046)\n\nplt.tight_layout()\nplt.show()\n\n# Create RGB composite for reference\nrgb_composite = np.stack([band_dict['red'], band_dict['green'], band_dict['blue']], axis=2)\nrgb_composite = np.clip(rgb_composite * 2.5, 0, 1)  # Enhance contrast\n\nplt.figure(figsize=(8, 8))\nplt.imshow(rgb_composite)\nplt.title('RGB Composite (Red-Green-Blue)')\nplt.axis('off')\nplt.show()\n\n\nSynthetic HLS data shape: (6, 224, 224)\nData type: float64\nValue range: [0.000, 0.553]"
  },
  {
    "objectID": "course-materials/interactive-sessions/session1_data_foundations.html#tokenizing-satellite-imagery",
    "href": "course-materials/interactive-sessions/session1_data_foundations.html#tokenizing-satellite-imagery",
    "title": "Week 1 Interactive Session: Geospatial Data Foundations",
    "section": "3. Tokenizing Satellite Imagery",
    "text": "3. Tokenizing Satellite Imagery\nJust as there are different approaches to text tokenization (character, word, subword), we need specialized approaches for satellite imagery. The key challenge: images have no natural token boundaries like spaces and punctuation in text.\n\nText Tokenization Review\nLet‚Äôs quickly review how text tokenization works:\n\n\nCode\nimport re\n\n# Text tokenization example\ntext = \"Hello, world. This, is a test.\"\n\n# Simple regex tokenization\ntext_tokens = re.split(r'([,.]|\\s)', text)\ntext_tokens = [item for item in text_tokens if item.strip()]\nprint(f\"Text tokens: {text_tokens}\")\nprint(f\"Number of tokens: {len(text_tokens)}\")\n\n# Text has natural boundaries: spaces, punctuation\n# Each token has semantic meaning: \"Hello\", \"world\", \"test\"\n\n\nText tokens: ['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\nNumber of tokens: 10\n\n\n\n\nGeospatial Tokenization Challenge\n\n\nCode\n# Unlike text, images have no obvious boundaries\nprint(\"GEOSPATIAL TOKENIZATION CHALLENGE:\")\nprint(\"üå≥ Where does the 'tree' token begin and end?\")\nprint(\"üçÉ Should each leaf be a separate token?\") \nprint(\"üè† Is a house one token or multiple (roof, walls, door)?\")\nprint(\"‚òÅÔ∏è How do we handle missing data (clouds)?\")\nprint(\"üìê How do we preserve spatial relationships?\")\n\n\nGEOSPATIAL TOKENIZATION CHALLENGE:\nüå≥ Where does the 'tree' token begin and end?\nüçÉ Should each leaf be a separate token?\nüè† Is a house one token or multiple (roof, walls, door)?\n‚òÅÔ∏è How do we handle missing data (clouds)?\nüìê How do we preserve spatial relationships?\n\n\n\n\nPatch-Based Tokenization (Vision Transformer Style)\nFollowing the approach used by Prithvi and other vision foundation models:\n\n\nCode\nclass PatchTokenizer:\n    \"\"\"\n    Patch-based tokenizer for satellite imagery\n    Similar to Vision Transformer approach\n    \"\"\"\n    \n    def __init__(self, patch_size=16, flatten_patches=True):\n        self.patch_size = patch_size\n        self.flatten_patches = flatten_patches\n        \n    def extract_patches(self, image):\n        \"\"\"\n        Extract non-overlapping patches from multi-band image\n        \n        Args:\n            image: Multi-band image of shape (bands, height, width)\n            \n        Returns:\n            patches: Array of shape (num_patches, bands * patch_size * patch_size)\n        \"\"\"\n        bands, height, width = image.shape\n        \n        # Calculate number of patches\n        patches_h = height // self.patch_size\n        patches_w = width // self.patch_size\n        \n        # Crop image to fit exact number of patches\n        cropped_h = patches_h * self.patch_size\n        cropped_w = patches_w * self.patch_size\n        cropped_image = image[:, :cropped_h, :cropped_w]\n        \n        # Reshape into patches\n        patches = cropped_image.reshape(\n            bands, patches_h, self.patch_size, patches_w, self.patch_size\n        )\n        # Rearrange dimensions: (patches_h, patches_w, bands, patch_size, patch_size)\n        patches = patches.transpose(1, 3, 0, 2, 4)\n        # Flatten spatial patch dimensions: (num_patches, bands, patch_size, patch_size)\n        patches = patches.reshape(-1, bands, self.patch_size, self.patch_size)\n        \n        if self.flatten_patches:\n            # Flatten each patch: (num_patches, bands * patch_size * patch_size)\n            patches = patches.reshape(patches.shape[0], -1)\n            \n        return patches\n    \n    def get_patch_positions(self, image_shape):\n        \"\"\"Get 2D positions of patches for positional encoding\"\"\"\n        _, height, width = image_shape\n        patches_h = height // self.patch_size\n        patches_w = width // self.patch_size\n        \n        positions = []\n        for i in range(patches_h):\n            for j in range(patches_w):\n                positions.append([i, j])\n        \n        return np.array(positions)\n\n# Test patch tokenization with different patch sizes\npatch_sizes = [8, 16, 32]\n\nprint(\"PATCH TOKENIZATION COMPARISON:\")\nprint(\"=\" * 50)\n\nfor patch_size in patch_sizes:\n    tokenizer = PatchTokenizer(patch_size=patch_size)\n    patches = tokenizer.extract_patches(hls_image)\n    positions = tokenizer.get_patch_positions(hls_image.shape)\n    \n    print(f\"\\nPatch size {patch_size}x{patch_size}:\")\n    print(f\"  Number of patches: {patches.shape[0]}\")\n    print(f\"  Patch dimension: {patches.shape[1]}\")\n    print(f\"  Token compression ratio: {hls_image.size / patches.size:.1f}x\")\n    print(f\"  Spatial positions shape: {positions.shape}\")\n\n\nPATCH TOKENIZATION COMPARISON:\n==================================================\n\nPatch size 8x8:\n  Number of patches: 784\n  Patch dimension: 384\n  Token compression ratio: 1.0x\n  Spatial positions shape: (784, 2)\n\nPatch size 16x16:\n  Number of patches: 196\n  Patch dimension: 1536\n  Token compression ratio: 1.0x\n  Spatial positions shape: (196, 2)\n\nPatch size 32x32:\n  Number of patches: 49\n  Patch dimension: 6144\n  Token compression ratio: 1.0x\n  Spatial positions shape: (49, 2)\n\n\n\n\nVisualizing Patch Extraction\n\n\nCode\ndef visualize_patch_extraction(image, patch_size=16):\n    \"\"\"Visualize how image is divided into patches\"\"\"\n    # Use RGB bands for visualization\n    rgb_image = np.stack([image[2], image[1], image[0]], axis=2)  # Red, Green, Blue\n    rgb_image = np.clip(rgb_image * 2.5, 0, 1)  # Enhance for visibility\n    \n    height, width = rgb_image.shape[:2]\n    patches_h = height // patch_size\n    patches_w = width // patch_size\n    \n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n    \n    # Original image\n    axes[0].imshow(rgb_image)\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n    \n    # Image with patch boundaries\n    axes[1].imshow(rgb_image)\n    \n    # Draw patch boundaries\n    for i in range(0, patches_h + 1):\n        y = i * patch_size\n        axes[1].axhline(y=y, color='red', linewidth=1, alpha=0.7)\n    \n    for j in range(0, patches_w + 1):\n        x = j * patch_size\n        axes[1].axvline(x=x, color='red', linewidth=1, alpha=0.7)\n    \n    axes[1].set_title(f'Patches ({patch_size}x{patch_size}): {patches_h}√ó{patches_w} = {patches_h*patches_w} patches')\n    axes[1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualize patch extraction for our HLS data\nvisualize_patch_extraction(hls_image, patch_size=16)"
  },
  {
    "objectID": "course-materials/interactive-sessions/session1_data_foundations.html#building-pytorch-data-loaders-for-geospatial-data",
    "href": "course-materials/interactive-sessions/session1_data_foundations.html#building-pytorch-data-loaders-for-geospatial-data",
    "title": "Week 1 Interactive Session: Geospatial Data Foundations",
    "section": "4. Building PyTorch Data Loaders for Geospatial Data",
    "text": "4. Building PyTorch Data Loaders for Geospatial Data\nNow we build PyTorch data loaders for satellite imagery. Instead of text sequences, we create spatial-temporal sequences of satellite patches.\n\nGeospatialDataset for Patch Sequences\n\n\nCode\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass GeospatialDatasetV1(Dataset):\n    \"\"\"\n    Geospatial dataset for foundation model training\n    Creates sequences of image patches with spatial and temporal relationships\n    \"\"\"\n    \n    def __init__(self, images, patch_size=16, max_length=256, stride=None, \n                 include_positions=True, mask_ratio=0.0):\n        \"\"\"\n        Args:\n            images: List of multi-band images or single image\n            patch_size: Size of patches to extract\n            max_length: Maximum sequence length (number of patches)\n            stride: Stride for overlapping sequences (default: max_length for non-overlapping)\n            include_positions: Whether to include spatial position information\n            mask_ratio: Ratio of patches to mask (for MAE pretraining)\n        \"\"\"\n        self.patch_size = patch_size\n        self.max_length = max_length\n        self.stride = stride or max_length\n        self.include_positions = include_positions\n        self.mask_ratio = mask_ratio\n        \n        self.input_patches = []\n        self.target_patches = []\n        self.positions = []\n        self.masks = []\n        \n        # Initialize tokenizer\n        self.tokenizer = PatchTokenizer(patch_size=patch_size)\n        \n        # Process images into patch sequences\n        if not isinstance(images, list):\n            images = [images]\n            \n        for image in images:\n            self._process_image(image)\n    \n    def _process_image(self, image):\n        \"\"\"Process a single image into patch sequences\"\"\"\n        # Extract patches\n        patches = self.tokenizer.extract_patches(image)\n        positions = self.tokenizer.get_patch_positions(image.shape)\n        \n        # Create sequences using sliding window approach\n        for i in range(0, len(patches) - self.max_length + 1, self.stride):\n            # Input sequence\n            input_sequence = patches[i:i + self.max_length]\n            \n            # Target sequence (for next-patch prediction or reconstruction)\n            if self.mask_ratio &gt; 0:\n                # For masked autoencoder: target is same as input, mask indicates what to predict\n                target_sequence = input_sequence.copy()\n                mask = self._create_random_mask(self.max_length)\n            else:\n                # For autoregressive: target is shifted by one patch\n                if i + self.max_length &lt; len(patches):\n                    target_sequence = patches[i + 1:i + self.max_length + 1]\n                    mask = np.ones(self.max_length, dtype=bool)  # No masking\n                else:\n                    continue\n            \n            # Position sequence\n            pos_sequence = positions[i:i + self.max_length] if self.include_positions else None\n            \n            self.input_patches.append(torch.from_numpy(input_sequence).float())\n            self.target_patches.append(torch.from_numpy(target_sequence).float())\n            if pos_sequence is not None:\n                self.positions.append(torch.from_numpy(pos_sequence).long())\n            self.masks.append(torch.from_numpy(mask).bool())\n    \n    def _create_random_mask(self, sequence_length):\n        \"\"\"Create random mask for masked autoencoder training\"\"\"\n        num_masked = int(sequence_length * self.mask_ratio)\n        mask = np.ones(sequence_length, dtype=bool)\n        \n        if num_masked &gt; 0:\n            mask_indices = np.random.choice(sequence_length, num_masked, replace=False)\n            mask[mask_indices] = False\n        \n        return mask\n    \n    def __len__(self):\n        return len(self.input_patches)\n    \n    def __getitem__(self, idx):\n        item = {\n            'input_patches': self.input_patches[idx],\n            'target_patches': self.target_patches[idx], \n            'mask': self.masks[idx]\n        }\n        \n        if self.include_positions:\n            item['positions'] = self.positions[idx]\n            \n        return item\n\n# Test the geospatial dataset\nprint(\"CREATING GEOSPATIAL DATASET:\")\nprint(\"=\" * 40)\n\n# Create dataset with different configurations\ndataset_configs = [\n    {\"name\": \"Autoregressive\", \"mask_ratio\": 0.0, \"max_length\": 64},\n    {\"name\": \"Masked Autoencoder\", \"mask_ratio\": 0.75, \"max_length\": 64},\n]\n\nfor config in dataset_configs:\n    dataset = GeospatialDatasetV1(\n        hls_image, \n        patch_size=16, \n        max_length=config[\"max_length\"],\n        mask_ratio=config[\"mask_ratio\"]\n    )\n    \n    print(f\"\\n{config['name']} Dataset:\")\n    print(f\"  Number of sequences: {len(dataset)}\")\n    print(f\"  Patch size: 16x16\")\n    print(f\"  Sequence length: {config['max_length']} patches\")\n    print(f\"  Mask ratio: {config['mask_ratio']:.0%}\")\n    \n    # Check first item\n    item = dataset[0]\n    print(f\"  Input shape: {item['input_patches'].shape}\")\n    print(f\"  Target shape: {item['target_patches'].shape}\")\n    print(f\"  Mask shape: {item['mask'].shape}\")\n    if 'positions' in item:\n        print(f\"  Positions shape: {item['positions'].shape}\")\n\n\nCREATING GEOSPATIAL DATASET:\n========================================\n\nAutoregressive Dataset:\n  Number of sequences: 3\n  Patch size: 16x16\n  Sequence length: 64 patches\n  Mask ratio: 0%\n  Input shape: torch.Size([64, 1536])\n  Target shape: torch.Size([64, 1536])\n  Mask shape: torch.Size([64])\n  Positions shape: torch.Size([64, 2])\n\nMasked Autoencoder Dataset:\n  Number of sequences: 3\n  Patch size: 16x16\n  Sequence length: 64 patches\n  Mask ratio: 75%\n  Input shape: torch.Size([64, 1536])\n  Target shape: torch.Size([64, 1536])\n  Mask shape: torch.Size([64])\n  Positions shape: torch.Size([64, 2])\n\n\n\n\nCreating Data Loaders\n\n\nCode\ndef create_geospatial_dataloader(images, batch_size=4, patch_size=16, max_length=64, \n                               mask_ratio=0.75, shuffle=True, num_workers=0):\n    \"\"\"\n    Create geospatial data loader for foundation model training\n    \"\"\"\n    \n    # Create dataset\n    dataset = GeospatialDatasetV1(\n        images=images,\n        patch_size=patch_size,\n        max_length=max_length,\n        mask_ratio=mask_ratio\n    )\n    \n    # Create dataloader  \n    dataloader = DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=shuffle, \n        num_workers=num_workers,\n        drop_last=True  # For consistent batch sizes\n    )\n    \n    return dataloader\n\n# Create data loader for our HLS data\nbatch_size = 2\nmax_length = 32  # Shorter sequences for demonstration\ndataloader = create_geospatial_dataloader(\n    hls_image,\n    batch_size=batch_size,\n    max_length=max_length,\n    mask_ratio=0.75  # Prithvi-style masking\n)\n\nprint(f\"GEOSPATIAL DATA LOADER:\")\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Max sequence length: {max_length}\")\nprint(f\"Total batches: {len(dataloader)}\")\n\n# Test batch loading\nfor batch_idx, batch in enumerate(dataloader):\n    print(f\"\\nBatch {batch_idx + 1}:\")\n    print(f\"  Input patches: {batch['input_patches'].shape}\")\n    print(f\"  Target patches: {batch['target_patches'].shape}\")\n    print(f\"  Masks: {batch['mask'].shape}\")\n    print(f\"  Positions: {batch['positions'].shape}\")\n    \n    # Show masking statistics\n    mask_ratio = (~batch['mask']).float().mean().item()\n    print(f\"  Actual mask ratio: {mask_ratio:.1%}\")\n    \n    if batch_idx &gt;= 1:  # Only show first 2 batches\n        break\n\n\nGEOSPATIAL DATA LOADER:\nBatch size: 2\nMax sequence length: 32\nTotal batches: 3\n\nBatch 1:\n  Input patches: torch.Size([2, 32, 1536])\n  Target patches: torch.Size([2, 32, 1536])\n  Masks: torch.Size([2, 32])\n  Positions: torch.Size([2, 32, 2])\n  Actual mask ratio: 75.0%\n\nBatch 2:\n  Input patches: torch.Size([2, 32, 1536])\n  Target patches: torch.Size([2, 32, 1536])\n  Masks: torch.Size([2, 32])\n  Positions: torch.Size([2, 32, 2])\n  Actual mask ratio: 75.0%"
  },
  {
    "objectID": "course-materials/interactive-sessions/session1_data_foundations.html#handling-missing-data-the-cloud-problem",
    "href": "course-materials/interactive-sessions/session1_data_foundations.html#handling-missing-data-the-cloud-problem",
    "title": "Week 1 Interactive Session: Geospatial Data Foundations",
    "section": "5. Handling Missing Data: The Cloud Problem",
    "text": "5. Handling Missing Data: The Cloud Problem\nUnlike text data (where missing words are rare), satellite imagery has massive missing data problems:\n\n\n\n\n\n\nScale of Missing Data in Satellite Imagery\n\n\n\n\n40-60% of optical satellite images contain clouds\nSeasonal variation: Up to 80% cloud cover in tropical regions during rainy season\n\nGeographic bias: Persistent cloud cover creates data deserts\n\nThis is like having 40-60% of words in text be &lt;UNK&gt; tokens!\n\n\n\nCloud-Aware Data Loading\n\n\nCode\ndef create_cloudy_satellite_image(clean_image):\n    \"\"\"Simulate realistic cloud contamination\"\"\"\n    cloudy_image = clean_image.copy()\n    \n    # Create realistic cloud patterns\n    height, width = clean_image.shape[1], clean_image.shape[2]\n    np.random.seed(123)\n    \n    # Large cloud system\n    center_y, center_x = height//3, 2*width//3\n    y, x = np.ogrid[:height, :width]\n    cloud1 = ((x - center_x)**2 + (y - center_y)**2) &lt; (height/6)**2\n    \n    # Smaller cloud patches\n    cloud2 = ((x - width//4)**2 + (y - 3*height//4)**2) &lt; (height/10)**2\n    \n    # Combine clouds\n    cloud_mask = np.logical_or(cloud1, cloud2)\n    \n    # Apply clouds (bright white values)\n    cloudy_image[:, cloud_mask] = 0.9  # Clouds are bright in all bands\n    \n    return cloudy_image, cloud_mask\n\n# Create cloudy version of our data\ncloudy_hls, cloud_mask = create_cloudy_satellite_image(hls_image)\n\n# Visualize the cloud problem\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Clean image\nrgb_clean = np.stack([hls_image[2], hls_image[1], hls_image[0]], axis=2)\nrgb_clean = np.clip(rgb_clean * 2.5, 0, 1)\naxes[0].imshow(rgb_clean)\naxes[0].set_title('Clean Image\\n(What we want)')\naxes[0].axis('off')\n\n# Cloudy image  \nrgb_cloudy = np.stack([cloudy_hls[2], cloudy_hls[1], cloudy_hls[0]], axis=2)\nrgb_cloudy = np.clip(rgb_cloudy * 2.5, 0, 1)\naxes[1].imshow(rgb_cloudy)\naxes[1].set_title('Reality: Cloudy Image\\n(What we actually get)')\naxes[1].axis('off')\n\n# Cloud mask\naxes[2].imshow(cloud_mask, cmap='RdBu_r')\naxes[2].set_title(f'Cloud Mask\\nCoverage: {cloud_mask.mean():.1%}')\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCloud-Aware Dataset\n\n\nCode\nclass CloudAwareGeospatialDataset(GeospatialDatasetV1):\n    \"\"\"\n    Extended dataset that handles cloud contamination\n    \"\"\"\n    \n    def __init__(self, images, cloud_masks=None, cloud_threshold=0.5, **kwargs):\n        \"\"\"\n        Args:\n            cloud_masks: Optional cloud masks for each image\n            cloud_threshold: Threshold for considering a patch contaminated\n        \"\"\"\n        self.cloud_masks = cloud_masks\n        self.cloud_threshold = cloud_threshold\n        \n        super().__init__(images, **kwargs)\n    \n    def _process_image(self, image):\n        \"\"\"Override to handle cloud contamination\"\"\"\n        # Extract patches normally\n        patches = self.tokenizer.extract_patches(image)\n        positions = self.tokenizer.get_patch_positions(image.shape)\n        \n        # Handle cloud masks if provided\n        if self.cloud_masks is not None:\n            # Process cloud mask to patch level\n            cloud_mask = self.cloud_masks[0] if len(self.cloud_masks) == 1 else self.cloud_masks[0]\n            \n            # Expand cloud mask to match image bands\n            cloud_mask_expanded = np.expand_dims(cloud_mask, axis=0)\n            cloud_patches = self.tokenizer.extract_patches(cloud_mask_expanded)\n            \n            # Calculate contamination ratio for each patch\n            contamination_ratios = cloud_patches.mean(axis=1)\n            is_contaminated = contamination_ratios &gt; self.cloud_threshold\n        else:\n            is_contaminated = np.zeros(len(patches), dtype=bool)\n        \n        # Create sequences, tracking cloud contamination\n        for i in range(0, len(patches) - self.max_length + 1, self.stride):\n            input_sequence = patches[i:i + self.max_length]\n            contamination_sequence = is_contaminated[i:i + self.max_length]\n            \n            # Skip sequences that are too contaminated\n            if contamination_sequence.mean() &gt; 0.8:  # Skip if &gt;80% contaminated\n                continue\n                \n            # Create mask that combines random masking + cloud masking\n            if self.mask_ratio &gt; 0:\n                target_sequence = input_sequence.copy()\n                random_mask = self._create_random_mask(self.max_length)\n                # Don't attend to cloud-contaminated patches\n                combined_mask = random_mask & (~contamination_sequence)\n            else:\n                if i + self.max_length &lt; len(patches):\n                    target_sequence = patches[i + 1:i + self.max_length + 1]\n                    combined_mask = ~contamination_sequence\n                else:\n                    continue\n            \n            pos_sequence = positions[i:i + self.max_length] if self.include_positions else None\n            \n            self.input_patches.append(torch.from_numpy(input_sequence).float())\n            self.target_patches.append(torch.from_numpy(target_sequence).float())\n            if pos_sequence is not None:\n                self.positions.append(torch.from_numpy(pos_sequence).long())\n            self.masks.append(torch.from_numpy(combined_mask).bool())\n\n# Test cloud-aware dataset\ncloud_aware_dataset = CloudAwareGeospatialDataset(\n    [cloudy_hls],\n    cloud_masks=[cloud_mask],\n    patch_size=16,\n    max_length=32,\n    mask_ratio=0.75\n)\n\nprint(f\"CLOUD-AWARE DATASET:\")\nprint(f\"Original HLS sequences: {len(GeospatialDatasetV1([hls_image], patch_size=16, max_length=32, mask_ratio=0.75))}\")\nprint(f\"Cloud-aware sequences: {len(cloud_aware_dataset)}\")\nprint(f\"Reduction due to cloud filtering: {(1 - len(cloud_aware_dataset)/len(GeospatialDatasetV1([hls_image], patch_size=16, max_length=32, mask_ratio=0.75)))*100:.1f}%\")\n\n\nCLOUD-AWARE DATASET:\nOriginal HLS sequences: 6\nCloud-aware sequences: 6\nReduction due to cloud filtering: 0.0%"
  },
  {
    "objectID": "course-materials/interactive-sessions/session1_data_foundations.html#temporal-sequences-time-series-as-documents",
    "href": "course-materials/interactive-sessions/session1_data_foundations.html#temporal-sequences-time-series-as-documents",
    "title": "Week 1 Interactive Session: Geospatial Data Foundations",
    "section": "6. Temporal Sequences: Time Series as ‚ÄúDocuments‚Äù",
    "text": "6. Temporal Sequences: Time Series as ‚ÄúDocuments‚Äù\nWe create temporal ‚Äúdocuments‚Äù from time series of satellite imagery, similar to how text documents are constructed from sentences:\n\n\nCode\ndef create_temporal_sequence(location_images, dates, max_sequence_length=12):\n    \"\"\"\n    Create temporal sequences from satellite imagery time series\n    Similar to how text documents are constructed from sentences\n    \"\"\"\n    \n    # Each location becomes a \"document\"\n    # Each date becomes a \"sentence\" of patches\n    # Each patch becomes a \"token\"\n    \n    temporal_tokens = []\n    temporal_positions = []\n    \n    tokenizer = PatchTokenizer(patch_size=16)\n    \n    for img_idx, (image, date) in enumerate(zip(location_images, dates)):\n        # Extract patches from this date\n        patches = tokenizer.extract_patches(image)\n        \n        # Add temporal encoding to each patch\n        temporal_encoding = np.full(len(patches), img_idx)  # Time step\n        \n        temporal_tokens.append(patches)\n        temporal_positions.append(temporal_encoding)\n    \n    # Concatenate temporal sequence (like concatenating sentences)\n    full_sequence = np.concatenate(temporal_tokens, axis=0)\n    temporal_positions = np.concatenate(temporal_positions)\n    \n    # Truncate if too long (like text sequence length limits)\n    if len(full_sequence) &gt; max_sequence_length:\n        full_sequence = full_sequence[:max_sequence_length]\n        temporal_positions = temporal_positions[:max_sequence_length]\n    \n    return full_sequence, temporal_positions\n\n# Simulate a time series of the same location\ntime_series = []\ndates = ['2023-01', '2023-04', '2023-07', '2023-10']  # Seasonal progression\n\nfor season in range(4):\n    # Create seasonal variations of the same scene\n    seasonal_image = hls_image.copy()\n    \n    # Simulate seasonal changes\n    if season == 1:  # Spring: more green\n        seasonal_image[1, :, :] += 0.2  # Green band\n    elif season == 2:  # Summer: brighter, more green\n        seasonal_image *= 1.1\n        seasonal_image[1, :, :] += 0.3  # Much more green\n    elif season == 3:  # Fall: more red/brown\n        seasonal_image[2, :, :] += 0.2  # Red band\n        seasonal_image[1, :, :] -= 0.1  # Less green\n    \n    seasonal_image = np.clip(seasonal_image, 0, 1)\n    time_series.append(seasonal_image)\n\n# Create temporal sequence\ntemporal_sequence, time_positions = create_temporal_sequence(time_series, dates)\n\nprint(f\"TEMPORAL SEQUENCE:\")\nprint(f\"Temporal sequence length: {len(temporal_sequence)} tokens\")\nprint(f\"Time positions: {np.unique(time_positions, return_counts=True)}\")\nprint(f\"Tokens per time step: {np.bincount(time_positions)}\")\n\n\nTEMPORAL SEQUENCE:\nTemporal sequence length: 12 tokens\nTime positions: (array([0]), array([12]))\nTokens per time step: [12]"
  },
  {
    "objectID": "course-materials/interactive-sessions/session1_data_foundations.html#summary-building-data-foundations-for-gfms",
    "href": "course-materials/interactive-sessions/session1_data_foundations.html#summary-building-data-foundations-for-gfms",
    "title": "Week 1 Interactive Session: Geospatial Data Foundations",
    "section": "Summary: Building Data Foundations for GFMs",
    "text": "Summary: Building Data Foundations for GFMs\nWe‚Äôve successfully built a comprehensive data preparation framework for geospatial foundation models:\n\nKey Components Developed\n\n\n\n\n\n\n\nData Challenge\nOur Geospatial Solution\n\n\n\n\nTokenization\nPatch-based spatial extraction with multi-spectral support\n\n\nVocabulary\nContinuous projection (no discrete vocabulary needed)\n\n\nSequences\nSpatial patches and temporal series from satellite imagery\n\n\nMissing Data\nCloud-aware contamination handling (40-60% data loss)\n\n\nPositional Encoding\n2D spatial + temporal position encoding\n\n\nData Loading\nSliding window over image patches with masking support\n\n\n\n\n\nWhat We Built\n\nPatchTokenizer: Converts satellite imagery to spatial tokens\nGeospatialDatasetV1: PyTorch dataset for image patch sequences\nCloudAwareGeospatialDataset: Handles missing data from clouds\nTemporal sequences: Time series as multi-image ‚Äúdocuments‚Äù\nComplete data loaders: Ready for foundation model training\n\n\n\nWeek 1 Deliverable: Complete Geospatial Data Pipeline\n\n\nCode\nprint(\"üéØ WEEK 1 DELIVERABLE COMPLETE:\")\nprint(\"=\" * 50)\nprint(\"‚úÖ Data downloading and loading\")\nprint(\"‚úÖ Multi-spectral tokenization\") \nprint(\"‚úÖ Patch-based sequence creation\")\nprint(\"‚úÖ Cloud-aware missing data handling\")\nprint(\"‚úÖ Temporal sequence construction\")\nprint(\"‚úÖ PyTorch data loaders ready for training\")\nprint(\"\\nüöÄ Ready for Week 2: Spatial-Temporal Attention Mechanisms!\")\n\n\nüéØ WEEK 1 DELIVERABLE COMPLETE:\n==================================================\n‚úÖ Data downloading and loading\n‚úÖ Multi-spectral tokenization\n‚úÖ Patch-based sequence creation\n‚úÖ Cloud-aware missing data handling\n‚úÖ Temporal sequence construction\n‚úÖ PyTorch data loaders ready for training\n\nüöÄ Ready for Week 2: Spatial-Temporal Attention Mechanisms!\n\n\n\n\nNext Steps\nThis data pipeline foundation enables: - Week 2: Building custom attention mechanisms for spatial-temporal relationships - Week 3: Assembling complete Vision Transformer architecture - Week 4: Implementing masked autoencoder pretraining - Weeks 5-10: Training, evaluation, and deployment\nWe‚Äôve built a robust geospatial data preparation system that handles the unique challenges of satellite imagery while incorporating proven concepts from foundation model data loading approaches."
  },
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "Syllabus.html#geog-288kc-building-geospatial-foundation-models",
    "href": "Syllabus.html#geog-288kc-building-geospatial-foundation-models",
    "title": "",
    "section": "GEOG 288KC: Building Geospatial Foundation Models",
    "text": "GEOG 288KC: Building Geospatial Foundation Models\nFall 2025\nFridays 9am-12pm + optional lab office hours Fridays 2pm-5pm\n\n\nCourse Overview\nThis course teaches students to build geospatial foundation models (GFMs) from scratch for remote sensing and environmental monitoring. Following the framework from ‚ÄúBuild a Large Language Model (From Scratch)‚Äù by Sebastian Raschka, students will implement every component of the foundation model pipeline‚Äîfrom geospatial tokenization and attention mechanisms to training loops and deployment‚Äîwhile developing their own working foundation model for a chosen geospatial application.\n\n\n\nPrerequisites\n\nStudents should have some experience with remote sensing, geospatial data, or ML (e.g., Python, Earth Engine, PyTorch).\nBecause this is a project-driven seminar, students should have a topic area in which they are interested in applying GeoFMs.\n\n\n\n\nApplications\nTo apply, students should submit a paragraph at the form link below describing their past experience with remote sensing, geospatial data, and ML, as well as their interest in building (not just using) Geospatial Foundation Models. They should describe a specific geospatial problem they want to solve by building a custom foundation model. The more clearly defined the target application and any existing datasets the better, though students will refine their approach as they learn to build complete GFM pipelines.\nhttps://forms.gle/Q1iDp2kuZuX1avMPA\n\n\n‚Äî\n\n\nCourse Structure: 3 Stages, 9 Steps\nFollowing Raschka‚Äôs framework for building foundation models:\n\nüèóÔ∏è Stage 1: Build GFM Architecture (Weeks 1-3)\n\nWeek 1: Geospatial Data Foundations (Step 1: Data preparation and sampling)\nWeek 2: Spatial-Temporal Attention Mechanisms (Step 2: Attention mechanism)\n\nWeek 3: Complete GFM Architecture (Step 3: LLM Architecture)\n\n\n\nüöÄ Stage 2: Training a Foundation Model (Weeks 4-7)\n\nWeek 4: Pretraining Implementation (Step 4: Pretraining)\nWeek 5: Training Loop Optimization (Step 5: Training loop)\nWeek 6: Model Evaluation & Analysis (Step 6: Model evaluation)\nWeek 7: Integration with Existing Models (Step 7: Load pretrained weights)\n\n\n\nüéØ Stage 3: Model Application (Weeks 8-10)\n\nWeek 8: Task-Specific Fine-tuning (Step 8: Fine-tuning)\nWeek 9: Model Implementation & Deployment (Step 9: Model implementation)\nWeek 10: Project Presentations & Future Directions (Integration and synthesis)\n\n\n\n\n\nDeliverables\nStage 1: Architecture (Weeks 1-3) * Week 1: Geospatial data pipeline with tokenization strategy * Week 3: Working GFM architecture (~10M parameters)\nStage 2: Training (Weeks 4-7) * Week 4: Active pretraining pipeline with monitoring * Week 6: Comprehensive model evaluation report * Week 7: Integration with existing models (Prithvi comparison)\nStage 3: Application (Weeks 8-10) * Week 8: Fine-tuned model for specific geospatial task * Week 9: Deployable model with API and documentation * Week 10: Final presentation of complete GFM pipeline (15 min demo + Q&A)\nOptional: Submit our foundation model to Hugging Face for broader visibility\n\n\n\nGrading\nThis course will be assessed on a pass/fail basis. Passing requires consistent attendance and participation and submission of all deliverables."
  }
]