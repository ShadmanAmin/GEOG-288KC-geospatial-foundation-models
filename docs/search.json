[
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Geospatial Foundation Models and Applications",
    "section": "Course Description",
    "text": "Course Description\nThis project-driven seminar introduces students to state-of-the-art geospatial foundation models (GFMs) for remote sensing and environmental monitoring. Through a mix of lecture, discussion, and hands-on labs, students will explore frontier techniques in geospatial AI—including self-supervised learning, masked autoencoders, multimodal sensor fusion, and scalable inference pipelines—while developing their own applied project.\nThe goal of GEOG 288KC (Geospatial Foundation Models and Applications) is to equip students with cutting-edge skills in applying foundation models to environmental challenges. By the end of the course, students should be able to:\n\nUnderstand and apply state-of-the-art geospatial foundation models for environmental monitoring\nFine-tune and adapt large-scale models for specific remote sensing applications\nBuild scalable processing pipelines using Earth Engine, TorchGeo, and cloud computing platforms\nCreate interactive applications and APIs for model deployment and visualization\nEvaluate and interpret foundation model outputs using robust validation frameworks\nCollaborate on applied research projects and communicate results to diverse audiences"
  },
  {
    "objectID": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "href": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "title": "Geospatial Foundation Models and Applications",
    "section": "Getting Started with the UCSB AI Sandbox",
    "text": "Getting Started with the UCSB AI Sandbox\nHere are detailed instructions for setting up your environment on the UCSB AI Sandbox, including foundation model installation and GPU optimization."
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Geospatial Foundation Models and Applications",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\nKelly Caylor\nEmail: caylor@ucsb.edu\nLearn more: Bren profile\n\n\n\n\nTA\n\n\n\n\n\n\n\nAnna Boser\nEmail: anaboser@ucsb.edu\nLearn more: Bren profile"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html",
    "href": "course-materials/cheatsheets/rasterio_basics.html",
    "title": "Working with Rasterio",
    "section": "",
    "text": "Rasterio is the essential Python library for reading and writing geospatial raster data.\n\n\nCode\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\n\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#introduction-to-rasterio",
    "href": "course-materials/cheatsheets/rasterio_basics.html#introduction-to-rasterio",
    "title": "Working with Rasterio",
    "section": "",
    "text": "Rasterio is the essential Python library for reading and writing geospatial raster data.\n\n\nCode\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\n\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#creating-sample-data",
    "href": "course-materials/cheatsheets/rasterio_basics.html#creating-sample-data",
    "title": "Working with Rasterio",
    "section": "Creating Sample Data",
    "text": "Creating Sample Data\nSince we may not have actual satellite imagery files, let’s create some sample raster data:\n\n\nCode\n# Create sample raster data\ndef create_sample_raster():\n    # Create a simple 100x100 raster with some pattern\n    height, width = 100, 100\n    \n    # Create coordinate grids\n    x = np.linspace(-2, 2, width)\n    y = np.linspace(-2, 2, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Create a pattern (e.g., circular pattern)\n    data = np.sin(np.sqrt(X**2 + Y**2)) * 255\n    data = data.astype(np.uint8)\n    \n    return data\n\nsample_data = create_sample_raster()\nprint(f\"Sample data shape: {sample_data.shape}\")\nprint(f\"Data type: {sample_data.dtype}\")\nprint(f\"Data range: {sample_data.min()} to {sample_data.max()}\")\n\n\nSample data shape: (100, 100)\nData type: uint8\nData range: 7 to 254"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#working-with-raster-arrays",
    "href": "course-materials/cheatsheets/rasterio_basics.html#working-with-raster-arrays",
    "title": "Working with Rasterio",
    "section": "Working with Raster Arrays",
    "text": "Working with Raster Arrays\n\nBasic array operations\n\n\nCode\n# Basic statistics\nprint(f\"Mean: {np.mean(sample_data):.2f}\")\nprint(f\"Standard deviation: {np.std(sample_data):.2f}\")\nprint(f\"Unique values: {len(np.unique(sample_data))}\")\n\n# Histogram of values\nplt.figure(figsize=(8, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Sample Raster Data')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.hist(sample_data.flatten(), bins=50, alpha=0.7)\nplt.title('Histogram of Values')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\nMean: 215.01\nStandard deviation: 46.13\nUnique values: 182\n\n\n\n\n\n\n\n\n\n\n\nArray masking and filtering\n\n\nCode\n# Create masks\nhigh_values = sample_data &gt; 200\nlow_values = sample_data &lt; 50\n\nprint(f\"Pixels with high values (&gt;200): {np.sum(high_values)}\")\nprint(f\"Pixels with low values (&lt;50): {np.sum(low_values)}\")\n\n# Apply mask\nmasked_data = np.where(high_values, sample_data, 0)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Original Data')\n\nplt.subplot(1, 3, 2)\nplt.imshow(high_values, cmap='gray')\nplt.title('High Value Mask')\n\nplt.subplot(1, 3, 3)\nplt.imshow(masked_data, cmap='viridis')\nplt.title('Masked Data')\n\nplt.tight_layout()\nplt.show()\n\n\nPixels with high values (&gt;200): 7364\nPixels with low values (&lt;50): 76"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#raster-metadata-and-transforms",
    "href": "course-materials/cheatsheets/rasterio_basics.html#raster-metadata-and-transforms",
    "title": "Working with Rasterio",
    "section": "Raster Metadata and Transforms",
    "text": "Raster Metadata and Transforms\n\nUnderstanding geospatial transforms\n\n\nCode\nfrom rasterio.transform import from_bounds\n\n# Create a sample transform (geographic coordinates)\nwest, south, east, north = -120.0, 35.0, -115.0, 40.0  # Bounding box\ntransform = from_bounds(west, south, east, north, 100, 100)\n\nprint(f\"Transform: {transform}\")\nprint(f\"Pixel width: {transform[0]}\")\nprint(f\"Pixel height: {abs(transform[4])}\")\n\n# Convert pixel coordinates to geographic coordinates\ndef pixel_to_geo(row, col, transform):\n    x, y = rasterio.transform.xy(transform, row, col)\n    return x, y\n\n# Example: center pixel\ncenter_row, center_col = 50, 50\ngeo_x, geo_y = pixel_to_geo(center_row, center_col, transform)\nprint(f\"Center pixel ({center_row}, {center_col}) -&gt; Geographic: ({geo_x:.2f}, {geo_y:.2f})\")\n\n\nTransform: | 0.05, 0.00,-120.00|\n| 0.00,-0.05, 40.00|\n| 0.00, 0.00, 1.00|\nPixel width: 0.05\nPixel height: 0.05\nCenter pixel (50, 50) -&gt; Geographic: (-117.47, 37.48)\n\n\n\n\nCreating profiles for writing rasters\n\n\nCode\n# Create a profile for writing raster data\nprofile = {\n    'driver': 'GTiff',\n    'dtype': 'uint8',\n    'nodata': None,\n    'width': 100,\n    'height': 100,\n    'count': 1,\n    'crs': 'EPSG:4326',  # WGS84 lat/lon\n    'transform': transform\n}\n\nprint(\"Raster profile:\")\nfor key, value in profile.items():\n    print(f\"  {key}: {value}\")\n\n\nRaster profile:\n  driver: GTiff\n  dtype: uint8\n  nodata: None\n  width: 100\n  height: 100\n  count: 1\n  crs: EPSG:4326\n  transform: | 0.05, 0.00,-120.00|\n| 0.00,-0.05, 40.00|\n| 0.00, 0.00, 1.00|"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#multi-band-operations",
    "href": "course-materials/cheatsheets/rasterio_basics.html#multi-band-operations",
    "title": "Working with Rasterio",
    "section": "Multi-band Operations",
    "text": "Multi-band Operations\n\nWorking with multi-band data\n\n\nCode\n# Create multi-band sample data (RGB-like)\ndef create_multiband_sample():\n    height, width = 100, 100\n    bands = 3\n    \n    # Create different patterns for each band\n    x = np.linspace(-2, 2, width)\n    y = np.linspace(-2, 2, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Band 1: Circular pattern\n    band1 = (np.sin(np.sqrt(X**2 + Y**2)) * 127 + 127).astype(np.uint8)\n    \n    # Band 2: Linear gradient\n    band2 = (np.linspace(0, 255, width) * np.ones((height, 1))).astype(np.uint8)\n    \n    # Band 3: Checkerboard pattern\n    band3 = ((X + Y) &gt; 0).astype(np.uint8) * 255\n    \n    return np.stack([band1, band2, band3])\n\nmultiband_data = create_multiband_sample()\nprint(f\"Multiband shape: {multiband_data.shape}\")\nprint(f\"Shape format: (bands, height, width)\")\n\n\nMultiband shape: (3, 100, 100)\nShape format: (bands, height, width)\n\n\n\n\nVisualizing multi-band data\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\n# Individual bands\nfor i in range(3):\n    row, col = i // 2, i % 2\n    axes[row, col].imshow(multiband_data[i], cmap='gray')\n    axes[row, col].set_title(f'Band {i+1}')\n\n# RGB composite (transpose for matplotlib)\nrgb_composite = np.transpose(multiband_data, (1, 2, 0))\naxes[1, 1].imshow(rgb_composite)\naxes[1, 1].set_title('RGB Composite')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#band-math-and-indices",
    "href": "course-materials/cheatsheets/rasterio_basics.html#band-math-and-indices",
    "title": "Working with Rasterio",
    "section": "Band Math and Indices",
    "text": "Band Math and Indices\n\nCalculate vegetation index (NDVI-like)\n\n\nCode\n# Simulate NIR and Red bands\nnir_band = multiband_data[0].astype(np.float32)\nred_band = multiband_data[1].astype(np.float32)\n\n# Calculate NDVI-like index\n# NDVI = (NIR - Red) / (NIR + Red)\nndvi = (nir_band - red_band) / (nir_band + red_band + 1e-8)  # Small value to avoid division by zero\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(nir_band, cmap='RdYlBu_r')\nplt.title('NIR-like Band')\nplt.colorbar()\n\nplt.subplot(1, 3, 2)\nplt.imshow(red_band, cmap='Reds')\nplt.title('Red-like Band')\nplt.colorbar()\n\nplt.subplot(1, 3, 3)\nplt.imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.title('NDVI-like Index')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {np.mean(ndvi):.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI range: -0.211 to 1.000\nNDVI mean: 0.353"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#resampling-and-reprojection-concepts",
    "href": "course-materials/cheatsheets/rasterio_basics.html#resampling-and-reprojection-concepts",
    "title": "Working with Rasterio",
    "section": "Resampling and Reprojection Concepts",
    "text": "Resampling and Reprojection Concepts\n\nUnderstanding resampling methods\n\n\nCode\nfrom scipy import ndimage\n\n# Demonstrate different resampling methods\noriginal = sample_data\n\n# Downsample (reduce resolution)\ndownsampled = ndimage.zoom(original, 0.5, order=1)  # Linear interpolation\n\n# Upsample (increase resolution) \nupsampled = ndimage.zoom(original, 2.0, order=1)  # Linear interpolation\n\nplt.figure(figsize=(15, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(original, cmap='viridis')\nplt.title(f'Original ({original.shape[0]}x{original.shape[1]})')\n\nplt.subplot(1, 3, 2)\nplt.imshow(downsampled, cmap='viridis')\nplt.title(f'Downsampled ({downsampled.shape[0]}x{downsampled.shape[1]})')\n\nplt.subplot(1, 3, 3)\nplt.imshow(upsampled, cmap='viridis')\nplt.title(f'Upsampled ({upsampled.shape[0]}x{upsampled.shape[1]})')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Original size: {original.shape}\")\nprint(f\"Downsampled size: {downsampled.shape}\")  \nprint(f\"Upsampled size: {upsampled.shape}\")\n\n\n\n\n\n\n\n\n\nOriginal size: (100, 100)\nDownsampled size: (50, 50)\nUpsampled size: (200, 200)"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#common-rasterio-patterns",
    "href": "course-materials/cheatsheets/rasterio_basics.html#common-rasterio-patterns",
    "title": "Working with Rasterio",
    "section": "Common Rasterio Patterns",
    "text": "Common Rasterio Patterns\n\nWorking with windows and blocks\n\n\nCode\n# Simulate processing large rasters in blocks\ndef process_in_blocks(data, block_size=50):\n    \"\"\"Process data in blocks to simulate handling large rasters\"\"\"\n    height, width = data.shape\n    processed = np.zeros_like(data)\n    \n    for row in range(0, height, block_size):\n        for col in range(0, width, block_size):\n            # Define window bounds\n            row_end = min(row + block_size, height)\n            col_end = min(col + block_size, width)\n            \n            # Extract block\n            block = data[row:row_end, col:col_end]\n            \n            # Process block (example: enhance contrast)\n            processed_block = np.clip(block * 1.2, 0, 255)\n            \n            # Write back to result\n            processed[row:row_end, col:col_end] = processed_block\n            \n            print(f\"Processed block: ({row}:{row_end}, {col}:{col_end})\")\n    \n    return processed\n\n# Process sample data in blocks\nprocessed_data = process_in_blocks(sample_data, block_size=25)\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Original')\n\nplt.subplot(1, 2, 2)\nplt.imshow(processed_data, cmap='viridis')\nplt.title('Processed (Enhanced)')\n\nplt.tight_layout()\nplt.show()\n\n\nProcessed block: (0:25, 0:25)\nProcessed block: (0:25, 25:50)\nProcessed block: (0:25, 50:75)\nProcessed block: (0:25, 75:100)\nProcessed block: (25:50, 0:25)\nProcessed block: (25:50, 25:50)\nProcessed block: (25:50, 50:75)\nProcessed block: (25:50, 75:100)\nProcessed block: (50:75, 0:25)\nProcessed block: (50:75, 25:50)\nProcessed block: (50:75, 50:75)\nProcessed block: (50:75, 75:100)\nProcessed block: (75:100, 0:25)\nProcessed block: (75:100, 25:50)\nProcessed block: (75:100, 50:75)\nProcessed block: (75:100, 75:100)"
  },
  {
    "objectID": "course-materials/cheatsheets/rasterio_basics.html#summary",
    "href": "course-materials/cheatsheets/rasterio_basics.html#summary",
    "title": "Working with Rasterio",
    "section": "Summary",
    "text": "Summary\nKey Rasterio concepts covered: - Reading and understanding raster data structure - Working with single and multi-band imagery - Coordinate transforms and geospatial metadata\n- Band math and index calculations - Resampling and processing techniques - Block-based processing for large datasets"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html",
    "href": "course-materials/cheatsheets/plotting_satellite.html",
    "title": "Plotting Satellite Imagery",
    "section": "",
    "text": "Effective visualization is crucial for understanding satellite imagery and geospatial data. This cheatsheet covers essential plotting techniques.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#introduction",
    "href": "course-materials/cheatsheets/plotting_satellite.html#introduction",
    "title": "Plotting Satellite Imagery",
    "section": "",
    "text": "Effective visualization is crucial for understanding satellite imagery and geospatial data. This cheatsheet covers essential plotting techniques.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#creating-sample-satellite-data",
    "href": "course-materials/cheatsheets/plotting_satellite.html#creating-sample-satellite-data",
    "title": "Plotting Satellite Imagery",
    "section": "Creating Sample Satellite Data",
    "text": "Creating Sample Satellite Data\n\n\nCode\ndef create_sample_satellite_scene(size=256):\n    \"\"\"Create a realistic-looking satellite scene\"\"\"\n    \n    # Create coordinate grids\n    x = np.linspace(0, 10, size)\n    y = np.linspace(0, 10, size)\n    X, Y = np.meshgrid(x, y)\n    \n    # Simulate different land cover types\n    # Water bodies (low values)\n    water = np.exp(-((X - 2)**2 + (Y - 7)**2) / 2) * 0.3\n    water += np.exp(-((X - 8)**2 + (Y - 3)**2) / 4) * 0.25\n    \n    # Forest/vegetation (medium-high values)\n    forest = np.exp(-((X - 6)**2 + (Y - 8)**2) / 8) * 0.7\n    forest += np.exp(-((X - 3)**2 + (Y - 2)**2) / 6) * 0.6\n    \n    # Urban areas (varied values)\n    urban = np.exp(-((X - 7)**2 + (Y - 5)**2) / 3) * 0.8\n    \n    # Combine and add noise\n    scene = water + forest + urban\n    noise = np.random.normal(0, 0.05, scene.shape)\n    scene = np.clip(scene + noise, 0, 1)\n    \n    # Scale to typical satellite data range\n    return (scene * 255).astype(np.uint8)\n\n# Create sample scenes for different bands\nred_band = create_sample_satellite_scene()\ngreen_band = create_sample_satellite_scene() * 1.1\nblue_band = create_sample_satellite_scene() * 0.8\nnir_band = create_sample_satellite_scene() * 1.3\n\nprint(f\"Band shapes: {red_band.shape}\")\nprint(f\"Data ranges - Red: {red_band.min()}-{red_band.max()}\")\n\n\nBand shapes: (256, 256)\nData ranges - Red: 0-255"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#single-band-visualization",
    "href": "course-materials/cheatsheets/plotting_satellite.html#single-band-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Single Band Visualization",
    "text": "Single Band Visualization\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Different colormaps for single bands\ncmaps = ['gray', 'viridis', 'plasma', 'RdYlBu_r']\nband_names = ['Grayscale', 'Viridis', 'Plasma', 'Red-Yellow-Blue']\n\nfor i, (cmap, name) in enumerate(zip(cmaps, band_names)):\n    ax = axes[i//2, i%2]\n    im = ax.imshow(red_band, cmap=cmap, interpolation='bilinear')\n    ax.set_title(f'{name} Colormap')\n    ax.axis('off')\n    plt.colorbar(im, ax=ax, shrink=0.8)\n\nplt.suptitle('Single Band Visualization with Different Colormaps', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#rgb-composite-images",
    "href": "course-materials/cheatsheets/plotting_satellite.html#rgb-composite-images",
    "title": "Plotting Satellite Imagery",
    "section": "RGB Composite Images",
    "text": "RGB Composite Images\n\n\nCode\ndef create_rgb_composite(r, g, b, enhance=True):\n    \"\"\"Create RGB composite with optional enhancement\"\"\"\n    \n    # Stack bands\n    rgb = np.stack([r, g, b], axis=-1)\n    \n    if enhance:\n        # Simple linear stretch enhancement\n        for i in range(3):\n            band = rgb[:,:,i].astype(float)\n            # Stretch to 2-98 percentile\n            p2, p98 = np.percentile(band, (2, 98))\n            band = np.clip((band - p2) / (p98 - p2), 0, 1)\n            rgb[:,:,i] = (band * 255).astype(np.uint8)\n    \n    return rgb\n\n# Create different composites\ntrue_color = create_rgb_composite(red_band, green_band, blue_band)\nfalse_color = create_rgb_composite(nir_band, red_band, green_band)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\naxes[0].imshow(true_color)\naxes[0].set_title('True Color Composite (RGB)')\naxes[0].axis('off')\n\naxes[1].imshow(false_color)\naxes[1].set_title('False Color Composite (NIR-Red-Green)')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0]."
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#spectral-indices-visualization",
    "href": "course-materials/cheatsheets/plotting_satellite.html#spectral-indices-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Spectral Indices Visualization",
    "text": "Spectral Indices Visualization\n\n\nCode\n# Calculate NDVI\ndef calculate_ndvi(nir, red):\n    \"\"\"Calculate Normalized Difference Vegetation Index\"\"\"\n    nir_f = nir.astype(float)\n    red_f = red.astype(float)\n    return (nir_f - red_f) / (nir_f + red_f + 1e-8)\n\n# Calculate other indices\nndvi = calculate_ndvi(nir_band, red_band)\n\n# Water index (using blue and NIR)\nndwi = calculate_ndvi(green_band, nir_band)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# NDVI\nim1 = axes[0].imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1, interpolation='bilinear')\naxes[0].set_title('NDVI (Vegetation Index)')\naxes[0].axis('off')\nplt.colorbar(im1, ax=axes[0], shrink=0.8)\n\n# NDWI  \nim2 = axes[1].imshow(ndwi, cmap='RdYlBu', vmin=-1, vmax=1, interpolation='bilinear')\naxes[1].set_title('NDWI (Water Index)')\naxes[1].axis('off')\nplt.colorbar(im2, ax=axes[1], shrink=0.8)\n\n# Histogram of NDVI values\naxes[2].hist(ndvi.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\naxes[2].set_xlabel('NDVI Value')\naxes[2].set_ylabel('Frequency')\naxes[2].set_title('NDVI Distribution')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {ndvi.mean():.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI range: -1.000 to 1.000\nNDVI mean: 0.123"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#custom-colormaps-and-classification",
    "href": "course-materials/cheatsheets/plotting_satellite.html#custom-colormaps-and-classification",
    "title": "Plotting Satellite Imagery",
    "section": "Custom Colormaps and Classification",
    "text": "Custom Colormaps and Classification\n\n\nCode\n# Create land cover classification\ndef classify_land_cover(ndvi, ndwi):\n    \"\"\"Simple land cover classification\"\"\"\n    classification = np.zeros_like(ndvi, dtype=int)\n    \n    # Water (high NDWI, low NDVI)\n    water_mask = (ndwi &gt; 0.3) & (ndvi &lt; 0.1)\n    classification[water_mask] = 1\n    \n    # Vegetation (high NDVI)\n    veg_mask = ndvi &gt; 0.4\n    classification[veg_mask] = 2\n    \n    # Bare soil/urban (low NDVI, low NDWI)  \n    bare_mask = (ndvi &lt; 0.2) & (ndwi &lt; 0.1)\n    classification[bare_mask] = 3\n    \n    return classification\n\n# Classify the scene\nland_cover = classify_land_cover(ndvi, ndwi)\n\n# Create custom colormap\ncolors = ['black', 'blue', 'green', 'brown', 'gray']\nn_classes = len(np.unique(land_cover))\ncustom_cmap = ListedColormap(colors[:n_classes])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Show classification\nim1 = axes[0].imshow(land_cover, cmap=custom_cmap, interpolation='nearest')\naxes[0].set_title('Land Cover Classification')\naxes[0].axis('off')\n\n# Custom legend\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='black', label='Unclassified'),\n    Patch(facecolor='blue', label='Water'),\n    Patch(facecolor='green', label='Vegetation'), \n    Patch(facecolor='brown', label='Bare Soil/Urban')\n]\naxes[0].legend(handles=legend_elements, loc='upper right')\n\n# Classification statistics\nunique_classes, counts = np.unique(land_cover, return_counts=True)\npercentages = counts / counts.sum() * 100\n\naxes[1].bar(['Unclassified', 'Water', 'Vegetation', 'Bare/Urban'][:len(unique_classes)], \n           percentages, color=['black', 'blue', 'green', 'brown'][:len(unique_classes)])\naxes[1].set_ylabel('Percentage of Pixels')\naxes[1].set_title('Land Cover Distribution')\naxes[1].grid(True, alpha=0.3)\n\nfor i, (cls, pct) in enumerate(zip(unique_classes, percentages)):\n    axes[1].text(i, pct + 1, f'{pct:.1f}%', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#advanced-visualization-techniques",
    "href": "course-materials/cheatsheets/plotting_satellite.html#advanced-visualization-techniques",
    "title": "Plotting Satellite Imagery",
    "section": "Advanced Visualization Techniques",
    "text": "Advanced Visualization Techniques\n\n\nCode\n# Multi-scale visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Full scene\naxes[0, 0].imshow(true_color)\naxes[0, 0].set_title('Full Scene')\naxes[0, 0].axis('off')\n\n# Add rectangle showing zoom area\nzoom_area = Rectangle((100, 100), 80, 80, linewidth=2, \n                     edgecolor='red', facecolor='none')\naxes[0, 0].add_patch(zoom_area)\n\n# Zoomed areas with different processing\nzoom_slice = (slice(100, 180), slice(100, 180))\n\n# Zoomed true color\naxes[0, 1].imshow(true_color[zoom_slice])\naxes[0, 1].set_title('Zoomed True Color')\naxes[0, 1].axis('off')\n\n# Zoomed false color\naxes[0, 2].imshow(false_color[zoom_slice])\naxes[0, 2].set_title('Zoomed False Color')\naxes[0, 2].axis('off')\n\n# Zoomed NDVI\nim1 = axes[1, 0].imshow(ndvi[zoom_slice], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[1, 0].set_title('Zoomed NDVI')\naxes[1, 0].axis('off')\nplt.colorbar(im1, ax=axes[1, 0], shrink=0.8)\n\n# Zoomed classification\naxes[1, 1].imshow(land_cover[zoom_slice], cmap=custom_cmap, interpolation='nearest')\naxes[1, 1].set_title('Zoomed Classification')\naxes[1, 1].axis('off')\n\n# Profile plot\ncenter_row = land_cover.shape[0] // 2\nprofile_ndvi = ndvi[center_row, :]\nprofile_x = np.arange(len(profile_ndvi))\n\naxes[1, 2].plot(profile_x, profile_ndvi, 'g-', linewidth=2)\naxes[1, 2].axhline(y=0, color='k', linestyle='--', alpha=0.5)\naxes[1, 2].axhline(y=0.4, color='r', linestyle='--', alpha=0.5, label='Vegetation threshold')\naxes[1, 2].set_xlabel('Pixel Position')\naxes[1, 2].set_ylabel('NDVI Value')\naxes[1, 2].set_title('NDVI Profile (Center Row)')\naxes[1, 2].grid(True, alpha=0.3)\naxes[1, 2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [42.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [42.0..255.0]."
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#time-series-visualization",
    "href": "course-materials/cheatsheets/plotting_satellite.html#time-series-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Time Series Visualization",
    "text": "Time Series Visualization\n\n\nCode\n# Simulate time series of NDVI\ndef create_ndvi_time_series(n_dates=12):\n    \"\"\"Create sample NDVI time series\"\"\"\n    dates = []\n    ndvi_values = []\n    \n    # Simulate seasonal pattern\n    base_ndvi = 0.4\n    seasonal_amplitude = 0.3\n    \n    for i in range(n_dates):\n        # Simulate monthly data\n        month_angle = (i / 12) * 2 * np.pi\n        seasonal_component = seasonal_amplitude * np.sin(month_angle + np.pi/2)  # Peak in summer\n        noise = np.random.normal(0, 0.05)\n        \n        ndvi_val = base_ndvi + seasonal_component + noise\n        ndvi_values.append(ndvi_val)\n        dates.append(f'Month {i+1}')\n    \n    return dates, ndvi_values\n\n# Create sample time series for different land cover types\ndates, forest_ndvi = create_ndvi_time_series()\n_, cropland_ndvi = create_ndvi_time_series()  \n_, urban_ndvi = create_ndvi_time_series()\n\n# Adjust for different land cover characteristics\ncropland_ndvi = [v * 0.8 + 0.1 for v in cropland_ndvi]  # Lower peak, higher base\nurban_ndvi = [v * 0.3 + 0.15 for v in urban_ndvi]  # Much lower overall\n\nplt.figure(figsize=(12, 6))\n\nplt.plot(dates, forest_ndvi, 'g-o', label='Forest', linewidth=2, markersize=6)\nplt.plot(dates, cropland_ndvi, 'orange', marker='s', label='Cropland', linewidth=2, markersize=6)  \nplt.plot(dates, urban_ndvi, 'gray', marker='^', label='Urban', linewidth=2, markersize=6)\n\nplt.xlabel('Time Period')\nplt.ylabel('NDVI Value')\nplt.title('NDVI Time Series by Land Cover Type')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.ylim(-0.1, 0.8)\n\n# Add shaded regions for seasons\nsummer_months = [4, 5, 6, 7]  # Months 5-8\nplt.axvspan(summer_months[0]-0.5, summer_months[-1]+0.5, alpha=0.2, color='yellow', label='Summer')\n\nplt.tight_layout()\nplt.show()\n\n# Print some statistics\nprint(\"NDVI Statistics by Land Cover:\")\nprint(f\"Forest - Mean: {np.mean(forest_ndvi):.3f}, Std: {np.std(forest_ndvi):.3f}\")\nprint(f\"Cropland - Mean: {np.mean(cropland_ndvi):.3f}, Std: {np.std(cropland_ndvi):.3f}\")\nprint(f\"Urban - Mean: {np.mean(urban_ndvi):.3f}, Std: {np.std(urban_ndvi):.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI Statistics by Land Cover:\nForest - Mean: 0.416, Std: 0.203\nCropland - Mean: 0.420, Std: 0.162\nUrban - Mean: 0.273, Std: 0.064"
  },
  {
    "objectID": "course-materials/cheatsheets/plotting_satellite.html#summary",
    "href": "course-materials/cheatsheets/plotting_satellite.html#summary",
    "title": "Plotting Satellite Imagery",
    "section": "Summary",
    "text": "Summary\nKey visualization techniques covered:\n\nSingle band visualization with different colormaps\nRGB composite creation and enhancement\nSpectral indices (NDVI, NDWI) visualization\n\nCustom colormaps and classification maps\nMulti-scale visualization with zoom and profiles\nTime series plotting for temporal analysis\n\nThese techniques form the foundation for effective satellite imagery analysis and presentation."
  },
  {
    "objectID": "course-materials/week8.html",
    "href": "course-materials/week8.html",
    "title": "Week 8: Advanced Development and Optimization",
    "section": "",
    "text": "Focus on refining project implementations, improving system performance, and preparing robust, scalable solutions with comprehensive evaluation and documentation.\n\n\n\nOptimize system performance and computational efficiency\nImplement advanced features and functionality enhancements\nEstablish comprehensive evaluation and robustness testing\nPrepare systems for deployment and scaling considerations\n\n\n\n\n\nPerformance Optimization: Memory usage, computational efficiency, GPU utilization\nAdvanced Features: Enhanced functionality, user interface improvements, automation\nScalability: Distributed processing, cloud integration, resource management\nRobustness: Error handling, edge case management, system reliability\n\n\n\n\n\nImplement performance optimizations and efficiency improvements\nAdd advanced features and enhanced functionality\nConduct comprehensive testing and validation across scenarios\nPrepare deployment configurations and scaling strategies\nCreate user documentation and system guides\n\n\n\n\nOptimization Workshops (9am-10:30am): - Monday: Performance profiling and computational optimization - Tuesday: Advanced feature development and system enhancement - Wednesday: Deployment strategies and scalability planning - Thursday: Comprehensive evaluation and validation techniques\nDevelopment Time (10:30am-12pm): - Independent optimization and feature development work - System testing and validation across multiple scenarios - Documentation preparation and user guide creation - Final debugging and system refinement\n\n\n\nComprehensive System Demo (Due: End of Week 8) - Advanced Demo (7-10 minutes): Full system functionality with edge cases - Performance Analysis: Efficiency metrics, scalability assessment, resource usage - Feature Showcase: Advanced capabilities and enhanced functionality - Deployment Documentation: Installation guides, configuration instructions, user manuals\n\n\n\n\nComputational Efficiency: Algorithm optimization, parallel processing, memory management\nSystem Architecture: Modular design, component interaction, API development\nUser Experience: Interface improvements, workflow automation, result visualization\nDeployment Readiness: Configuration management, environment portability, monitoring\n\n\n\n\n\nDistributed computing and cloud deployment patterns\nModel serving and API development best practices\nAdvanced visualization and interactive dashboard creation\nSystem monitoring, logging, and error tracking implementation\n\n\n\n\n\nPerformance optimization guides and profiling tools\nAdvanced deployment patterns and containerization\nScalable architecture design principles\nSystem monitoring and observability frameworks\n\n\n\n\nWeek 9 will focus on final project polish, comprehensive documentation, and presentation preparation."
  },
  {
    "objectID": "course-materials/week8.html#week-8-overview",
    "href": "course-materials/week8.html#week-8-overview",
    "title": "Week 8: Advanced Development and Optimization",
    "section": "",
    "text": "Focus on refining project implementations, improving system performance, and preparing robust, scalable solutions with comprehensive evaluation and documentation.\n\n\n\nOptimize system performance and computational efficiency\nImplement advanced features and functionality enhancements\nEstablish comprehensive evaluation and robustness testing\nPrepare systems for deployment and scaling considerations\n\n\n\n\n\nPerformance Optimization: Memory usage, computational efficiency, GPU utilization\nAdvanced Features: Enhanced functionality, user interface improvements, automation\nScalability: Distributed processing, cloud integration, resource management\nRobustness: Error handling, edge case management, system reliability\n\n\n\n\n\nImplement performance optimizations and efficiency improvements\nAdd advanced features and enhanced functionality\nConduct comprehensive testing and validation across scenarios\nPrepare deployment configurations and scaling strategies\nCreate user documentation and system guides\n\n\n\n\nOptimization Workshops (9am-10:30am): - Monday: Performance profiling and computational optimization - Tuesday: Advanced feature development and system enhancement - Wednesday: Deployment strategies and scalability planning - Thursday: Comprehensive evaluation and validation techniques\nDevelopment Time (10:30am-12pm): - Independent optimization and feature development work - System testing and validation across multiple scenarios - Documentation preparation and user guide creation - Final debugging and system refinement\n\n\n\nComprehensive System Demo (Due: End of Week 8) - Advanced Demo (7-10 minutes): Full system functionality with edge cases - Performance Analysis: Efficiency metrics, scalability assessment, resource usage - Feature Showcase: Advanced capabilities and enhanced functionality - Deployment Documentation: Installation guides, configuration instructions, user manuals\n\n\n\n\nComputational Efficiency: Algorithm optimization, parallel processing, memory management\nSystem Architecture: Modular design, component interaction, API development\nUser Experience: Interface improvements, workflow automation, result visualization\nDeployment Readiness: Configuration management, environment portability, monitoring\n\n\n\n\n\nDistributed computing and cloud deployment patterns\nModel serving and API development best practices\nAdvanced visualization and interactive dashboard creation\nSystem monitoring, logging, and error tracking implementation\n\n\n\n\n\nPerformance optimization guides and profiling tools\nAdvanced deployment patterns and containerization\nScalable architecture design principles\nSystem monitoring and observability frameworks\n\n\n\n\nWeek 9 will focus on final project polish, comprehensive documentation, and presentation preparation."
  },
  {
    "objectID": "course-materials/week6.html",
    "href": "course-materials/week6.html",
    "title": "Week 6: Project Initiation and Planning",
    "section": "",
    "text": "Transition to intensive project development phase with structured planning sessions, technical guidance, and initial implementation milestones.\n\n\n\nDefine clear project scope, objectives, and success criteria\nEstablish technical architecture and implementation roadmap\nBegin core development with proper project structure\nSet up reproducible development environments and workflows\n\n\n\n\n\nProject Definition: Problem statement, data sources, expected outcomes\nTechnical Planning: Architecture design, technology stack, implementation phases\nResource Management: Computational requirements, data access, timeline planning\nQuality Assurance: Testing strategies, validation approaches, documentation standards\n\n\n\n\n\nFinalize project proposal with detailed specifications\nSet up project repository with proper structure and documentation\nImplement core data loading and preprocessing pipeline\nEstablish development workflow and version control practices\nBegin initial model experiments and baseline implementations\n\n\n\n\nStructured Guidance (9am-11am): - One-on-one project planning meetings with instructor - Technical architecture review and feedback - Resource identification and access setup - Timeline validation and milestone definition\nDevelopment Time (11am-12pm): - Independent implementation work - Peer collaboration and knowledge sharing - Technical problem-solving and debugging - Progress documentation and repository setup\n\n\n\nProject Proposal and Initial Implementation (Due: End of Week 6) - Written Proposal (2-3 pages): Problem statement, data sources, methodology, timeline - Repository Setup: Structured codebase with documentation, environment setup - Initial Pipeline: Working data loading and basic preprocessing functionality - Baseline Results: Initial experiments or proof-of-concept implementation\n\n\n\n\nProject repository structure and best practices\nReproducible environment setup (conda, Docker, requirements)\nData management and version control strategies\nComputational resource allocation and optimization\n\n\n\n\n\nProject proposal templates and examples\nRepository structure guidelines\nUCSB AI Sandbox resource allocation\nTechnical consultation scheduling system\n\n\n\n\nWeek 7 will focus on intensive development with regular progress check-ins and technical workshops."
  },
  {
    "objectID": "course-materials/week6.html#week-6-overview",
    "href": "course-materials/week6.html#week-6-overview",
    "title": "Week 6: Project Initiation and Planning",
    "section": "",
    "text": "Transition to intensive project development phase with structured planning sessions, technical guidance, and initial implementation milestones.\n\n\n\nDefine clear project scope, objectives, and success criteria\nEstablish technical architecture and implementation roadmap\nBegin core development with proper project structure\nSet up reproducible development environments and workflows\n\n\n\n\n\nProject Definition: Problem statement, data sources, expected outcomes\nTechnical Planning: Architecture design, technology stack, implementation phases\nResource Management: Computational requirements, data access, timeline planning\nQuality Assurance: Testing strategies, validation approaches, documentation standards\n\n\n\n\n\nFinalize project proposal with detailed specifications\nSet up project repository with proper structure and documentation\nImplement core data loading and preprocessing pipeline\nEstablish development workflow and version control practices\nBegin initial model experiments and baseline implementations\n\n\n\n\nStructured Guidance (9am-11am): - One-on-one project planning meetings with instructor - Technical architecture review and feedback - Resource identification and access setup - Timeline validation and milestone definition\nDevelopment Time (11am-12pm): - Independent implementation work - Peer collaboration and knowledge sharing - Technical problem-solving and debugging - Progress documentation and repository setup\n\n\n\nProject Proposal and Initial Implementation (Due: End of Week 6) - Written Proposal (2-3 pages): Problem statement, data sources, methodology, timeline - Repository Setup: Structured codebase with documentation, environment setup - Initial Pipeline: Working data loading and basic preprocessing functionality - Baseline Results: Initial experiments or proof-of-concept implementation\n\n\n\n\nProject repository structure and best practices\nReproducible environment setup (conda, Docker, requirements)\nData management and version control strategies\nComputational resource allocation and optimization\n\n\n\n\n\nProject proposal templates and examples\nRepository structure guidelines\nUCSB AI Sandbox resource allocation\nTechnical consultation scheduling system\n\n\n\n\nWeek 7 will focus on intensive development with regular progress check-ins and technical workshops."
  },
  {
    "objectID": "course-materials/week4.html",
    "href": "course-materials/week4.html",
    "title": "Week 4: Multi-modal and Generative Models",
    "section": "",
    "text": "Explore cutting-edge multi-modal and generative approaches in geospatial foundation models, including sensor fusion and synthetic data generation.\n\n\n\nUnderstand multi-modal fusion techniques for different satellite sensors\nExplore generative models for geospatial applications\nLearn about vision-language models for remote sensing\nInvestigate synthetic data generation and augmentation strategies\n\n\n\n\n\nMulti-modal Architectures: Sensor fusion, cross-modal attention, joint embeddings\nGenerative Models: GANs, VAEs, diffusion models for satellite imagery\nVision-Language Models: CLIP-based approaches, remote sensing captioning\nSynthetic Data: Data augmentation, domain adaptation, privacy preservation\n\n\n\n\n\nMulti-sensor fusion workshop (optical + SAR + LiDAR)\nExperiment with generative models for data augmentation\nExplore vision-language models for scene understanding\nProject progress check-ins and technical consultations\n\n\n\n\n\nImplementing multi-modal fusion architectures\nWorking with generative adversarial networks\nUsing pre-trained vision-language models\nEvaluating synthetic vs. real data quality\n\n\n\n\nBuilding a multi-modal classifier combining Sentinel-1 and Sentinel-2 data\n\n\n\n\nCLIP for Remote Sensing\nMulti-modal learning in Earth observation survey\nGenerative models for geospatial data tutorial\n\n\n\n\nWeek 5 provides dedicated time for semi-independent project work with optional proposal workshops."
  },
  {
    "objectID": "course-materials/week4.html#week-4-overview",
    "href": "course-materials/week4.html#week-4-overview",
    "title": "Week 4: Multi-modal and Generative Models",
    "section": "",
    "text": "Explore cutting-edge multi-modal and generative approaches in geospatial foundation models, including sensor fusion and synthetic data generation.\n\n\n\nUnderstand multi-modal fusion techniques for different satellite sensors\nExplore generative models for geospatial applications\nLearn about vision-language models for remote sensing\nInvestigate synthetic data generation and augmentation strategies\n\n\n\n\n\nMulti-modal Architectures: Sensor fusion, cross-modal attention, joint embeddings\nGenerative Models: GANs, VAEs, diffusion models for satellite imagery\nVision-Language Models: CLIP-based approaches, remote sensing captioning\nSynthetic Data: Data augmentation, domain adaptation, privacy preservation\n\n\n\n\n\nMulti-sensor fusion workshop (optical + SAR + LiDAR)\nExperiment with generative models for data augmentation\nExplore vision-language models for scene understanding\nProject progress check-ins and technical consultations\n\n\n\n\n\nImplementing multi-modal fusion architectures\nWorking with generative adversarial networks\nUsing pre-trained vision-language models\nEvaluating synthetic vs. real data quality\n\n\n\n\nBuilding a multi-modal classifier combining Sentinel-1 and Sentinel-2 data\n\n\n\n\nCLIP for Remote Sensing\nMulti-modal learning in Earth observation survey\nGenerative models for geospatial data tutorial\n\n\n\n\nWeek 5 provides dedicated time for semi-independent project work with optional proposal workshops."
  },
  {
    "objectID": "course-materials/week2.html",
    "href": "course-materials/week2.html",
    "title": "Week 2: Working with Geospatial Data and Model Outputs",
    "section": "",
    "text": "This week focuses on the practical aspects of working with geospatial data and understanding outputs from pre-trained foundation models.\n\n\n\nMaster geospatial data formats and preprocessing pipelines\nUnderstand foundation model input requirements and output interpretations\nLearn to work with multi-spectral and multi-temporal data\nDevelop skills in data visualization and analysis workflows\n\n\n\n\n\nData Formats: GeoTIFF, NetCDF, Zarr, Cloud Optimized GeoTIFF (COG)\nPreprocessing Pipelines: Normalization, tiling, temporal alignment, cloud masking\nModel I/O: Input preparation, batch processing, output post-processing\nVisualization: Interactive maps, time series plots, embedding spaces\n\n\n\n\n\nHands-on data preprocessing workshop\nWorking with satellite imagery from different sensors (Landsat, Sentinel, MODIS)\nExtracting and visualizing embeddings from pre-trained models\nBuilding data loading pipelines with TorchGeo\n\n\n\n\n\nGDAL and rasterio for geospatial data handling\nxarray for multi-dimensional data analysis\nTorchGeo for PyTorch-based geospatial workflows\nEarth Engine for large-scale data access\n\n\n\n\nBuilding an end-to-end pipeline from raw satellite data to foundation model inference\n\n\n\n\nTorchGeo Documentation\nEarth Engine Python API\nCloud Optimized GeoTIFF specification\n\n\n\n\nWeek 3 will cover fine-tuning foundation models for specific applications and developing initial project proposals."
  },
  {
    "objectID": "course-materials/week2.html#week-2-overview",
    "href": "course-materials/week2.html#week-2-overview",
    "title": "Week 2: Working with Geospatial Data and Model Outputs",
    "section": "",
    "text": "This week focuses on the practical aspects of working with geospatial data and understanding outputs from pre-trained foundation models.\n\n\n\nMaster geospatial data formats and preprocessing pipelines\nUnderstand foundation model input requirements and output interpretations\nLearn to work with multi-spectral and multi-temporal data\nDevelop skills in data visualization and analysis workflows\n\n\n\n\n\nData Formats: GeoTIFF, NetCDF, Zarr, Cloud Optimized GeoTIFF (COG)\nPreprocessing Pipelines: Normalization, tiling, temporal alignment, cloud masking\nModel I/O: Input preparation, batch processing, output post-processing\nVisualization: Interactive maps, time series plots, embedding spaces\n\n\n\n\n\nHands-on data preprocessing workshop\nWorking with satellite imagery from different sensors (Landsat, Sentinel, MODIS)\nExtracting and visualizing embeddings from pre-trained models\nBuilding data loading pipelines with TorchGeo\n\n\n\n\n\nGDAL and rasterio for geospatial data handling\nxarray for multi-dimensional data analysis\nTorchGeo for PyTorch-based geospatial workflows\nEarth Engine for large-scale data access\n\n\n\n\nBuilding an end-to-end pipeline from raw satellite data to foundation model inference\n\n\n\n\nTorchGeo Documentation\nEarth Engine Python API\nCloud Optimized GeoTIFF specification\n\n\n\n\nWeek 3 will cover fine-tuning foundation models for specific applications and developing initial project proposals."
  },
  {
    "objectID": "course-materials/week1.html",
    "href": "course-materials/week1.html",
    "title": "Week 1: Introduction to Geospatial Foundation Models",
    "section": "",
    "text": "This week introduces the fundamental concepts of geospatial foundation models (GFMs) and explores their revolutionary applications in remote sensing and environmental monitoring.\n\n\n\nUnderstand what geospatial foundation models are and how they differ from traditional ML approaches\nExplore key architectures and training approaches for GFMs\nExamine real-world applications and case studies\nReview the current landscape of available models and platforms\n\n\n\n\n\nFoundation Model Fundamentals: Self-supervised learning, masked autoencoders, transformer architectures\nGeospatial Adaptations: Handling multi-spectral data, spatial relationships, temporal sequences\nAvailable Models: Prithvi, SatMAE, GeoFM, and other state-of-the-art models\nApplication Domains: Land cover classification, change detection, crop monitoring, disaster response\n\n\n\n\n\nInteractive demo with pre-trained geospatial foundation models\nHands-on exploration of model architectures\nDiscussion of project ideas and feasibility\n\n\n\n\n\nPrithvi Foundation Model\nSatMAE Paper and Implementation\nReading: “Foundation Models for Earth Observation” survey paper\n\n\n\n\nIntroduction to working with pre-trained GFMs using HuggingFace and PyTorch\n\n\n\nWeek 2 will dive into working with geospatial data formats and understanding pre-trained model outputs."
  },
  {
    "objectID": "course-materials/week1.html#week-1-overview",
    "href": "course-materials/week1.html#week-1-overview",
    "title": "Week 1: Introduction to Geospatial Foundation Models",
    "section": "",
    "text": "This week introduces the fundamental concepts of geospatial foundation models (GFMs) and explores their revolutionary applications in remote sensing and environmental monitoring.\n\n\n\nUnderstand what geospatial foundation models are and how they differ from traditional ML approaches\nExplore key architectures and training approaches for GFMs\nExamine real-world applications and case studies\nReview the current landscape of available models and platforms\n\n\n\n\n\nFoundation Model Fundamentals: Self-supervised learning, masked autoencoders, transformer architectures\nGeospatial Adaptations: Handling multi-spectral data, spatial relationships, temporal sequences\nAvailable Models: Prithvi, SatMAE, GeoFM, and other state-of-the-art models\nApplication Domains: Land cover classification, change detection, crop monitoring, disaster response\n\n\n\n\n\nInteractive demo with pre-trained geospatial foundation models\nHands-on exploration of model architectures\nDiscussion of project ideas and feasibility\n\n\n\n\n\nPrithvi Foundation Model\nSatMAE Paper and Implementation\nReading: “Foundation Models for Earth Observation” survey paper\n\n\n\n\nIntroduction to working with pre-trained GFMs using HuggingFace and PyTorch\n\n\n\nWeek 2 will dive into working with geospatial data formats and understanding pre-trained model outputs."
  },
  {
    "objectID": "course-materials/resources/course_resources.html",
    "href": "course-materials/resources/course_resources.html",
    "title": "GeoAI Course Resources",
    "section": "",
    "text": "Handbook of Geospatial Artificial Intelligence (Gao, Hu, Li, 2023) – Comprehensive reference with 30+ topics, code examples, datasets.\nDeep Learning for the Earth Sciences (Camps-Valls, Tuia, Zhu, 2021) – Earth data meets deep learning; ideal for understanding EO models.\nGeoAI and Human Geography (Xiao Huang, 2025) – Connects GeoAI with environmental and human systems.\n\n\n\n\n\nBoutayeb et al., 2024 – A comprehensive survey of GeoAI methods and applications."
  },
  {
    "objectID": "course-materials/resources/course_resources.html#essential-background-and-foundational-resources",
    "href": "course-materials/resources/course_resources.html#essential-background-and-foundational-resources",
    "title": "GeoAI Course Resources",
    "section": "",
    "text": "Handbook of Geospatial Artificial Intelligence (Gao, Hu, Li, 2023) – Comprehensive reference with 30+ topics, code examples, datasets.\nDeep Learning for the Earth Sciences (Camps-Valls, Tuia, Zhu, 2021) – Earth data meets deep learning; ideal for understanding EO models.\nGeoAI and Human Geography (Xiao Huang, 2025) – Connects GeoAI with environmental and human systems.\n\n\n\n\n\nBoutayeb et al., 2024 – A comprehensive survey of GeoAI methods and applications."
  },
  {
    "objectID": "course-materials/resources/course_resources.html#week-by-week-resources",
    "href": "course-materials/resources/course_resources.html#week-by-week-resources",
    "title": "GeoAI Course Resources",
    "section": "Week-by-Week Resources",
    "text": "Week-by-Week Resources\n\nWeek 1: Introduction to Geospatial Foundation Models\n\n🌐 Development Seed Blog (2024): Using Foundation Models for Earth Observation\n🚀 NASA/IBM Release: Prithvi HLS Foundation Model\n☁️ AWS ML Blog (2025): Deploying GeoFMs on AWS\n\n\n\nWeek 2: Working with Geospatial Data\n\n📚 Book: Learning Geospatial Analysis with Python (4th ed., 2023)\n🧰 TorchGeo Docs: https://pytorch.org/geo\n🌍 OpenGeoAI Toolkit: https://github.com/opengeos/geoai\n\n\n\nWeek 3: Loading & Using Foundation Models\n\n🤗 Hugging Face Prithvi Models: https://huggingface.co/ibm-nasa-geospatial\n📓 Demo Notebooks: Available on Hugging Face model cards.\n🧪 AWS GeoFM Deployment Examples: Code and pipelines via GitHub.\n🧠 IEEE GRSS Blog: Self-Supervised Geospatial Backbones\n\n\n\nWeek 4: Multi-modal & Generative Models\n\n🛰️ Sensor Fusion via TorchGeo: Stack Sentinel-1/2 & others.\n🌐 Presto, Clay, DOFA Models: Explained in DevSeed & AWS blogs.\n🧪 DiffusionSat Example: Medium Tutorial on Generating EO Images\n\n\n\nWeek 5: Model Adaptation & Evaluation\n\n🔌 Adapters for GeoFM: Explained via Development Seed blog.\n📊 Evaluation Metrics: IoU, mAP, F1 from Boutayeb et al. (2024) + SpaceNet.\n📁 Radiant Earth MLHub: https://mlhub.earth – Public geospatial ML datasets and benchmarks."
  },
  {
    "objectID": "course-materials/resources/course_resources.html#interactive-sessions",
    "href": "course-materials/resources/course_resources.html#interactive-sessions",
    "title": "GeoAI Course Resources",
    "section": "Interactive Sessions",
    "text": "Interactive Sessions\n\n1️⃣ PyTorch & TorchGeo Fundamentals\n\nTorchGeo docs & sample loaders (CRS, tile sampling, multispectral).\n\n\n\n2️⃣ Loading & Using Foundation Models\n\nHugging Face GeoFM models (Prithvi, Clay) and notebook demos.\n\n\n\n3️⃣ Multi-modal & Sensor Fusion\n\nDOFA & Clay examples; stacking Sentinel data with TorchGeo.\n\n\n\n4️⃣ Model Adaptation & Evaluation\n\nFine-tuning using TerraTorch and evaluation on public datasets.\n\n\n\n5️⃣ Scalable Analysis & Deployment\n\n📚 Geospatial Data Analytics on AWS (2023)\n☁️ AWS SageMaker + GeoFM repo (scalable inference)"
  },
  {
    "objectID": "course-materials/resources/course_resources.html#general-tools-repos",
    "href": "course-materials/resources/course_resources.html#general-tools-repos",
    "title": "GeoAI Course Resources",
    "section": "📦 General Tools & Repos",
    "text": "📦 General Tools & Repos\n\n🔧 OpenGeoAI: https://github.com/opengeos/geoai\n🛰️ IBM/NASA TerraTorch: https://github.com/ibm/terrapytorch\n📍 Radiant MLHub Datasets: https://mlhub.earth\n🧪 SpaceNet Challenges: https://spacenet.ai/challenges/"
  },
  {
    "objectID": "course-materials/resources/course_resources.html#deployment-and-project-resources",
    "href": "course-materials/resources/course_resources.html#deployment-and-project-resources",
    "title": "GeoAI Course Resources",
    "section": "🧭 Deployment and Project Resources",
    "text": "🧭 Deployment and Project Resources\n\n🔧 **Flask/Streamlit for D"
  },
  {
    "objectID": "course-materials/projects/project-application-template.html",
    "href": "course-materials/projects/project-application-template.html",
    "title": "Project Application Template",
    "section": "",
    "text": "Student Name: [Your Name]\nEmail: [Your Email]\nDate: [Submission Date]\n\n\nPlease describe your past experience with:\nRemote Sensing and Geospatial Data (2-3 sentences): - [Describe your experience with satellite imagery, GIS, remote sensing tools, etc.]\nMachine Learning and Deep Learning (2-3 sentences): - [Describe your experience with Python, PyTorch, model training, etc.]\nSpecific Tools and Platforms (1-2 sentences): - [Mention experience with Earth Engine, QGIS, ArcGIS, cloud platforms, etc.]\n\n\n\nApplication Domain (2-3 sentences): - [Describe the specific application area you’re interested in exploring] - [Examples: crop monitoring, deforestation detection, disaster response, urban planning, etc.]\nResearch Questions or Goals (2-3 sentences): - [What specific questions do you hope to answer or problems to solve?] - [What would successful outcomes look like for your project?]\n\n\n\nAvailable Data (1-2 sentences): - [Do you have access to specific datasets, ground truth data, or study areas?] - [If not, describe what types of data you would need to acquire]\nComputational Resources (1 sentence): - [Describe your current access to computational resources, cloud credits, etc.]\n\n\n\nInitial Ideas (2-3 sentences): - [Any preliminary ideas about which foundation models or techniques to explore?] - [This can be very preliminary - you’ll refine throughout the course]\n\n\n\nWhat you hope to learn (2-3 sentences): - [What specific skills or knowledge do you want to gain from this course?] - [How does this fit into your broader academic or career goals?]"
  },
  {
    "objectID": "course-materials/projects/project-application-template.html#project-application-due-week-0",
    "href": "course-materials/projects/project-application-template.html#project-application-due-week-0",
    "title": "Project Application Template",
    "section": "",
    "text": "Student Name: [Your Name]\nEmail: [Your Email]\nDate: [Submission Date]\n\n\nPlease describe your past experience with:\nRemote Sensing and Geospatial Data (2-3 sentences): - [Describe your experience with satellite imagery, GIS, remote sensing tools, etc.]\nMachine Learning and Deep Learning (2-3 sentences): - [Describe your experience with Python, PyTorch, model training, etc.]\nSpecific Tools and Platforms (1-2 sentences): - [Mention experience with Earth Engine, QGIS, ArcGIS, cloud platforms, etc.]\n\n\n\nApplication Domain (2-3 sentences): - [Describe the specific application area you’re interested in exploring] - [Examples: crop monitoring, deforestation detection, disaster response, urban planning, etc.]\nResearch Questions or Goals (2-3 sentences): - [What specific questions do you hope to answer or problems to solve?] - [What would successful outcomes look like for your project?]\n\n\n\nAvailable Data (1-2 sentences): - [Do you have access to specific datasets, ground truth data, or study areas?] - [If not, describe what types of data you would need to acquire]\nComputational Resources (1 sentence): - [Describe your current access to computational resources, cloud credits, etc.]\n\n\n\nInitial Ideas (2-3 sentences): - [Any preliminary ideas about which foundation models or techniques to explore?] - [This can be very preliminary - you’ll refine throughout the course]\n\n\n\nWhat you hope to learn (2-3 sentences): - [What specific skills or knowledge do you want to gain from this course?] - [How does this fit into your broader academic or career goals?]"
  },
  {
    "objectID": "course-materials/lectures/lecture6_cloud_computing.html",
    "href": "course-materials/lectures/lecture6_cloud_computing.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "course-materials/lectures/lecture4_multimodal.html",
    "href": "course-materials/lectures/lecture4_multimodal.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "course-materials/lectures/lecture2_geospatial_data.html",
    "href": "course-materials/lectures/lecture2_geospatial_data.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "course-materials/interactive-sessions/session5_deployment.html",
    "href": "course-materials/interactive-sessions/session5_deployment.html",
    "title": "Session 5: Scalable Analysis & Deployment",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/interactive-sessions/session3_multimodal.html",
    "href": "course-materials/interactive-sessions/session3_multimodal.html",
    "title": "Session 3: Multi-modal & Sensor Fusion",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/interactive-sessions/session1_pytorch_torchgeo.html",
    "href": "course-materials/interactive-sessions/session1_pytorch_torchgeo.html",
    "title": "Session 1: PyTorch & TorchGeo Fundamentals",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/cheatsheets/torchgeo_basics.html",
    "href": "course-materials/cheatsheets/torchgeo_basics.html",
    "title": "TorchGeo Datasets & Transforms",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/cheatsheets/matplotlib_geospatial.html",
    "href": "course-materials/cheatsheets/matplotlib_geospatial.html",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/cheatsheets/folium_basics.html",
    "href": "course-materials/cheatsheets/folium_basics.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/cheatsheets/earth_engine_basics.html",
    "href": "course-materials/cheatsheets/earth_engine_basics.html",
    "title": "Earth Engine Basics",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#geospatial-data-fundamentals",
    "href": "cheatsheets.html#geospatial-data-fundamentals",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#pytorch-for-geospatial-ai",
    "href": "cheatsheets.html#pytorch-for-geospatial-ai",
    "title": "Cheatsheets",
    "section": "🔥 PyTorch for Geospatial AI",
    "text": "🔥 PyTorch for Geospatial AI\n\nPyTorch Tensors & GPU Operations\nTorchGeo Datasets & Transforms\nData Loading for Satellite Imagery"
  },
  {
    "objectID": "cheatsheets.html#foundation-models-huggingface",
    "href": "cheatsheets.html#foundation-models-huggingface",
    "title": "Cheatsheets",
    "section": "🤗 Foundation Models & HuggingFace",
    "text": "🤗 Foundation Models & HuggingFace\n\nLoading Pre-trained Models\nModel Inference & Feature Extraction\nFine-tuning Strategies"
  },
  {
    "objectID": "cheatsheets.html#visualization-analysis",
    "href": "cheatsheets.html#visualization-analysis",
    "title": "Cheatsheets",
    "section": "📊 Visualization & Analysis",
    "text": "📊 Visualization & Analysis\n\nPlotting Satellite Imagery\nInteractive Maps with Folium\nGeospatial Plotting with Matplotlib"
  },
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "Syllabus.html#geog-288kc-geospatial-foundation-models-and-applications",
    "href": "Syllabus.html#geog-288kc-geospatial-foundation-models-and-applications",
    "title": "",
    "section": "GEOG 288KC: Geospatial Foundation Models and Applications",
    "text": "GEOG 288KC: Geospatial Foundation Models and Applications\nFall 2025\nFridays 9am-12pm + optional lab office hours Fridays 2pm-5pm\n\n\nCourse Overview\nThis course introduces students to state-of-the-art geospatial foundation models (GFMs) for remote sensing and environmental monitoring. Through a mix of lecture, discussion, and hands-on labs, students will explore frontier techniques in geospatial AI—including self-supervised learning, masked autoencoders, multimodal sensor fusion, and scalable inference pipelines—while developing their own applied project.\n\n\n\nPrerequisites\n\nStudents should have some experience with remote sensing, geospatial data, or ML (e.g., Python, Earth Engine, PyTorch).\nBecause this is a project-driven seminar, students should have a topic area in which they are interested in applying GeoFMs.\n\n\n\n\nApplications\nTo apply, students should submit a paragraph at the form link below describing their past experience with remote sensing, geospatial data, and ML, as well as their interest in Geospatial Foundation Models. They should also describe their application of interest. The more clearly defined the potential project goal and any existing fine-tuning data the better, though students will also have the opportunity to refine their ideas throughout the course as we learn more about GeoFMs and what they are capable of.\nhttps://forms.gle/Q1iDp2kuZuX1avMPA\n\n\n—\n\n\nTentative weekly Topics\n\nWeek 0: Getting set up for geospatial AI in the UCSB AI Sandbox + Project Applications\n\nWeek 1: Introduction to geospatial foundation models and their applications\nWeek 2: Working with geospatial data and pretrained model outputs\n\nWeek 3: Fine-tuning foundation models and early project ideation + Project Proposals\n\nWeek 4: Multi-modal and generative models for remote sensing\n\nWeek 5: Semi-independent project work and optional proposal workshops\n\nWeek 6: Model adaptation, efficient fine-tuning, and evaluation strategies\n\nWeek 7: Scalable analysis pipelines using Earth Engine, TorchGeo, and cloud tools + Initial MVPs\n\nWeek 8: Building lightweight APIs and applications for model inference and visualization\n\nWeek 9: Project workshops and synthesis\n\nWeek 10: Final project presentations and discussion of future directions\n\n\n\n\nDeliverables\n\nProject Application (Week 0): 1-paragraph summary of past experience and application interest area\nProject Proposal (Week 3): brief plan + feasibility check\nInitial MVP (Week 7): First implementation of project workflow and tests/examples\nFinal Presentation (Week 10): 10–15 min demo + reflection\nOptional: Submit your project to Hugging Face or GitHub for broader visibility\n\n\n\n\nGrading\nThis course will be assessed on a pass/fail basis. Passing requires consistent attendance and participation and submission of all deliverables."
  },
  {
    "objectID": "course-materials/cheatsheets/dataloader_satellite.html",
    "href": "course-materials/cheatsheets/dataloader_satellite.html",
    "title": "Data Loading for Satellite Imagery",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/cheatsheets/finetuning_basics.html",
    "href": "course-materials/cheatsheets/finetuning_basics.html",
    "title": "Fine-tuning Strategies",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/cheatsheets/loading_models.html",
    "href": "course-materials/cheatsheets/loading_models.html",
    "title": "Loading Pre-trained Models",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/cheatsheets/model_inference.html",
    "href": "course-materials/cheatsheets/model_inference.html",
    "title": "Model Inference & Feature Extraction",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/cheatsheets/xarray_basics.html",
    "href": "course-materials/cheatsheets/xarray_basics.html",
    "title": "Xarray for Multi-dimensional Data",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/interactive-sessions/session2_foundation_models.html",
    "href": "course-materials/interactive-sessions/session2_foundation_models.html",
    "title": "Session 2: Loading & Using Foundation Models",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/interactive-sessions/session4_adaptation.html",
    "href": "course-materials/interactive-sessions/session4_adaptation.html",
    "title": "Session 4: Model Adaptation & Evaluation",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/lectures/lecture1_architectures.html",
    "href": "course-materials/lectures/lecture1_architectures.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "course-materials/lectures/lecture3_finetuning.html",
    "href": "course-materials/lectures/lecture3_finetuning.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "course-materials/lectures/lecture5_evaluation.html",
    "href": "course-materials/lectures/lecture5_evaluation.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "course-materials/projects/mvp-template.html",
    "href": "course-materials/projects/mvp-template.html",
    "title": "Initial MVP Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Your Project Title]\nPresentation Date: [Date]\n\n\nProblem Recap (30 seconds): - [Brief reminder of the problem you’re solving]\nCurrent Status (1 minute): - [What have you accomplished so far?] - [What is working in your current implementation?]\n\n\n\nSystem Architecture:\n[Include a simple diagram or flowchart of your system]\nData Input → Preprocessing → Model → Post-processing → Output\nKey Components Implemented: - [x] Data loading and preprocessing pipeline - [x] Model setup and configuration\n- [x] Basic training/fine-tuning workflow - [ ] Advanced evaluation metrics - [ ] Visualization dashboard - [ ] Scalable deployment\nTechnology Stack: - Data Processing: [e.g., Earth Engine, rasterio, xarray] - Model Framework: [e.g., PyTorch, TorchGeo, HuggingFace] - Visualization: [e.g., Matplotlib, Folium, Plotly] - Infrastructure: [e.g., UCSB AI Sandbox, local development]\n\n\n\nDemo Script (prepare talking points for each step):\n\nData Loading Demo (1 minute):\n\n[Show loading and preprocessing of your specific dataset]\n[Highlight any data challenges you’ve solved]\n\nModel Inference Demo (2 minutes):\n\n[Run your model on sample data]\n[Show inputs and outputs clearly]\n\nResults Visualization (1-2 minutes):\n\n[Display results in an intuitive format]\n[Compare with baselines or ground truth if available]\n\n\n\n\n\nQuantitative Results: - Dataset Size: [Number of samples, spatial/temporal coverage] - Model Performance: [Key metrics with numbers] - Processing Time: [Inference speed, training time]\nExample Results Table: | Metric | Baseline | Your Model | Notes | |——–|———-|————|——-| | Accuracy | XX% | YY% | [Context] | | F1-Score | XX | YY | [Context] | | Processing Speed | XX sec | YY sec | [Context] |\nQualitative Observations: - [What patterns or insights have you discovered?] - [What works well? What doesn’t work as expected?]\n\n\n\nTechnical Challenges Overcome: 1. Challenge: [Specific technical problem] Solution: [How you solved it]\n\nChallenge: [Another problem] Solution: [Your approach]\n\nCurrent Limitations: - [What are the current limitations of your system?] - [What assumptions are you making?]\n\n\n\nImmediate Goals (Week 8): - [ ] [Specific technical improvement] - [ ] [Additional evaluation or validation] - [ ] [User interface or API development]\nFinal Implementation Goals (Week 9-10): - [ ] [Advanced features or optimizations] - [ ] [Comprehensive evaluation and comparison] - [ ] [Documentation and reproducibility]\nRisks and Contingencies: - [What could go wrong? How will you adapt?]\n\n\n\nSpecific Areas Where You’d Like Input: 1. [Technical question about approach or implementation] 2. [Question about evaluation methodology]\n3. [Question about scope or next steps]\nOpen Discussion: - [Any other areas where peer feedback would be valuable]"
  },
  {
    "objectID": "course-materials/projects/mvp-template.html#initial-mvp-presentation-due-week-7",
    "href": "course-materials/projects/mvp-template.html#initial-mvp-presentation-due-week-7",
    "title": "Initial MVP Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Your Project Title]\nPresentation Date: [Date]\n\n\nProblem Recap (30 seconds): - [Brief reminder of the problem you’re solving]\nCurrent Status (1 minute): - [What have you accomplished so far?] - [What is working in your current implementation?]\n\n\n\nSystem Architecture:\n[Include a simple diagram or flowchart of your system]\nData Input → Preprocessing → Model → Post-processing → Output\nKey Components Implemented: - [x] Data loading and preprocessing pipeline - [x] Model setup and configuration\n- [x] Basic training/fine-tuning workflow - [ ] Advanced evaluation metrics - [ ] Visualization dashboard - [ ] Scalable deployment\nTechnology Stack: - Data Processing: [e.g., Earth Engine, rasterio, xarray] - Model Framework: [e.g., PyTorch, TorchGeo, HuggingFace] - Visualization: [e.g., Matplotlib, Folium, Plotly] - Infrastructure: [e.g., UCSB AI Sandbox, local development]\n\n\n\nDemo Script (prepare talking points for each step):\n\nData Loading Demo (1 minute):\n\n[Show loading and preprocessing of your specific dataset]\n[Highlight any data challenges you’ve solved]\n\nModel Inference Demo (2 minutes):\n\n[Run your model on sample data]\n[Show inputs and outputs clearly]\n\nResults Visualization (1-2 minutes):\n\n[Display results in an intuitive format]\n[Compare with baselines or ground truth if available]\n\n\n\n\n\nQuantitative Results: - Dataset Size: [Number of samples, spatial/temporal coverage] - Model Performance: [Key metrics with numbers] - Processing Time: [Inference speed, training time]\nExample Results Table: | Metric | Baseline | Your Model | Notes | |——–|———-|————|——-| | Accuracy | XX% | YY% | [Context] | | F1-Score | XX | YY | [Context] | | Processing Speed | XX sec | YY sec | [Context] |\nQualitative Observations: - [What patterns or insights have you discovered?] - [What works well? What doesn’t work as expected?]\n\n\n\nTechnical Challenges Overcome: 1. Challenge: [Specific technical problem] Solution: [How you solved it]\n\nChallenge: [Another problem] Solution: [Your approach]\n\nCurrent Limitations: - [What are the current limitations of your system?] - [What assumptions are you making?]\n\n\n\nImmediate Goals (Week 8): - [ ] [Specific technical improvement] - [ ] [Additional evaluation or validation] - [ ] [User interface or API development]\nFinal Implementation Goals (Week 9-10): - [ ] [Advanced features or optimizations] - [ ] [Comprehensive evaluation and comparison] - [ ] [Documentation and reproducibility]\nRisks and Contingencies: - [What could go wrong? How will you adapt?]\n\n\n\nSpecific Areas Where You’d Like Input: 1. [Technical question about approach or implementation] 2. [Question about evaluation methodology]\n3. [Question about scope or next steps]\nOpen Discussion: - [Any other areas where peer feedback would be valuable]"
  },
  {
    "objectID": "course-materials/projects/mvp-template.html#mvp-demonstration-checklist",
    "href": "course-materials/projects/mvp-template.html#mvp-demonstration-checklist",
    "title": "Initial MVP Template",
    "section": "MVP Demonstration Checklist",
    "text": "MVP Demonstration Checklist\n\nBefore Your Presentation:\n\nTest all code and demos on presentation computer\nPrepare backup slides in case live demo fails\nTime your presentation (aim for 10-12 minutes + 3-5 for Q&A)\nHave clear talking points for each section\nSave sample outputs/screenshots as backup\n\n\n\nDuring Presentation:\n\nSpeak clearly and at appropriate pace\nShow your screen clearly for demos\nEngage with questions and feedback\nStay within time limits\nBe honest about current limitations\n\n\n\nAfter Presentation:\n\nTake notes on feedback received\nUpdate project plan based on suggestions\nSchedule follow-up consultations if needed"
  },
  {
    "objectID": "course-materials/projects/project-proposal-template.html",
    "href": "course-materials/projects/project-proposal-template.html",
    "title": "Project Proposal Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Descriptive Project Title]\nDate: [Submission Date]\n\n\nProblem Description (3-4 sentences): - [Clearly describe the problem you’re addressing] - [Why is this problem important? What is the broader significance?] - [What are the current limitations or gaps in existing approaches?]\nResearch Objectives (2-3 bullet points): - [Specific, measurable goals for your project] - [What specific outcomes will you deliver?]\n\n\n\nPrimary Dataset(s): - Source: [e.g., Landsat, Sentinel, MODIS, custom dataset] - Spatial Coverage: [Geographic extent] - Temporal Coverage: [Time period] - Resolution: [Spatial and temporal resolution] - Size: [Approximate data volume]\nGround Truth/Labels (if applicable): - [Description of labeled data for training/validation] - [Source and quality of labels]\nData Availability Assessment: - [x] Data is readily accessible - [ ] Data requires special access/permissions - [ ] Data needs to be collected/generated - [ ] Alternative data sources identified\n\n\n\nFoundation Model Selection: - Primary Model: [e.g., Prithvi, SatMAE, custom architecture] - Justification: [Why is this model appropriate for your problem?] - Baseline Comparisons: [What will you compare against?]\nMethodology Overview (4-5 sentences): - [High-level description of your approach] - [Key technical components and workflow] - [How will you adapt/fine-tune the foundation model?]\nKey Technical Components: - [ ] Data preprocessing pipeline - [ ] Model fine-tuning/adaptation - [ ] Evaluation framework - [ ] Visualization/interpretation tools - [ ] Scalable inference pipeline\n\n\n\nWeek 4-5: [Specific goals and deliverables] Week 6: [Specific goals and deliverables]\nWeek 7 (MVP): [What will your MVP demonstration include?] Week 8-9: [Final implementation goals] Week 10: [Final presentation preparation]\n\n\n\nPerformance Metrics: - [How will you measure success? Specific metrics] - [Quantitative measures: accuracy, F1, IoU, etc.] - [Qualitative measures: interpretability, usability, etc.]\nValidation Approach: - [Cross-validation strategy] - [Train/validation/test splits] - [Spatial/temporal validation considerations]\n\n\n\nTechnical Risks and Mitigation: - Risk 1: [Description] → Mitigation: [How you’ll address it] - Risk 2: [Description] → Mitigation: [How you’ll address it]\nResource Requirements: - Computational: [GPU hours, memory requirements, storage] - Data: [Download time, storage space, preprocessing time] - Time: [Realistic assessment of scope given 10-week timeframe]\nBackup Plans: - [What will you do if primary approach doesn’t work?] - [How can you ensure you have a working system for final presentation?]\n\n\n\nPrimary Deliverables: - [What specific outputs will your project produce?] - [Code repository, trained models, analysis results, etc.]\nPotential Impact: - [How could this work be used in practice?] - [What are the broader implications of success?]\nFuture Work: - [How could this project be extended beyond the course?] - [What are the next logical steps?]\n\n\n\nKey Literature (3-5 papers): - [List relevant papers that inform your approach]\nSoftware/Tools: - [List key libraries, platforms, tools you’ll use]\nAdditional Resources: - [Any other resources, datasets, collaborations, etc.]"
  },
  {
    "objectID": "course-materials/projects/project-proposal-template.html#project-proposal-due-week-3",
    "href": "course-materials/projects/project-proposal-template.html#project-proposal-due-week-3",
    "title": "Project Proposal Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Descriptive Project Title]\nDate: [Submission Date]\n\n\nProblem Description (3-4 sentences): - [Clearly describe the problem you’re addressing] - [Why is this problem important? What is the broader significance?] - [What are the current limitations or gaps in existing approaches?]\nResearch Objectives (2-3 bullet points): - [Specific, measurable goals for your project] - [What specific outcomes will you deliver?]\n\n\n\nPrimary Dataset(s): - Source: [e.g., Landsat, Sentinel, MODIS, custom dataset] - Spatial Coverage: [Geographic extent] - Temporal Coverage: [Time period] - Resolution: [Spatial and temporal resolution] - Size: [Approximate data volume]\nGround Truth/Labels (if applicable): - [Description of labeled data for training/validation] - [Source and quality of labels]\nData Availability Assessment: - [x] Data is readily accessible - [ ] Data requires special access/permissions - [ ] Data needs to be collected/generated - [ ] Alternative data sources identified\n\n\n\nFoundation Model Selection: - Primary Model: [e.g., Prithvi, SatMAE, custom architecture] - Justification: [Why is this model appropriate for your problem?] - Baseline Comparisons: [What will you compare against?]\nMethodology Overview (4-5 sentences): - [High-level description of your approach] - [Key technical components and workflow] - [How will you adapt/fine-tune the foundation model?]\nKey Technical Components: - [ ] Data preprocessing pipeline - [ ] Model fine-tuning/adaptation - [ ] Evaluation framework - [ ] Visualization/interpretation tools - [ ] Scalable inference pipeline\n\n\n\nWeek 4-5: [Specific goals and deliverables] Week 6: [Specific goals and deliverables]\nWeek 7 (MVP): [What will your MVP demonstration include?] Week 8-9: [Final implementation goals] Week 10: [Final presentation preparation]\n\n\n\nPerformance Metrics: - [How will you measure success? Specific metrics] - [Quantitative measures: accuracy, F1, IoU, etc.] - [Qualitative measures: interpretability, usability, etc.]\nValidation Approach: - [Cross-validation strategy] - [Train/validation/test splits] - [Spatial/temporal validation considerations]\n\n\n\nTechnical Risks and Mitigation: - Risk 1: [Description] → Mitigation: [How you’ll address it] - Risk 2: [Description] → Mitigation: [How you’ll address it]\nResource Requirements: - Computational: [GPU hours, memory requirements, storage] - Data: [Download time, storage space, preprocessing time] - Time: [Realistic assessment of scope given 10-week timeframe]\nBackup Plans: - [What will you do if primary approach doesn’t work?] - [How can you ensure you have a working system for final presentation?]\n\n\n\nPrimary Deliverables: - [What specific outputs will your project produce?] - [Code repository, trained models, analysis results, etc.]\nPotential Impact: - [How could this work be used in practice?] - [What are the broader implications of success?]\nFuture Work: - [How could this project be extended beyond the course?] - [What are the next logical steps?]\n\n\n\nKey Literature (3-5 papers): - [List relevant papers that inform your approach]\nSoftware/Tools: - [List key libraries, platforms, tools you’ll use]\nAdditional Resources: - [Any other resources, datasets, collaborations, etc.]"
  },
  {
    "objectID": "course-materials/week0.html",
    "href": "course-materials/week0.html",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/week0.html#week-0-overview",
    "href": "course-materials/week0.html#week-0-overview",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/week10.html",
    "href": "course-materials/week10.html",
    "title": "Week 10: Final Presentations",
    "section": "",
    "text": "The culminating week of the course featuring final project presentations, demonstrations, and forward-looking discussions about the future of geospatial foundation models.\n\n\n\nPresent completed projects with professional-quality demonstrations\nEvaluate and provide constructive feedback on peer projects\nIdentify connections between different project approaches and applications\nEnvision future developments in geospatial foundation models\n\n\n\n\nSession 1 (9am-10:30am): Project Presentations Group A - 5-6 presentations, 10-15 minutes each including Q&A - Live demonstrations and technical deep-dives - Peer evaluation and feedback forms\nBreak (10:30am-10:45am)\nSession 2 (10:45am-12pm): Project Presentations Group B + Synthesis - 5-6 presentations, 10-15 minutes each including Q&A - Course synthesis and future directions discussion - Celebration and reflection on learning journey\n\n\n\nFinal Presentation (Week 10) Each presentation should include: - Problem and Motivation (2-3 minutes): Clear problem statement and significance - Technical Approach (3-4 minutes): Methodology, model architecture, key innovations - Live Demonstration (4-5 minutes): Working system with real examples - Results and Impact (2-3 minutes): Quantitative results, validation, broader implications - Reflection and Future Work (1-2 minutes): Lessons learned, next steps - Q&A Session (3-5 minutes): Peer questions and discussion\n\n\n\n\nTechnical Quality: Implementation completeness, methodology soundness\nInnovation: Novel approaches, creative problem-solving, technical depth\nCommunication: Clarity, organization, visual aids, demonstration quality\nImpact: Practical significance, broader applications, contribution to field\n\n\n\n\n\nEmerging Trends: Latest developments in foundation model architectures\nApplication Domains: Unexplored areas and promising applications\nTechnical Challenges: Scalability, interpretability, robustness\nSocietal Impact: Environmental monitoring, disaster response, climate science\nFuture Research: Open problems, collaboration opportunities\n\n\n\n\nHugging Face/GitHub Submission: - Students may optionally submit polished projects to public repositories - Opportunity for broader visibility and community engagement - Instructor support for preparing submission-quality documentation\nConference/Workshop Submissions: - Guidance on adapting projects for academic conferences - Identification of relevant venues and submission opportunities - Support for preparing extended abstracts or full papers\n\n\n\n\nProject Repository: Complete, documented codebase with examples\nFinal Report: Comprehensive technical document (5-10 pages)\nPresentation Materials: Slides and any supporting materials\nPeer Evaluations: Constructive feedback on all peer presentations\n\n\n\n\n\nIndividual reflection essays on key learnings and growth\nCourse feedback and suggestions for future iterations\nIdentification of personal next steps and continued learning goals\n\n\n\n\n\nPresentation templates and guidelines\nProject submission checklists\nConference and journal venue lists\nNetworking and collaboration opportunities\n\n\n\n\nRecognition of outstanding projects and contributions to course community!"
  },
  {
    "objectID": "course-materials/week10.html#week-10-overview",
    "href": "course-materials/week10.html#week-10-overview",
    "title": "Week 10: Final Presentations",
    "section": "",
    "text": "The culminating week of the course featuring final project presentations, demonstrations, and forward-looking discussions about the future of geospatial foundation models.\n\n\n\nPresent completed projects with professional-quality demonstrations\nEvaluate and provide constructive feedback on peer projects\nIdentify connections between different project approaches and applications\nEnvision future developments in geospatial foundation models\n\n\n\n\nSession 1 (9am-10:30am): Project Presentations Group A - 5-6 presentations, 10-15 minutes each including Q&A - Live demonstrations and technical deep-dives - Peer evaluation and feedback forms\nBreak (10:30am-10:45am)\nSession 2 (10:45am-12pm): Project Presentations Group B + Synthesis - 5-6 presentations, 10-15 minutes each including Q&A - Course synthesis and future directions discussion - Celebration and reflection on learning journey\n\n\n\nFinal Presentation (Week 10) Each presentation should include: - Problem and Motivation (2-3 minutes): Clear problem statement and significance - Technical Approach (3-4 minutes): Methodology, model architecture, key innovations - Live Demonstration (4-5 minutes): Working system with real examples - Results and Impact (2-3 minutes): Quantitative results, validation, broader implications - Reflection and Future Work (1-2 minutes): Lessons learned, next steps - Q&A Session (3-5 minutes): Peer questions and discussion\n\n\n\n\nTechnical Quality: Implementation completeness, methodology soundness\nInnovation: Novel approaches, creative problem-solving, technical depth\nCommunication: Clarity, organization, visual aids, demonstration quality\nImpact: Practical significance, broader applications, contribution to field\n\n\n\n\n\nEmerging Trends: Latest developments in foundation model architectures\nApplication Domains: Unexplored areas and promising applications\nTechnical Challenges: Scalability, interpretability, robustness\nSocietal Impact: Environmental monitoring, disaster response, climate science\nFuture Research: Open problems, collaboration opportunities\n\n\n\n\nHugging Face/GitHub Submission: - Students may optionally submit polished projects to public repositories - Opportunity for broader visibility and community engagement - Instructor support for preparing submission-quality documentation\nConference/Workshop Submissions: - Guidance on adapting projects for academic conferences - Identification of relevant venues and submission opportunities - Support for preparing extended abstracts or full papers\n\n\n\n\nProject Repository: Complete, documented codebase with examples\nFinal Report: Comprehensive technical document (5-10 pages)\nPresentation Materials: Slides and any supporting materials\nPeer Evaluations: Constructive feedback on all peer presentations\n\n\n\n\n\nIndividual reflection essays on key learnings and growth\nCourse feedback and suggestions for future iterations\nIdentification of personal next steps and continued learning goals\n\n\n\n\n\nPresentation templates and guidelines\nProject submission checklists\nConference and journal venue lists\nNetworking and collaboration opportunities\n\n\n\n\nRecognition of outstanding projects and contributions to course community!"
  },
  {
    "objectID": "course-materials/week3.html",
    "href": "course-materials/week3.html",
    "title": "Week 3: Fine-tuning Foundation Models",
    "section": "",
    "text": "This week covers techniques for fine-tuning geospatial foundation models for specific applications, along with developing concrete project proposals.\n\n\n\nUnderstand different fine-tuning strategies for foundation models\nLearn parameter-efficient fine-tuning techniques (LoRA, adapters)\nDevelop skills in transfer learning for geospatial applications\nFormulate detailed project proposals with clear objectives\n\n\n\n\n\nFine-tuning Strategies: Full fine-tuning vs. parameter-efficient methods\nTransfer Learning: Domain adaptation, few-shot learning, zero-shot capabilities\nModel Architecture Modifications: Adding task-specific heads, multi-task learning\nTraining Considerations: Data requirements, computational resources, evaluation metrics\n\n\n\n\n\nHands-on fine-tuning workshop with sample datasets\nProject ideation sessions and feasibility discussions\nTechnical review of project proposals\nGroup feedback and refinement sessions\n\n\n\n\n\nImplementing LoRA and adapter-based fine-tuning\nSetting up training pipelines with PyTorch Lightning\nModel evaluation and validation strategies\nManaging computational resources in UCSB AI Sandbox\n\n\n\n\nFine-tuning a pre-trained model for land cover classification\n\n\n\nProject Proposal (Due: End of Week 3) - Clear problem statement and research objectives - Dataset description and availability assessment - Technical approach and model selection rationale - Timeline and milestone planning - Feasibility analysis and risk mitigation\n\n\n\n\nLoRA Implementation Examples\nParameter-Efficient Fine-Tuning survey paper\nPyTorch Lightning documentation\n\n\n\n\nWeek 4 will explore multi-modal and generative models for remote sensing applications."
  },
  {
    "objectID": "course-materials/week3.html#week-3-overview",
    "href": "course-materials/week3.html#week-3-overview",
    "title": "Week 3: Fine-tuning Foundation Models",
    "section": "",
    "text": "This week covers techniques for fine-tuning geospatial foundation models for specific applications, along with developing concrete project proposals.\n\n\n\nUnderstand different fine-tuning strategies for foundation models\nLearn parameter-efficient fine-tuning techniques (LoRA, adapters)\nDevelop skills in transfer learning for geospatial applications\nFormulate detailed project proposals with clear objectives\n\n\n\n\n\nFine-tuning Strategies: Full fine-tuning vs. parameter-efficient methods\nTransfer Learning: Domain adaptation, few-shot learning, zero-shot capabilities\nModel Architecture Modifications: Adding task-specific heads, multi-task learning\nTraining Considerations: Data requirements, computational resources, evaluation metrics\n\n\n\n\n\nHands-on fine-tuning workshop with sample datasets\nProject ideation sessions and feasibility discussions\nTechnical review of project proposals\nGroup feedback and refinement sessions\n\n\n\n\n\nImplementing LoRA and adapter-based fine-tuning\nSetting up training pipelines with PyTorch Lightning\nModel evaluation and validation strategies\nManaging computational resources in UCSB AI Sandbox\n\n\n\n\nFine-tuning a pre-trained model for land cover classification\n\n\n\nProject Proposal (Due: End of Week 3) - Clear problem statement and research objectives - Dataset description and availability assessment - Technical approach and model selection rationale - Timeline and milestone planning - Feasibility analysis and risk mitigation\n\n\n\n\nLoRA Implementation Examples\nParameter-Efficient Fine-Tuning survey paper\nPyTorch Lightning documentation\n\n\n\n\nWeek 4 will explore multi-modal and generative models for remote sensing applications."
  },
  {
    "objectID": "course-materials/week5.html",
    "href": "course-materials/week5.html",
    "title": "Week 5: Model Adaptation and Evaluation",
    "section": "",
    "text": "Focus on advanced model adaptation techniques and robust evaluation methodologies for geospatial foundation models.\n\n\n\nMaster efficient fine-tuning techniques for large models\nDesign comprehensive evaluation frameworks\nUnderstand model interpretability and explainability methods\nLearn best practices for model validation in geospatial contexts\n\n\n\n\n\nEfficient Fine-tuning: QLoRA, AdaLoRA, prompt tuning, in-context learning\nEvaluation Strategies: Cross-validation, spatial validation, temporal validation\nInterpretability: Attention visualization, feature attribution, saliency maps\nRobustness Testing: Out-of-distribution performance, adversarial examples\n\n\n\n\n\nImplement advanced fine-tuning techniques for computational efficiency\nDesign evaluation protocols specific to geospatial applications\nApply interpretability methods to understand model decisions\nConduct robustness analysis and failure mode investigation\n\n\n\n\n\nQLoRA implementation for memory-efficient fine-tuning\nSpatial cross-validation techniques\nGradCAM and attention visualization for satellite imagery\nStatistical significance testing for model comparisons\n\n\n\n\nComprehensive model evaluation workshop: from metrics to interpretability\n\n\n\n\nPerformance Metrics: Accuracy, F1, IoU, spatial consistency\nValidation Strategies: Leave-one-region-out, temporal splits\nInterpretability Analysis: Feature importance, decision boundaries\nRobustness Assessment: Domain shift, seasonal variations, sensor differences\n\n\n\n\n\nQLoRA Paper and Implementation\nSpatial validation techniques for Earth observation\nModel interpretability tools and libraries\n\n\n\n\nWeek 6 will cover scalable analysis pipelines using Earth Engine, TorchGeo, and cloud tools for production deployment."
  },
  {
    "objectID": "course-materials/week5.html#week-5-overview",
    "href": "course-materials/week5.html#week-5-overview",
    "title": "Week 5: Model Adaptation and Evaluation",
    "section": "",
    "text": "Focus on advanced model adaptation techniques and robust evaluation methodologies for geospatial foundation models.\n\n\n\nMaster efficient fine-tuning techniques for large models\nDesign comprehensive evaluation frameworks\nUnderstand model interpretability and explainability methods\nLearn best practices for model validation in geospatial contexts\n\n\n\n\n\nEfficient Fine-tuning: QLoRA, AdaLoRA, prompt tuning, in-context learning\nEvaluation Strategies: Cross-validation, spatial validation, temporal validation\nInterpretability: Attention visualization, feature attribution, saliency maps\nRobustness Testing: Out-of-distribution performance, adversarial examples\n\n\n\n\n\nImplement advanced fine-tuning techniques for computational efficiency\nDesign evaluation protocols specific to geospatial applications\nApply interpretability methods to understand model decisions\nConduct robustness analysis and failure mode investigation\n\n\n\n\n\nQLoRA implementation for memory-efficient fine-tuning\nSpatial cross-validation techniques\nGradCAM and attention visualization for satellite imagery\nStatistical significance testing for model comparisons\n\n\n\n\nComprehensive model evaluation workshop: from metrics to interpretability\n\n\n\n\nPerformance Metrics: Accuracy, F1, IoU, spatial consistency\nValidation Strategies: Leave-one-region-out, temporal splits\nInterpretability Analysis: Feature importance, decision boundaries\nRobustness Assessment: Domain shift, seasonal variations, sensor differences\n\n\n\n\n\nQLoRA Paper and Implementation\nSpatial validation techniques for Earth observation\nModel interpretability tools and libraries\n\n\n\n\nWeek 6 will cover scalable analysis pipelines using Earth Engine, TorchGeo, and cloud tools for production deployment."
  },
  {
    "objectID": "course-materials/week7.html",
    "href": "course-materials/week7.html",
    "title": "Week 7: Core Development and Implementation",
    "section": "",
    "text": "Intensive development week focused on implementing core project functionality with structured technical workshops, peer collaboration, and regular progress check-ins.\n\n\n\nImplement core model training, fine-tuning, or inference workflows\nEstablish robust evaluation and validation methodologies\nBuild scalable data processing and analysis pipelines\nCreate preliminary results and validate technical approaches\n\n\n\n\n\nModel Implementation: Training loops, fine-tuning strategies, inference optimization\nData Workflows: Efficient processing, feature engineering, quality validation\nEvaluation Frameworks: Metrics implementation, validation strategies, result analysis\nSystem Integration: Component interaction, error handling, performance monitoring\n\n\n\n\n\nImplement core model functionality (training/fine-tuning/inference)\nComplete data preprocessing and feature engineering pipeline\nEstablish evaluation methodology and initial validation results\nCreate visualization and result analysis workflows\nConduct technical code review and optimization\n\n\n\n\nTechnical Workshops (9am-10:30am): - Monday/Wednesday: Advanced debugging and optimization strategies - Tuesday/Thursday: Model evaluation and validation techniques - All sessions include peer code reviews and collaborative problem-solving\nDevelopment Time (10:30am-12pm): - Independent implementation work with instructor availability - Small group collaboration and knowledge sharing - Technical consultation and troubleshooting sessions - Progress documentation and milestone tracking\n\n\n\nWorking System Demonstration (Due: End of Week 7) - Live Demo (5-7 minutes): Functional system with real examples - Technical Documentation: Code structure, key design decisions, performance characteristics - Validation Results: Preliminary performance metrics and analysis - Progress Report: Completed milestones, current challenges, revised timeline\n\n\n\n\nDevelopment Best Practices: Code organization, testing frameworks, documentation\nPerformance Monitoring: Computational efficiency, resource utilization, bottleneck identification\nVersion Control: Branching strategies, collaborative workflows, milestone tracking\nEnvironment Management: Reproducible setups, dependency management, deployment preparation\n\n\n\n\n\nStructured code review sessions with feedback protocols\nKnowledge sharing presentations on technical solutions\nCollaborative debugging and problem-solving workshops\nCross-project learning and technique exchange\n\n\n\n\n\nAdvanced development pattern guides\nModel optimization and debugging tools\nPerformance profiling and monitoring utilities\nCollaborative development workflows\n\n\n\n\nWeek 8 will focus on project refinement, advanced features, and scalability improvements."
  },
  {
    "objectID": "course-materials/week7.html#week-7-overview",
    "href": "course-materials/week7.html#week-7-overview",
    "title": "Week 7: Core Development and Implementation",
    "section": "",
    "text": "Intensive development week focused on implementing core project functionality with structured technical workshops, peer collaboration, and regular progress check-ins.\n\n\n\nImplement core model training, fine-tuning, or inference workflows\nEstablish robust evaluation and validation methodologies\nBuild scalable data processing and analysis pipelines\nCreate preliminary results and validate technical approaches\n\n\n\n\n\nModel Implementation: Training loops, fine-tuning strategies, inference optimization\nData Workflows: Efficient processing, feature engineering, quality validation\nEvaluation Frameworks: Metrics implementation, validation strategies, result analysis\nSystem Integration: Component interaction, error handling, performance monitoring\n\n\n\n\n\nImplement core model functionality (training/fine-tuning/inference)\nComplete data preprocessing and feature engineering pipeline\nEstablish evaluation methodology and initial validation results\nCreate visualization and result analysis workflows\nConduct technical code review and optimization\n\n\n\n\nTechnical Workshops (9am-10:30am): - Monday/Wednesday: Advanced debugging and optimization strategies - Tuesday/Thursday: Model evaluation and validation techniques - All sessions include peer code reviews and collaborative problem-solving\nDevelopment Time (10:30am-12pm): - Independent implementation work with instructor availability - Small group collaboration and knowledge sharing - Technical consultation and troubleshooting sessions - Progress documentation and milestone tracking\n\n\n\nWorking System Demonstration (Due: End of Week 7) - Live Demo (5-7 minutes): Functional system with real examples - Technical Documentation: Code structure, key design decisions, performance characteristics - Validation Results: Preliminary performance metrics and analysis - Progress Report: Completed milestones, current challenges, revised timeline\n\n\n\n\nDevelopment Best Practices: Code organization, testing frameworks, documentation\nPerformance Monitoring: Computational efficiency, resource utilization, bottleneck identification\nVersion Control: Branching strategies, collaborative workflows, milestone tracking\nEnvironment Management: Reproducible setups, dependency management, deployment preparation\n\n\n\n\n\nStructured code review sessions with feedback protocols\nKnowledge sharing presentations on technical solutions\nCollaborative debugging and problem-solving workshops\nCross-project learning and technique exchange\n\n\n\n\n\nAdvanced development pattern guides\nModel optimization and debugging tools\nPerformance profiling and monitoring utilities\nCollaborative development workflows\n\n\n\n\nWeek 8 will focus on project refinement, advanced features, and scalability improvements."
  },
  {
    "objectID": "course-materials/week9.html",
    "href": "course-materials/week9.html",
    "title": "Week 9: Project Workshops and Synthesis",
    "section": "",
    "text": "Dedicated time for final project development with structured workshops to synthesize course concepts and prepare for final presentations.\n\n\n\nIntegrate all course concepts into cohesive final projects\nRefine project implementations based on feedback and testing\nPrepare comprehensive project documentation and presentations\nSynthesize lessons learned and identify future research directions\n\n\n\n\nWorkshop Sessions (9am-11am): - Project refinement and polishing workshop - Presentation preparation and storytelling techniques - Technical documentation and reproducibility best practices - Synthesis discussion: Challenges and opportunities in GeoAI\nIndividual Work Time (11am-12pm): - Final implementation and testing - Documentation completion - Presentation materials preparation - Peer review and feedback sessions\n\n\n\n\nComplete final project implementation and testing\nPrepare comprehensive project documentation\nCreate presentation materials and demo recordings\nConduct peer code reviews and provide feedback\nSynthesize key learnings and identify future work\n\n\n\n\n\nTechnical Documentation: Code repository, installation instructions, usage examples\nProject Report: Problem statement, methodology, results, discussion\nPresentation Materials: Slides, demo video, live demonstration\nReproducibility Package: Data, code, environment specifications\n\n\n\n\n\nTechnical Challenges: Common pitfalls, lessons learned, best practices\nResearch Opportunities: Identified gaps, future directions, open problems\nReal-world Applications: Deployment considerations, user needs, impact assessment\nEthical Considerations: Bias, fairness, environmental impact, data privacy\n\n\n\n\nExtended support for: - Final debugging and optimization - Advanced visualization and result analysis - Presentation rehearsals and feedback - Repository organization and documentation\n\n\n\n\nCode review partnerships for quality assurance\nPractice presentations with constructive feedback\nCross-project learning and knowledge sharing\nCollaborative problem-solving for technical challenges\n\n\n\n\n\nProject documentation templates\nPresentation guidelines and rubrics\nCode review checklists\nFinal project submission requirements\n\n\n\n\nWeek 10 will feature final project presentations and discussion of future directions in geospatial foundation models."
  },
  {
    "objectID": "course-materials/week9.html#week-9-overview",
    "href": "course-materials/week9.html#week-9-overview",
    "title": "Week 9: Project Workshops and Synthesis",
    "section": "",
    "text": "Dedicated time for final project development with structured workshops to synthesize course concepts and prepare for final presentations.\n\n\n\nIntegrate all course concepts into cohesive final projects\nRefine project implementations based on feedback and testing\nPrepare comprehensive project documentation and presentations\nSynthesize lessons learned and identify future research directions\n\n\n\n\nWorkshop Sessions (9am-11am): - Project refinement and polishing workshop - Presentation preparation and storytelling techniques - Technical documentation and reproducibility best practices - Synthesis discussion: Challenges and opportunities in GeoAI\nIndividual Work Time (11am-12pm): - Final implementation and testing - Documentation completion - Presentation materials preparation - Peer review and feedback sessions\n\n\n\n\nComplete final project implementation and testing\nPrepare comprehensive project documentation\nCreate presentation materials and demo recordings\nConduct peer code reviews and provide feedback\nSynthesize key learnings and identify future work\n\n\n\n\n\nTechnical Documentation: Code repository, installation instructions, usage examples\nProject Report: Problem statement, methodology, results, discussion\nPresentation Materials: Slides, demo video, live demonstration\nReproducibility Package: Data, code, environment specifications\n\n\n\n\n\nTechnical Challenges: Common pitfalls, lessons learned, best practices\nResearch Opportunities: Identified gaps, future directions, open problems\nReal-world Applications: Deployment considerations, user needs, impact assessment\nEthical Considerations: Bias, fairness, environmental impact, data privacy\n\n\n\n\nExtended support for: - Final debugging and optimization - Advanced visualization and result analysis - Presentation rehearsals and feedback - Repository organization and documentation\n\n\n\n\nCode review partnerships for quality assurance\nPractice presentations with constructive feedback\nCross-project learning and knowledge sharing\nCollaborative problem-solving for technical challenges\n\n\n\n\n\nProject documentation templates\nPresentation guidelines and rubrics\nCode review checklists\nFinal project submission requirements\n\n\n\n\nWeek 10 will feature final project presentations and discussion of future directions in geospatial foundation models."
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html",
    "href": "course-materials/cheatsheets/pytorch_tensors.html",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "",
    "text": "Code\nimport torch\nimport numpy as np\n\n# From Python lists\ntensor_from_list = torch.tensor([[1, 2], [3, 4]])\nprint(f\"From list: {tensor_from_list}\")\n\n# From NumPy arrays\nnumpy_array = np.array([[1, 2], [3, 4]])\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(f\"From numpy: {tensor_from_numpy}\")\n\n# Common initialization patterns\nzeros_tensor = torch.zeros(3, 3)\nones_tensor = torch.ones(2, 4)\nrandom_tensor = torch.randn(2, 3)  # Normal distribution\n\nprint(f\"Zeros: {zeros_tensor}\")\nprint(f\"Random: {random_tensor}\")\n\n\nFrom list: tensor([[1, 2],\n        [3, 4]])\nFrom numpy: tensor([[1, 2],\n        [3, 4]])\nZeros: tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nRandom: tensor([[ 0.7016,  0.0190,  0.9809],\n        [-0.8297, -0.5010,  1.3637]])\n\n\n\n\n\n\n\nCode\n# Check tensor properties\nsample_tensor = torch.randn(3, 4, 5)\n\nprint(f\"Shape: {sample_tensor.shape}\")\nprint(f\"Size: {sample_tensor.size()}\")\nprint(f\"Data type: {sample_tensor.dtype}\")\nprint(f\"Device: {sample_tensor.device}\")\nprint(f\"Number of dimensions: {sample_tensor.ndim}\")\nprint(f\"Total elements: {sample_tensor.numel()}\")\n\n\nShape: torch.Size([3, 4, 5])\nSize: torch.Size([3, 4, 5])\nData type: torch.float32\nDevice: cpu\nNumber of dimensions: 3\nTotal elements: 60"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#creating-tensors",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#creating-tensors",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "",
    "text": "Code\nimport torch\nimport numpy as np\n\n# From Python lists\ntensor_from_list = torch.tensor([[1, 2], [3, 4]])\nprint(f\"From list: {tensor_from_list}\")\n\n# From NumPy arrays\nnumpy_array = np.array([[1, 2], [3, 4]])\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(f\"From numpy: {tensor_from_numpy}\")\n\n# Common initialization patterns\nzeros_tensor = torch.zeros(3, 3)\nones_tensor = torch.ones(2, 4)\nrandom_tensor = torch.randn(2, 3)  # Normal distribution\n\nprint(f\"Zeros: {zeros_tensor}\")\nprint(f\"Random: {random_tensor}\")\n\n\nFrom list: tensor([[1, 2],\n        [3, 4]])\nFrom numpy: tensor([[1, 2],\n        [3, 4]])\nZeros: tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nRandom: tensor([[ 0.7016,  0.0190,  0.9809],\n        [-0.8297, -0.5010,  1.3637]])\n\n\n\n\n\n\n\nCode\n# Check tensor properties\nsample_tensor = torch.randn(3, 4, 5)\n\nprint(f\"Shape: {sample_tensor.shape}\")\nprint(f\"Size: {sample_tensor.size()}\")\nprint(f\"Data type: {sample_tensor.dtype}\")\nprint(f\"Device: {sample_tensor.device}\")\nprint(f\"Number of dimensions: {sample_tensor.ndim}\")\nprint(f\"Total elements: {sample_tensor.numel()}\")\n\n\nShape: torch.Size([3, 4, 5])\nSize: torch.Size([3, 4, 5])\nData type: torch.float32\nDevice: cpu\nNumber of dimensions: 3\nTotal elements: 60"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#gpu-operations",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#gpu-operations",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "GPU Operations",
    "text": "GPU Operations\n\nCheck GPU availability\n\n\nCode\n# Check if CUDA is available\ncuda_available = torch.cuda.is_available()\nprint(f\"CUDA available: {cuda_available}\")\n\nif cuda_available:\n    gpu_count = torch.cuda.device_count()\n    gpu_name = torch.cuda.get_device_name(0)\n    print(f\"Number of GPUs: {gpu_count}\")\n    print(f\"GPU name: {gpu_name}\")\nelse:\n    print(\"Running on CPU\")\n\n\nCUDA available: False\nRunning on CPU\n\n\n\n\nMoving tensors to GPU\n\n\nCode\n# Create a tensor\ncpu_tensor = torch.randn(3, 3)\nprint(f\"Original device: {cpu_tensor.device}\")\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpu_tensor = cpu_tensor.to(device)\nprint(f\"After moving to device: {gpu_tensor.device}\")\n\n# Alternative methods\nif torch.cuda.is_available():\n    # Method 2: .cuda() method\n    gpu_tensor2 = cpu_tensor.cuda()\n    print(f\"Using .cuda(): {gpu_tensor2.device}\")\n    \n    # Method 3: Create directly on GPU\n    direct_gpu = torch.randn(3, 3, device='cuda')\n    print(f\"Created on GPU: {direct_gpu.device}\")\n\n\nOriginal device: cpu\nAfter moving to device: cpu"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#basic-tensor-operations",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#basic-tensor-operations",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\n\nMathematical operations\n\n\nCode\n# Create sample tensors\na = torch.randn(3, 3)\nb = torch.randn(3, 3)\n\n# Element-wise operations\naddition = a + b\nsubtraction = a - b  \nmultiplication = a * b\ndivision = a / b\n\nprint(f\"Addition shape: {addition.shape}\")\nprint(f\"Subtraction mean: {subtraction.mean():.3f}\")\n\n# Matrix operations\nmatmul = torch.matmul(a, b)  # Matrix multiplication\ntranspose = a.t()  # Transpose\n\nprint(f\"Matrix multiplication shape: {matmul.shape}\")\nprint(f\"Transpose shape: {transpose.shape}\")\n\n\nAddition shape: torch.Size([3, 3])\nSubtraction mean: -0.073\nMatrix multiplication shape: torch.Size([3, 3])\nTranspose shape: torch.Size([3, 3])\n\n\n\n\nUseful tensor methods\n\n\nCode\n# Statistical operations\ndata = torch.randn(4, 5)\n\nprint(f\"Mean: {data.mean():.3f}\")\nprint(f\"Standard deviation: {data.std():.3f}\")\nprint(f\"Min: {data.min():.3f}\")\nprint(f\"Max: {data.max():.3f}\")\n\n# Reshaping\nreshaped = data.view(2, 10)  # Reshape to 2x10\nflattened = data.flatten()   # Flatten to 1D\n\nprint(f\"Original shape: {data.shape}\")\nprint(f\"Reshaped: {reshaped.shape}\")\nprint(f\"Flattened: {flattened.shape}\")\n\n\nMean: -0.380\nStandard deviation: 1.090\nMin: -2.545\nMax: 1.585\nOriginal shape: torch.Size([4, 5])\nReshaped: torch.Size([2, 10])\nFlattened: torch.Size([20])"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#memory-management",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#memory-management",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Memory Management",
    "text": "Memory Management\n\nGPU memory monitoring\n\n\nCode\nif torch.cuda.is_available():\n    # Check current memory usage\n    allocated = torch.cuda.memory_allocated() / (1024**2)  # Convert to MB\n    reserved = torch.cuda.memory_reserved() / (1024**2)\n    \n    print(f\"GPU memory allocated: {allocated:.1f} MB\")\n    print(f\"GPU memory reserved: {reserved:.1f} MB\")\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    print(\"GPU cache cleared\")\nelse:\n    print(\"GPU memory monitoring not available (running on CPU)\")\n\n\nGPU memory monitoring not available (running on CPU)\n\n\n\n\nBest practices\n\n\nCode\n# Use context managers for temporary operations\ndef memory_efficient_operation():\n    with torch.no_grad():  # Disable gradient computation\n        large_tensor = torch.randn(1000, 1000)\n        result = large_tensor.mean()\n        return result\n\nresult = memory_efficient_operation()\nprint(f\"Result: {result:.3f}\")\n\n# The large_tensor is automatically garbage collected\n\n\nResult: -0.001"
  },
  {
    "objectID": "course-materials/cheatsheets/pytorch_tensors.html#converting-between-formats",
    "href": "course-materials/cheatsheets/pytorch_tensors.html#converting-between-formats",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Converting Between Formats",
    "text": "Converting Between Formats\n\nPyTorch ↔︎ NumPy\n\n\nCode\n# PyTorch to NumPy\ntorch_tensor = torch.randn(2, 3)\nnumpy_array = torch_tensor.numpy()  # Only works on CPU tensors\n\nprint(f\"PyTorch tensor: {torch_tensor}\")\nprint(f\"NumPy array: {numpy_array}\")\n\n# NumPy to PyTorch\nnew_torch = torch.from_numpy(numpy_array)\nprint(f\"Back to PyTorch: {new_torch}\")\n\n\nPyTorch tensor: tensor([[-1.9366,  0.4574, -0.2636],\n        [ 0.3120,  0.1357, -1.4863]])\nNumPy array: [[-1.9365865   0.45741966 -0.26364863]\n [ 0.31196123  0.13574891 -1.4862579 ]]\nBack to PyTorch: tensor([[-1.9366,  0.4574, -0.2636],\n        [ 0.3120,  0.1357, -1.4863]])\n\n\n\n\nHandling GPU tensors\n\n\nCode\nif torch.cuda.is_available():\n    # GPU tensor must be moved to CPU first\n    gpu_tensor = torch.randn(2, 3).cuda()\n    cpu_numpy = gpu_tensor.cpu().numpy()\n    print(f\"GPU tensor converted to NumPy: {cpu_numpy.shape}\")\nelse:\n    print(\"GPU conversion example not available (running on CPU)\")\n\n\nGPU conversion example not available (running on CPU)"
  }
]