[
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Building Geospatial Foundation Models",
    "section": "Course Description",
    "text": "Course Description\nThis project-driven seminar teaches students to build geospatial foundation models (GFMs) from scratch. Students implement every layer of the pipeline—from data pipelines and tokenization through attention mechanisms, full architectures, pretraining, evaluation, and deployment—culminating in a working end-to-end GFM tailored to a chosen geospatial application.\nBy the end of the course, students will be able to:\n\nDesign and implement geospatial data pipelines for multi-spectral, spatial, and temporal data\nBuild attention mechanisms and assemble transformer-based architectures for geospatial inputs\nPretrain using masked autoencoding and evaluate learned representations\nFine-tune models for specific Earth observation tasks\nDeploy models via APIs and interactive interfaces with honest performance analysis"
  },
  {
    "objectID": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "href": "index.html#getting-started-with-the-ucsb-ai-sandbox",
    "title": "Building Geospatial Foundation Models",
    "section": "Getting Started with the UCSB AI Sandbox",
    "text": "Getting Started with the UCSB AI Sandbox\nHere are detailed instructions for setting up your environment on the UCSB AI Sandbox, including foundation model installation and GPU optimization."
  },
  {
    "objectID": "index.html#course-structure-3-stages-10-weeks",
    "href": "index.html#course-structure-3-stages-10-weeks",
    "title": "Building Geospatial Foundation Models",
    "section": "Course Structure: 3 Stages, 10 Weeks",
    "text": "Course Structure: 3 Stages, 10 Weeks\n\n\n\n\n\nflowchart TD\n    subgraph Stage1 [\"🏗️ Stage 1: Build GFM Architecture\"]\n        direction LR\n        W1[\"📊&lt;br/&gt;Week 1&lt;br/&gt;Data Foundations&lt;br/&gt;Pipelines & Tokenization\"] --&gt; W2[\"🧠&lt;br/&gt;Week 2&lt;br/&gt;Attention Mechanisms&lt;br/&gt;Spatial-Temporal Focus\"]\n        W2 --&gt; W3[\"🏛️&lt;br/&gt;Week 3&lt;br/&gt;Complete Architecture&lt;br/&gt;Vision Transformer\"]\n    end\n    \n    subgraph Stage2 [\"🚀 Stage 2: Train Foundation Model\"]\n        direction LR\n        W4[\"🎭&lt;br/&gt;Week 4&lt;br/&gt;Pretraining&lt;br/&gt;Masked Autoencoder\"] --&gt; W5[\"⚡&lt;br/&gt;Week 5&lt;br/&gt;Training Optimization&lt;br/&gt;Stability & Efficiency\"]\n        W5 --&gt; W6[\"📈&lt;br/&gt;Week 6&lt;br/&gt;Evaluation & Analysis&lt;br/&gt;Embeddings & Probing\"]\n        W6 --&gt; W7[\"🔗&lt;br/&gt;Week 7&lt;br/&gt;Model Integration&lt;br/&gt;Prithvi, SatMAE\"]\n    end\n    \n    subgraph Stage3 [\"🎯 Stage 3: Apply & Deploy\"]\n        direction LR\n        W8[\"🎯&lt;br/&gt;Week 8&lt;br/&gt;Fine-tuning&lt;br/&gt;Task-Specific Training\"] --&gt; W9[\"🚀&lt;br/&gt;Week 9&lt;br/&gt;Deployment&lt;br/&gt;APIs & Interfaces\"]\n        W9 --&gt; W10[\"🎤&lt;br/&gt;Week 10&lt;br/&gt;Presentations&lt;br/&gt;Project Synthesis\"]\n    end\n    \n    Stage1 --&gt; Stage2\n    Stage2 --&gt; Stage3\n    \n    style Stage1 fill:#e3f2fd\n    style Stage2 fill:#fff3e0  \n    style Stage3 fill:#e8f5e8\n    style W1 fill:#bbdefb\n    style W4 fill:#ffe0b2\n    style W8 fill:#c8e6c8\n\n\n\n\n\n\n\n🏗️ Stage 1: Build GFM Architecture (Weeks 1-3)\n\nWeek 1: Geospatial Data Foundations (data pipelines, tokenization, loaders)\nWeek 2: Spatial-Temporal Attention Mechanisms (from-scratch implementation)\nWeek 3: Complete GFM Architecture (Vision Transformer for geospatial)\n\n\n\n🚀 Stage 2: Train a Foundation Model (Weeks 4-7)\n\nWeek 4: Pretraining Implementation (masked autoencoder)\nWeek 5: Training Loop Optimization (stability, efficiency, mixed precision)\nWeek 6: Model Evaluation & Analysis (embeddings, probing, reconstructions)\nWeek 7: Integration with Existing Models (Prithvi, SatMAE)\n\n\n\n🎯 Stage 3: Apply & Deploy (Weeks 8-10)\n\nWeek 8: Task-Specific Fine-tuning (efficient strategies, few-shot)\nWeek 9: Model Implementation & Deployment (APIs, UI, benchmarking)\nWeek 10: Project Presentations & Synthesis\n\nQuick links:\n\nWeekly materials: see navbar → 🗓️ weekly materials\nInteractive sessions: see navbar → 💻 interactive sessions"
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "Building Geospatial Foundation Models",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\nInstructor\n\n\n\n\n\n\n\nKelly Caylor\nEmail: caylor@ucsb.edu\nLearn more: Bren profile\n\n\n\n\nTA\n\n\n\n\n\n\n\nAnna Boser\nEmail: anaboser@ucsb.edu\nLearn more: Bren profile"
  },
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#geospatial-data-fundamentals",
    "href": "cheatsheets.html#geospatial-data-fundamentals",
    "title": "Cheatsheets",
    "section": "",
    "text": "Working with Rasterio\nEarth Engine Basics\nXarray for Multi-dimensional Data"
  },
  {
    "objectID": "cheatsheets.html#pytorch-for-geospatial-ai",
    "href": "cheatsheets.html#pytorch-for-geospatial-ai",
    "title": "Cheatsheets",
    "section": "🔥 PyTorch for Geospatial AI",
    "text": "🔥 PyTorch for Geospatial AI\n\nPyTorch Tensors & GPU Operations\nTorchGeo Datasets & Transforms\nData Loading for Satellite Imagery"
  },
  {
    "objectID": "cheatsheets.html#foundation-models-huggingface",
    "href": "cheatsheets.html#foundation-models-huggingface",
    "title": "Cheatsheets",
    "section": "🤗 Foundation Models & HuggingFace",
    "text": "🤗 Foundation Models & HuggingFace\n\nLoading Pre-trained Models\nModel Inference & Feature Extraction\nFine-tuning Strategies"
  },
  {
    "objectID": "cheatsheets.html#visualization-analysis",
    "href": "cheatsheets.html#visualization-analysis",
    "title": "Cheatsheets",
    "section": "📊 Visualization & Analysis",
    "text": "📊 Visualization & Analysis\n\nPlotting Satellite Imagery\nInteractive Maps with Folium\nGeospatial Plotting with Matplotlib"
  },
  {
    "objectID": "course-materials/week8.html",
    "href": "course-materials/week8.html",
    "title": "Real-World Application",
    "section": "",
    "text": "This week adapts foundation models for specific tasks using efficient fine-tuning strategies.\n\n\n\nAdapt foundation models for specific tasks\nImplement efficient fine-tuning strategies\nHandle limited labeled data\nEvaluate task performance\n\n\n\n\n\nFull Fine-tuning vs. Parameter-efficient: LoRA and adapter strategies\nFew-shot Learning: Working with limited labeled data\nTask-specific Data: Preparation and augmentation strategies\nMulti-task Learning: Joint training for multiple objectives\nFine-tuning Evaluation: Task-specific performance metrics\n\n\n\n\n\nImplement LoRA and adapter-based fine-tuning\nPrepare task-specific datasets\nCompare full vs. parameter-efficient fine-tuning\nEvaluate few-shot learning capabilities\nOptimize for limited data scenarios\n\n\n\n\nSession 8: Task-Specific Fine-tuning - Efficient fine-tuning strategies and performance optimization\n\n\n\n\nOptimization and feature development work\nSystem testing and validation across scenarios\nDocumentation and user guide creation\nFinal debugging and system refinement\n\n\n\n\nFine-tuned model for chosen application with performance analysis and comparison\n\n\n\n\nComputational Efficiency: Algorithm optimization, parallel processing, memory management\nSystem Architecture: Modular design, component interaction, API development\nUser Experience: Interface improvements, workflow automation, result visualization\nDeployment Readiness: Configuration management, environment portability, monitoring\n\n\n\n\n\nDistributed computing and cloud deployment patterns\nModel serving and API development best practices\nAdvanced visualization and interactive dashboard creation\nSystem monitoring, logging, and error tracking implementation\n\n\n\n\n\nPerformance optimization guides and profiling tools\nAdvanced deployment patterns and containerization\nScalable architecture design principles\nSystem monitoring and observability frameworks\n\n\n\n\nWeek 9 will focus on final project polish, comprehensive documentation, and presentation preparation."
  },
  {
    "objectID": "course-materials/week8.html#week-8-overview",
    "href": "course-materials/week8.html#week-8-overview",
    "title": "Real-World Application",
    "section": "",
    "text": "This week adapts foundation models for specific tasks using efficient fine-tuning strategies.\n\n\n\nAdapt foundation models for specific tasks\nImplement efficient fine-tuning strategies\nHandle limited labeled data\nEvaluate task performance\n\n\n\n\n\nFull Fine-tuning vs. Parameter-efficient: LoRA and adapter strategies\nFew-shot Learning: Working with limited labeled data\nTask-specific Data: Preparation and augmentation strategies\nMulti-task Learning: Joint training for multiple objectives\nFine-tuning Evaluation: Task-specific performance metrics\n\n\n\n\n\nImplement LoRA and adapter-based fine-tuning\nPrepare task-specific datasets\nCompare full vs. parameter-efficient fine-tuning\nEvaluate few-shot learning capabilities\nOptimize for limited data scenarios\n\n\n\n\nSession 8: Task-Specific Fine-tuning - Efficient fine-tuning strategies and performance optimization\n\n\n\n\nOptimization and feature development work\nSystem testing and validation across scenarios\nDocumentation and user guide creation\nFinal debugging and system refinement\n\n\n\n\nFine-tuned model for chosen application with performance analysis and comparison\n\n\n\n\nComputational Efficiency: Algorithm optimization, parallel processing, memory management\nSystem Architecture: Modular design, component interaction, API development\nUser Experience: Interface improvements, workflow automation, result visualization\nDeployment Readiness: Configuration management, environment portability, monitoring\n\n\n\n\n\nDistributed computing and cloud deployment patterns\nModel serving and API development best practices\nAdvanced visualization and interactive dashboard creation\nSystem monitoring, logging, and error tracking implementation\n\n\n\n\n\nPerformance optimization guides and profiling tools\nAdvanced deployment patterns and containerization\nScalable architecture design principles\nSystem monitoring and observability frameworks\n\n\n\n\nWeek 9 will focus on final project polish, comprehensive documentation, and presentation preparation."
  },
  {
    "objectID": "course-materials/week9.html",
    "href": "course-materials/week9.html",
    "title": "Real-World Application",
    "section": "",
    "text": "This week deploys models for production use, optimizes for inference, and builds user-friendly interfaces.\n\n\n\nDeploy models for production use\nOptimize models for inference\nBuild user-friendly interfaces\nDocument model capabilities\n\n\n\n\n\nModel Optimization: Quantization and optimization for inference\nAPI Development: Building inference APIs with FastAPI\nUser Interface: Creation of interactive web interfaces\nModel Documentation: Model cards and capability documentation\nPerformance Benchmarking: Speed and accuracy analysis\n\n\n\n\n\nOptimize model for efficient inference\nBuild deployment API with documentation\nCreate user interface for model interaction\nWrite comprehensive model cards\nBenchmark performance and document capabilities\n\n\n\n\nDeployable model with documentation: API, user interface, model card, and performance benchmarks\n\n\n\nSession 9: Model Implementation & Deployment - Real-time deployment challenges with live APIs and immediate feedback\n\n\n\nExtended support for: - Final debugging and optimization - Advanced visualization and result analysis - Presentation rehearsals and feedback - Repository organization and documentation\n\n\n\n\nCode review partnerships for quality assurance\nPractice presentations with constructive feedback\nCross-project learning and knowledge sharing\nCollaborative problem-solving for technical challenges\n\n\n\n\n\nProject documentation templates\nPresentation guidelines and rubrics\nCode review checklists\nFinal project submission requirements\n\n\n\n\nWeek 10 will feature final project presentations and discussion of future directions in geospatial foundation models."
  },
  {
    "objectID": "course-materials/week9.html#week-9-overview",
    "href": "course-materials/week9.html#week-9-overview",
    "title": "Real-World Application",
    "section": "",
    "text": "This week deploys models for production use, optimizes for inference, and builds user-friendly interfaces.\n\n\n\nDeploy models for production use\nOptimize models for inference\nBuild user-friendly interfaces\nDocument model capabilities\n\n\n\n\n\nModel Optimization: Quantization and optimization for inference\nAPI Development: Building inference APIs with FastAPI\nUser Interface: Creation of interactive web interfaces\nModel Documentation: Model cards and capability documentation\nPerformance Benchmarking: Speed and accuracy analysis\n\n\n\n\n\nOptimize model for efficient inference\nBuild deployment API with documentation\nCreate user interface for model interaction\nWrite comprehensive model cards\nBenchmark performance and document capabilities\n\n\n\n\nDeployable model with documentation: API, user interface, model card, and performance benchmarks\n\n\n\nSession 9: Model Implementation & Deployment - Real-time deployment challenges with live APIs and immediate feedback\n\n\n\nExtended support for: - Final debugging and optimization - Advanced visualization and result analysis - Presentation rehearsals and feedback - Repository organization and documentation\n\n\n\n\nCode review partnerships for quality assurance\nPractice presentations with constructive feedback\nCross-project learning and knowledge sharing\nCollaborative problem-solving for technical challenges\n\n\n\n\n\nProject documentation templates\nPresentation guidelines and rubrics\nCode review checklists\nFinal project submission requirements\n\n\n\n\nWeek 10 will feature final project presentations and discussion of future directions in geospatial foundation models."
  },
  {
    "objectID": "course-materials/week1.html",
    "href": "course-materials/week1.html",
    "title": "Foundation Building",
    "section": "",
    "text": "This week establishes the foundation for building geospatial foundation models by mastering data preparation and preprocessing pipelines.\n\n\n\nUnderstand geospatial data as foundation model input\nImplement robust data preprocessing pipelines\nHandle missing data (clouds, gaps) effectively\nCreate temporal sequences from satellite imagery\n\n\n\n\n\nGeospatial Tokenization: Convert satellite imagery to model-ready tokens\nMulti-spectral Data: Properties and preprocessing of different bands\nCloud Masking: Strategies for handling missing data\nTemporal Sequences: Building time series from satellite imagery\nData Loaders: Efficient batch processing for training\n\n\n\n\n\nBuild complete geospatial tokenization pipeline\nImplement cloud masking and missing data strategies\nCreate temporal sequence datasets\nOptimize data loading for large-scale training\n\n\n\n\n\nMulti-spectral satellite data properties\nCloud masking algorithms and implementations\nPyTorch data loading best practices\n\n\n\n\nSession 1: Geospatial Data Foundations - Hands-on implementation of geospatial data preprocessing pipeline\n\n\n\nComplete geospatial data pipeline capable of processing satellite imagery for foundation model training\n\n\n\nWeek 2 will implement spatial-temporal attention mechanisms from scratch."
  },
  {
    "objectID": "course-materials/week1.html#week-1-overview",
    "href": "course-materials/week1.html#week-1-overview",
    "title": "Foundation Building",
    "section": "",
    "text": "This week establishes the foundation for building geospatial foundation models by mastering data preparation and preprocessing pipelines.\n\n\n\nUnderstand geospatial data as foundation model input\nImplement robust data preprocessing pipelines\nHandle missing data (clouds, gaps) effectively\nCreate temporal sequences from satellite imagery\n\n\n\n\n\nGeospatial Tokenization: Convert satellite imagery to model-ready tokens\nMulti-spectral Data: Properties and preprocessing of different bands\nCloud Masking: Strategies for handling missing data\nTemporal Sequences: Building time series from satellite imagery\nData Loaders: Efficient batch processing for training\n\n\n\n\n\nBuild complete geospatial tokenization pipeline\nImplement cloud masking and missing data strategies\nCreate temporal sequence datasets\nOptimize data loading for large-scale training\n\n\n\n\n\nMulti-spectral satellite data properties\nCloud masking algorithms and implementations\nPyTorch data loading best practices\n\n\n\n\nSession 1: Geospatial Data Foundations - Hands-on implementation of geospatial data preprocessing pipeline\n\n\n\nComplete geospatial data pipeline capable of processing satellite imagery for foundation model training\n\n\n\nWeek 2 will implement spatial-temporal attention mechanisms from scratch."
  },
  {
    "objectID": "course-materials/week0.html",
    "href": "course-materials/week0.html",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/week0.html#week-0-overview",
    "href": "course-materials/week0.html#week-0-overview",
    "title": "Week 0: Getting Started",
    "section": "",
    "text": "Welcome to GEOG 288KC: Geospatial Foundation Models and Applications! This week focuses on getting everyone set up with the computational environment and submitting project applications.\n\n\n\nSet up access to UCSB AI Sandbox\nUnderstand course structure and expectations\nSubmit project application with research interest area\nReview prerequisite knowledge areas\n\n\n\n\n\nComplete UCSB AI Sandbox setup and account access\nSubmit project application describing experience and research interests\nReview course syllabus and deliverable timeline\nSet up development environment (Python, PyTorch, Earth Engine access)\n\n\n\n\n\nUCSB AI Sandbox Documentation\nProject Application Form\nCourse GitHub Repository\n\n\n\n\nProject Application (Due: End of Week 0) - 1-paragraph summary of past experience with remote sensing, geospatial data, and ML - Description of application interest area for Geospatial Foundation Models - Any existing fine-tuning data or project ideas\n\n\n\nWeek 1 will introduce the fundamentals of geospatial foundation models and their applications in remote sensing and environmental monitoring."
  },
  {
    "objectID": "course-materials/week2.html",
    "href": "course-materials/week2.html",
    "title": "Foundation Building",
    "section": "",
    "text": "This week implements attention mechanisms from scratch, adapting them specifically for spatial-temporal geospatial relationships.\n\n\n\nImplement self-attention from scratch\nAdapt attention for spatial relationships\nAdd temporal attention for time series\nUnderstand positional encoding for 2D/3D data\n\n\n\n\n\nMulti-head Self-Attention: Implementation from mathematical formulations\n2D Positional Encoding: Spatial position encoding for image patches\nTemporal Encoding: Time series attention for satellite sequences\nCross-attention: Multi-modal data fusion mechanisms\nAttention Visualization: Understanding learned attention patterns\n\n\n\n\n\nImplement multi-head attention from scratch\nBuild 2D positional encoding for spatial patches\nCreate temporal attention for time series data\nVisualize and interpret attention patterns\n\n\n\n\n\nMathematical implementation of attention mechanisms\nPyTorch tensor operations and gradient computation\nAttention pattern visualization\nDebugging dimension mismatches and training issues\n\n\n\n\nSession 2: Spatial-Temporal Attention Mechanisms - Live coding session: Building attention mechanisms with intentional errors and debugging\n\n\n\nCustom attention module for geospatial data with spatial and temporal components\n\n\n\n\nAttention mechanism mathematical foundations\nTransformer architecture deep dive\nSpatial attention for computer vision\n\n\n\n\nWeek 3 will assemble the complete GFM architecture using our custom attention modules."
  },
  {
    "objectID": "course-materials/week2.html#week-2-overview",
    "href": "course-materials/week2.html#week-2-overview",
    "title": "Foundation Building",
    "section": "",
    "text": "This week implements attention mechanisms from scratch, adapting them specifically for spatial-temporal geospatial relationships.\n\n\n\nImplement self-attention from scratch\nAdapt attention for spatial relationships\nAdd temporal attention for time series\nUnderstand positional encoding for 2D/3D data\n\n\n\n\n\nMulti-head Self-Attention: Implementation from mathematical formulations\n2D Positional Encoding: Spatial position encoding for image patches\nTemporal Encoding: Time series attention for satellite sequences\nCross-attention: Multi-modal data fusion mechanisms\nAttention Visualization: Understanding learned attention patterns\n\n\n\n\n\nImplement multi-head attention from scratch\nBuild 2D positional encoding for spatial patches\nCreate temporal attention for time series data\nVisualize and interpret attention patterns\n\n\n\n\n\nMathematical implementation of attention mechanisms\nPyTorch tensor operations and gradient computation\nAttention pattern visualization\nDebugging dimension mismatches and training issues\n\n\n\n\nSession 2: Spatial-Temporal Attention Mechanisms - Live coding session: Building attention mechanisms with intentional errors and debugging\n\n\n\nCustom attention module for geospatial data with spatial and temporal components\n\n\n\n\nAttention mechanism mathematical foundations\nTransformer architecture deep dive\nSpatial attention for computer vision\n\n\n\n\nWeek 3 will assemble the complete GFM architecture using our custom attention modules."
  },
  {
    "objectID": "course-materials/week3.html",
    "href": "course-materials/week3.html",
    "title": "Foundation Building",
    "section": "",
    "text": "This week assembles a complete Vision Transformer architecture adapted for geospatial foundation models, integrating data pipelines and attention mechanisms.\n\n\n\nAssemble complete Vision Transformer architecture\nHandle multi-spectral input processing\nImplement memory-efficient designs\nValidate architecture through testing\n\n\n\n\n\nTransformer Encoder Blocks: Layer normalization and residual connections\nMulti-spectral Input Embedding: Handling different numbers of spectral bands\nArchitecture Testing: Forward pass validation and gradient checking\nMemory Optimization: Efficient attention and activation checkpointing\nModel Scaling: Understanding parameter count and computational requirements\n\n\n\n\n\nBuild complete transformer encoder architecture\nImplement multi-spectral input processing\nTest architecture with sample geospatial data\nOptimize memory usage and computational efficiency\n\n\n\n\n\nPyTorch module composition and inheritance\nMemory profiling and optimization\nArchitecture validation and testing\nUnderstanding model complexity and scaling\n\n\n\n\nSession 3: Complete GFM Architecture - Collaborative architecture decisions: Debating design choices and testing empirically\n\n\n\nWorking GFM architecture (~10M parameters) capable of processing multi-spectral satellite imagery\n\n\n\n\nVision Transformer architecture details\nEfficient transformer implementations\nMemory optimization techniques for large models\n\n\n\n\nWeek 4 begins Stage 2 with masked autoencoder pretraining implementation."
  },
  {
    "objectID": "course-materials/week3.html#week-3-overview",
    "href": "course-materials/week3.html#week-3-overview",
    "title": "Foundation Building",
    "section": "",
    "text": "This week assembles a complete Vision Transformer architecture adapted for geospatial foundation models, integrating data pipelines and attention mechanisms.\n\n\n\nAssemble complete Vision Transformer architecture\nHandle multi-spectral input processing\nImplement memory-efficient designs\nValidate architecture through testing\n\n\n\n\n\nTransformer Encoder Blocks: Layer normalization and residual connections\nMulti-spectral Input Embedding: Handling different numbers of spectral bands\nArchitecture Testing: Forward pass validation and gradient checking\nMemory Optimization: Efficient attention and activation checkpointing\nModel Scaling: Understanding parameter count and computational requirements\n\n\n\n\n\nBuild complete transformer encoder architecture\nImplement multi-spectral input processing\nTest architecture with sample geospatial data\nOptimize memory usage and computational efficiency\n\n\n\n\n\nPyTorch module composition and inheritance\nMemory profiling and optimization\nArchitecture validation and testing\nUnderstanding model complexity and scaling\n\n\n\n\nSession 3: Complete GFM Architecture - Collaborative architecture decisions: Debating design choices and testing empirically\n\n\n\nWorking GFM architecture (~10M parameters) capable of processing multi-spectral satellite imagery\n\n\n\n\nVision Transformer architecture details\nEfficient transformer implementations\nMemory optimization techniques for large models\n\n\n\n\nWeek 4 begins Stage 2 with masked autoencoder pretraining implementation."
  },
  {
    "objectID": "course-materials/week7.html",
    "href": "course-materials/week7.html",
    "title": "System Training",
    "section": "",
    "text": "This week integrates with existing foundation models and compares custom implementations against state-of-the-art.\n\n\n\nLoad and use pretrained foundation models\nCompare custom vs. state-of-the-art models\nUnderstand when to build vs. use existing models\nImplement model ensembling\n\n\n\n\n\nLoading Prithvi and SatMAE: Working with pretrained weights\nArchitecture Compatibility: Adapting existing models to new data\nPerformance Comparison: Custom models vs. state-of-the-art\nTransfer Learning: Strategies for model adaptation\nModel Selection: Criteria for choosing when to build vs. use\n\n\n\n\n\nLoad and configure Prithvi and SatMAE models\nAdapt architectures for compatibility with course data\nCompare performance against custom models\nImplement transfer learning strategies\nCreate model selection framework\n\n\n\n\nSession 7: Integration with Existing Models - Working with Prithvi, SatMAE, and performance comparison\n\n\n\n\nIndependent implementation with instructor feedback\nSmall-group collaboration and knowledge sharing\nTroubleshooting sessions and milestone tracking\n\n\n\n\nIntegrated system using multiple models with performance comparison and model selection guidelines\n\n\n\n\nDevelopment Best Practices: Code organization, testing frameworks, documentation\nPerformance Monitoring: Computational efficiency, resource utilization, bottleneck identification\nVersion Control: Branching strategies, collaborative workflows, milestone tracking\nEnvironment Management: Reproducible setups, dependency management, deployment preparation\n\n\n\n\n\nStructured code review sessions with feedback protocols\nKnowledge sharing presentations on technical solutions\nCollaborative debugging and problem-solving workshops\nCross-project learning and technique exchange\n\n\n\n\n\nAdvanced development pattern guides\nModel optimization and debugging tools\nPerformance profiling and monitoring utilities\nCollaborative development workflows\n\n\n\n\nWeek 8 will focus on project refinement, advanced features, and scalability improvements."
  },
  {
    "objectID": "course-materials/week7.html#week-7-overview",
    "href": "course-materials/week7.html#week-7-overview",
    "title": "System Training",
    "section": "",
    "text": "This week integrates with existing foundation models and compares custom implementations against state-of-the-art.\n\n\n\nLoad and use pretrained foundation models\nCompare custom vs. state-of-the-art models\nUnderstand when to build vs. use existing models\nImplement model ensembling\n\n\n\n\n\nLoading Prithvi and SatMAE: Working with pretrained weights\nArchitecture Compatibility: Adapting existing models to new data\nPerformance Comparison: Custom models vs. state-of-the-art\nTransfer Learning: Strategies for model adaptation\nModel Selection: Criteria for choosing when to build vs. use\n\n\n\n\n\nLoad and configure Prithvi and SatMAE models\nAdapt architectures for compatibility with course data\nCompare performance against custom models\nImplement transfer learning strategies\nCreate model selection framework\n\n\n\n\nSession 7: Integration with Existing Models - Working with Prithvi, SatMAE, and performance comparison\n\n\n\n\nIndependent implementation with instructor feedback\nSmall-group collaboration and knowledge sharing\nTroubleshooting sessions and milestone tracking\n\n\n\n\nIntegrated system using multiple models with performance comparison and model selection guidelines\n\n\n\n\nDevelopment Best Practices: Code organization, testing frameworks, documentation\nPerformance Monitoring: Computational efficiency, resource utilization, bottleneck identification\nVersion Control: Branching strategies, collaborative workflows, milestone tracking\nEnvironment Management: Reproducible setups, dependency management, deployment preparation\n\n\n\n\n\nStructured code review sessions with feedback protocols\nKnowledge sharing presentations on technical solutions\nCollaborative debugging and problem-solving workshops\nCross-project learning and technique exchange\n\n\n\n\n\nAdvanced development pattern guides\nModel optimization and debugging tools\nPerformance profiling and monitoring utilities\nCollaborative development workflows\n\n\n\n\nWeek 8 will focus on project refinement, advanced features, and scalability improvements."
  },
  {
    "objectID": "course-materials/week6.html",
    "href": "course-materials/week6.html",
    "title": "System Training",
    "section": "",
    "text": "This week evaluates representation quality, assesses reconstruction performance, and compares against baselines.\n\n\n\nEvaluate representation quality\nAssess reconstruction performance\nCompare against baselines\nUnderstand learned features\n\n\n\n\n\nEmbedding Visualization: t-SNE, UMAP for learned representations\nReconstruction Quality: Assessment metrics and visual analysis\nLinear Probing: Evaluation of learned features for downstream tasks\nFeature Interpretation: Understanding what the model has learned\nAblation Studies: Component importance analysis\n\n\n\n\n\nGenerate and visualize learned embeddings\nEvaluate reconstruction quality on test data\nImplement linear probing for classification tasks\nAnalyze learned attention patterns and features\nCompare against random initialization baselines\n\n\n\n\nSession 6: Model Evaluation & Analysis - Comprehensive evaluation with embedding visualization and performance analysis\n\n\n\nComprehensive evaluation report with embedding visualizations, reconstruction analysis, and baseline comparisons\n\n\n\n\nProject repository structure and best practices\nReproducible environment setup (conda, Docker, requirements)\nData management and version control strategies\nComputational resource allocation and optimization\n\n\n\n\n\nProject proposal templates and examples\nRepository structure guidelines\nUCSB AI Sandbox resource allocation\nTechnical consultation scheduling system\n\n\n\n\nWeek 7 will focus on intensive development with regular progress check-ins and technical workshops."
  },
  {
    "objectID": "course-materials/week6.html#week-6-overview",
    "href": "course-materials/week6.html#week-6-overview",
    "title": "System Training",
    "section": "",
    "text": "This week evaluates representation quality, assesses reconstruction performance, and compares against baselines.\n\n\n\nEvaluate representation quality\nAssess reconstruction performance\nCompare against baselines\nUnderstand learned features\n\n\n\n\n\nEmbedding Visualization: t-SNE, UMAP for learned representations\nReconstruction Quality: Assessment metrics and visual analysis\nLinear Probing: Evaluation of learned features for downstream tasks\nFeature Interpretation: Understanding what the model has learned\nAblation Studies: Component importance analysis\n\n\n\n\n\nGenerate and visualize learned embeddings\nEvaluate reconstruction quality on test data\nImplement linear probing for classification tasks\nAnalyze learned attention patterns and features\nCompare against random initialization baselines\n\n\n\n\nSession 6: Model Evaluation & Analysis - Comprehensive evaluation with embedding visualization and performance analysis\n\n\n\nComprehensive evaluation report with embedding visualizations, reconstruction analysis, and baseline comparisons\n\n\n\n\nProject repository structure and best practices\nReproducible environment setup (conda, Docker, requirements)\nData management and version control strategies\nComputational resource allocation and optimization\n\n\n\n\n\nProject proposal templates and examples\nRepository structure guidelines\nUCSB AI Sandbox resource allocation\nTechnical consultation scheduling system\n\n\n\n\nWeek 7 will focus on intensive development with regular progress check-ins and technical workshops."
  },
  {
    "objectID": "course-materials/week4.html",
    "href": "course-materials/week4.html",
    "title": "System Training",
    "section": "",
    "text": "This week implements masked autoencoder pretraining for geospatial foundation models, setting up the complete training pipeline.\n\n\n\nImplement masked autoencoder objective\nSet up distributed training pipeline\nMonitor training progress effectively\nHandle large-scale geospatial datasets\n\n\n\n\n\nMasked Patch Reconstruction: Prithvi-style self-supervised learning\nTraining Data Preparation: Augmentation and batch creation\nLoss Functions: Reconstruction loss for masked patches\nDistributed Training: Multi-GPU setup and synchronization\nMonitoring and Logging: Training metrics and visualization\n\n\n\n\n\nImplement masked autoencoder training objective\nSet up distributed training infrastructure\nCreate training data augmentation pipeline\nBuild monitoring and logging system\n\n\n\n\n\nSelf-supervised learning implementation\nDistributed training with PyTorch Lightning\nTraining monitoring and debugging\nLarge-scale data handling\n\n\n\n\nSession 4: Pretraining Implementation - Launch first pretraining run with real satellite data\n\n\n\nActive pretraining pipeline with monitoring and checkpointing\n\n\n\n\nMasked Autoencoder (MAE) paper and implementation\nPrithvi training methodology\nDistributed training best practices\n\n\n\n\nWeek 5 will optimize the training loop for stability and efficiency."
  },
  {
    "objectID": "course-materials/week4.html#week-4-overview",
    "href": "course-materials/week4.html#week-4-overview",
    "title": "System Training",
    "section": "",
    "text": "This week implements masked autoencoder pretraining for geospatial foundation models, setting up the complete training pipeline.\n\n\n\nImplement masked autoencoder objective\nSet up distributed training pipeline\nMonitor training progress effectively\nHandle large-scale geospatial datasets\n\n\n\n\n\nMasked Patch Reconstruction: Prithvi-style self-supervised learning\nTraining Data Preparation: Augmentation and batch creation\nLoss Functions: Reconstruction loss for masked patches\nDistributed Training: Multi-GPU setup and synchronization\nMonitoring and Logging: Training metrics and visualization\n\n\n\n\n\nImplement masked autoencoder training objective\nSet up distributed training infrastructure\nCreate training data augmentation pipeline\nBuild monitoring and logging system\n\n\n\n\n\nSelf-supervised learning implementation\nDistributed training with PyTorch Lightning\nTraining monitoring and debugging\nLarge-scale data handling\n\n\n\n\nSession 4: Pretraining Implementation - Launch first pretraining run with real satellite data\n\n\n\nActive pretraining pipeline with monitoring and checkpointing\n\n\n\n\nMasked Autoencoder (MAE) paper and implementation\nPrithvi training methodology\nDistributed training best practices\n\n\n\n\nWeek 5 will optimize the training loop for stability and efficiency."
  },
  {
    "objectID": "course-materials/week5.html",
    "href": "course-materials/week5.html",
    "title": "System Training",
    "section": "",
    "text": "This week optimizes training for stability and efficiency, handling geospatial-specific training challenges.\n\n\n\nOptimize training for stability and efficiency\nHandle geospatial-specific training challenges\nImplement advanced optimization techniques\nDebug training issues\n\n\n\n\n\nLearning Rate Scheduling: Warm-up, cosine annealing, adaptive strategies\nGradient Management: Clipping, accumulation, normalization\nMixed Precision Training: Automatic mixed precision (AMP) implementation\nMissing Data Handling: Training with cloud-contaminated patches\nTraining Stability: Convergence analysis and debugging\n\n\n\n\n\nImplement advanced learning rate schedulers\nAdd gradient clipping and accumulation\nEnable mixed precision training\nDebug training instabilities and convergence issues\n\n\n\n\n\nAdvanced PyTorch optimization\nTraining stability analysis\nPerformance profiling and optimization\nSystematic debugging of training issues\n\n\n\n\nSession 5: Training Loop Optimization - Performance archaeology: Investigating training curves and optimization dynamics\n\n\n\nOptimized training loop with monitoring and stability guarantees\n\n\n\n\nTraining stability best practices\nMixed precision training guides\nOptimization theory for deep learning\n\n\n\n\nWeek 6 will evaluate model performance and analyze learned representations."
  },
  {
    "objectID": "course-materials/week5.html#week-5-overview",
    "href": "course-materials/week5.html#week-5-overview",
    "title": "System Training",
    "section": "",
    "text": "This week optimizes training for stability and efficiency, handling geospatial-specific training challenges.\n\n\n\nOptimize training for stability and efficiency\nHandle geospatial-specific training challenges\nImplement advanced optimization techniques\nDebug training issues\n\n\n\n\n\nLearning Rate Scheduling: Warm-up, cosine annealing, adaptive strategies\nGradient Management: Clipping, accumulation, normalization\nMixed Precision Training: Automatic mixed precision (AMP) implementation\nMissing Data Handling: Training with cloud-contaminated patches\nTraining Stability: Convergence analysis and debugging\n\n\n\n\n\nImplement advanced learning rate schedulers\nAdd gradient clipping and accumulation\nEnable mixed precision training\nDebug training instabilities and convergence issues\n\n\n\n\n\nAdvanced PyTorch optimization\nTraining stability analysis\nPerformance profiling and optimization\nSystematic debugging of training issues\n\n\n\n\nSession 5: Training Loop Optimization - Performance archaeology: Investigating training curves and optimization dynamics\n\n\n\nOptimized training loop with monitoring and stability guarantees\n\n\n\n\nTraining stability best practices\nMixed precision training guides\nOptimization theory for deep learning\n\n\n\n\nWeek 6 will evaluate model performance and analyze learned representations."
  },
  {
    "objectID": "course-materials/week10.html",
    "href": "course-materials/week10.html",
    "title": "Real-World Application",
    "section": "",
    "text": "This week presents complete model pipelines, synthesizes course learnings, and identifies future research directions.\n\n\n\nPresent complete model pipeline\nSynthesize course learnings\nIdentify future research directions\nPlan continued development\n\n\n\n\n\nFinal Project Presentations: Complete foundation model pipeline demonstrations\nPerformance Analysis: Comparison with state-of-the-art models\nScaling Strategies: Discussion of computational and data requirements\nFuture Work: Identification of research and development opportunities\nCourse Reflection: Synthesis of build-first learning approach\n\n\n\n\nComplete foundation model pipeline with: - Working architecture built from scratch - Trained model with evaluation results - Deployed system with API and interface - Comparison with existing foundation models - Documentation and presentation materials\n\n\n\nSession 10: Project Presentations & Synthesis - Present complete foundation model systems and synthesize course learnings\n\n\n\n\nPresent complete foundation model systems\nDemonstrate end-to-end pipelines with live data\nCompare performance against state-of-the-art\nDiscuss scaling and future development\nReflect on course transformation from consumers to creators\n\n\n\n\n\nEmerging Trends: Latest developments in foundation model architectures\nApplication Domains: Unexplored areas and promising applications\nTechnical Challenges: Scalability, interpretability, robustness\nSocietal Impact: Environmental monitoring, disaster response, climate science\nFuture Research: Open problems, collaboration opportunities\n\n\n\n\nHugging Face/GitHub Submission: - Students may optionally submit polished projects to public repositories - Opportunity for broader visibility and community engagement - Instructor support for preparing submission-quality documentation\nConference/Workshop Submissions: - Guidance on adapting projects for academic conferences - Identification of relevant venues and submission opportunities - Support for preparing extended abstracts or full papers\n\n\n\n\nProject Repository: Complete, documented codebase with examples\nFinal Report: Comprehensive technical document (5-10 pages)\nPresentation Materials: Slides and any supporting materials\nPeer Evaluations: Constructive feedback on all peer presentations\n\n\n\n\n\nIndividual reflection essays on key learnings and growth\nCourse feedback and suggestions for future iterations\nIdentification of personal next steps and continued learning goals\n\n\n\n\n\nPresentation templates and guidelines\nProject submission checklists\nConference and journal venue lists\nNetworking and collaboration opportunities\n\n\n\n\nRecognition of outstanding projects and contributions to course community!"
  },
  {
    "objectID": "course-materials/week10.html#week-10-overview",
    "href": "course-materials/week10.html#week-10-overview",
    "title": "Real-World Application",
    "section": "",
    "text": "This week presents complete model pipelines, synthesizes course learnings, and identifies future research directions.\n\n\n\nPresent complete model pipeline\nSynthesize course learnings\nIdentify future research directions\nPlan continued development\n\n\n\n\n\nFinal Project Presentations: Complete foundation model pipeline demonstrations\nPerformance Analysis: Comparison with state-of-the-art models\nScaling Strategies: Discussion of computational and data requirements\nFuture Work: Identification of research and development opportunities\nCourse Reflection: Synthesis of build-first learning approach\n\n\n\n\nComplete foundation model pipeline with: - Working architecture built from scratch - Trained model with evaluation results - Deployed system with API and interface - Comparison with existing foundation models - Documentation and presentation materials\n\n\n\nSession 10: Project Presentations & Synthesis - Present complete foundation model systems and synthesize course learnings\n\n\n\n\nPresent complete foundation model systems\nDemonstrate end-to-end pipelines with live data\nCompare performance against state-of-the-art\nDiscuss scaling and future development\nReflect on course transformation from consumers to creators\n\n\n\n\n\nEmerging Trends: Latest developments in foundation model architectures\nApplication Domains: Unexplored areas and promising applications\nTechnical Challenges: Scalability, interpretability, robustness\nSocietal Impact: Environmental monitoring, disaster response, climate science\nFuture Research: Open problems, collaboration opportunities\n\n\n\n\nHugging Face/GitHub Submission: - Students may optionally submit polished projects to public repositories - Opportunity for broader visibility and community engagement - Instructor support for preparing submission-quality documentation\nConference/Workshop Submissions: - Guidance on adapting projects for academic conferences - Identification of relevant venues and submission opportunities - Support for preparing extended abstracts or full papers\n\n\n\n\nProject Repository: Complete, documented codebase with examples\nFinal Report: Comprehensive technical document (5-10 pages)\nPresentation Materials: Slides and any supporting materials\nPeer Evaluations: Constructive feedback on all peer presentations\n\n\n\n\n\nIndividual reflection essays on key learnings and growth\nCourse feedback and suggestions for future iterations\nIdentification of personal next steps and continued learning goals\n\n\n\n\n\nPresentation templates and guidelines\nProject submission checklists\nConference and journal venue lists\nNetworking and collaboration opportunities\n\n\n\n\nRecognition of outstanding projects and contributions to course community!"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html",
    "title": "Working with Geospatial Raster Data",
    "section": "",
    "text": "In this session, we learn how to transform raw geospatial raster data into model‑ready embeddings. We’ll work with a tiny multi‑band GeoTIFF, build small, deterministic processing steps, and show visible outputs after every step. The goal is to build intuition that mirrors LLM pipelines (tokenization → embeddings) while grounding everything in images (patches → embeddings).\n\n\n\nLoad and inspect small multi‑band rasters (CRS, resolution, dtype, bands)\nNormalize and organize bands for consistent modeling\nExtract fixed‑size patches\nAdd contextual features (location/time “tokens”)\nSample training patches correctly and create patch embeddings with shapes you can verify\n\n\n\n\nThis session follows a complete data processing pipeline organized into three logical phases that transform raw satellite imagery into model-ready embeddings:\n\n\n\n\n\nflowchart TD\n    subgraph Phase1 [\"🔬 Phase 1: Pixel-Level Pre-processing\"]\n        direction LR\n        A[\"🛰️ Raw Data&lt;br/&gt;Inspection&lt;br/&gt;Section 1.1\"] --&gt; B[\"📐 Band&lt;br/&gt;Normalization&lt;br/&gt;Section 1.2\"]\n        B --&gt; C[\"🔬 Dimensionality&lt;br/&gt;Reduction&lt;br/&gt;Section 1.3\"]\n    end\n    \n    subgraph Phase2 [\"🗺️ Phase 2: Spatial-Structure Processing\"]\n        direction LR\n        D[\"✂️ Patch&lt;br/&gt;Extraction&lt;br/&gt;Section 2.1\"] --&gt; E[\"🏷️ Patch&lt;br/&gt;Metadata&lt;br/&gt;Section 2.2\"]\n        E --&gt; F[\"🌍 Spatial-Temporal&lt;br/&gt;Context&lt;br/&gt;Section 2.3\"]\n        F --&gt; G[\"📊 Training&lt;br/&gt;Sampling&lt;br/&gt;Section 2.4\"]\n    end\n    \n    subgraph Phase3 [\"🧠 Phase 3: Model-Ready Processing\"]\n        direction LR\n        H[\"🧠 Patch&lt;br/&gt;Embeddings&lt;br/&gt;Section 3.1\"] --&gt; I[\"📍 Positional&lt;br/&gt;Encoding&lt;br/&gt;Section 3.2\"]\n    end\n    \n    Phase1 --&gt; Phase2\n    Phase2 --&gt; Phase3\n    \n    style Phase1 fill:#e3f2fd\n    style Phase2 fill:#fff3e0  \n    style Phase3 fill:#e8f5e8\n\n\n\n\n\n\n\n\n\nBefore we get started, let’s setup our computational environment by loading necessary libraries and seeding our computations.\n\n\nWhy these imports: file paths (Path), randomness (random, numpy), and plotting (matplotlib) are used throughout the session.\n\n\nCode\nimport os, random\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(\"Imports OK\")\n\n\nImports OK\n\n\n\n\n\nWe use fixed seeds to ensure that our randomness is consistent every time we run our example code. This allows your results to match the rendered output on the course site.\n\n\nCode\nRNG_SEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(RNG_SEED)\nrandom.seed(RNG_SEED)\nnp.random.seed(RNG_SEED)\nprint(\"Seeds set:\", RNG_SEED)\n\n\nSeeds set: 42\n\n\n\n\n\nWe’ll store the sample raster in a local data/ directory and set a default patch size.\n\n\nCode\n# Use absolute path to ensure we find the data file\nDATA_DIR = Path(__file__).parent.parent.parent.parent / \"data\" if \"__file__\" in globals() else Path(\"../../../data\")\nDATA_DIR.mkdir(exist_ok=True)\nDATA_PATH = DATA_DIR / \"landcover_sample.tif\"\nPATCH_SIZE = 64\nprint(\"DATA_PATH:\", DATA_PATH)\nprint(\"DATA_PATH exists:\", DATA_PATH.exists())\nprint(\"Current working directory:\", Path.cwd())\nprint(\"PATCH_SIZE:\", PATCH_SIZE)\n\n\nDATA_PATH: ../../../data/landcover_sample.tif\nDATA_PATH exists: True\nCurrent working directory: /Users/kellycaylor/dev/geoAI/book/course-materials/weekly-sessions\nPATCH_SIZE: 64\n\n\nLet’s verify the data file exists:\n\n\nCode\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(\"✓ Data file found!\")\nprint(f\"  Path: {DATA_PATH}\")\nprint(f\"  Size (KB): {round(DATA_PATH.stat().st_size/1024,1)}\")\n\n\n✓ Data file found!\n  Path: ../../../data/landcover_sample.tif\n  Size (KB): 12.6"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#learning-objectives",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#learning-objectives",
    "title": "Working with Geospatial Raster Data",
    "section": "",
    "text": "Load and inspect small multi‑band rasters (CRS, resolution, dtype, bands)\nNormalize and organize bands for consistent modeling\nExtract fixed‑size patches\nAdd contextual features (location/time “tokens”)\nSample training patches correctly and create patch embeddings with shapes you can verify"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#session-roadmap",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#session-roadmap",
    "title": "Working with Geospatial Raster Data",
    "section": "",
    "text": "This session follows a complete data processing pipeline organized into three logical phases that transform raw satellite imagery into model-ready embeddings:\n\n\n\n\n\nflowchart TD\n    subgraph Phase1 [\"🔬 Phase 1: Pixel-Level Pre-processing\"]\n        direction LR\n        A[\"🛰️ Raw Data&lt;br/&gt;Inspection&lt;br/&gt;Section 1.1\"] --&gt; B[\"📐 Band&lt;br/&gt;Normalization&lt;br/&gt;Section 1.2\"]\n        B --&gt; C[\"🔬 Dimensionality&lt;br/&gt;Reduction&lt;br/&gt;Section 1.3\"]\n    end\n    \n    subgraph Phase2 [\"🗺️ Phase 2: Spatial-Structure Processing\"]\n        direction LR\n        D[\"✂️ Patch&lt;br/&gt;Extraction&lt;br/&gt;Section 2.1\"] --&gt; E[\"🏷️ Patch&lt;br/&gt;Metadata&lt;br/&gt;Section 2.2\"]\n        E --&gt; F[\"🌍 Spatial-Temporal&lt;br/&gt;Context&lt;br/&gt;Section 2.3\"]\n        F --&gt; G[\"📊 Training&lt;br/&gt;Sampling&lt;br/&gt;Section 2.4\"]\n    end\n    \n    subgraph Phase3 [\"🧠 Phase 3: Model-Ready Processing\"]\n        direction LR\n        H[\"🧠 Patch&lt;br/&gt;Embeddings&lt;br/&gt;Section 3.1\"] --&gt; I[\"📍 Positional&lt;br/&gt;Encoding&lt;br/&gt;Section 3.2\"]\n    end\n    \n    Phase1 --&gt; Phase2\n    Phase2 --&gt; Phase3\n    \n    style Phase1 fill:#e3f2fd\n    style Phase2 fill:#fff3e0  \n    style Phase3 fill:#e8f5e8"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#setting-up",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#setting-up",
    "title": "Working with Geospatial Raster Data",
    "section": "",
    "text": "Before we get started, let’s setup our computational environment by loading necessary libraries and seeding our computations.\n\n\nWhy these imports: file paths (Path), randomness (random, numpy), and plotting (matplotlib) are used throughout the session.\n\n\nCode\nimport os, random\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(\"Imports OK\")\n\n\nImports OK\n\n\n\n\n\nWe use fixed seeds to ensure that our randomness is consistent every time we run our example code. This allows your results to match the rendered output on the course site.\n\n\nCode\nRNG_SEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(RNG_SEED)\nrandom.seed(RNG_SEED)\nnp.random.seed(RNG_SEED)\nprint(\"Seeds set:\", RNG_SEED)\n\n\nSeeds set: 42\n\n\n\n\n\nWe’ll store the sample raster in a local data/ directory and set a default patch size.\n\n\nCode\n# Use absolute path to ensure we find the data file\nDATA_DIR = Path(__file__).parent.parent.parent.parent / \"data\" if \"__file__\" in globals() else Path(\"../../../data\")\nDATA_DIR.mkdir(exist_ok=True)\nDATA_PATH = DATA_DIR / \"landcover_sample.tif\"\nPATCH_SIZE = 64\nprint(\"DATA_PATH:\", DATA_PATH)\nprint(\"DATA_PATH exists:\", DATA_PATH.exists())\nprint(\"Current working directory:\", Path.cwd())\nprint(\"PATCH_SIZE:\", PATCH_SIZE)\n\n\nDATA_PATH: ../../../data/landcover_sample.tif\nDATA_PATH exists: True\nCurrent working directory: /Users/kellycaylor/dev/geoAI/book/course-materials/weekly-sessions\nPATCH_SIZE: 64\n\n\nLet’s verify the data file exists:\n\n\nCode\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(\"✓ Data file found!\")\nprint(f\"  Path: {DATA_PATH}\")\nprint(f\"  Size (KB): {round(DATA_PATH.stat().st_size/1024,1)}\")\n\n\n✓ Data file found!\n  Path: ../../../data/landcover_sample.tif\n  Size (KB): 12.6"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#raw-data-inspection",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#raw-data-inspection",
    "title": "Working with Geospatial Raster Data",
    "section": "1.1 🛰️ Raw Data Inspection",
    "text": "1.1 🛰️ Raw Data Inspection\n\nReading Geospatial Data\nBefore we start working with the data in Python, it’s important to understand what kinds of information a geospatial raster dataset contains.\nA single raster file stores two broad categories of information:\n\nPixel values – the actual measurements.\n\nEach pixel has one or more bands (channels) that store numeric values.\n\nBands might represent reflectance in different wavelengths, elevation, temperature, or derived indices like NDVI.\n\nPixel values are stored in a numeric data type (e.g., integers or floats) and have a defined range (min/max).\n\nSpatial metadata – information that describes where and how those measurements fit on Earth.\n\nLocation reference — how pixel positions map to real-world coordinates.\n\nPixel resolution — the width and height of each pixel in ground units (e.g., meters).\n\nGrid layout and orientation — how rows and columns are aligned in space.\n\nCoordinate system — the mathematical model that defines positions on Earth (projection).\n\nTransform — a mapping from pixel coordinates (row/col) to real-world coordinates (x/y).\n\n\nWhen we open a raster with rasterio, we can access both the measurement values and the spatial metadata. This lets us work with the data not just as an image, but as a set of geographically-anchored measurements we can relate to location, time, and other datasets. Let’s take a look at our sample image.\n\n\nRasterio image reading and metadata inspection\n\n\nCode\nimport rasterio as rio\nimport pyproj\n\nwith rio.open(DATA_PATH) as src:\n    arr = src.read()                          # (bands, height, width)\n    band_count, height, width = arr.shape\n    dtype = arr.dtype\n\n    # CRS and EPSG\n    crs = src.crs\n    epsg = crs.to_epsg()\n    transform = src.transform  # Save transform for later use\n\n    # Get a human-readable CRS name\n    crs_info = pyproj.CRS.from_user_input(crs)\n    crs_name = crs_info.name\n\n    # Resolution in CRS units\n    res_x, res_y = src.res\n\n# --- Print summary ---\nprint(f\"Shape: {arr.shape}\")\nprint(f\"Bands: {band_count}  |  Height: {height} px  Width: {width} px |  Data type: {dtype}\")\nprint(f\"CRS: {crs_name}\" + (f\" (EPSG:{epsg})\" if epsg else \"\"))\nprint(f\"Resolution: {res_x:.2f} × {res_y:.2f} ground units per pixel\")\nprint(\"Per-band min/max:\")\nfor i, b in enumerate(arr, start=1):\n    print(f\"  Band {i}: {float(b.min()):.3f} – {float(b.max()):.3f}\")\n\n\nShape: (3, 64, 64)\nBands: 3  |  Height: 64 px  Width: 64 px |  Data type: uint8\nCRS: Projection: Transverse Mercator; Datum: WGS84; Ellipsoid: WGS84\nResolution: 0.25 × 0.25 ground units per pixel\nPer-band min/max:\n  Band 1: 0.000 – 254.000\n  Band 2: 0.000 – 254.000\n  Band 3: 0.000 – 254.000\n\n\nNotice how the output gives us the fundamental “identity” of our geospatial data. The arr.shape tells us we have a 3D array(bands × height × width), the Coordinate Reference System (CRS) anchors this data to a specific location on Earth, and the resolution information tells us the real-world size of each pixel. Finally, the per-band min/max values reveal the dynamic range of our spectral measurements.\nAlthough the pixel values in this image are stored as uint8, which supports a range from 0 to 255, all three bands in our dataset have maximum values of 254. This usually happens because the largest possible value in the data type is often reserved as a NoData flag, marking pixels with no valid measurement. This upper limit is specific to this dataset, and the use of NoData flags is dataset dependent.\nImage metadata is crucial because satellite imagery comes with precise spatial and spectral calibration. Each pixel value either represents actual physical measurements (like surface reflectance) at a specific geographic location, or a specific category that maps to a classification based on the dataset. Our model needs to understand not just “what values doe this pixel contain?” but also “where is this pixel?” and “what does this pixel represent?”\n\n\n\n\n\n\nDigital Numbers\n\n\n\nThe raw pixel values in many geospatial images are stored as integers to reduce file size and improve processing speed. These integer values are called Digital Numbers (DNs).\nA DN is an instrument-recorded value that may need to be scaled or converted to physical units (such as reflectance or temperature) before analysis.\n\n\n\n\nDN visualization\n\n\nCode\n# Display each band's raw DNs\nnum_bands = arr.shape[0]\nfig, axes = plt.subplots(1, num_bands, figsize=(4*num_bands, 4))\n\n# Handle case where we only have one band\nif num_bands == 1:\n    axes = [axes]\n\nfor i in range(num_bands):\n    band = arr[i]\n    data_min, data_max = float(band.min()), float(band.max())\n    \n    im = axes[i].imshow(band, cmap='viridis', vmin=data_min, vmax=data_max)\n    axes[i].set_title(f\"Band {i+1}: Raw Values\\n(Range: {data_min:.0f} - {data_max:.0f})\")\n    axes[i].axis('off')\n    \n    # Add colorbar for each band\n    plt.colorbar(im, ax=axes[i], label='Digital Number (DN)', shrink=0.8)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#band-normalization",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#band-normalization",
    "title": "Working with Geospatial Raster Data",
    "section": "1.2 📐 Band Normalization",
    "text": "1.2 📐 Band Normalization\nScaling bands to consistent ranges for stable model training\nNow that we’ve examined our raw data, we need to prepare it for neural network processing. While the raw digital numbers are meaningful for interpretation, they present several challenges when used directly as input to models. Different spectral bands often have vastly different value ranges, and neural networks are sensitive to the scale of input features.\n\nWhy normalize geospatial data?\nNormalization serves several critical purposes in geospatial machine learning:\nScale Invariance: Different spectral bands capture different physical phenomena and often have dramatically different value ranges. For example, near-infrared reflectance might range from 0-4000 DN, while visible bands range from 0-255 DN. Without normalization, the model would be dominated by the larger-valued bands.\nTraining Stability: Neural networks use gradient-based optimization, which works best when input features have similar scales. Large differences in feature magnitude can cause unstable gradients, slow convergence, or poor performance.\nSensor Consistency: When working with data from multiple sensors or time periods, normalization helps remove acquisition-specific biases while preserving the relative patterns that matter for learning.\nActivation Function Compatibility: Many neural network activation functions (like sigmoid or tanh) work optimally when inputs are in specific ranges. Normalized inputs help ensure we’re operating in the most effective part of these functions.\n\n\nCommon normalization approaches\nLet’s examine the most common normalization strategies for geospatial data:\n\nMin-Max Normalization (what we’ll use)\nScales each band to a fixed range, typically [0,1]:\nnormalized = (value - min) / (max - min)\nPros: Preserves the distribution shape, guarantees output range\nCons: Sensitive to outliers, range depends on specific dataset\n\n\nZ-Score Standardization\nCenters data around zero with unit variance:\nstandardized = (value - mean) / std\nPros: Removes mean/variance effects, handles outliers better\nCons: Output range is unbounded, can lose interpretability\n\n\nRobust Scaling\nUses median and interquartile range instead of mean/std:\nrobust = (value - median) / (75th_percentile - 25th_percentile)\nPros: Very robust to outliers\nCons: More complex, can compress dynamic range\nFor this exercise, we’ll use min-max normalization because it’s intuitive and preserves the relative structure of our data while ensuring all bands contribute equally to the model.\n\n\nCode\n# Convert to float32 for numerical precision in calculations\narr = arr.astype(np.float32)\nprint(f\"Data type after conversion: {arr.dtype}\")\n\n# Calculate per-band statistics\n# Reshape to (bands, pixels) for easier computation\npixel_values = arr.reshape(arr.shape[0], -1)\nmins = pixel_values.min(axis=1)\nmaxs = pixel_values.max(axis=1)\nranges = maxs - mins\n\nprint(\"Per-band statistics before normalization:\")\nfor i in range(len(mins)):\n    print(f\"  Band {i+1}: min={mins[i]:.1f}, max={maxs[i]:.1f}, range={ranges[i]:.1f}\")\n\n\nData type after conversion: float32\nPer-band statistics before normalization:\n  Band 1: min=0.0, max=254.0, range=254.0\n  Band 2: min=0.0, max=254.0, range=254.0\n  Band 3: min=0.0, max=254.0, range=254.0\n\n\n\n\nCode\n# Apply min-max normalization: (x - min) / (max - min)\n# Add small epsilon to prevent division by zero\neps = 1e-8\narr_norm = (arr - mins[:, None, None]) / (np.maximum(ranges, eps)[:, None, None])\n\nprint(\"\\nNormalized to [0,1] range:\")\nprint(\"Per-band min/max after normalization:\")\nfor i, band in enumerate(arr_norm):\n    print(f\"  Band {i+1}: {float(band.min()):.3f} to {float(band.max()):.3f}\")\n\n\n\nNormalized to [0,1] range:\nPer-band min/max after normalization:\n  Band 1: 0.000 to 1.000\n  Band 2: 0.000 to 1.000\n  Band 3: 0.000 to 1.000\n\n\n\n\n\nVisualizing the normalization effect\nLet’s examine how normalization changes the distribution of pixel values. This visualization helps us understand both what we’ve gained (comparable scales) and what we’ve preserved (relative patterns within each band).\n\n\nCode\n# Compare distributions before and after normalization\nfig, axes = plt.subplots(2, 3, figsize=(12, 6))\n\nfor i in range(min(3, arr.shape[0])):\n    # Raw data histogram\n    axes[0, i].hist(arr[i].ravel(), bins=50, color='gray', alpha=0.7, edgecolor='black')\n    axes[0, i].set_title(f\"Band {i+1}: Raw DNs\")\n    axes[0, i].set_xlabel(\"Digital Number\")\n    axes[0, i].set_ylabel(\"Frequency\")\n    \n    # Normalized data histogram  \n    axes[1, i].hist(arr_norm[i].ravel(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n    axes[1, i].set_title(f\"Band {i+1}: Normalized [0,1]\")\n    axes[1, i].set_xlabel(\"Normalized Value\")\n    axes[1, i].set_ylabel(\"Frequency\")\n    axes[1, i].set_xlim(0, 1)  # Fix scale to show [0,1] range\n\nplt.suptitle(\"Distribution Comparison: Raw vs. Normalized\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the histograms show that while the scales have changed dramatically, the shape of each distribution—the relative patterns of light and dark areas—remains exactly the same. We’ve made the bands comparable without losing the information that distinguishes different materials and land cover types.\n\n\n\n\n\n\nHistorgram equalization\n\n\n\nWhat to notice\n\nThe raw data shows very different scales across bands (different x-axis ranges)\nAfter normalization, all bands span [0,1] but keep their unique distribution shapes\nThis transformation makes bands equally “visible” to the neural network while preserving the spectral signatures that matter for classification\n\n\n\nWhy this matters: Proper normalization is essential for stable training and ensures all spectral bands contribute meaningfully to the model’s understanding of the landscape.\n\n\n\n\n\n\nDeeper Dive: Normalization Method Comparison\n\n\n\nFor a comprehensive analysis of different normalization approaches used in state-of-the-art geospatial foundation models (including Prithvi, SatMAE, and Clay), see our detailed Normalization Methods Comparison. This example compares computational performance, robustness to outliers, and practical trade-offs between five different normalization strategies."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#tiling-and-patch-extraction",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#tiling-and-patch-extraction",
    "title": "Working with Geospatial Raster Data",
    "section": "1.3 Tiling and patch extraction ✂️",
    "text": "1.3 Tiling and patch extraction ✂️\nThis section covers the patch extraction step in our roadmap: splitting large images into manageable “tokens” for model processing.\n\nWhy do we need patches?\nWhen working with satellite imagery, we face a fundamental challenge: scale mismatch. A typical satellite image might be 10,000×10,000 pixels or larger, but neural networks work best with much smaller, consistent input sizes. But there’s more to it than just computational limits. Patches serve several critical functions in geospatial machine learning:\n🧠 Cognitive Focus: Just as humans focus on local areas when interpreting landscapes, neural networks learn more effectively when they can concentrate on coherent spatial neighborhoods rather than trying to process vast areas simultaneously.\n⚡ Computational Efficiency: Smaller patches fit in GPU memory and enable parallel processing. We can train on batches of patches rather than one massive image at a time.\n🎯 Consistent Learning: Fixed-size patches ensure the model sees consistent input dimensions, enabling it to learn spatial patterns at a specific scale.\n🔄 Data Augmentation: From one large image, we can extract hundreds or thousands of training patches, dramatically increasing our dataset size.\n\n\nThe tokenization analogy\nThe relationship between patches and tokenization runs deeper than you might first think. Let’s explore this analogy:\n\n\n\n\n\n\n\n\nAspect\nText Processing\nGeospatial Processing\n\n\n\n\nRaw Input\nDocuments, articles, books\nLarge satellite images, map tiles\n\n\nAtomic Unit\nWords/subwords → tokens\nPixels → patches\n\n\nWhy Split?\nModels can’t process infinite text\nModels can’t process arbitrarily large images\n\n\nContext Window\nLimited token context (e.g., 4K tokens)\nLimited spatial context (e.g., 64×64 pixels)\n\n\nSemantic Coherence\nTokens preserve word meaning\nPatches preserve local spatial patterns\n\n\nPosition Matters\nWord order affects meaning\nSpatial arrangement affects interpretation\n\n\nOverlapping Context\nSliding windows for long documents\nOverlapping patches for spatial continuity\n\n\n\n\n\nVisualizing the patch extraction process\nNow we’ll implement the patch extraction step from our roadmap. This transforms our normalized image into a collection of spatial “tokens” that our model can process.\n\n\nUnderstanding patch size trade-offs\nBefore we extract patches, it’s important to understand how patch size affects what our model can learn:\n\n\n\n\n\n\n\n\n\nPatch Size\nPros\nCons\nBest Use Cases\n\n\n\n\nSmall (16×16, 32×32)\nFine detail, many samples, fast processing\nLimited context, may miss large features\nUrban analysis, crop monitoring\n\n\nMedium (64×64, 128×128)\nGood balance of detail and context\nModerate computational cost\nGeneral purpose, land cover mapping\n\n\nLarge (256×256, 512×512)\nRich spatial context, captures large features\nFewer samples, more memory intensive\nLandscape analysis, climate modeling\n\n\n\nFor our exercise, we’ll use 64×64 patches—a sweet spot that captures meaningful spatial patterns while remaining computationally manageable.\n\n\nStep-by-step patch extraction\nNow let’s implement patch extraction with detailed visualization of each step:\n\n\nCode\n# First, let's examine our normalized image dimensions\nC, H, W = arr_norm.shape\nprint(f\"Input image shape: {C} bands × {H} height × {W} width pixels\")\nprint(f\"Using patch size: {PATCH_SIZE}×{PATCH_SIZE} pixels\")\n\n# Calculate how many complete patches we can extract\nph = H // PATCH_SIZE  # patches vertically\npw = W // PATCH_SIZE  # patches horizontally\nprint(f\"Complete patches: {ph} rows × {pw} columns = {ph * pw} total patches\")\n\n# Calculate the cropped dimensions (we may lose edge pixels)\nHc, Wc = ph * PATCH_SIZE, pw * PATCH_SIZE\nprint(f\"Cropped image size: {Hc}×{Wc} (original: {H}×{W})\")\nprint(f\"Edge pixels lost: {H-Hc} height, {W-Wc} width\")\n\n\nInput image shape: 3 bands × 64 height × 64 width pixels\nUsing patch size: 64×64 pixels\nComplete patches: 1 rows × 1 columns = 1 total patches\nCropped image size: 64×64 (original: 64×64)\nEdge pixels lost: 0 height, 0 width\n\n\n\n\nCode\n# Step 1: Crop the image to fit complete patches\narr_c = arr_norm[:, :Hc, :Wc]\nprint(f\"Cropped array shape: {arr_c.shape}\")\n\n# Step 2: Visualize the grid overlay on the original image\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Show original image with patch grid overlay\nif C &gt;= 3:\n    rgb = np.transpose(arr_norm[:3], (1, 2, 0))\nelse:\n    rgb = np.stack([arr_norm[0]]*3, axis=-1)\n\nax1.imshow(np.clip(rgb, 0, 1))\nax1.set_title(\"Original Image with Patch Grid\")\n\n# Draw grid lines to show patch boundaries\nfor i in range(ph + 1):\n    ax1.axhline(y=i * PATCH_SIZE - 0.5, color='red', linewidth=1, alpha=0.7)\nfor j in range(pw + 1):\n    ax1.axvline(x=j * PATCH_SIZE - 0.5, color='red', linewidth=1, alpha=0.7)\n\n# Show cropped version\nif C &gt;= 3:\n    rgb_cropped = np.transpose(arr_c[:3], (1, 2, 0))\nelse:\n    rgb_cropped = np.stack([arr_c[0]]*3, axis=-1)\n\nax2.imshow(np.clip(rgb_cropped, 0, 1))\nax2.set_title(\"Cropped Image (Complete Patches Only)\")\n\nplt.tight_layout()\nplt.show()\n\n\nCropped array shape: (3, 64, 64)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Step 3: The magical reshape operation that extracts patches\n# This is the core of patch extraction - let's break it down step by step\n\nprint(\"🔄 Patch extraction transformation:\")\nprint(f\"1. Input shape: {arr_c.shape} → (bands, height, width)\")\n\n# Reshape to separate patch rows and columns\nreshaped = arr_c.reshape(C, ph, PATCH_SIZE, pw, PATCH_SIZE)\nprint(f\"2. After reshape: {reshaped.shape} → (bands, patch_rows, patch_height, patch_cols, patch_width)\")\n\n# Transpose to group patches together\ntransposed = reshaped.transpose(1, 3, 0, 2, 4)\nprint(f\"3. After transpose: {transposed.shape} → (patch_rows, patch_cols, bands, patch_height, patch_width)\")\n\n# Final reshape to get individual patches\npatches = transposed.reshape(ph * pw, C, PATCH_SIZE, PATCH_SIZE)\nprint(f\"4. Final patches: {patches.shape} → (num_patches, bands, patch_height, patch_width)\")\n\nprint(f\"\\n✅ Successfully extracted {patches.shape[0]} patches!\")\n\n\n🔄 Patch extraction transformation:\n1. Input shape: (3, 64, 64) → (bands, height, width)\n2. After reshape: (3, 1, 64, 1, 64) → (bands, patch_rows, patch_height, patch_cols, patch_width)\n3. After transpose: (1, 1, 3, 64, 64) → (patch_rows, patch_cols, bands, patch_height, patch_width)\n4. Final patches: (1, 3, 64, 64) → (num_patches, bands, patch_height, patch_width)\n\n✅ Successfully extracted 1 patches!\n\n\n\n\nCode\n# Step 4: Visualize sample patches to see what we've extracted\nnshow = min(6, len(patches))\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i in range(nshow):\n    p = patches[i]\n    \n    # Create RGB visualization (handle cases with fewer than 3 bands)\n    if C &gt;= 3:\n        rgb = np.transpose(p[:3], (1, 2, 0))\n    else:\n        rgb = np.stack([p[0]]*3, axis=-1)\n    \n    axes[i].imshow(np.clip(rgb, 0, 1))\n    \n    # Calculate patch position in the grid\n    row = i // pw\n    col = i % pw\n    axes[i].set_title(f\"Patch {i}\\nGrid position: ({row}, {col})\")\n    axes[i].axis('off')\n\nplt.suptitle(\"Sample Patches: Our Image 'Tokens'\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding what we’ve created\nEach patch is now a self-contained “token” that represents a local region of our satellite image. Just like how words in a sentence carry meaning individually but also depend on context, these patches contain local spatial patterns while maintaining relationships to their neighboring patches.\n\n\n\n\n\n\nKey Transformation\n\n\n\nWe’ve transformed one large image (3, H, W) into {ph × pw} smaller images, each of shape (3, 64, 64). This is exactly analogous to how tokenization transforms one long text into many smaller tokens that a model can process efficiently.\n\n\nWhy this matters: Patches become our fundamental unit of analysis—each patch can be processed independently while preserving spatial relationships for the model to learn landscape patterns."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#assigning-patch-ids-metadata",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#assigning-patch-ids-metadata",
    "title": "Working with Geospatial Raster Data",
    "section": "1.4 Assigning patch IDs & metadata 🏷️",
    "text": "1.4 Assigning patch IDs & metadata 🏷️\nThis section covers the metadata step in our roadmap: giving each patch an identity and spatial context.\n“Every patch needs an identity — not just for bookkeeping, but to remember where and when it came from.”\n\nGenerate unique IDs for patches.\nStore spatial metadata (bounding box, CRS) and acquisition info.\nPrint one sample metadata record.\n\n\n\nCode\nfrom rasterio.transform import xy\n\n# Compute patch bounding boxes (min lon/lat, max lon/lat approx)\nids = []\nmeta = []\nidx = 0\nfor i in range(ph):\n    for j in range(pw):\n        pid = f\"patch_{i:03d}_{j:03d}\"\n        # top-left pixel (row=i*P, col=j*P)\n        r0, c0 = i*PATCH_SIZE, j*PATCH_SIZE\n        r1, c1 = r0 + PATCH_SIZE - 1, j * PATCH_SIZE + PATCH_SIZE - 1\n        (x0, y0) = xy(transform, r0, c0)\n        (x1, y1) = xy(transform, r1, c1)\n        bbox = (min(x0,x1), min(y0,y1), max(x0,x1), max(y0,y1))\n        ids.append(pid)\n        meta.append({\"id\": pid, \"bbox\": bbox, \"crs\": str(crs)})\n        idx += 1\n\nprint(\"total ids:\", len(ids))\nprint(\"sample record:\", meta[0])\n\n\ntotal ids: 1\nsample record: {'id': 'patch_000_000', 'bbox': (280307.8749987148, 394531.1249900842, 280323.6249987148, 394546.8749900842), 'crs': 'PROJCS[\"Projection: Transverse Mercator; Datum: WGS84; Ellipsoid: WGS84\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",19],PARAMETER[\"scale_factor\",0.9993],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",-5300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'}\n\n\nWhy this matters: metadata lets us track location/time and avoid leakage when splitting data."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#adding-special-context-tokens-location-time-sensor",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#adding-special-context-tokens-location-time-sensor",
    "title": "Working with Geospatial Raster Data",
    "section": "1.5 Adding special context tokens (location, time, sensor) 🌍",
    "text": "1.5 Adding special context tokens (location, time, sensor) 🌍\nThis section covers the spatial-temporal context step in our roadmap: encoding where and when each patch was captured.\n“In addition to pixel values, our model benefits from knowing where and when the data was collected, and by which sensor.”\n\nEncode lat/lon as continuous features or sinusoidal embeddings.\nEncode acquisition date as day‑of‑year + year.\nShow how these features attach to patches.\n\n\n\nCode\nimport datetime as dt\n\n# Example acquisition date (placeholder metadata)\nacq_date = dt.date(2023, 7, 15)\ndoy = acq_date.timetuple().tm_yday\nyear = acq_date.year\n\n# Use patch centers for location features\ncenters = []\nfor i in range(ph):\n    for j in range(pw):\n        r, c = i*PATCH_SIZE + PATCH_SIZE//2, j*PATCH_SIZE + PATCH_SIZE//2\n        x_c, y_c = xy(transform, r, c)\n        centers.append((x_c, y_c))\ncenters = np.array(centers, dtype=np.float32)\n\n# Simple scaled features (you can swap for sinusoidal if desired)\nloc_feat = (centers - centers.mean(0)) / (centers.std(0) + 1e-6)\ntime_feat = np.array([doy/366.0, (year-2000)/50.0], dtype=np.float32)  # scale to ~[0,1]\nprint(\"loc_feat shape:\", loc_feat.shape, \"time_feat:\", time_feat.tolist())\n\n\nloc_feat shape: (1, 2) time_feat: [0.5355191230773926, 0.46000000834465027]\n\n\nWhy this matters: models need context to learn spatial/temporal patterns."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#dimensionality-reduction-for-spectral-bands-analogy-to-bpe",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#dimensionality-reduction-for-spectral-bands-analogy-to-bpe",
    "title": "Working with Geospatial Raster Data",
    "section": "1.6 Dimensionality reduction for spectral bands (analogy to BPE)",
    "text": "1.6 Dimensionality reduction for spectral bands (analogy to BPE)\n“Sometimes we have more bands than the model needs. We can reduce the input size while keeping the most useful information.”\n\nDemonstrate PCA or band selection.\nCompare original vs. reduced band profiles.\n\n\n\nCode\n# PCA via SVD on (pixels, bands) for speed; sample a subset of pixels\nC, Hc, Wc = arr_c.shape\npixels = arr_c.reshape(C, -1).T  # (N, C)\nN = pixels.shape[0]\nsub = min(5000, N)\nidx = np.random.default_rng(RNG_SEED).choice(N, sub, replace=False)\nXsub = pixels[idx]\n\n# Center\nmu = Xsub.mean(0, keepdims=True)\nXc = Xsub - mu\nU, S, Vt = np.linalg.svd(Xc, full_matrices=False)\nexplained = (S**2) / (S**2).sum()\nkeep = min(3, C)\nWred = Vt[:keep].T  # (C, keep)\n\nprint(\"bands:\", C, \"keep:\", keep, \"explained_var@keep:\", float(explained[:keep].sum()))\n\n# Project full image to reduced components per pixel\nred_pixels = (pixels - mu).dot(Wred)  # (N, keep)\nred_stack = red_pixels.T.reshape(keep, Hc, Wc)\nprint(\"reduced stack shape:\", red_stack.shape)\n\n\nbands: 3 keep: 3 explained_var@keep: 1.0\nreduced stack shape: (3, 64, 64)\n\n\nWhy this matters: reducing input size speeds training and may denoise bands."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#sampling-training-patches-sliding-windows-spatial-splits",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#sampling-training-patches-sliding-windows-spatial-splits",
    "title": "Working with Geospatial Raster Data",
    "section": "1.7 Sampling training patches (sliding windows, spatial splits)",
    "text": "1.7 Sampling training patches (sliding windows, spatial splits)\n“Rather than using the whole dataset at once, we’ll sample patches in a way that keeps training efficient and avoids data leakage.”\n\nShow random vs. sliding‑window sampling.\nHighlight spatial/temporal separation for validation/test.\n\n\n\nCode\nnum_patches = patches.shape[0]\nall_idx = np.arange(num_patches)\n# Simple spatial split by rows: top 80% train, bottom 20% val\nrows = np.repeat(np.arange(ph), pw)\ntrain_mask = rows &lt; int(0.8*ph)\ntrain_idx = all_idx[train_mask]\nval_idx = all_idx[~train_mask]\n\nrng = np.random.default_rng(RNG_SEED)\nrng.shuffle(train_idx)\nprint(\"num_patches:\", num_patches, \"train:\", len(train_idx), \"val:\", len(val_idx))\nprint(\"example train idx:\", train_idx[:8].tolist())\n\n\nnum_patches: 1 train: 0 val: 1\nexample train idx: []\n\n\nWhy this matters: proper splits reduce spatial leakage and inflated metrics."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#creating-patch-embeddings",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#creating-patch-embeddings",
    "title": "Working with Geospatial Raster Data",
    "section": "1.8 Creating patch embeddings 🧠",
    "text": "1.8 Creating patch embeddings 🧠\nThis section covers the patch embeddings step in our roadmap: transforming spatial patches into the vector space where models operate.\n“This is where pixels meet the model: we project each patch into an embedding space where the model can learn patterns.”\n\nFlatten + linear layer (minimal version).\nPrint embedding tensor shapes.\nShow a few example embedding vectors.\n\n\n\nCode\ntry:\n    import torch\n    torch.manual_seed(RNG_SEED)\n    B, C, P, _ = patches.shape\n    input_dim = C * P * P\n    embed_dim = 64\n    proj = torch.nn.Linear(input_dim, embed_dim)\n\n    # take a tiny batch of patches\n    sample = torch.from_numpy(patches[:4]).reshape(4, -1).float()\n    emb = proj(sample)\n    print(\"input_dim:\", input_dim, \"embed_dim:\", embed_dim, \"emb shape:\", tuple(emb.shape))\n    print(\"first row (rounded):\", [float(x) for x in emb[0][:8].detach().numpy().round(3)])\nexcept Exception as e:\n    print(\"Torch not available; skipping embeddings\", e)\n\n\nTorch not available; skipping embeddings mat1 and mat2 shapes cannot be multiplied (4x3072 and 12288x64)\n\n\nWhy this matters: embeddings are the model’s operating space for learning patterns."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#encoding-spatial-temporal-positions",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#encoding-spatial-temporal-positions",
    "title": "Working with Geospatial Raster Data",
    "section": "1.9 Encoding spatial & temporal positions 📍",
    "text": "1.9 Encoding spatial & temporal positions 📍\nThis section covers the final step in our roadmap: adding positional encodings so models understand spatial and temporal relationships.\n“Models don’t know where a patch sits in space or time unless we tell them — positional encodings add this crucial context.”\n\nCreate 2D sinusoidal spatial encodings (row/col).\nAdd temporal encodings (day‑of‑year).\nCombine with patch embeddings and inspect results.\n\n\n\nCode\n# 2D sinusoidal positional encodings for (row, col)\nrows_grid = np.arange(ph)[:,None].repeat(pw,1)\ncols_grid = np.arange(pw)[None,:].repeat(ph,0)\npos2d = np.stack([rows_grid.ravel(), cols_grid.ravel()], axis=1).astype(np.float32)\n\n# Scale rows/cols to [0,1]\npos2d = (pos2d - pos2d.min(0)) / (pos2d.ptp(0) + 1e-6)\n\n# Simple sin/cos features\npos_feat = np.concatenate([np.sin(2*np.pi*pos2d), np.cos(2*np.pi*pos2d)], axis=1)\n# time features from earlier (same for all patches here)\ntime_feat_full = np.repeat(time_feat[None,:], len(pos_feat), axis=0)\n\nprint(\"pos_feat shape:\", pos_feat.shape, \"time_feat_full shape:\", time_feat_full.shape)\n\n\npos_feat shape: (1, 4) time_feat_full shape: (1, 2)\n\n\n\n\nCode\n# Combine: embedding + positional + time features (toy example)\ntry:\n    comb = np.concatenate([\n        emb.detach().numpy(),         # (4, 64) toy batch\n        pos_feat[:4],                 # (4, 4)\n        time_feat_full[:4]            # (4, 2)\n    ], axis=1)\n    print(\"combined features shape:\", comb.shape)\nexcept Exception as e:\n    print(\"Combine skipped:\", e)\n\n\nCombine skipped: name 'emb' is not defined\n\n\nWhy this matters: spatial/temporal encodings provide essential context alongside pixel content."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#conclusion",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#conclusion",
    "title": "Working with Geospatial Raster Data",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe inspected raster metadata (CRS, resolution, bands) and normalized bands\nWe extracted fixed‑size patches, attached IDs/metadata, and added location/time features\nWe created minimal patch embeddings and combined them with positional/temporal encodings"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations copy.html#resources",
    "href": "course-materials/weekly-sessions/session1_data_foundations copy.html#resources",
    "title": "Working with Geospatial Raster Data",
    "section": "Resources",
    "text": "Resources\n\nQuarto: https://quarto.org/docs\nRasterio: https://rasterio.readthedocs.io/\nNumPy: https://numpy.org/doc/\nPyTorch: https://pytorch.org/docs/stable/index.html\nVision Transformers (ViT): https://arxiv.org/abs/2010.11929"
  },
  {
    "objectID": "course-materials/weekly-sessions/session2_attention_mechanisms.html",
    "href": "course-materials/weekly-sessions/session2_attention_mechanisms.html",
    "title": "Week 2 Interactive Session: Spatial-Temporal Attention Mechanisms",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html",
    "title": "Working with Geospatial Raster Data",
    "section": "",
    "text": "In this session, we learn how to transform raw geospatial raster data into model‑ready embeddings. We’ll work with a tiny multi‑band GeoTIFF, build small, deterministic processing steps, and show visible outputs after every step. The goal is to build intuition that mirrors LLM pipelines (tokenization → embeddings) while grounding everything in images (patches → embeddings).\n\n\n\nLoad and inspect small multi‑band rasters (CRS, resolution, dtype, bands)\nNormalize and organize bands for consistent modeling\nExtract fixed‑size patches\nAdd contextual features (location/time “tokens”)\nSample training patches correctly and create patch embeddings with shapes you can verify\n\n\n\n\nThis session follows a complete data processing pipeline organized into three logical phases that transform raw satellite imagery into model-ready embeddings:\n\n\n\n\n\nflowchart TD\n    subgraph Phase1 [\"🔬 Phase 1: Pixel-Level Pre-processing\"]\n        direction LR\n        A[\"🛰️ Raw Data&lt;br/&gt;Inspection&lt;br/&gt;Section 1.1\"] --&gt; B[\"📐 Band&lt;br/&gt;Normalization&lt;br/&gt;Section 1.2\"]\n        B --&gt; C[\"🔬 Dimensionality&lt;br/&gt;Reduction&lt;br/&gt;Section 1.3\"]\n    end\n    \n    subgraph Phase2 [\"🗺️ Phase 2: Spatial-Structure Processing\"]\n        direction LR\n        D[\"✂️ Patch&lt;br/&gt;Extraction&lt;br/&gt;Section 2.1\"] --&gt; E[\"🏷️ Patch&lt;br/&gt;Metadata&lt;br/&gt;Section 2.2\"]\n        E --&gt; F[\"🌍 Spatial-Temporal&lt;br/&gt;Context&lt;br/&gt;Section 2.3\"]\n        F --&gt; G[\"📊 Training&lt;br/&gt;Sampling&lt;br/&gt;Section 2.4\"]\n    end\n    \n    subgraph Phase3 [\"🧠 Phase 3: Model-Ready Processing\"]\n        direction LR\n        H[\"🧠 Patch&lt;br/&gt;Embeddings&lt;br/&gt;Section 3.1\"] --&gt; I[\"📍 Positional&lt;br/&gt;Encoding&lt;br/&gt;Section 3.2\"]\n    end\n    \n    Phase1 --&gt; Phase2\n    Phase2 --&gt; Phase3\n    \n    style Phase1 fill:#e3f2fd\n    style Phase2 fill:#fff3e0  \n    style Phase3 fill:#e8f5e8\n\n\n\n\n\n\n\n\n\nBefore we get started, let’s setup our computational environment by loading necessary libraries and seeding our computations.\n\n\nWhy these imports: file paths (Path), randomness (random, numpy), and plotting (matplotlib) are used throughout the session.\n\n\nCode\nimport os, random\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(\"Imports OK\")\n\n\nImports OK\n\n\n\n\n\nWe use fixed seeds to ensure that our randomness is consistent every time we run our example code. This allows your results to match the rendered output on the course site.\n\n\nCode\nRNG_SEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(RNG_SEED)\nrandom.seed(RNG_SEED)\nnp.random.seed(RNG_SEED)\nprint(\"Seeds set:\", RNG_SEED)\n\n\nSeeds set: 42\n\n\n\n\n\nWe’ll store the sample raster in a local data/ directory and set a default patch size.\n\n\nCode\n# Use absolute path to ensure we find the data file\nDATA_DIR = Path(__file__).parent.parent.parent.parent / \"data\" if \"__file__\" in globals() else Path(\"../../../data\")\nDATA_DIR.mkdir(exist_ok=True)\nDATA_PATH = DATA_DIR / \"landcover_sample.tif\"\nPATCH_SIZE = 64\nprint(\"DATA_PATH:\", DATA_PATH)\nprint(\"DATA_PATH exists:\", DATA_PATH.exists())\nprint(\"Current working directory:\", Path.cwd())\nprint(\"PATCH_SIZE:\", PATCH_SIZE)\n\n\nDATA_PATH: ../../../data/landcover_sample.tif\nDATA_PATH exists: True\nCurrent working directory: /Users/kellycaylor/dev/geoAI/book/course-materials/weekly-sessions\nPATCH_SIZE: 64\n\n\nIf the file is missing, we’ll download a small sample.\n\n\nCode\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(\"✓ Data file found!\")\nprint(f\"  Path: {DATA_PATH}\")\nprint(f\"  Size (KB): {round(DATA_PATH.stat().st_size/1024,1)}\")\n\n\n✓ Data file found!\n  Path: ../../../data/landcover_sample.tif\n  Size (KB): 12.6"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#learning-objectives",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#learning-objectives",
    "title": "Working with Geospatial Raster Data",
    "section": "",
    "text": "Load and inspect small multi‑band rasters (CRS, resolution, dtype, bands)\nNormalize and organize bands for consistent modeling\nExtract fixed‑size patches\nAdd contextual features (location/time “tokens”)\nSample training patches correctly and create patch embeddings with shapes you can verify"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#session-roadmap",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#session-roadmap",
    "title": "Working with Geospatial Raster Data",
    "section": "",
    "text": "This session follows a complete data processing pipeline organized into three logical phases that transform raw satellite imagery into model-ready embeddings:\n\n\n\n\n\nflowchart TD\n    subgraph Phase1 [\"🔬 Phase 1: Pixel-Level Pre-processing\"]\n        direction LR\n        A[\"🛰️ Raw Data&lt;br/&gt;Inspection&lt;br/&gt;Section 1.1\"] --&gt; B[\"📐 Band&lt;br/&gt;Normalization&lt;br/&gt;Section 1.2\"]\n        B --&gt; C[\"🔬 Dimensionality&lt;br/&gt;Reduction&lt;br/&gt;Section 1.3\"]\n    end\n    \n    subgraph Phase2 [\"🗺️ Phase 2: Spatial-Structure Processing\"]\n        direction LR\n        D[\"✂️ Patch&lt;br/&gt;Extraction&lt;br/&gt;Section 2.1\"] --&gt; E[\"🏷️ Patch&lt;br/&gt;Metadata&lt;br/&gt;Section 2.2\"]\n        E --&gt; F[\"🌍 Spatial-Temporal&lt;br/&gt;Context&lt;br/&gt;Section 2.3\"]\n        F --&gt; G[\"📊 Training&lt;br/&gt;Sampling&lt;br/&gt;Section 2.4\"]\n    end\n    \n    subgraph Phase3 [\"🧠 Phase 3: Model-Ready Processing\"]\n        direction LR\n        H[\"🧠 Patch&lt;br/&gt;Embeddings&lt;br/&gt;Section 3.1\"] --&gt; I[\"📍 Positional&lt;br/&gt;Encoding&lt;br/&gt;Section 3.2\"]\n    end\n    \n    Phase1 --&gt; Phase2\n    Phase2 --&gt; Phase3\n    \n    style Phase1 fill:#e3f2fd\n    style Phase2 fill:#fff3e0  \n    style Phase3 fill:#e8f5e8"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#setting-up",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#setting-up",
    "title": "Working with Geospatial Raster Data",
    "section": "",
    "text": "Before we get started, let’s setup our computational environment by loading necessary libraries and seeding our computations.\n\n\nWhy these imports: file paths (Path), randomness (random, numpy), and plotting (matplotlib) are used throughout the session.\n\n\nCode\nimport os, random\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(\"Imports OK\")\n\n\nImports OK\n\n\n\n\n\nWe use fixed seeds to ensure that our randomness is consistent every time we run our example code. This allows your results to match the rendered output on the course site.\n\n\nCode\nRNG_SEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(RNG_SEED)\nrandom.seed(RNG_SEED)\nnp.random.seed(RNG_SEED)\nprint(\"Seeds set:\", RNG_SEED)\n\n\nSeeds set: 42\n\n\n\n\n\nWe’ll store the sample raster in a local data/ directory and set a default patch size.\n\n\nCode\n# Use absolute path to ensure we find the data file\nDATA_DIR = Path(__file__).parent.parent.parent.parent / \"data\" if \"__file__\" in globals() else Path(\"../../../data\")\nDATA_DIR.mkdir(exist_ok=True)\nDATA_PATH = DATA_DIR / \"landcover_sample.tif\"\nPATCH_SIZE = 64\nprint(\"DATA_PATH:\", DATA_PATH)\nprint(\"DATA_PATH exists:\", DATA_PATH.exists())\nprint(\"Current working directory:\", Path.cwd())\nprint(\"PATCH_SIZE:\", PATCH_SIZE)\n\n\nDATA_PATH: ../../../data/landcover_sample.tif\nDATA_PATH exists: True\nCurrent working directory: /Users/kellycaylor/dev/geoAI/book/course-materials/weekly-sessions\nPATCH_SIZE: 64\n\n\nIf the file is missing, we’ll download a small sample.\n\n\nCode\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(\"✓ Data file found!\")\nprint(f\"  Path: {DATA_PATH}\")\nprint(f\"  Size (KB): {round(DATA_PATH.stat().st_size/1024,1)}\")\n\n\n✓ Data file found!\n  Path: ../../../data/landcover_sample.tif\n  Size (KB): 12.6"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#raw-data-inspection",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#raw-data-inspection",
    "title": "Working with Geospatial Raster Data",
    "section": "1.1 🛰️ Raw Data Inspection",
    "text": "1.1 🛰️ Raw Data Inspection\n\nReading Geospatial Data\nBefore we start working with the data in Python, it’s important to understand what kinds of information a geospatial raster dataset contains.\nA single raster file stores two broad categories of information:\n\nPixel values – the actual measurements.\n\nEach pixel has one or more bands (channels) that store numeric values.\n\nBands might represent reflectance in different wavelengths, elevation, temperature, or derived indices like NDVI.\n\nPixel values are stored in a numeric data type (e.g., integers or floats) and have a defined range (min/max).\n\nSpatial metadata – information that describes where and how those measurements fit on Earth.\n\nLocation reference — how pixel positions map to real-world coordinates.\n\nPixel resolution — the width and height of each pixel in ground units (e.g., meters).\n\nGrid layout and orientation — how rows and columns are aligned in space.\n\nCoordinate system — the mathematical model that defines positions on Earth (projection).\n\nTransform — a mapping from pixel coordinates (row/col) to real-world coordinates (x/y).\n\n\nWhen we open a raster with rasterio, we can access both the measurement values and the spatial metadata. This lets us work with the data not just as an image, but as a set of geographically-anchored measurements we can relate to location, time, and other datasets. Let’s take a look at our sample image.\n\n\nRasterio image reading and metadata inspection\n\n\nCode\nimport rasterio as rio\nimport pyproj\n\nwith rio.open(DATA_PATH) as src:\n    arr = src.read()                          # (bands, height, width)\n    band_count, height, width = arr.shape\n    dtype = arr.dtype\n\n    # CRS and EPSG\n    crs = src.crs\n    epsg = crs.to_epsg()\n    transform = src.transform  # Save transform for later use\n\n    # Get a human-readable CRS name\n    crs_info = pyproj.CRS.from_user_input(crs)\n    crs_name = crs_info.name\n\n    # Resolution in CRS units\n    res_x, res_y = src.res\n\n# --- Print summary ---\nprint(f\"Shape: {arr.shape}\")\nprint(f\"Bands: {band_count}  |  Height: {height} px  Width: {width} px |  Data type: {dtype}\")\nprint(f\"CRS: {crs_name}\" + (f\" (EPSG:{epsg})\" if epsg else \"\"))\nprint(f\"Resolution: {res_x:.2f} × {res_y:.2f} ground units per pixel\")\nprint(\"Per-band min/max:\")\nfor i, b in enumerate(arr, start=1):\n    print(f\"  Band {i}: {float(b.min()):.3f} – {float(b.max()):.3f}\")\n\n\nShape: (3, 64, 64)\nBands: 3  |  Height: 64 px  Width: 64 px |  Data type: uint8\nCRS: Projection: Transverse Mercator; Datum: WGS84; Ellipsoid: WGS84\nResolution: 0.25 × 0.25 ground units per pixel\nPer-band min/max:\n  Band 1: 0.000 – 254.000\n  Band 2: 0.000 – 254.000\n  Band 3: 0.000 – 254.000\n\n\nNotice how the output gives us the fundamental “identity” of our geospatial data. The arr.shape tells us we have a 3D array(bands × height × width), the Coordinate Reference System (CRS) anchors this data to a specific location on Earth, and the resolution information tells us the real-world size of each pixel. Finally, the per-band min/max values reveal the dynamic range of our spectral measurements.\nAlthough the pixel values in this image are stored as uint8, which supports a range from 0 to 255, all three bands in our dataset have maximum values of 254. This usually happens because the largest possible value in the data type is often reserved as a NoData flag, marking pixels with no valid measurement. This upper limit is specific to this dataset, and the use of NoData flags is dataset dependent.\nImage metadata is crucial because satellite imagery comes with precise spatial and spectral calibration. Each pixel value either represents actual physical measurements (like surface reflectance) at a specific geographic location, or a specific category that maps to a classification based on the dataset. Our model needs to understand not just “what values doe this pixel contain?” but also “where is this pixel?” and “what does this pixel represent?”\n\n\n\n\n\n\nDigital Numbers\n\n\n\nThe raw pixel values in many geospatial images are stored as integers to reduce file size and improve processing speed. These integer values are called Digital Numbers (DNs).\nA DN is an instrument-recorded value that may need to be scaled or converted to physical units (such as reflectance or temperature) before analysis.\n\n\n\n\nDN visualization\n\n\nCode\n# Display each band's raw DNs\nnum_bands = arr.shape[0]\nfig, axes = plt.subplots(1, num_bands, figsize=(4*num_bands, 4))\n\n# Handle case where we only have one band\nif num_bands == 1:\n    axes = [axes]\n\nfor i in range(num_bands):\n    band = arr[i]\n    data_min, data_max = float(band.min()), float(band.max())\n    \n    im = axes[i].imshow(band, cmap='viridis', vmin=data_min, vmax=data_max)\n    axes[i].set_title(f\"Band {i+1}: Raw Values\\n(Range: {data_min:.0f} - {data_max:.0f})\")\n    axes[i].axis('off')\n    \n    # Add colorbar for each band\n    plt.colorbar(im, ax=axes[i], label='Digital Number (DN)', shrink=0.8)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#band-normalization",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#band-normalization",
    "title": "Working with Geospatial Raster Data",
    "section": "1.2 📐 Band Normalization",
    "text": "1.2 📐 Band Normalization\nScaling bands to consistent ranges for stable model training\nNow that we’ve examined our raw data, we need to prepare it for neural network processing. While the raw digital numbers are meaningful for interpretation, they present several challenges when used directly as input to models. Different spectral bands often have vastly different value ranges, and neural networks are sensitive to the scale of input features.\n\nWhy normalize geospatial data?\nNormalization serves several critical purposes in geospatial machine learning:\nScale Invariance: Different spectral bands capture different physical phenomena and often have dramatically different value ranges. For example, near-infrared reflectance might range from 0-4000 DN, while visible bands range from 0-255 DN. Without normalization, the model would be dominated by the larger-valued bands.\nTraining Stability: Neural networks use gradient-based optimization, which works best when input features have similar scales. Large differences in feature magnitude can cause unstable gradients, slow convergence, or poor performance.\nSensor Consistency: When working with data from multiple sensors or time periods, normalization helps remove acquisition-specific biases while preserving the relative patterns that matter for learning.\nActivation Function Compatibility: Many neural network activation functions (like sigmoid or tanh) work optimally when inputs are in specific ranges. Normalized inputs help ensure we’re operating in the most effective part of these functions.\n\n\nCommon normalization approaches\nLet’s examine the most common normalization strategies for geospatial data:\n\nMin-Max Normalization (what we’ll use)\nScales each band to a fixed range, typically [0,1]:\nnormalized = (value - min) / (max - min)\nPros: Preserves the distribution shape, guarantees output range\nCons: Sensitive to outliers, range depends on specific dataset\n\n\nZ-Score Standardization\nCenters data around zero with unit variance:\nstandardized = (value - mean) / std\nPros: Removes mean/variance effects, handles outliers better\nCons: Output range is unbounded, can lose interpretability\n\n\nRobust Scaling\nUses median and interquartile range instead of mean/std:\nrobust = (value - median) / (75th_percentile - 25th_percentile)\nPros: Very robust to outliers\nCons: More complex, can compress dynamic range\nFor this exercise, we’ll use min-max normalization because it’s intuitive and preserves the relative structure of our data while ensuring all bands contribute equally to the model.\n\n\nCode\n# Convert to float32 for numerical precision in calculations\narr = arr.astype(np.float32)\nprint(f\"Data type after conversion: {arr.dtype}\")\n\n# Calculate per-band statistics\n# Reshape to (bands, pixels) for easier computation\npixel_values = arr.reshape(arr.shape[0], -1)\nmins = pixel_values.min(axis=1)\nmaxs = pixel_values.max(axis=1)\nranges = maxs - mins\n\nprint(\"Per-band statistics before normalization:\")\nfor i in range(len(mins)):\n    print(f\"  Band {i+1}: min={mins[i]:.1f}, max={maxs[i]:.1f}, range={ranges[i]:.1f}\")\n\n\nData type after conversion: float32\nPer-band statistics before normalization:\n  Band 1: min=0.0, max=254.0, range=254.0\n  Band 2: min=0.0, max=254.0, range=254.0\n  Band 3: min=0.0, max=254.0, range=254.0\n\n\n\n\nCode\n# Apply min-max normalization: (x - min) / (max - min)\n# Add small epsilon to prevent division by zero\neps = 1e-8\narr_norm = (arr - mins[:, None, None]) / (np.maximum(ranges, eps)[:, None, None])\n\nprint(\"\\nNormalized to [0,1] range:\")\nprint(\"Per-band min/max after normalization:\")\nfor i, band in enumerate(arr_norm):\n    print(f\"  Band {i+1}: {float(band.min()):.3f} to {float(band.max()):.3f}\")\n\n\n\nNormalized to [0,1] range:\nPer-band min/max after normalization:\n  Band 1: 0.000 to 1.000\n  Band 2: 0.000 to 1.000\n  Band 3: 0.000 to 1.000\n\n\n\n\n\nVisualizing the normalization effect\nLet’s examine how normalization changes the distribution of pixel values. This visualization helps us understand both what we’ve gained (comparable scales) and what we’ve preserved (relative patterns within each band).\n\n\nCode\n# Compare distributions before and after normalization\nfig, axes = plt.subplots(2, 3, figsize=(12, 6))\n\nfor i in range(min(3, arr.shape[0])):\n    # Raw data histogram\n    axes[0, i].hist(arr[i].ravel(), bins=50, color='gray', alpha=0.7, edgecolor='black')\n    axes[0, i].set_title(f\"Band {i+1}: Raw DNs\")\n    axes[0, i].set_xlabel(\"Digital Number\")\n    axes[0, i].set_ylabel(\"Frequency\")\n    \n    # Normalized data histogram  \n    axes[1, i].hist(arr_norm[i].ravel(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n    axes[1, i].set_title(f\"Band {i+1}: Normalized [0,1]\")\n    axes[1, i].set_xlabel(\"Normalized Value\")\n    axes[1, i].set_ylabel(\"Frequency\")\n    axes[1, i].set_xlim(0, 1)  # Fix scale to show [0,1] range\n\nplt.suptitle(\"Distribution Comparison: Raw vs. Normalized\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the histograms show that while the scales have changed dramatically, the shape of each distribution—the relative patterns of light and dark areas—remains exactly the same. We’ve made the bands comparable without losing the information that distinguishes different materials and land cover types.\n\n\n\n\n\n\nHistorgram equalization\n\n\n\nWhat to notice\n\nThe raw data shows very different scales across bands (different x-axis ranges)\nAfter normalization, all bands span [0,1] but keep their unique distribution shapes\nThis transformation makes bands equally “visible” to the neural network while preserving the spectral signatures that matter for classification\n\n\n\nWhy this matters: Proper normalization is essential for stable training and ensures all spectral bands contribute meaningfully to the model’s understanding of the landscape.\n\n\n\n\n\n\nDeeper Dive: Normalization Method Comparison\n\n\n\nFor a comprehensive analysis of different normalization approaches used in state-of-the-art geospatial foundation models (including Prithvi, SatMAE, and Clay), see our detailed Normalization Methods Comparison. This example compares computational performance, robustness to outliers, and practical trade-offs between five different normalization strategies."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#tiling-and-patch-extraction",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#tiling-and-patch-extraction",
    "title": "Working with Geospatial Raster Data",
    "section": "1.3 Tiling and patch extraction ✂️",
    "text": "1.3 Tiling and patch extraction ✂️\nThis section covers the patch extraction step in our roadmap: splitting large images into manageable “tokens” for model processing.\n\nWhy do we need patches?\nWhen working with satellite imagery, we face a fundamental challenge: scale mismatch. A typical satellite image might be 10,000×10,000 pixels or larger, but neural networks work best with much smaller, consistent input sizes. But there’s more to it than just computational limits. Patches serve several critical functions in geospatial machine learning:\n🧠 Cognitive Focus: Just as humans focus on local areas when interpreting landscapes, neural networks learn more effectively when they can concentrate on coherent spatial neighborhoods rather than trying to process vast areas simultaneously.\n⚡ Computational Efficiency: Smaller patches fit in GPU memory and enable parallel processing. We can train on batches of patches rather than one massive image at a time.\n🎯 Consistent Learning: Fixed-size patches ensure the model sees consistent input dimensions, enabling it to learn spatial patterns at a specific scale.\n🔄 Data Augmentation: From one large image, we can extract hundreds or thousands of training patches, dramatically increasing our dataset size.\n\n\nThe tokenization analogy\nThe relationship between patches and tokenization runs deeper than you might first think. Let’s explore this analogy:\n\n\n\n\n\n\n\n\nAspect\nText Processing\nGeospatial Processing\n\n\n\n\nRaw Input\nDocuments, articles, books\nLarge satellite images, map tiles\n\n\nAtomic Unit\nWords/subwords → tokens\nPixels → patches\n\n\nWhy Split?\nModels can’t process infinite text\nModels can’t process arbitrarily large images\n\n\nContext Window\nLimited token context (e.g., 4K tokens)\nLimited spatial context (e.g., 64×64 pixels)\n\n\nSemantic Coherence\nTokens preserve word meaning\nPatches preserve local spatial patterns\n\n\nPosition Matters\nWord order affects meaning\nSpatial arrangement affects interpretation\n\n\nOverlapping Context\nSliding windows for long documents\nOverlapping patches for spatial continuity\n\n\n\n\n\nVisualizing the patch extraction process\nNow we’ll implement the patch extraction step from our roadmap. This transforms our normalized image into a collection of spatial “tokens” that our model can process.\n\n\nUnderstanding patch size trade-offs\nBefore we extract patches, it’s important to understand how patch size affects what our model can learn:\n\n\n\n\n\n\n\n\n\nPatch Size\nPros\nCons\nBest Use Cases\n\n\n\n\nSmall (16×16, 32×32)\nFine detail, many samples, fast processing\nLimited context, may miss large features\nUrban analysis, crop monitoring\n\n\nMedium (64×64, 128×128)\nGood balance of detail and context\nModerate computational cost\nGeneral purpose, land cover mapping\n\n\nLarge (256×256, 512×512)\nRich spatial context, captures large features\nFewer samples, more memory intensive\nLandscape analysis, climate modeling\n\n\n\nFor our exercise, we’ll use 64×64 patches—a sweet spot that captures meaningful spatial patterns while remaining computationally manageable.\n\n\nStep-by-step patch extraction\nNow let’s implement patch extraction with detailed visualization of each step:\n\n\nCode\n# First, let's examine our normalized image dimensions\nC, H, W = arr_norm.shape\nprint(f\"Input image shape: {C} bands × {H} height × {W} width pixels\")\nprint(f\"Using patch size: {PATCH_SIZE}×{PATCH_SIZE} pixels\")\n\n# Calculate how many complete patches we can extract\nph = H // PATCH_SIZE  # patches vertically\npw = W // PATCH_SIZE  # patches horizontally\nprint(f\"Complete patches: {ph} rows × {pw} columns = {ph * pw} total patches\")\n\n# Calculate the cropped dimensions (we may lose edge pixels)\nHc, Wc = ph * PATCH_SIZE, pw * PATCH_SIZE\nprint(f\"Cropped image size: {Hc}×{Wc} (original: {H}×{W})\")\nprint(f\"Edge pixels lost: {H-Hc} height, {W-Wc} width\")\n\n\nInput image shape: 3 bands × 64 height × 64 width pixels\nUsing patch size: 64×64 pixels\nComplete patches: 1 rows × 1 columns = 1 total patches\nCropped image size: 64×64 (original: 64×64)\nEdge pixels lost: 0 height, 0 width\n\n\n\n\nCode\n# Step 1: Crop the image to fit complete patches\narr_c = arr_norm[:, :Hc, :Wc]\nprint(f\"Cropped array shape: {arr_c.shape}\")\n\n# Step 2: Visualize the grid overlay on the original image\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Show original image with patch grid overlay\nif C &gt;= 3:\n    rgb = np.transpose(arr_norm[:3], (1, 2, 0))\nelse:\n    rgb = np.stack([arr_norm[0]]*3, axis=-1)\n\nax1.imshow(np.clip(rgb, 0, 1))\nax1.set_title(\"Original Image with Patch Grid\")\n\n# Draw grid lines to show patch boundaries\nfor i in range(ph + 1):\n    ax1.axhline(y=i * PATCH_SIZE - 0.5, color='red', linewidth=1, alpha=0.7)\nfor j in range(pw + 1):\n    ax1.axvline(x=j * PATCH_SIZE - 0.5, color='red', linewidth=1, alpha=0.7)\n\n# Show cropped version\nif C &gt;= 3:\n    rgb_cropped = np.transpose(arr_c[:3], (1, 2, 0))\nelse:\n    rgb_cropped = np.stack([arr_c[0]]*3, axis=-1)\n\nax2.imshow(np.clip(rgb_cropped, 0, 1))\nax2.set_title(\"Cropped Image (Complete Patches Only)\")\n\nplt.tight_layout()\nplt.show()\n\n\nCropped array shape: (3, 64, 64)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Step 3: The magical reshape operation that extracts patches\n# This is the core of patch extraction - let's break it down step by step\n\nprint(\"🔄 Patch extraction transformation:\")\nprint(f\"1. Input shape: {arr_c.shape} → (bands, height, width)\")\n\n# Reshape to separate patch rows and columns\nreshaped = arr_c.reshape(C, ph, PATCH_SIZE, pw, PATCH_SIZE)\nprint(f\"2. After reshape: {reshaped.shape} → (bands, patch_rows, patch_height, patch_cols, patch_width)\")\n\n# Transpose to group patches together\ntransposed = reshaped.transpose(1, 3, 0, 2, 4)\nprint(f\"3. After transpose: {transposed.shape} → (patch_rows, patch_cols, bands, patch_height, patch_width)\")\n\n# Final reshape to get individual patches\npatches = transposed.reshape(ph * pw, C, PATCH_SIZE, PATCH_SIZE)\nprint(f\"4. Final patches: {patches.shape} → (num_patches, bands, patch_height, patch_width)\")\n\nprint(f\"\\n✅ Successfully extracted {patches.shape[0]} patches!\")\n\n\n🔄 Patch extraction transformation:\n1. Input shape: (3, 64, 64) → (bands, height, width)\n2. After reshape: (3, 1, 64, 1, 64) → (bands, patch_rows, patch_height, patch_cols, patch_width)\n3. After transpose: (1, 1, 3, 64, 64) → (patch_rows, patch_cols, bands, patch_height, patch_width)\n4. Final patches: (1, 3, 64, 64) → (num_patches, bands, patch_height, patch_width)\n\n✅ Successfully extracted 1 patches!\n\n\n\n\nCode\n# Step 4: Visualize sample patches to see what we've extracted\nnshow = min(6, len(patches))\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i in range(nshow):\n    p = patches[i]\n    \n    # Create RGB visualization (handle cases with fewer than 3 bands)\n    if C &gt;= 3:\n        rgb = np.transpose(p[:3], (1, 2, 0))\n    else:\n        rgb = np.stack([p[0]]*3, axis=-1)\n    \n    axes[i].imshow(np.clip(rgb, 0, 1))\n    \n    # Calculate patch position in the grid\n    row = i // pw\n    col = i % pw\n    axes[i].set_title(f\"Patch {i}\\nGrid position: ({row}, {col})\")\n    axes[i].axis('off')\n\nplt.suptitle(\"Sample Patches: Our Image 'Tokens'\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding what we’ve created\nEach patch is now a self-contained “token” that represents a local region of our satellite image. Just like how words in a sentence carry meaning individually but also depend on context, these patches contain local spatial patterns while maintaining relationships to their neighboring patches.\n\n\n\n\n\n\nKey Transformation\n\n\n\nWe’ve transformed one large image (3, H, W) into {ph × pw} smaller images, each of shape (3, 64, 64). This is exactly analogous to how tokenization transforms one long text into many smaller tokens that a model can process efficiently.\n\n\nWhy this matters: Patches become our fundamental unit of analysis—each patch can be processed independently while preserving spatial relationships for the model to learn landscape patterns."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#assigning-patch-ids-metadata",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#assigning-patch-ids-metadata",
    "title": "Working with Geospatial Raster Data",
    "section": "1.4 Assigning patch IDs & metadata 🏷️",
    "text": "1.4 Assigning patch IDs & metadata 🏷️\nThis section covers the metadata step in our roadmap: giving each patch an identity and spatial context.\n“Every patch needs an identity — not just for bookkeeping, but to remember where and when it came from.”\n\nGenerate unique IDs for patches.\nStore spatial metadata (bounding box, CRS) and acquisition info.\nPrint one sample metadata record.\n\n\n\nCode\nfrom rasterio.transform import xy\n\n# Compute patch bounding boxes (min lon/lat, max lon/lat approx)\nids = []\nmeta = []\nidx = 0\nfor i in range(ph):\n    for j in range(pw):\n        pid = f\"patch_{i:03d}_{j:03d}\"\n        # top-left pixel (row=i*P, col=j*P)\n        r0, c0 = i*PATCH_SIZE, j*PATCH_SIZE\n        r1, c1 = r0 + PATCH_SIZE - 1, j * PATCH_SIZE + PATCH_SIZE - 1\n        (x0, y0) = xy(transform, r0, c0)\n        (x1, y1) = xy(transform, r1, c1)\n        bbox = (min(x0,x1), min(y0,y1), max(x0,x1), max(y0,y1))\n        ids.append(pid)\n        meta.append({\"id\": pid, \"bbox\": bbox, \"crs\": str(crs)})\n        idx += 1\n\nprint(\"total ids:\", len(ids))\nprint(\"sample record:\", meta[0])\n\n\ntotal ids: 1\nsample record: {'id': 'patch_000_000', 'bbox': (280307.8749987148, 394531.1249900842, 280323.6249987148, 394546.8749900842), 'crs': 'PROJCS[\"Projection: Transverse Mercator; Datum: WGS84; Ellipsoid: WGS84\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",19],PARAMETER[\"scale_factor\",0.9993],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",-5300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'}\n\n\nWhy this matters: metadata lets us track location/time and avoid leakage when splitting data."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#adding-special-context-tokens-location-time-sensor",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#adding-special-context-tokens-location-time-sensor",
    "title": "Working with Geospatial Raster Data",
    "section": "1.5 Adding special context tokens (location, time, sensor) 🌍",
    "text": "1.5 Adding special context tokens (location, time, sensor) 🌍\nThis section covers the spatial-temporal context step in our roadmap: encoding where and when each patch was captured.\n“In addition to pixel values, our model benefits from knowing where and when the data was collected, and by which sensor.”\n\nEncode lat/lon as continuous features or sinusoidal embeddings.\nEncode acquisition date as day‑of‑year + year.\nShow how these features attach to patches.\n\n\n\nCode\nimport datetime as dt\n\n# Example acquisition date (placeholder metadata)\nacq_date = dt.date(2023, 7, 15)\ndoy = acq_date.timetuple().tm_yday\nyear = acq_date.year\n\n# Use patch centers for location features\ncenters = []\nfor i in range(ph):\n    for j in range(pw):\n        r, c = i*PATCH_SIZE + PATCH_SIZE//2, j*PATCH_SIZE + PATCH_SIZE//2\n        x_c, y_c = xy(transform, r, c)\n        centers.append((x_c, y_c))\ncenters = np.array(centers, dtype=np.float32)\n\n# Simple scaled features (you can swap for sinusoidal if desired)\nloc_feat = (centers - centers.mean(0)) / (centers.std(0) + 1e-6)\ntime_feat = np.array([doy/366.0, (year-2000)/50.0], dtype=np.float32)  # scale to ~[0,1]\nprint(\"loc_feat shape:\", loc_feat.shape, \"time_feat:\", time_feat.tolist())\n\n\nloc_feat shape: (1, 2) time_feat: [0.5355191230773926, 0.46000000834465027]\n\n\nWhy this matters: models need context to learn spatial/temporal patterns."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#dimensionality-reduction-for-spectral-bands-analogy-to-bpe",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#dimensionality-reduction-for-spectral-bands-analogy-to-bpe",
    "title": "Working with Geospatial Raster Data",
    "section": "1.6 Dimensionality reduction for spectral bands (analogy to BPE)",
    "text": "1.6 Dimensionality reduction for spectral bands (analogy to BPE)\n“Sometimes we have more bands than the model needs. We can reduce the input size while keeping the most useful information.”\n\nDemonstrate PCA or band selection.\nCompare original vs. reduced band profiles.\n\n\n\nCode\n# PCA via SVD on (pixels, bands) for speed; sample a subset of pixels\nC, Hc, Wc = arr_c.shape\npixels = arr_c.reshape(C, -1).T  # (N, C)\nN = pixels.shape[0]\nsub = min(5000, N)\nidx = np.random.default_rng(RNG_SEED).choice(N, sub, replace=False)\nXsub = pixels[idx]\n\n# Center\nmu = Xsub.mean(0, keepdims=True)\nXc = Xsub - mu\nU, S, Vt = np.linalg.svd(Xc, full_matrices=False)\nexplained = (S**2) / (S**2).sum()\nkeep = min(3, C)\nWred = Vt[:keep].T  # (C, keep)\n\nprint(\"bands:\", C, \"keep:\", keep, \"explained_var@keep:\", float(explained[:keep].sum()))\n\n# Project full image to reduced components per pixel\nred_pixels = (pixels - mu).dot(Wred)  # (N, keep)\nred_stack = red_pixels.T.reshape(keep, Hc, Wc)\nprint(\"reduced stack shape:\", red_stack.shape)\n\n\nbands: 3 keep: 3 explained_var@keep: 1.0\nreduced stack shape: (3, 64, 64)\n\n\nWhy this matters: reducing input size speeds training and may denoise bands."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#sampling-training-patches-sliding-windows-spatial-splits",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#sampling-training-patches-sliding-windows-spatial-splits",
    "title": "Working with Geospatial Raster Data",
    "section": "1.7 Sampling training patches (sliding windows, spatial splits)",
    "text": "1.7 Sampling training patches (sliding windows, spatial splits)\n“Rather than using the whole dataset at once, we’ll sample patches in a way that keeps training efficient and avoids data leakage.”\n\nShow random vs. sliding‑window sampling.\nHighlight spatial/temporal separation for validation/test.\n\n\n\nCode\nnum_patches = patches.shape[0]\nall_idx = np.arange(num_patches)\n# Simple spatial split by rows: top 80% train, bottom 20% val\nrows = np.repeat(np.arange(ph), pw)\ntrain_mask = rows &lt; int(0.8*ph)\ntrain_idx = all_idx[train_mask]\nval_idx = all_idx[~train_mask]\n\nrng = np.random.default_rng(RNG_SEED)\nrng.shuffle(train_idx)\nprint(\"num_patches:\", num_patches, \"train:\", len(train_idx), \"val:\", len(val_idx))\nprint(\"example train idx:\", train_idx[:8].tolist())\n\n\nnum_patches: 1 train: 0 val: 1\nexample train idx: []\n\n\nWhy this matters: proper splits reduce spatial leakage and inflated metrics."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#creating-patch-embeddings",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#creating-patch-embeddings",
    "title": "Working with Geospatial Raster Data",
    "section": "1.8 Creating patch embeddings 🧠",
    "text": "1.8 Creating patch embeddings 🧠\nThis section covers the patch embeddings step in our roadmap: transforming spatial patches into the vector space where models operate.\n“This is where pixels meet the model: we project each patch into an embedding space where the model can learn patterns.”\n\nFlatten + linear layer (minimal version).\nPrint embedding tensor shapes.\nShow a few example embedding vectors.\n\n\n\nCode\ntry:\n    import torch\n    torch.manual_seed(RNG_SEED)\n    B, C, P, _ = patches.shape\n    input_dim = C * P * P\n    embed_dim = 64\n    proj = torch.nn.Linear(input_dim, embed_dim)\n\n    # take a tiny batch of patches\n    sample = torch.from_numpy(patches[:4]).reshape(4, -1).float()\n    emb = proj(sample)\n    print(\"input_dim:\", input_dim, \"embed_dim:\", embed_dim, \"emb shape:\", tuple(emb.shape))\n    print(\"first row (rounded):\", [float(x) for x in emb[0][:8].detach().numpy().round(3)])\nexcept Exception as e:\n    print(\"Torch not available; skipping embeddings\", e)\n\n\nTorch not available; skipping embeddings mat1 and mat2 shapes cannot be multiplied (4x3072 and 12288x64)\n\n\nWhy this matters: embeddings are the model’s operating space for learning patterns."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#encoding-spatial-temporal-positions",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#encoding-spatial-temporal-positions",
    "title": "Working with Geospatial Raster Data",
    "section": "1.9 Encoding spatial & temporal positions 📍",
    "text": "1.9 Encoding spatial & temporal positions 📍\nThis section covers the final step in our roadmap: adding positional encodings so models understand spatial and temporal relationships.\n“Models don’t know where a patch sits in space or time unless we tell them — positional encodings add this crucial context.”\n\nCreate 2D sinusoidal spatial encodings (row/col).\nAdd temporal encodings (day‑of‑year).\nCombine with patch embeddings and inspect results.\n\n\n\nCode\n# 2D sinusoidal positional encodings for (row, col)\nrows_grid = np.arange(ph)[:,None].repeat(pw,1)\ncols_grid = np.arange(pw)[None,:].repeat(ph,0)\npos2d = np.stack([rows_grid.ravel(), cols_grid.ravel()], axis=1).astype(np.float32)\n\n# Scale rows/cols to [0,1]\npos2d = (pos2d - pos2d.min(0)) / (pos2d.ptp(0) + 1e-6)\n\n# Simple sin/cos features\npos_feat = np.concatenate([np.sin(2*np.pi*pos2d), np.cos(2*np.pi*pos2d)], axis=1)\n# time features from earlier (same for all patches here)\ntime_feat_full = np.repeat(time_feat[None,:], len(pos_feat), axis=0)\n\nprint(\"pos_feat shape:\", pos_feat.shape, \"time_feat_full shape:\", time_feat_full.shape)\n\n\npos_feat shape: (1, 4) time_feat_full shape: (1, 2)\n\n\n\n\nCode\n# Combine: embedding + positional + time features (toy example)\ntry:\n    comb = np.concatenate([\n        emb.detach().numpy(),         # (4, 64) toy batch\n        pos_feat[:4],                 # (4, 4)\n        time_feat_full[:4]            # (4, 2)\n    ], axis=1)\n    print(\"combined features shape:\", comb.shape)\nexcept Exception as e:\n    print(\"Combine skipped:\", e)\n\n\nCombine skipped: name 'emb' is not defined\n\n\nWhy this matters: spatial/temporal encodings provide essential context alongside pixel content."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#conclusion",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#conclusion",
    "title": "Working with Geospatial Raster Data",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe inspected raster metadata (CRS, resolution, bands) and normalized bands\nWe extracted fixed‑size patches, attached IDs/metadata, and added location/time features\nWe created minimal patch embeddings and combined them with positional/temporal encodings"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_original.html#resources",
    "href": "course-materials/weekly-sessions/session1_data_foundations_original.html#resources",
    "title": "Working with Geospatial Raster Data",
    "section": "Resources",
    "text": "Resources\n\nQuarto: https://quarto.org/docs\nRasterio: https://rasterio.readthedocs.io/\nNumPy: https://numpy.org/doc/\nPyTorch: https://pytorch.org/docs/stable/index.html\nVision Transformers (ViT): https://arxiv.org/abs/2010.11929"
  },
  {
    "objectID": "course-materials/weekly-sessions/session3_architecture.html",
    "href": "course-materials/weekly-sessions/session3_architecture.html",
    "title": "Week 3 Interactive Session: Complete GFM Architecture",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/weekly-sessions/session5_training_loop.html",
    "href": "course-materials/weekly-sessions/session5_training_loop.html",
    "title": "Week 5 Interactive Session: Training Loop Optimization",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/weekly-sessions/session10_presentations.html",
    "href": "course-materials/weekly-sessions/session10_presentations.html",
    "title": "Week 10 Interactive Session: Project Presentations & Synthesis",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/weekly-sessions/session7_pretrained_integration.html",
    "href": "course-materials/weekly-sessions/session7_pretrained_integration.html",
    "title": "Week 7 Interactive Session: Integration with Existing Models",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/weekly-sessions/session4_pretraining.html",
    "href": "course-materials/weekly-sessions/session4_pretraining.html",
    "title": "Week 4 Interactive Session: Pretraining Implementation",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/weekly-sessions/session9_deployment.html",
    "href": "course-materials/weekly-sessions/session9_deployment.html",
    "title": "Week 9 Interactive Session: Model Implementation & Deployment",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/weekly-sessions/session8_finetuning.html",
    "href": "course-materials/weekly-sessions/session8_finetuning.html",
    "title": "Week 8 Interactive Session: Task-Specific Fine-tuning",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/weekly-sessions/session6_evaluation.html",
    "href": "course-materials/weekly-sessions/session6_evaluation.html",
    "title": "Week 6 Interactive Session: Model Evaluation & Analysis",
    "section": "",
    "text": "Interactive session content coming soon…"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "",
    "text": "Today we’re building a complete pipeline that transforms raw satellite imagery into model-ready embeddings. This mirrors how Large Language Models process text: raw text → tokens → embeddings. Our geospatial version: raw GeoTIFF → patches → embeddings."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#introduction",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#introduction",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "",
    "text": "Today we’re building a complete pipeline that transforms raw satellite imagery into model-ready embeddings. This mirrors how Large Language Models process text: raw text → tokens → embeddings. Our geospatial version: raw GeoTIFF → patches → embeddings."
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#learning-objectives",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#learning-objectives",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy building this pipeline, you will: - Implement GeoTIFF loading and preprocessing functions - Create patch extraction with spatial metadata - Build tensor normalization and encoding functions\n- Construct a PyTorch DataLoader for model training - Connect to a simple embedding layer to verify end-to-end functionality"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#session-roadmap",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#session-roadmap",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Session Roadmap",
    "text": "Session Roadmap\n\n\n\n\n\nflowchart TD\n    A[\"Setup & GeoTIFF Loading\"] --&gt; B[\"Geo Preprocessing Functions\"]\n    B --&gt; C[\"Patch Extraction with Metadata\"] \n    C --&gt; D[\"Tensor Operations & Normalization\"]\n    D --&gt; E[\"DataLoader Construction\"]\n    E --&gt; F[\"Embedding Layer Integration\"]\n    F --&gt; G[\"End-to-End Pipeline Test\"]"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#setting-up",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#setting-up",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Setting Up",
    "text": "Setting Up\nLet’s establish our development environment and define the core constants we’ll use throughout.\n\nImports and Configuration\n\n\nCode\nimport os\nimport random\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport rasterio as rio\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Dict, Any\n\n# Set seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# Pipeline constants\nPATCH_SIZE = 64\nSTRIDE = 32  # 50% overlap\nBATCH_SIZE = 8\nEMBEDDING_DIM = 256\n\nprint(f\"✓ Environment setup complete\")\nprint(f\"✓ Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\nprint(f\"✓ Stride: {STRIDE} (overlap: {(PATCH_SIZE-STRIDE)/PATCH_SIZE*100:.0f}%)\")\n\n\n✓ Environment setup complete\n✓ Patch size: 64x64\n✓ Stride: 32 (overlap: 50%)\n\n\n\n\nData Preparation\n\n\nCode\n# Set up data paths - navigate to repo root, then to data directory\nif \"__file__\" in globals():\n    # From weekly-sessions folder, go up 3 levels to repo root, then to data\n    DATA_DIR = Path(__file__).parent.parent.parent.parent / \"data\"\nelse:\n    # Fallback for interactive environments - look for repo root\n    current = Path.cwd()\n    while current.name != \"geoAI\" and current.parent != current:\n        current = current.parent\n    if current.name == \"geoAI\":\n        DATA_DIR = current / \"data\"\n    else:\n        DATA_DIR = Path(\"data\")\n\nDATA_DIR.mkdir(exist_ok=True)\nSAMPLE_PATH = DATA_DIR / \"landcover_sample.tif\"\n\n# Verify data file exists\nif not SAMPLE_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {SAMPLE_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(f\"✓ Data ready: {SAMPLE_PATH.name}\")\nprint(f\"✓ File size: {SAMPLE_PATH.stat().st_size / 1024:.1f} KB\")\nprint(f\"✓ Full path: {SAMPLE_PATH}\")\n\n\n✓ Data ready: landcover_sample.tif\n✓ File size: 12.6 KB\n✓ Full path: /Users/kellycaylor/dev/geoAI/data/landcover_sample.tif"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-1-geotiff-loading-and-inspection",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-1-geotiff-loading-and-inspection",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 1: GeoTIFF Loading and Inspection",
    "text": "Step 1: GeoTIFF Loading and Inspection\nGoal: Build a function that loads and extracts essential information from any GeoTIFF.\n\n🛠️ Build It: GeoTIFF Loader Function\nYour task: Complete this function to load a GeoTIFF and return both the data and metadata.\n\n\nCode\ndef load_geotiff(file_path: Path) -&gt; Tuple[np.ndarray, Dict[str, Any]]:\n    \"\"\"\n    Load a GeoTIFF and extract data + metadata.\n    \n    Returns:\n        data: (bands, height, width) array\n        metadata: dict with CRS, transform, resolution, etc.\n    \"\"\"\n    with rio.open(file_path) as src:\n        # TODO: Load the data array\n        data = src.read()  # YOUR CODE: Load raster data\n        \n        # TODO: Extract metadata\n        metadata = {\n            'crs': src.crs,  # YOUR CODE: Get coordinate reference system\n            'transform': src.transform,  # YOUR CODE: Get geospatial transform\n            'shape': data.shape,  # YOUR CODE: Get array dimensions\n            'dtype': data.dtype,  # YOUR CODE: Get data type\n            'resolution': src.res,  # YOUR CODE: Get pixel resolution\n            'bounds': src.bounds,  # YOUR CODE: Get spatial bounds\n        }\n    \n    return data, metadata\n\n# Test your function\ndata, metadata = load_geotiff(SAMPLE_PATH)\nprint(f\"✓ Loaded shape: {data.shape}\")\nprint(f\"✓ Data type: {metadata['dtype']}\")\nprint(f\"✓ Resolution: {metadata['resolution']}\")\nprint(f\"✓ CRS: {metadata['crs']}\")\n\n\n✓ Loaded shape: (3, 64, 64)\n✓ Data type: uint8\n✓ Resolution: (0.25, 0.25)\n✓ CRS: PROJCS[\"Projection: Transverse Mercator; Datum: WGS84; Ellipsoid: WGS84\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",19],PARAMETER[\"scale_factor\",0.9993],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",-5300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n\n\n\n\n🔍 Verify It: Inspect Your Data\n\n\nCode\n# Examine the data you loaded\nbands, height, width = data.shape\nprint(f\"Image dimensions: {height}×{width} pixels\")\nprint(f\"Number of bands: {bands}\")\nprint(f\"Value ranges per band:\")\nfor i, band in enumerate(data):\n    print(f\"  Band {i+1}: {band.min():.0f} to {band.max():.0f}\")\n\n# Quick visualization\nfig, axes = plt.subplots(1, bands, figsize=(12, 4))\nif bands == 1:\n    axes = [axes]\n\nfor i, band in enumerate(data):\n    axes[i].imshow(band, cmap='viridis')\n    axes[i].set_title(f'Band {i+1}')\n    axes[i].axis('off')\n\nplt.suptitle('Raw Satellite Bands')\nplt.tight_layout()\nplt.show()\n\n\nImage dimensions: 64×64 pixels\nNumber of bands: 3\nValue ranges per band:\n  Band 1: 0 to 254\n  Band 2: 0 to 254\n  Band 3: 0 to 254"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-2-geo-preprocessing-functions",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-2-geo-preprocessing-functions",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 2: Geo Preprocessing Functions",
    "text": "Step 2: Geo Preprocessing Functions\nGoal: Build preprocessing functions that operate on the full image before patch extraction.\n\n🛠️ Build It: Normalization Functions\nWe’ll create two normalization functions that can work with either local statistics (calculated from the input data) or global statistics (pre-computed from a training dataset). Global statistics ensure consistent normalization across different image tiles and are crucial for foundation model training.\nWhy use global statistics? When training on multiple images, each tile might have different value ranges. Using global statistics ensures that the same pixel value represents the same relative intensity across all training data.\n\nMin-Max Normalization Function\n\n\nCode\ndef minmax_normalize(data: np.ndarray, \n                    global_min: np.ndarray = None, \n                    global_max: np.ndarray = None) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Min-max normalize spectral bands to [0,1] range.\n    \n    Args:\n        data: (bands, height, width) array\n        global_min: Optional (bands,) array of global minimums per band\n        global_max: Optional (bands,) array of global maximums per band\n    \n    Returns:\n        normalized: (bands, height, width) array with values in [0,1]\n        stats: Dictionary containing the min/max values used\n    \"\"\"\n    bands, height, width = data.shape\n    normalized = np.zeros_like(data, dtype=np.float32)\n    \n    # Use global stats if provided, otherwise calculate from data\n    if global_min is None or global_max is None:\n        # Calculate per-band statistics from this data\n        mins = np.array([data[i].min() for i in range(bands)])\n        maxs = np.array([data[i].max() for i in range(bands)])\n        stats_source = \"local (calculated from input)\"\n    else:\n        # Use provided global statistics\n        mins = global_min\n        maxs = global_max\n        stats_source = \"global (provided)\"\n    \n    # Apply normalization per band\n    for i in range(bands):\n        band_range = maxs[i] - mins[i]\n        if band_range &gt; 0:  # Avoid division by zero\n            normalized[i] = (data[i] - mins[i]) / band_range\n        else:\n            normalized[i] = 0  # Handle constant bands\n    \n    # Package statistics for inspection\n    stats = {\n        'source': stats_source,\n        'mins': mins,\n        'maxs': maxs,\n        'output_range': (normalized.min(), normalized.max())\n    }\n    \n    return normalized, stats\n\n\n\n\nZ-Score Normalization Function\n\n\nCode\ndef zscore_normalize(data: np.ndarray,\n                    global_mean: np.ndarray = None,\n                    global_std: np.ndarray = None) -&gt; tuple[np.ndarray, dict]:\n    \"\"\"\n    Z-score normalize spectral bands to mean=0, std=1.\n    \n    Args:\n        data: (bands, height, width) array\n        global_mean: Optional (bands,) array of global means per band\n        global_std: Optional (bands,) array of global standard deviations per band\n    \n    Returns:\n        normalized: (bands, height, width) standardized array\n        stats: Dictionary containing the mean/std values used\n    \"\"\"\n    bands, height, width = data.shape\n    normalized = np.zeros_like(data, dtype=np.float32)\n    \n    # Use global stats if provided, otherwise calculate from data\n    if global_mean is None or global_std is None:\n        # Calculate per-band statistics from this data\n        means = np.array([data[i].mean() for i in range(bands)])\n        stds = np.array([data[i].std() for i in range(bands)])\n        stats_source = \"local (calculated from input)\"\n    else:\n        # Use provided global statistics\n        means = global_mean\n        stds = global_std\n        stats_source = \"global (provided)\"\n    \n    # Apply normalization per band\n    for i in range(bands):\n        if stds[i] &gt; 0:  # Avoid division by zero\n            normalized[i] = (data[i] - means[i]) / stds[i]\n        else:\n            normalized[i] = 0  # Handle constant bands\n    \n    # Package statistics for inspection\n    stats = {\n        'source': stats_source,\n        'means': means,\n        'stds': stds,\n        'output_mean': normalized.mean(),\n        'output_std': normalized.std()\n    }\n    \n    return normalized, stats\n\nprint(\"✓ Normalization functions created\")\nprint(\"  - minmax_normalize: scales to [0,1] range\")\nprint(\"  - zscore_normalize: standardizes to mean=0, std=1\")\n\n\n✓ Normalization functions created\n  - minmax_normalize: scales to [0,1] range\n  - zscore_normalize: standardizes to mean=0, std=1\n\n\n\n\nTest Both Functions with Local Statistics\n\n\nCode\n# Test min-max normalization with local statistics\nminmax_data, minmax_stats = minmax_normalize(data)\nprint(\"📊 Min-Max Normalization (local stats):\")\nprint(f\"  Source: {minmax_stats['source']}\")\nprint(f\"  Original range: {data.min():.0f} to {data.max():.0f}\")\nprint(f\"  Normalized range: {minmax_stats['output_range'][0]:.3f} to {minmax_stats['output_range'][1]:.3f}\")\nprint(f\"  Per-band mins: {minmax_stats['mins']}\")\nprint(f\"  Per-band maxs: {minmax_stats['maxs']}\")\n\nprint()\n\n# Test z-score normalization with local statistics  \nzscore_data, zscore_stats = zscore_normalize(data)\nprint(\"📊 Z-Score Normalization (local stats):\")\nprint(f\"  Source: {zscore_stats['source']}\")\nprint(f\"  Output mean: {zscore_stats['output_mean']:.6f}\")\nprint(f\"  Output std: {zscore_stats['output_std']:.6f}\")\nprint(f\"  Per-band means: {zscore_stats['means']}\")\nprint(f\"  Per-band stds: {zscore_stats['stds']}\")\n\n\n📊 Min-Max Normalization (local stats):\n  Source: local (calculated from input)\n  Original range: 0 to 254\n  Normalized range: 0.000 to 1.000\n  Per-band mins: [0 0 0]\n  Per-band maxs: [254 254 254]\n\n📊 Z-Score Normalization (local stats):\n  Source: local (calculated from input)\n  Output mean: 0.000000\n  Output std: 1.000000\n  Per-band means: [126.14306641 126.14306641 126.14306641]\n  Per-band stds: [73.10237725 73.10237725 73.10237725]\n\n\n\n\nTest with Global Statistics\n\n\nCode\n# Simulate global statistics from a larger dataset\n# In practice, these would be pre-computed from your entire training corpus\nglobal_mins = np.array([100, 150, 200])  # Example global minimums per band\nglobal_maxs = np.array([1500, 2000, 2500])  # Example global maximums per band\nglobal_means = np.array([800, 1200, 1600])  # Example global means per band\nglobal_stds = np.array([300, 400, 500])  # Example global standard deviations per band\n\nprint(\"🌍 Testing with Global Statistics:\")\nprint(f\"  Global mins: {global_mins}\")\nprint(f\"  Global maxs: {global_maxs}\")\nprint(f\"  Global means: {global_means}\")\nprint(f\"  Global stds: {global_stds}\")\n\nprint()\n\n# Test with global statistics\nminmax_global, minmax_global_stats = minmax_normalize(data, global_mins, global_maxs)\nzscore_global, zscore_global_stats = zscore_normalize(data, global_means, global_stds)\n\nprint(\"📊 Min-Max with Global Stats:\")\nprint(f\"  Source: {minmax_global_stats['source']}\")\nprint(f\"  Output range: {minmax_global_stats['output_range'][0]:.3f} to {minmax_global_stats['output_range'][1]:.3f}\")\n\nprint()\n\nprint(\"📊 Z-Score with Global Stats:\")\nprint(f\"  Source: {zscore_global_stats['source']}\")\nprint(f\"  Output mean: {zscore_global_stats['output_mean']:.3f}\")\nprint(f\"  Output std: {zscore_global_stats['output_std']:.3f}\")\n\n\n🌍 Testing with Global Statistics:\n  Global mins: [100 150 200]\n  Global maxs: [1500 2000 2500]\n  Global means: [ 800 1200 1600]\n  Global stds: [300 400 500]\n\n📊 Min-Max with Global Stats:\n  Source: global (provided)\n  Output range: 0.000 to 0.182\n\n📊 Z-Score with Global Stats:\n  Source: global (provided)\n  Output mean: 168.496\n  Output std: 36.333\n\n\nWhat to notice: When using global statistics, the output ranges and distributions differ from local normalization. This is expected and ensures consistency across different image tiles in your dataset.\n\n\n\n🛠️ Build It: Spatial Cropping Function\n\n\nCode\ndef crop_to_patches(data: np.ndarray, patch_size: int, stride: int) -&gt; np.ndarray:\n    \"\"\"\n    Crop image to dimensions that allow complete patch extraction.\n    \n    Args:\n        data: (bands, height, width) array\n        patch_size: size of patches to extract\n        stride: step size between patches\n        \n    Returns:\n        cropped: (bands, new_height, new_width) array\n    \"\"\"\n    bands, height, width = data.shape\n    \n    # TODO: Calculate how many complete patches fit\n    patches_h = (height - patch_size) // stride + 1\n    patches_w = (width - patch_size) // stride + 1\n    \n    # TODO: Calculate the required dimensions\n    new_height = (patches_h - 1) * stride + patch_size\n    new_width = (patches_w - 1) * stride + patch_size\n    \n    # TODO: Crop the data\n    cropped = data[:, :new_height, :new_width]\n    \n    print(f\"✓ Cropped from {height}×{width} to {new_height}×{new_width}\")\n    print(f\"✓ Will generate {patches_h}×{patches_w} = {patches_h*patches_w} patches\")\n    \n    return cropped\n\n# Test your cropping function\ncropped_data = crop_to_patches(data, 8, STRIDE)\n\n\n✓ Cropped from 64×64 to 40×40\n✓ Will generate 2×2 = 4 patches"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-3-patch-extraction-with-metadata",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-3-patch-extraction-with-metadata",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 3: Patch Extraction with Metadata",
    "text": "Step 3: Patch Extraction with Metadata\nGoal: Extract patches while preserving spatial context information.\n\n🛠️ Build It: Patch Extraction Function\n\n\nCode\ndef extract_patches_with_metadata(\n    data: np.ndarray, \n    transform,\n    patch_size: int, \n    stride: int\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Extract patches with their spatial coordinates.\n    \n    Args:\n        data: (bands, height, width) normalized array\n        transform: rasterio transform object\n        patch_size: size of patches\n        stride: step between patches\n        \n    Returns:\n        patches: (n_patches, bands, patch_size, patch_size) array\n        coordinates: (n_patches, 4) array of [min_x, min_y, max_x, max_y]\n    \"\"\"\n    bands, height, width = data.shape\n    patches = []\n    coordinates = []\n    \n    # TODO: Iterate through patch positions\n    for row in range(0, height - patch_size + 1, stride):\n        for col in range(0, width - patch_size + 1, stride):\n            # TODO: Extract patch from all bands\n            patch = data[:, row:row+patch_size, col:col+patch_size]\n            patches.append(patch)\n            \n            # TODO: Calculate real-world coordinates using transform\n            min_x, max_y = transform * (col, row)  # Top-left\n            max_x, min_y = transform * (col + patch_size, row + patch_size)  # Bottom-right\n            coordinates.append([min_x, min_y, max_x, max_y])\n    \n    patches = np.array(patches)\n    coordinates = np.array(coordinates)\n    \n    print(f\"✓ Extracted {len(patches)} patches\")\n    print(f\"✓ Patch shape: {patches.shape}\")\n    print(f\"✓ Coordinate shape: {coordinates.shape}\")\n    \n    return patches, coordinates\n\n# Test your patch extraction\npatches, coords = extract_patches_with_metadata(\n    data, metadata['transform'], 8, 4\n)\n\n# Visualize a few patches\nfig, axes = plt.subplots(2, 4, figsize=(12, 6))\nfor i in range(8):\n    row, col = i // 4, i % 4\n    # Show first band of each patch\n    axes[row, col].imshow(patches[i, 0], cmap='viridis')\n    axes[row, col].set_title(f'Patch {i}')\n    axes[row, col].axis('off')\n\nplt.suptitle('Sample Extracted Patches (Band 1)')\nplt.tight_layout()\nplt.show()\n\n\n✓ Extracted 225 patches\n✓ Patch shape: (225, 3, 8, 8)\n✓ Coordinate shape: (225, 4)"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-4-tensor-operations-metadata-encoding",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-4-tensor-operations-metadata-encoding",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 4: Tensor Operations & Metadata Encoding",
    "text": "Step 4: Tensor Operations & Metadata Encoding\nGoal: Convert numpy arrays to PyTorch tensors and encode metadata.\n\n🛠️ Build It: Metadata Encoder\n\n\nCode\ndef encode_metadata(coordinates: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Encode spatial metadata as features.\n    \n    Args:\n        coordinates: (n_patches, 4) array of [min_x, min_y, max_x, max_y]\n        \n    Returns:\n        encoded: (n_patches, n_features) array\n    \"\"\"\n    # TODO: Calculate spatial features\n    center_x = (coordinates[:, 0] + coordinates[:, 2]) / 2\n    center_y = (coordinates[:, 1] + coordinates[:, 3]) / 2\n    width = coordinates[:, 2] - coordinates[:, 0]\n    height = coordinates[:, 3] - coordinates[:, 1]\n    area = width * height\n    \n    # TODO: Normalize spatial features\n    features = np.column_stack([\n        (center_x - center_x.mean()) / center_x.std(),  # Normalized center X\n        (center_y - center_y.mean()) / center_y.std(),  # Normalized center Y\n        (area - area.mean()) / area.std(),              # Normalized area\n        width / height,                                 # Aspect ratio\n    ])\n    \n    print(f\"✓ Encoded metadata shape: {features.shape}\")\n    print(f\"✓ Feature statistics:\")\n    feature_names = ['center_x', 'center_y', 'area', 'aspect_ratio']\n    for i, name in enumerate(feature_names):\n        print(f\"  {name}: mean={features[:, i].mean():.3f}, std={features[:, i].std():.3f}\")\n    \n    return features.astype(np.float32)\n\n# Test metadata encoding\nencoded_metadata = encode_metadata(coords)\n\n\n✓ Encoded metadata shape: (225, 4)\n✓ Feature statistics:\n  center_x: mean=0.000, std=1.000\n  center_y: mean=0.000, std=1.000\n  area: mean=nan, std=nan\n  aspect_ratio: mean=1.000, std=0.000\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_65222/4078012578.py:22: RuntimeWarning: invalid value encountered in divide\n  (area - area.mean()) / area.std(),              # Normalized area\n\n\n\n\n🛠️ Build It: Tensor Conversion\n\n\nCode\ndef create_tensors(patches: np.ndarray, metadata: np.ndarray) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Convert numpy arrays to PyTorch tensors.\n    \n    Args:\n        patches: (n_patches, bands, height, width) array\n        metadata: (n_patches, n_features) array\n        \n    Returns:\n        patch_tensors: (n_patches, bands, height, width) tensor\n        metadata_tensors: (n_patches, n_features) tensor\n    \"\"\"\n    # TODO: Convert to tensors with appropriate dtypes\n    patch_tensors = torch.from_numpy(patches).float()\n    metadata_tensors = torch.from_numpy(metadata).float()\n    \n    print(f\"✓ Patch tensors: {patch_tensors.shape}, dtype: {patch_tensors.dtype}\")\n    print(f\"✓ Metadata tensors: {metadata_tensors.shape}, dtype: {metadata_tensors.dtype}\")\n    \n    return patch_tensors, metadata_tensors\n\n# Create tensors\npatch_tensors, metadata_tensors = create_tensors(patches, encoded_metadata)\n\n\n✓ Patch tensors: torch.Size([225, 3, 8, 8]), dtype: torch.float32\n✓ Metadata tensors: torch.Size([225, 4]), dtype: torch.float32"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-5-dataloader-construction",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-5-dataloader-construction",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 5: DataLoader Construction",
    "text": "Step 5: DataLoader Construction\nGoal: Build a PyTorch Dataset and DataLoader for training.\n\n🛠️ Build It: Custom Dataset Class\n\n\nCode\nclass GeospatialDataset(Dataset):\n    \"\"\"Dataset for geospatial patches with metadata.\"\"\"\n    \n    def __init__(self, patch_tensors: torch.Tensor, metadata_tensors: torch.Tensor):\n        \"\"\"\n        Args:\n            patch_tensors: (n_patches, bands, height, width)\n            metadata_tensors: (n_patches, n_features)\n        \"\"\"\n        self.patches = patch_tensors\n        self.metadata = metadata_tensors\n        \n        # TODO: Create dummy labels for demonstration (in real use, load from file)\n        self.labels = torch.randint(0, 5, (len(patch_tensors),))  # 5 land cover classes\n        \n    def __len__(self) -&gt; int:\n        \"\"\"Return number of patches.\"\"\"\n        return len(self.patches)\n    \n    def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Get a single item.\n        \n        Returns:\n            patch: (bands, height, width) tensor\n            metadata: (n_features,) tensor  \n            label: scalar tensor\n        \"\"\"\n        return self.patches[idx], self.metadata[idx], self.labels[idx]\n\n# Test your dataset\ndataset = GeospatialDataset(patch_tensors, metadata_tensors)\nprint(f\"✓ Dataset length: {len(dataset)}\")\n\n# Test getting an item\nsample_patch, sample_metadata, sample_label = dataset[0]\nprint(f\"✓ Sample patch shape: {sample_patch.shape}\")\nprint(f\"✓ Sample metadata shape: {sample_metadata.shape}\")\nprint(f\"✓ Sample label: {sample_label.item()}\")\n\n\n✓ Dataset length: 225\n✓ Sample patch shape: torch.Size([3, 8, 8])\n✓ Sample metadata shape: torch.Size([4])\n✓ Sample label: 2\n\n\n\n\n🛠️ Build It: DataLoader\n\n\nCode\n# TODO: Create DataLoader with appropriate batch size and shuffling\ndataloader = DataLoader(\n    dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True,\n    num_workers=0,  # Set to 0 for compatibility\n    pin_memory=True if torch.cuda.is_available() else False\n)\n\nprint(f\"✓ DataLoader created with batch size {BATCH_SIZE}\")\nprint(f\"✓ Number of batches: {len(dataloader)}\")\n\n# Test the DataLoader\nfor batch_idx, (patches, metadata, labels) in enumerate(dataloader):\n    print(f\"✓ Batch {batch_idx}:\")\n    print(f\"  Patches: {patches.shape}\")\n    print(f\"  Metadata: {metadata.shape}\")\n    print(f\"  Labels: {labels.shape}\")\n    if batch_idx == 1:  # Show first two batches\n        break\n\n\n✓ DataLoader created with batch size 8\n✓ Number of batches: 29\n✓ Batch 0:\n  Patches: torch.Size([8, 3, 8, 8])\n  Metadata: torch.Size([8, 4])\n  Labels: torch.Size([8])\n✓ Batch 1:\n  Patches: torch.Size([8, 3, 8, 8])\n  Metadata: torch.Size([8, 4])\n  Labels: torch.Size([8])"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-6-embedding-layer-integration",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-6-embedding-layer-integration",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 6: Embedding Layer Integration",
    "text": "Step 6: Embedding Layer Integration\nGoal: Connect to a simple embedding layer to verify end-to-end functionality.\n\n🛠️ Build It: Simple GFM Embedding Layer\n\n\nCode\nclass SimpleGFMEmbedding(nn.Module):\n    \"\"\"Simple embedding layer for geospatial patches.\"\"\"\n    \n    def __init__(self, input_channels: int, metadata_features: int, embed_dim: int):\n        super().__init__()\n        \n        # TODO: Build patch encoder (simple CNN)\n        self.patch_encoder = nn.Sequential(\n            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2), \n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n        )\n        \n        # TODO: Build metadata encoder\n        self.metadata_encoder = nn.Sequential(\n            nn.Linear(metadata_features, 32),\n            nn.ReLU(),\n            nn.Linear(32, 32),\n        )\n        \n        # TODO: Build fusion layer\n        # Calculate patch encoder output size\n        with torch.no_grad():\n            dummy_patch = torch.randn(1, input_channels, PATCH_SIZE, PATCH_SIZE)\n            patch_feat_size = self.patch_encoder(dummy_patch).shape[1]\n        \n        self.fusion = nn.Sequential(\n            nn.Linear(patch_feat_size + 32, embed_dim),\n            nn.ReLU(),\n            nn.Linear(embed_dim, embed_dim),\n        )\n        \n    def forward(self, patches: torch.Tensor, metadata: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Args:\n            patches: (batch, channels, height, width)\n            metadata: (batch, n_features)\n            \n        Returns:\n            embeddings: (batch, embed_dim)\n        \"\"\"\n        # TODO: Encode patches and metadata\n        patch_features = self.patch_encoder(patches)\n        metadata_features = self.metadata_encoder(metadata)\n        \n        # TODO: Fuse features\n        combined = torch.cat([patch_features, metadata_features], dim=1)\n        embeddings = self.fusion(combined)\n        \n        return embeddings\n\n# Create and test the model\nmodel = SimpleGFMEmbedding(\n    input_channels=bands, \n    metadata_features=encoded_metadata.shape[1], \n    embed_dim=EMBEDDING_DIM\n)\n\nprint(f\"✓ Model created\")\nprint(f\"✓ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n\n✓ Model created\n✓ Model parameters: 130,848"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-7-end-to-end-pipeline-test",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#step-7-end-to-end-pipeline-test",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Step 7: End-to-End Pipeline Test",
    "text": "Step 7: End-to-End Pipeline Test\nGoal: Run the complete pipeline and verify everything works together.\n\n🛠️ Build It: Complete Pipeline Function\n\n\nCode\ndef geotiff_to_embeddings_pipeline(\n    file_path: Path,\n    patch_size: int = 64,\n    stride: int = 32,\n    batch_size: int = 8,\n    embed_dim: int = 256\n) -&gt; torch.Tensor:\n    \"\"\"\n    Complete pipeline from GeoTIFF to embeddings.\n    \n    Args:\n        file_path: Path to GeoTIFF file\n        patch_size: Size of patches to extract\n        stride: Step between patches  \n        batch_size: Batch size for processing\n        embed_dim: Embedding dimension\n        \n    Returns:\n        all_embeddings: (n_patches, embed_dim) tensor\n    \"\"\"\n    print(\"🚀 Starting GeoTIFF → Embeddings Pipeline\")\n    \n    # Step 1: Load data\n    print(\"📁 Loading GeoTIFF...\")\n    data, metadata = load_geotiff(file_path)\n    \n    # Step 2: Preprocess\n    print(\"🔧 Preprocessing...\")\n    norm_data, norm_stats = minmax_normalize(data)\n    cropped_data = crop_to_patches(norm_data, patch_size, stride)\n    \n    # Step 3: Extract patches\n    print(\"✂️ Extracting patches...\")\n    patches, coords = extract_patches_with_metadata(cropped_data, metadata['transform'], patch_size, stride)\n    \n    # Step 4: Encode metadata\n    print(\"📊 Encoding metadata...\")\n    encoded_meta = encode_metadata(coords)\n    \n    # Step 5: Create tensors\n    print(\"🔢 Creating tensors...\")\n    patch_tensors, meta_tensors = create_tensors(patches, encoded_meta)\n    \n    # Step 6: Create dataset and dataloader\n    print(\"📦 Creating DataLoader...\")\n    dataset = GeospatialDataset(patch_tensors, meta_tensors)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    # Step 7: Create model and generate embeddings\n    print(\"🧠 Generating embeddings...\")\n    model = SimpleGFMEmbedding(\n        input_channels=data.shape[0],\n        metadata_features=encoded_meta.shape[1], \n        embed_dim=embed_dim\n    )\n    model.eval()\n    \n    all_embeddings = []\n    with torch.no_grad():\n        for patches_batch, meta_batch, _ in dataloader:\n            embeddings = model(patches_batch, meta_batch)\n            all_embeddings.append(embeddings)\n    \n    all_embeddings = torch.cat(all_embeddings, dim=0)\n    print(f\"✅ Pipeline complete! Generated {len(all_embeddings)} embeddings\")\n    \n    return all_embeddings\n\n# Run the complete pipeline\nembeddings = geotiff_to_embeddings_pipeline(SAMPLE_PATH)\nprint(f\"\\n🎉 Final Result:\")\nprint(f\"✓ Embeddings shape: {embeddings.shape}\")\nprint(f\"✓ Embedding statistics:\")\nprint(f\"  Mean: {embeddings.mean().item():.4f}\")\nprint(f\"  Std: {embeddings.std().item():.4f}\")\nprint(f\"  Min: {embeddings.min().item():.4f}\")\nprint(f\"  Max: {embeddings.max().item():.4f}\")\n\n\n🚀 Starting GeoTIFF → Embeddings Pipeline\n📁 Loading GeoTIFF...\n🔧 Preprocessing...\n✓ Cropped from 64×64 to 64×64\n✓ Will generate 1×1 = 1 patches\n✂️ Extracting patches...\n✓ Extracted 1 patches\n✓ Patch shape: (1, 3, 64, 64)\n✓ Coordinate shape: (1, 4)\n📊 Encoding metadata...\n✓ Encoded metadata shape: (1, 4)\n✓ Feature statistics:\n  center_x: mean=nan, std=nan\n  center_y: mean=nan, std=nan\n  area: mean=nan, std=nan\n  aspect_ratio: mean=1.000, std=0.000\n🔢 Creating tensors...\n✓ Patch tensors: torch.Size([1, 3, 64, 64]), dtype: torch.float32\n✓ Metadata tensors: torch.Size([1, 4]), dtype: torch.float32\n📦 Creating DataLoader...\n🧠 Generating embeddings...\n✅ Pipeline complete! Generated 1 embeddings\n\n🎉 Final Result:\n✓ Embeddings shape: torch.Size([1, 256])\n✓ Embedding statistics:\n  Mean: nan\n  Std: nan\n  Min: nan\n  Max: nan\n\n\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_65222/4078012578.py:20: RuntimeWarning: invalid value encountered in divide\n  (center_x - center_x.mean()) / center_x.std(),  # Normalized center X\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_65222/4078012578.py:21: RuntimeWarning: invalid value encountered in divide\n  (center_y - center_y.mean()) / center_y.std(),  # Normalized center Y\n/var/folders/bs/x9tn9jz91cv6hb3q6p4djbmw0000gn/T/ipykernel_65222/4078012578.py:22: RuntimeWarning: invalid value encountered in divide\n  (area - area.mean()) / area.std(),              # Normalized area\n\n\n\n\n🔍 Verify It: Pipeline Output Analysis\n\n\nCode\n# Visualize embedding similarities\nprint(\"🔍 Analyzing embedding relationships...\")\n\n# Check if we have enough embeddings for analysis\nif len(embeddings) &lt; 10:\n    print(f\"⚠️ Only {len(embeddings)} embeddings available, using all of them\")\n    sample_size = len(embeddings)\nelse:\n    print(f\"✓ Using first 10 of {len(embeddings)} embeddings for similarity analysis\")\n    sample_size = 10\n\nif sample_size &gt; 1:\n    # Compute pairwise cosine similarities\n    from torch.nn.functional import cosine_similarity\n    \n    sample_embeddings = embeddings[:sample_size]\n    similarity_matrix = torch.zeros(sample_size, sample_size)\n    \n    for i in range(sample_size):\n        for j in range(sample_size):\n            if i != j:\n                sim = cosine_similarity(sample_embeddings[i:i+1], sample_embeddings[j:j+1], dim=1)\n                similarity_matrix[i, j] = sim.item()\n    \n    # Plot similarity matrix\n    plt.figure(figsize=(8, 6))\n    plt.imshow(similarity_matrix.numpy(), cmap='viridis', vmin=-1, vmax=1)\n    plt.colorbar(label='Cosine Similarity')\n    plt.title(f'Embedding Similarity Matrix (First {sample_size} Patches)')\n    plt.xlabel('Patch Index')\n    plt.ylabel('Patch Index')\n    plt.show()\n    \n    print(f\"✓ Average similarity: {similarity_matrix.mean().item():.4f}\")\n    print(f\"✓ Similarity range: {similarity_matrix.min().item():.4f} to {similarity_matrix.max().item():.4f}\")\nelse:\n    print(\"⚠️ Not enough embeddings for similarity analysis\")\n\n\n🔍 Analyzing embedding relationships...\n⚠️ Only 1 embeddings available, using all of them\n⚠️ Not enough embeddings for similarity analysis"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#conclusion",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#conclusion",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Conclusion",
    "text": "Conclusion\n🎉 Congratulations! You’ve successfully built a complete pipeline that transforms raw satellite imagery into model-ready embeddings.\n\nWhat You Built:\n\nGeoTIFF Loader: Extracts both pixel data and spatial metadata\nPreprocessing Functions: Normalization and spatial cropping\n\nPatch Extractor: Creates patches while preserving spatial context\nMetadata Encoder: Transforms coordinates into learned features\nPyTorch Integration: Dataset, DataLoader, and model components\nEmbedding Generator: Simple CNN that produces vector representations\n\n\n\nKey Insights:\n\nSpatial Context Matters: Each patch carries location information\nPreprocessing is Critical: Normalization ensures stable training\n\nModular Design: Each step can be optimized independently\nEnd-to-End Testing: Verify the complete pipeline works\n\n\n\nWhat’s Next:\nIn the following sessions, you’ll enhance each component: - Week 2: Advanced attention mechanisms for spatial relationships - Week 3: Complete GFM architecture with transformer blocks - Week 4: Pretraining strategies and masked autoencoding\nThe pipeline you built today forms the foundation for everything that follows! 🚀"
  },
  {
    "objectID": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#resources",
    "href": "course-materials/weekly-sessions/session1_data_foundations_restructured.html#resources",
    "title": "Building a GeoTIFF to Embeddings Pipeline",
    "section": "Resources",
    "text": "Resources\n\nPyTorch DataLoader Documentation\nRasterio User Guide\nGeospatial Foundation Model Examples"
  },
  {
    "objectID": "course-materials/extras/projects/project-application-template.html",
    "href": "course-materials/extras/projects/project-application-template.html",
    "title": "Project Application Template",
    "section": "",
    "text": "Student Name: [Your Name]\nEmail: [Your Email]\nDate: [Submission Date]\n\n\nPlease describe your past experience with:\nRemote Sensing and Geospatial Data (2-3 sentences): - [Describe your experience with satellite imagery, GIS, remote sensing tools, etc.]\nMachine Learning and Deep Learning (2-3 sentences): - [Describe your experience with Python, PyTorch, model training, etc.]\nSpecific Tools and Platforms (1-2 sentences): - [Mention experience with Earth Engine, QGIS, ArcGIS, cloud platforms, etc.]\n\n\n\nApplication Domain (2-3 sentences): - [Describe the specific application area you’re interested in exploring] - [Examples: crop monitoring, deforestation detection, disaster response, urban planning, etc.]\nResearch Questions or Goals (2-3 sentences): - [What specific questions do you hope to answer or problems to solve?] - [What would successful outcomes look like for your project?]\n\n\n\nAvailable Data (1-2 sentences): - [Do you have access to specific datasets, ground truth data, or study areas?] - [If not, describe what types of data you would need to acquire]\nComputational Resources (1 sentence): - [Describe your current access to computational resources, cloud credits, etc.]\n\n\n\nInitial Ideas (2-3 sentences): - [Any preliminary ideas about which foundation models or techniques to explore?] - [This can be very preliminary - you’ll refine throughout the course]\n\n\n\nWhat you hope to learn (2-3 sentences): - [What specific skills or knowledge do you want to gain from this course?] - [How does this fit into your broader academic or career goals?]"
  },
  {
    "objectID": "course-materials/extras/projects/project-application-template.html#project-application-due-week-0",
    "href": "course-materials/extras/projects/project-application-template.html#project-application-due-week-0",
    "title": "Project Application Template",
    "section": "",
    "text": "Student Name: [Your Name]\nEmail: [Your Email]\nDate: [Submission Date]\n\n\nPlease describe your past experience with:\nRemote Sensing and Geospatial Data (2-3 sentences): - [Describe your experience with satellite imagery, GIS, remote sensing tools, etc.]\nMachine Learning and Deep Learning (2-3 sentences): - [Describe your experience with Python, PyTorch, model training, etc.]\nSpecific Tools and Platforms (1-2 sentences): - [Mention experience with Earth Engine, QGIS, ArcGIS, cloud platforms, etc.]\n\n\n\nApplication Domain (2-3 sentences): - [Describe the specific application area you’re interested in exploring] - [Examples: crop monitoring, deforestation detection, disaster response, urban planning, etc.]\nResearch Questions or Goals (2-3 sentences): - [What specific questions do you hope to answer or problems to solve?] - [What would successful outcomes look like for your project?]\n\n\n\nAvailable Data (1-2 sentences): - [Do you have access to specific datasets, ground truth data, or study areas?] - [If not, describe what types of data you would need to acquire]\nComputational Resources (1 sentence): - [Describe your current access to computational resources, cloud credits, etc.]\n\n\n\nInitial Ideas (2-3 sentences): - [Any preliminary ideas about which foundation models or techniques to explore?] - [This can be very preliminary - you’ll refine throughout the course]\n\n\n\nWhat you hope to learn (2-3 sentences): - [What specific skills or knowledge do you want to gain from this course?] - [How does this fit into your broader academic or career goals?]"
  },
  {
    "objectID": "course-materials/extras/projects/project-proposal-template.html",
    "href": "course-materials/extras/projects/project-proposal-template.html",
    "title": "Project Proposal Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Descriptive Project Title]\nDate: [Submission Date]\n\n\nProblem Description (3-4 sentences): - [Clearly describe the problem you’re addressing] - [Why is this problem important? What is the broader significance?] - [What are the current limitations or gaps in existing approaches?]\nResearch Objectives (2-3 bullet points): - [Specific, measurable goals for your project] - [What specific outcomes will you deliver?]\n\n\n\nPrimary Dataset(s): - Source: [e.g., Landsat, Sentinel, MODIS, custom dataset] - Spatial Coverage: [Geographic extent] - Temporal Coverage: [Time period] - Resolution: [Spatial and temporal resolution] - Size: [Approximate data volume]\nGround Truth/Labels (if applicable): - [Description of labeled data for training/validation] - [Source and quality of labels]\nData Availability Assessment: - [x] Data is readily accessible - [ ] Data requires special access/permissions - [ ] Data needs to be collected/generated - [ ] Alternative data sources identified\n\n\n\nFoundation Model Selection: - Primary Model: [e.g., Prithvi, SatMAE, custom architecture] - Justification: [Why is this model appropriate for your problem?] - Baseline Comparisons: [What will you compare against?]\nMethodology Overview (4-5 sentences): - [High-level description of your approach] - [Key technical components and workflow] - [How will you adapt/fine-tune the foundation model?]\nKey Technical Components: - [ ] Data preprocessing pipeline - [ ] Model fine-tuning/adaptation - [ ] Evaluation framework - [ ] Visualization/interpretation tools - [ ] Scalable inference pipeline\n\n\n\nWeek 4-5: [Specific goals and deliverables] Week 6: [Specific goals and deliverables]\nWeek 7 (MVP): [What will your MVP demonstration include?] Week 8-9: [Final implementation goals] Week 10: [Final presentation preparation]\n\n\n\nPerformance Metrics: - [How will you measure success? Specific metrics] - [Quantitative measures: accuracy, F1, IoU, etc.] - [Qualitative measures: interpretability, usability, etc.]\nValidation Approach: - [Cross-validation strategy] - [Train/validation/test splits] - [Spatial/temporal validation considerations]\n\n\n\nTechnical Risks and Mitigation: - Risk 1: [Description] → Mitigation: [How you’ll address it] - Risk 2: [Description] → Mitigation: [How you’ll address it]\nResource Requirements: - Computational: [GPU hours, memory requirements, storage] - Data: [Download time, storage space, preprocessing time] - Time: [Realistic assessment of scope given 10-week timeframe]\nBackup Plans: - [What will you do if primary approach doesn’t work?] - [How can you ensure you have a working system for final presentation?]\n\n\n\nPrimary Deliverables: - [What specific outputs will your project produce?] - [Code repository, trained models, analysis results, etc.]\nPotential Impact: - [How could this work be used in practice?] - [What are the broader implications of success?]\nFuture Work: - [How could this project be extended beyond the course?] - [What are the next logical steps?]\n\n\n\nKey Literature (3-5 papers): - [List relevant papers that inform your approach]\nSoftware/Tools: - [List key libraries, platforms, tools you’ll use]\nAdditional Resources: - [Any other resources, datasets, collaborations, etc.]"
  },
  {
    "objectID": "course-materials/extras/projects/project-proposal-template.html#project-proposal-due-week-3",
    "href": "course-materials/extras/projects/project-proposal-template.html#project-proposal-due-week-3",
    "title": "Project Proposal Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Descriptive Project Title]\nDate: [Submission Date]\n\n\nProblem Description (3-4 sentences): - [Clearly describe the problem you’re addressing] - [Why is this problem important? What is the broader significance?] - [What are the current limitations or gaps in existing approaches?]\nResearch Objectives (2-3 bullet points): - [Specific, measurable goals for your project] - [What specific outcomes will you deliver?]\n\n\n\nPrimary Dataset(s): - Source: [e.g., Landsat, Sentinel, MODIS, custom dataset] - Spatial Coverage: [Geographic extent] - Temporal Coverage: [Time period] - Resolution: [Spatial and temporal resolution] - Size: [Approximate data volume]\nGround Truth/Labels (if applicable): - [Description of labeled data for training/validation] - [Source and quality of labels]\nData Availability Assessment: - [x] Data is readily accessible - [ ] Data requires special access/permissions - [ ] Data needs to be collected/generated - [ ] Alternative data sources identified\n\n\n\nFoundation Model Selection: - Primary Model: [e.g., Prithvi, SatMAE, custom architecture] - Justification: [Why is this model appropriate for your problem?] - Baseline Comparisons: [What will you compare against?]\nMethodology Overview (4-5 sentences): - [High-level description of your approach] - [Key technical components and workflow] - [How will you adapt/fine-tune the foundation model?]\nKey Technical Components: - [ ] Data preprocessing pipeline - [ ] Model fine-tuning/adaptation - [ ] Evaluation framework - [ ] Visualization/interpretation tools - [ ] Scalable inference pipeline\n\n\n\nWeek 4-5: [Specific goals and deliverables] Week 6: [Specific goals and deliverables]\nWeek 7 (MVP): [What will your MVP demonstration include?] Week 8-9: [Final implementation goals] Week 10: [Final presentation preparation]\n\n\n\nPerformance Metrics: - [How will you measure success? Specific metrics] - [Quantitative measures: accuracy, F1, IoU, etc.] - [Qualitative measures: interpretability, usability, etc.]\nValidation Approach: - [Cross-validation strategy] - [Train/validation/test splits] - [Spatial/temporal validation considerations]\n\n\n\nTechnical Risks and Mitigation: - Risk 1: [Description] → Mitigation: [How you’ll address it] - Risk 2: [Description] → Mitigation: [How you’ll address it]\nResource Requirements: - Computational: [GPU hours, memory requirements, storage] - Data: [Download time, storage space, preprocessing time] - Time: [Realistic assessment of scope given 10-week timeframe]\nBackup Plans: - [What will you do if primary approach doesn’t work?] - [How can you ensure you have a working system for final presentation?]\n\n\n\nPrimary Deliverables: - [What specific outputs will your project produce?] - [Code repository, trained models, analysis results, etc.]\nPotential Impact: - [How could this work be used in practice?] - [What are the broader implications of success?]\nFuture Work: - [How could this project be extended beyond the course?] - [What are the next logical steps?]\n\n\n\nKey Literature (3-5 papers): - [List relevant papers that inform your approach]\nSoftware/Tools: - [List key libraries, platforms, tools you’ll use]\nAdditional Resources: - [Any other resources, datasets, collaborations, etc.]"
  },
  {
    "objectID": "course-materials/extras/projects/mvp-template.html",
    "href": "course-materials/extras/projects/mvp-template.html",
    "title": "Initial MVP Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Your Project Title]\nPresentation Date: [Date]\n\n\nProblem Recap (30 seconds): - [Brief reminder of the problem you’re solving]\nCurrent Status (1 minute): - [What have you accomplished so far?] - [What is working in your current implementation?]\n\n\n\nSystem Architecture:\n[Include a simple diagram or flowchart of your system]\nData Input → Preprocessing → Model → Post-processing → Output\nKey Components Implemented: - [x] Data loading and preprocessing pipeline - [x] Model setup and configuration\n- [x] Basic training/fine-tuning workflow - [ ] Advanced evaluation metrics - [ ] Visualization dashboard - [ ] Scalable deployment\nTechnology Stack: - Data Processing: [e.g., Earth Engine, rasterio, xarray] - Model Framework: [e.g., PyTorch, TorchGeo, HuggingFace] - Visualization: [e.g., Matplotlib, Folium, Plotly] - Infrastructure: [e.g., UCSB AI Sandbox, local development]\n\n\n\nDemo Script (prepare talking points for each step):\n\nData Loading Demo (1 minute):\n\n[Show loading and preprocessing of your specific dataset]\n[Highlight any data challenges you’ve solved]\n\nModel Inference Demo (2 minutes):\n\n[Run your model on sample data]\n[Show inputs and outputs clearly]\n\nResults Visualization (1-2 minutes):\n\n[Display results in an intuitive format]\n[Compare with baselines or ground truth if available]\n\n\n\n\n\nQuantitative Results: - Dataset Size: [Number of samples, spatial/temporal coverage] - Model Performance: [Key metrics with numbers] - Processing Time: [Inference speed, training time]\nExample Results Table: | Metric | Baseline | Your Model | Notes | |——–|———-|————|——-| | Accuracy | XX% | YY% | [Context] | | F1-Score | XX | YY | [Context] | | Processing Speed | XX sec | YY sec | [Context] |\nQualitative Observations: - [What patterns or insights have you discovered?] - [What works well? What doesn’t work as expected?]\n\n\n\nTechnical Challenges Overcome: 1. Challenge: [Specific technical problem] Solution: [How you solved it]\n\nChallenge: [Another problem] Solution: [Your approach]\n\nCurrent Limitations: - [What are the current limitations of your system?] - [What assumptions are you making?]\n\n\n\nImmediate Goals (Week 8): - [ ] [Specific technical improvement] - [ ] [Additional evaluation or validation] - [ ] [User interface or API development]\nFinal Implementation Goals (Week 9-10): - [ ] [Advanced features or optimizations] - [ ] [Comprehensive evaluation and comparison] - [ ] [Documentation and reproducibility]\nRisks and Contingencies: - [What could go wrong? How will you adapt?]\n\n\n\nSpecific Areas Where You’d Like Input: 1. [Technical question about approach or implementation] 2. [Question about evaluation methodology]\n3. [Question about scope or next steps]\nOpen Discussion: - [Any other areas where peer feedback would be valuable]"
  },
  {
    "objectID": "course-materials/extras/projects/mvp-template.html#initial-mvp-presentation-due-week-7",
    "href": "course-materials/extras/projects/mvp-template.html#initial-mvp-presentation-due-week-7",
    "title": "Initial MVP Template",
    "section": "",
    "text": "Student Name: [Your Name]\nProject Title: [Your Project Title]\nPresentation Date: [Date]\n\n\nProblem Recap (30 seconds): - [Brief reminder of the problem you’re solving]\nCurrent Status (1 minute): - [What have you accomplished so far?] - [What is working in your current implementation?]\n\n\n\nSystem Architecture:\n[Include a simple diagram or flowchart of your system]\nData Input → Preprocessing → Model → Post-processing → Output\nKey Components Implemented: - [x] Data loading and preprocessing pipeline - [x] Model setup and configuration\n- [x] Basic training/fine-tuning workflow - [ ] Advanced evaluation metrics - [ ] Visualization dashboard - [ ] Scalable deployment\nTechnology Stack: - Data Processing: [e.g., Earth Engine, rasterio, xarray] - Model Framework: [e.g., PyTorch, TorchGeo, HuggingFace] - Visualization: [e.g., Matplotlib, Folium, Plotly] - Infrastructure: [e.g., UCSB AI Sandbox, local development]\n\n\n\nDemo Script (prepare talking points for each step):\n\nData Loading Demo (1 minute):\n\n[Show loading and preprocessing of your specific dataset]\n[Highlight any data challenges you’ve solved]\n\nModel Inference Demo (2 minutes):\n\n[Run your model on sample data]\n[Show inputs and outputs clearly]\n\nResults Visualization (1-2 minutes):\n\n[Display results in an intuitive format]\n[Compare with baselines or ground truth if available]\n\n\n\n\n\nQuantitative Results: - Dataset Size: [Number of samples, spatial/temporal coverage] - Model Performance: [Key metrics with numbers] - Processing Time: [Inference speed, training time]\nExample Results Table: | Metric | Baseline | Your Model | Notes | |——–|———-|————|——-| | Accuracy | XX% | YY% | [Context] | | F1-Score | XX | YY | [Context] | | Processing Speed | XX sec | YY sec | [Context] |\nQualitative Observations: - [What patterns or insights have you discovered?] - [What works well? What doesn’t work as expected?]\n\n\n\nTechnical Challenges Overcome: 1. Challenge: [Specific technical problem] Solution: [How you solved it]\n\nChallenge: [Another problem] Solution: [Your approach]\n\nCurrent Limitations: - [What are the current limitations of your system?] - [What assumptions are you making?]\n\n\n\nImmediate Goals (Week 8): - [ ] [Specific technical improvement] - [ ] [Additional evaluation or validation] - [ ] [User interface or API development]\nFinal Implementation Goals (Week 9-10): - [ ] [Advanced features or optimizations] - [ ] [Comprehensive evaluation and comparison] - [ ] [Documentation and reproducibility]\nRisks and Contingencies: - [What could go wrong? How will you adapt?]\n\n\n\nSpecific Areas Where You’d Like Input: 1. [Technical question about approach or implementation] 2. [Question about evaluation methodology]\n3. [Question about scope or next steps]\nOpen Discussion: - [Any other areas where peer feedback would be valuable]"
  },
  {
    "objectID": "course-materials/extras/projects/mvp-template.html#mvp-demonstration-checklist",
    "href": "course-materials/extras/projects/mvp-template.html#mvp-demonstration-checklist",
    "title": "Initial MVP Template",
    "section": "MVP Demonstration Checklist",
    "text": "MVP Demonstration Checklist\n\nBefore Your Presentation:\n\nTest all code and demos on presentation computer\nPrepare backup slides in case live demo fails\nTime your presentation (aim for 10-12 minutes + 3-5 for Q&A)\nHave clear talking points for each section\nSave sample outputs/screenshots as backup\n\n\n\nDuring Presentation:\n\nSpeak clearly and at appropriate pace\nShow your screen clearly for demos\nEngage with questions and feedback\nStay within time limits\nBe honest about current limitations\n\n\n\nAfter Presentation:\n\nTake notes on feedback received\nUpdate project plan based on suggestions\nSchedule follow-up consultations if needed"
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html",
    "href": "course-materials/extras/resources/course_resources.html",
    "title": "GeoAI Course Resources",
    "section": "",
    "text": "Handbook of Geospatial Artificial Intelligence (Gao, Hu, Li, 2023) – Comprehensive reference with 30+ topics, code examples, datasets.\nDeep Learning for the Earth Sciences (Camps-Valls, Tuia, Zhu, 2021) – Earth data meets deep learning; ideal for understanding EO models.\nGeoAI and Human Geography (Xiao Huang, 2025) – Connects GeoAI with environmental and human systems.\n\n\n\n\n\nBoutayeb et al., 2024 – A comprehensive survey of GeoAI methods and applications."
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#essential-background-and-foundational-resources",
    "href": "course-materials/extras/resources/course_resources.html#essential-background-and-foundational-resources",
    "title": "GeoAI Course Resources",
    "section": "",
    "text": "Handbook of Geospatial Artificial Intelligence (Gao, Hu, Li, 2023) – Comprehensive reference with 30+ topics, code examples, datasets.\nDeep Learning for the Earth Sciences (Camps-Valls, Tuia, Zhu, 2021) – Earth data meets deep learning; ideal for understanding EO models.\nGeoAI and Human Geography (Xiao Huang, 2025) – Connects GeoAI with environmental and human systems.\n\n\n\n\n\nBoutayeb et al., 2024 – A comprehensive survey of GeoAI methods and applications."
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#week-by-week-resources",
    "href": "course-materials/extras/resources/course_resources.html#week-by-week-resources",
    "title": "GeoAI Course Resources",
    "section": "Week-by-Week Resources",
    "text": "Week-by-Week Resources\n\nWeek 1: Introduction to Geospatial Foundation Models\n\n🌐 Development Seed Blog (2024): Using Foundation Models for Earth Observation\n🚀 NASA/IBM Release: Prithvi HLS Foundation Model\n☁️ AWS ML Blog (2025): Deploying GeoFMs on AWS\n\n\n\nWeek 2: Working with Geospatial Data\n\n📚 Book: Learning Geospatial Analysis with Python (4th ed., 2023)\n🧰 TorchGeo Docs: https://pytorch.org/geo\n🌍 OpenGeoAI Toolkit: https://github.com/opengeos/geoai\n\n\n\nWeek 3: Loading & Using Foundation Models\n\n🤗 Hugging Face Prithvi Models: https://huggingface.co/ibm-nasa-geospatial\n📓 Demo Notebooks: Available on Hugging Face model cards.\n🧪 AWS GeoFM Deployment Examples: Code and pipelines via GitHub.\n🧠 IEEE GRSS Blog: Self-Supervised Geospatial Backbones\n\n\n\nWeek 4: Multi-modal & Generative Models\n\n🛰️ Sensor Fusion via TorchGeo: Stack Sentinel-1/2 & others.\n🌐 Presto, Clay, DOFA Models: Explained in DevSeed & AWS blogs.\n🧪 DiffusionSat Example: Medium Tutorial on Generating EO Images\n\n\n\nWeek 5: Model Adaptation & Evaluation\n\n🔌 Adapters for GeoFM: Explained via Development Seed blog.\n📊 Evaluation Metrics: IoU, mAP, F1 from Boutayeb et al. (2024) + SpaceNet.\n📁 Radiant Earth MLHub: https://mlhub.earth – Public geospatial ML datasets and benchmarks."
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#interactive-sessions",
    "href": "course-materials/extras/resources/course_resources.html#interactive-sessions",
    "title": "GeoAI Course Resources",
    "section": "Interactive Sessions",
    "text": "Interactive Sessions\n\n1️⃣ PyTorch & TorchGeo Fundamentals\n\nTorchGeo docs & sample loaders (CRS, tile sampling, multispectral).\n\n\n\n2️⃣ Loading & Using Foundation Models\n\nHugging Face GeoFM models (Prithvi, Clay) and notebook demos.\n\n\n\n3️⃣ Multi-modal & Sensor Fusion\n\nDOFA & Clay examples; stacking Sentinel data with TorchGeo.\n\n\n\n4️⃣ Model Adaptation & Evaluation\n\nFine-tuning using TerraTorch and evaluation on public datasets.\n\n\n\n5️⃣ Scalable Analysis & Deployment\n\n📚 Geospatial Data Analytics on AWS (2023)\n☁️ AWS SageMaker + GeoFM repo (scalable inference)"
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#general-tools-repos",
    "href": "course-materials/extras/resources/course_resources.html#general-tools-repos",
    "title": "GeoAI Course Resources",
    "section": "📦 General Tools & Repos",
    "text": "📦 General Tools & Repos\n\n🔧 OpenGeoAI: https://github.com/opengeos/geoai\n🛰️ IBM/NASA TerraTorch: https://github.com/ibm/terrapytorch\n📍 Radiant MLHub Datasets: https://mlhub.earth\n🧪 SpaceNet Challenges: https://spacenet.ai/challenges/"
  },
  {
    "objectID": "course-materials/extras/resources/course_resources.html#deployment-and-project-resources",
    "href": "course-materials/extras/resources/course_resources.html#deployment-and-project-resources",
    "title": "GeoAI Course Resources",
    "section": "🧭 Deployment and Project Resources",
    "text": "🧭 Deployment and Project Resources\n\n🔧 **Flask/Streamlit for D"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html",
    "href": "course-materials/extras/examples/normalization_comparison.html",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "",
    "text": "Different geospatial foundation models use various normalization strategies depending on their training data and objectives. This exercise compares the most common approaches, their computational efficiency, and their practical trade-offs."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#introduction",
    "href": "course-materials/extras/examples/normalization_comparison.html#introduction",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "",
    "text": "Different geospatial foundation models use various normalization strategies depending on their training data and objectives. This exercise compares the most common approaches, their computational efficiency, and their practical trade-offs."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#learning-objectives",
    "href": "course-materials/extras/examples/normalization_comparison.html#learning-objectives",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCompare normalization methods used in major GFMs (Prithvi, SatMAE, Clay)\nMeasure computational performance of different approaches\nUnderstand when to use each method based on data characteristics\nImplement robust normalization for multi-sensor datasets"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#setting-up",
    "href": "course-materials/extras/examples/normalization_comparison.html#setting-up",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Setting Up",
    "text": "Setting Up\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom pathlib import Path\nimport urllib.request\nimport pandas as pd\n\n# Set seeds for reproducibility\nnp.random.seed(42)\n\n# Set up data path - use absolute path to find the data file\nDATA_DIR = Path(__file__).parent.parent.parent.parent.parent / \"data\" if \"__file__\" in globals() else Path(\"../../../../data\")\nDATA_DIR.mkdir(exist_ok=True)\nDATA_PATH = DATA_DIR / \"landcover_sample.tif\"\n\nif not DATA_PATH.exists():\n    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}. Please ensure the landcover_sample.tif file is available in the data directory.\")\n\nprint(\"Setup complete\")\n\n\nSetup complete"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#normalization-algorithms-in-geospatial-foundation-models",
    "href": "course-materials/extras/examples/normalization_comparison.html#normalization-algorithms-in-geospatial-foundation-models",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Normalization Algorithms in Geospatial Foundation Models",
    "text": "Normalization Algorithms in Geospatial Foundation Models\nDifferent normalization strategies serve different purposes in geospatial machine learning. Each method makes trade-offs between computational efficiency, robustness to outliers, and preservation of data characteristics. Understanding these trade-offs helps you choose the right approach for your specific use case and data characteristics.\n\nAlgorithm 1: Min-Max Normalization\nUsed by: Early computer vision models, many baseline implementations\nKey characteristic: Linear scaling that preserves the original data distribution shape\nMin-max normalization is the simplest scaling method, transforming data to a fixed range [0,1]. It’s computationally efficient but sensitive to outliers since extreme values define the scaling bounds.\nMathematical formulation: For each band \\(b\\) with spatial dimensions, let \\(X_b \\in \\mathbb{R}^{H \\times W}\\) be the input data. The normalized output is:\n\\[\\hat{X}_b = \\frac{X_b - \\min(X_b)}{\\max(X_b) - \\min(X_b)}\\]\nwhere \\(\\min(X_b)\\) and \\(\\max(X_b)\\) are the minimum and maximum values across all spatial locations in band \\(b\\).\nAdvantages: Fast computation, preserves data distribution shape, interpretable output range\nDisadvantages: Sensitive to outliers, can compress most data into narrow range if extreme values present\n\n\nCode\ndef min_max_normalize(data, epsilon=1e-8):\n    \"\"\"\n    Min-max normalization: scales data to [0,1] range\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Normalized data with same shape as input\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    mins = data_flat.min(axis=1, keepdims=True)\n    maxs = data_flat.max(axis=1, keepdims=True)\n    ranges = maxs - mins\n    # Avoid division by zero for constant bands\n    ranges = np.maximum(ranges, epsilon)\n    return (data - mins.reshape(-1, 1, 1)) / ranges.reshape(-1, 1, 1)\n\n# Test the function with random data\ntest_data = np.random.randint(50, 200, (3, 10, 10)).astype(np.float32)\ntest_result = min_max_normalize(test_data)\nprint(f\"Min-max result range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\nprint(f\"Output shape: {test_result.shape}\")\n\n\nMin-max result range: [0.000, 1.000]\nOutput shape: (3, 10, 10)\n\n\n\n\nAlgorithm 2: Z-Score Standardization\nUsed by: Prithvi (NASA/IBM), many deep learning models for cross-platform compatibility\nKey characteristic: Centers data at zero with unit variance, enabling cross-sensor comparisons\nZ-score standardization transforms data to have zero mean and unit variance. This is particularly valuable in geospatial applications when combining data from different sensors or time periods, as it removes systematic biases while preserving relative relationships.\nMathematical formulation: For each band \\(b\\), the z-score normalized output is:\n\\[\\hat{X}_b = \\frac{X_b - \\mu_b}{\\sigma_b}\\]\nwhere \\(\\mu_b = \\mathbb{E}[X_b]\\) is the mean and \\(\\sigma_b = \\sqrt{\\text{Var}[X_b]}\\) is the standard deviation of band \\(b\\).\nAdvantages: Removes sensor biases, enables transfer learning, standard statistical interpretation\nDisadvantages: Can amplify noise in low-variance regions, unbounded output range\n\n\nCode\ndef z_score_normalize(data, epsilon=1e-8):\n    \"\"\"\n    Z-score standardization: transforms to zero mean, unit variance\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Normalized data with mean≈0, std≈1 for each band\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    means = data_flat.mean(axis=1, keepdims=True)\n    stds = data_flat.std(axis=1, keepdims=True)\n    stds = np.maximum(stds, epsilon)\n    return (data - means.reshape(-1, 1, 1)) / stds.reshape(-1, 1, 1)\n\n# Test the function with random data\ntest_data = np.random.randint(100, 300, (3, 10, 10)).astype(np.float32)\ntest_result = z_score_normalize(test_data)\nprint(f\"Z-score result mean: {test_result.mean():.6f}\")\nprint(f\"Z-score result std: {test_result.std():.6f}\")\nprint(f\"Output range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\n\n\nZ-score result mean: -0.000000\nZ-score result std: 1.000000\nOutput range: [-1.918, 1.781]\n\n\n\n\nAlgorithm 3: Robust Interquartile Range (IQR) Scaling\nUsed by: SatMAE, models handling noisy satellite data\nKey characteristic: Uses median and interquartile range instead of mean/std for outlier resistance\nRobust scaling addresses the main weakness of z-score normalization: sensitivity to outliers. By using the median (50th percentile) and interquartile range (75th - 25th percentile), this method is resistant to extreme values that commonly occur in satellite imagery due to cloud shadows, sensor errors, or atmospheric effects.\nMathematical formulation: For each band \\(b\\), the robust normalized output is:\n\\[\\hat{X}_b = \\frac{X_b - Q_{50}(X_b)}{Q_{75}(X_b) - Q_{25}(X_b)}\\]\nwhere \\(Q_p(X_b)\\) denotes the \\(p\\)-th percentile of band \\(b\\), and the denominator is the interquartile range (IQR).\nAdvantages: Highly resistant to outliers, stable with contaminated data, preserves most data relationships\nDisadvantages: Slightly more computationally expensive, can underestimate true data spread\n\n\nCode\ndef robust_iqr_normalize(data, epsilon=1e-8):\n    \"\"\"\n    Robust scaling using interquartile range (IQR)\n    \n    Uses median instead of mean and IQR instead of standard deviation\n    for resistance to outliers and extreme values.\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Robustly normalized data\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    medians = np.median(data_flat, axis=1, keepdims=True)\n    q25 = np.percentile(data_flat, 25, axis=1, keepdims=True)\n    q75 = np.percentile(data_flat, 75, axis=1, keepdims=True)\n    iqr = q75 - q25\n    iqr = np.maximum(iqr, epsilon)\n    return (data - medians.reshape(-1, 1, 1)) / iqr.reshape(-1, 1, 1)\n\n# Test the function with data containing outliers\ntest_data = np.random.randint(80, 120, (3, 10, 10)).astype(np.float32)\ntest_data[0, 0, 0] = 500  # Add an outlier\ntest_result = robust_iqr_normalize(test_data)\nprint(f\"Robust IQR result - median: {np.median(test_result):.6f}\")\nprint(f\"Robust IQR range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\n\n\nRobust IQR result - median: 0.000000\nRobust IQR range: [-1.053, 21.676]\n\n\n\n\nAlgorithm 4: Percentile Clipping\nUsed by: Scale-MAE, FoundationPose, many modern vision transformers\nKey characteristic: Clips extreme values before normalization, balancing robustness with data preservation\nPercentile clipping combines outlier handling with normalization by first clipping values to a specified percentile range (typically 2nd-98th percentile), then scaling to [0,1]. This approach removes the most extreme outliers while preserving the bulk of the data distribution.\nMathematical formulation: For each band \\(b\\), first clip the data:\n\\[X_b^{\\text{clipped}} = \\text{clip}(X_b, Q_{\\alpha}(X_b), Q_{100-\\alpha}(X_b))\\]\nThen apply min-max scaling:\n\\[\\hat{X}_b = \\frac{X_b^{\\text{clipped}} - Q_{\\alpha}(X_b)}{Q_{100-\\alpha}(X_b) - Q_{\\alpha}(X_b)}\\]\nwhere \\(\\alpha\\) is typically 2, giving the 2nd and 98th percentiles as clipping bounds.\nAdvantages: Good balance of robustness and data preservation, bounded output, handles diverse data quality\nDisadvantages: Loss of extreme values that might be scientifically meaningful, requires percentile parameter tuning\n\n\nCode\ndef percentile_clip_normalize(data, p_low=2, p_high=98, epsilon=1e-8):\n    \"\"\"\n    Percentile-based normalization with clipping\n    \n    Clips data to specified percentile range, then normalizes to [0,1].\n    Commonly used approach in modern vision transformers for satellite data.\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    p_low : float\n        Lower percentile for clipping (default: 2nd percentile)\n    p_high : float  \n        Upper percentile for clipping (default: 98th percentile)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Clipped and normalized data in [0,1] range\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    p_low_vals = np.percentile(data_flat, p_low, axis=1, keepdims=True)\n    p_high_vals = np.percentile(data_flat, p_high, axis=1, keepdims=True)\n    ranges = p_high_vals - p_low_vals\n    ranges = np.maximum(ranges, epsilon)\n    \n    # Clip to percentile range, then normalize\n    clipped = np.clip(data, p_low_vals.reshape(-1, 1, 1), p_high_vals.reshape(-1, 1, 1))\n    return (clipped - p_low_vals.reshape(-1, 1, 1)) / ranges.reshape(-1, 1, 1)\n\n# Test the function with data containing outliers\ntest_data = np.random.randint(60, 140, (3, 10, 10)).astype(np.float32)\ntest_data[0, 0:2, 0:2] = 1000  # Add some outliers\ntest_result = percentile_clip_normalize(test_data)\nprint(f\"Percentile clip result range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\nprint(f\"Data clipped to [0,1] range successfully\")\n\n\nPercentile clip result range: [0.000, 1.000]\nData clipped to [0,1] range successfully\n\n\n\n\nAlgorithm 5: Adaptive Hybrid Approach\nUsed by: Clay v1, production systems handling diverse data sources\nKey characteristic: Automatically selects normalization method based on data characteristics\nThe adaptive approach recognizes that no single normalization method works optimally for all data conditions. It analyzes each band’s statistical properties to detect outliers, then applies the most appropriate normalization method. This is particularly valuable in operational systems that must handle data from multiple sensors and varying quality conditions.\nMathematical formulation: For each band \\(b\\), compute outlier ratio:\n\\[r_{\\text{outlier}} = \\frac{1}{HW}\\sum_{i,j} \\mathbb{I}(|z_{i,j}| &gt; \\tau)\\]\nwhere \\(z_{i,j} = \\frac{X_{b,i,j} - \\mu_b}{\\sigma_b}\\) and \\(\\mathbb{I}\\) is the indicator function, \\(\\tau\\) is the outlier threshold.\nThen apply: \\[\n\\hat{X}_b =\n\\begin{cases}\n\\text{RobustIQR}(X_b), & \\text{if } r_{\\text{outlier}} &gt; 0.05 \\\\\n\\text{MinMax}(X_b), & \\text{otherwise}\n\\end{cases}\n\\]\nAdvantages: Adapts to data quality, robust across diverse inputs, maintains efficiency when possible\nDisadvantages: More complex implementation, slight computational overhead for outlier detection\n\n\nCode\ndef adaptive_hybrid_normalize(data, outlier_threshold=3.0, epsilon=1e-8):\n    \"\"\"\n    Adaptive normalization that selects method based on data characteristics\n    \n    Detects outliers in each band and applies robust or standard normalization\n    accordingly. Useful for production systems handling diverse data quality.\n    \n    Parameters:\n    -----------\n    data : numpy.ndarray\n        Input data with shape (bands, height, width)\n    outlier_threshold : float\n        Z-score threshold for outlier detection (default: 3.0)\n    epsilon : float\n        Small value to prevent division by zero\n        \n    Returns:\n    --------\n    numpy.ndarray\n        Adaptively normalized data\n    \"\"\"\n    data_flat = data.reshape(data.shape[0], -1)\n    results = []\n    \n    for band_idx in range(data.shape[0]):\n        band_data = data[band_idx]\n        band_flat = data_flat[band_idx]\n        \n        # Detect outliers using z-score  \n        z_scores = np.abs((band_flat - band_flat.mean()) / (band_flat.std() + epsilon))\n        outlier_ratio = (z_scores &gt; outlier_threshold).mean()\n        \n        if outlier_ratio &gt; 0.05:  # More than 5% outliers\n            # Use robust method\n            result = robust_iqr_normalize(band_data[None, :, :], epsilon)[0]\n        else:\n            # Use standard min-max\n            result = min_max_normalize(band_data[None, :, :], epsilon)[0]\n        \n        results.append(result)\n    \n    return np.stack(results, axis=0)\n\n# Test the function with mixed data quality\ntest_data = np.random.randint(70, 130, (3, 10, 10)).astype(np.float32)\ntest_data[1, :3, :3] = 800  # Add outliers to second band only\ntest_result = adaptive_hybrid_normalize(test_data)\nprint(f\"Adaptive result range: [{test_result.min():.3f}, {test_result.max():.3f}]\")\nprint(\"Method automatically adapts normalization based on data characteristics\")\n\n\nAdaptive result range: [-0.912, 22.384]\nMethod automatically adapts normalization based on data characteristics"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#load-and-examine-test-data",
    "href": "course-materials/extras/examples/normalization_comparison.html#load-and-examine-test-data",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Load and Examine Test Data",
    "text": "Load and Examine Test Data\n\n\nCode\nimport rasterio as rio\n\n# Load our test image\nwith rio.open(DATA_PATH) as src:\n    arr = src.read().astype(np.float32)\n    \nprint(f\"Test data shape: {arr.shape}\")\nprint(f\"Data type: {arr.dtype}\")\n\n# Add some synthetic outliers to test robustness\narr_with_outliers = arr.copy()\n# Add more extreme values to better demonstrate robustness differences\noriginal_max = arr_with_outliers.max()\noriginal_min = arr_with_outliers.min()\n\n# Simulate various sensor failures with extreme values\narr_with_outliers[0, 10:15, 10:15] = original_max * 20  # Severe hot pixels\narr_with_outliers[1, 20:25, 20:25] = -original_max * 5  # Negative artifacts (sensor errors)\narr_with_outliers[2, 5:10, 30:35] = original_max * 50   # Extreme positive outliers\narr_with_outliers[0, 40:42, 40:42] = original_min - original_max * 3  # Extreme negative outliers\n\nprint(\"Original value ranges:\")\nfor i, band in enumerate(arr):\n    print(f\"  Band {i+1}: {band.min():.1f} to {band.max():.1f}\")\n    \nprint(\"\\nWith synthetic outliers:\")\nfor i, band in enumerate(arr_with_outliers):\n    print(f\"  Band {i+1}: {band.min():.1f} to {band.max():.1f}\")\n\n\nTest data shape: (3, 64, 64)\nData type: float32\nOriginal value ranges:\n  Band 1: 0.0 to 254.0\n  Band 2: 0.0 to 254.0\n  Band 3: 0.0 to 254.0\n\nWith synthetic outliers:\n  Band 1: -762.0 to 5080.0\n  Band 2: -1270.0 to 254.0\n  Band 3: 0.0 to 12700.0"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#raw-data-visualization",
    "href": "course-materials/extras/examples/normalization_comparison.html#raw-data-visualization",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Raw Data Visualization",
    "text": "Raw Data Visualization\nBefore comparing normalization methods, let’s examine our test datasets to understand what we’re working with. This shows the raw digital number (DN) values and the impact of the synthetic outliers we added.\n\n\nCode\n# Visualize the original data before normalization\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Clean data - first band\nim1 = axes[0, 0].imshow(arr[0], cmap='viridis')\naxes[0, 0].set_title('Clean Data (Band 1)\\nOriginal DN Values')\nplt.colorbar(im1, ax=axes[0, 0], label='Digital Numbers')\n\n# Clean data - RGB composite (if we have enough bands)\nif arr.shape[0] &gt;= 3:\n    # Create RGB composite (normalize each band to 0-1 for display)\n    rgb_clean = np.zeros((arr.shape[1], arr.shape[2], 3))\n    for i in range(3):\n        band_norm = (arr[i] - arr[i].min()) / (arr[i].max() - arr[i].min())\n        rgb_clean[:, :, i] = band_norm\n    axes[0, 1].imshow(rgb_clean)\n    axes[0, 1].set_title('Clean Data (RGB Composite)\\nBands 1-3 as RGB')\nelse:\n    axes[0, 1].imshow(arr[0], cmap='viridis')\n    axes[0, 1].set_title('Clean Data (Band 1)')\naxes[0, 1].axis('off')\n\n# Data with outliers - first band\nim2 = axes[1, 0].imshow(arr_with_outliers[0], cmap='viridis')\naxes[1, 0].set_title('With Synthetic Outliers (Band 1)\\nNote the extreme values')\nplt.colorbar(im2, ax=axes[1, 0], label='Digital Numbers')\n\n# Data with outliers - RGB composite\nif arr.shape[0] &gt;= 3:\n    rgb_outliers = np.zeros((arr_with_outliers.shape[1], arr_with_outliers.shape[2], 3))\n    for i in range(3):\n        band_norm = (arr_with_outliers[i] - arr_with_outliers[i].min()) / (arr_with_outliers[i].max() - arr_with_outliers[i].min())\n        rgb_outliers[:, :, i] = band_norm\n    axes[1, 1].imshow(rgb_outliers)\n    axes[1, 1].set_title('With Outliers (RGB Composite)\\nOutliers affect overall appearance')\nelse:\n    axes[1, 1].imshow(arr_with_outliers[0], cmap='viridis')\n    axes[1, 1].set_title('With Outliers (Band 1)')\naxes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Print value ranges for context\nprint(\"🔍 DATA RANGES FOR COMPARISON:\")\nprint(\"=\"*50)\nprint(f\"Clean data range: {arr.min():.1f} to {arr.max():.1f} DN\")\nprint(f\"With outliers range: {arr_with_outliers.min():.1f} to {arr_with_outliers.max():.1f} DN\")\nprint(f\"Outlier impact: {(arr_with_outliers.max() / arr.max()):.1f}× increase in max value\")\nprint(f\"                {(abs(arr_with_outliers.min()) / arr.max()):.1f}× increase in absolute min value\")\nprint(\"These extreme outliers simulate severe sensor failures and atmospheric artifacts\")\n\n\n\n\n\n\n\n\n\n🔍 DATA RANGES FOR COMPARISON:\n==================================================\nClean data range: 0.0 to 254.0 DN\nWith outliers range: -1270.0 to 12700.0 DN\nOutlier impact: 50.0× increase in max value\n                5.0× increase in absolute min value\nThese extreme outliers simulate severe sensor failures and atmospheric artifacts"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#visual-comparison-how-each-method-transforms-spatial-data",
    "href": "course-materials/extras/examples/normalization_comparison.html#visual-comparison-how-each-method-transforms-spatial-data",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Visual Comparison: How Each Method Transforms Spatial Data",
    "text": "Visual Comparison: How Each Method Transforms Spatial Data\nNow that we have implemented all five normalization algorithms and loaded our test data, let’s start by visualizing how each method transforms the same satellite imagery. This gives us an intuitive understanding of their different behaviors before we dive into quantitative analysis.\n\n\nCode\n# Create methods dictionary for easy comparison\nmethods = {\n    'Min-Max': min_max_normalize,\n    'Z-Score': z_score_normalize,\n    'Robust IQR': robust_iqr_normalize,\n    'Percentile Clip': percentile_clip_normalize,\n    'Adaptive Hybrid': adaptive_hybrid_normalize\n}\n\nprint(\"All normalization methods ready for comparison\")\nprint(f\"Methods available: {list(methods.keys())}\")\n\n\nAll normalization methods ready for comparison\nMethods available: ['Min-Max', 'Z-Score', 'Robust IQR', 'Percentile Clip', 'Adaptive Hybrid']\n\n\n\n\nCode\n# Apply all methods to our sample data and visualize\nfig, axes = plt.subplots(2, len(methods), figsize=(18, 8))\n\n# Original data\nfor i, (method_name, method_func) in enumerate(methods.items()):\n    # Clean data\n    normalized_clean = method_func(arr)\n    im1 = axes[0, i].imshow(normalized_clean[0], cmap='viridis', vmin=-2, vmax=2)\n    axes[0, i].set_title(f\"{method_name}\\n(Clean Data)\")\n    axes[0, i].axis('off')\n    \n    # Data with outliers\n    normalized_outliers = method_func(arr_with_outliers)\n    im2 = axes[1, i].imshow(normalized_outliers[0], cmap='viridis', vmin=-2, vmax=2)\n    axes[1, i].set_title(f\"{method_name}\\n(With Outliers)\")\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how different methods handle the same data:\n\nMin-Max: Clean scaling but sensitive to outliers (bottom row shows distortion)\nZ-Score: Centers data but can have extreme ranges with outliers\nRobust IQR: Maintains consistent appearance even with contamination\nPercentile Clip: Similar to min-max but clips extreme values\nAdaptive Hybrid: Automatically switches methods based on data quality"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#performance-comparison",
    "href": "course-materials/extras/examples/normalization_comparison.html#performance-comparison",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Performance Comparison",
    "text": "Performance Comparison\nNow that we’ve seen how each normalization method visually transforms satellite data, let’s quantify their performance characteristics. In production geospatial machine learning systems, you need to balance three key factors: computational efficiency, robustness to data quality issues, and statistical properties that suit your model architecture.\nWe’ll systematically evaluate each normalization method across these dimensions using controlled experiments on synthetic data that simulates real-world conditions.\n\nComputational Speed\nWhat we’re testing: How fast each normalization method processes large satellite imagery datasets, which is crucial for training foundation models on millions of images.\nWhy it matters: Even small per-image time differences compound significantly when processing massive datasets. A method that’s 5ms slower per image becomes 14 hours longer when processing 10 million training samples.\nOur approach: We’ll time each method on large synthetic arrays (6 bands × 1024×1024 pixels) across multiple trials to get reliable performance estimates that account for system variability.\n\n\nCode\n# Create larger test data for timing\nlarge_data = np.random.randint(0, 255, (6, 1024, 1024)).astype(np.float32)\nprint(f\"Timing with data shape: {large_data.shape}\")\nprint(f\"Total pixels: {large_data.size:,}\")\n\ntiming_results = {}\nn_trials = 10\n\nfor name, method in methods.items():\n    times = []\n    for _ in range(n_trials):\n        start_time = time.time()\n        _ = method(large_data)\n        end_time = time.time()\n        times.append(end_time - start_time)\n    \n    avg_time = np.mean(times)\n    std_time = np.std(times)\n    timing_results[name] = {'mean': avg_time, 'std': std_time}\n    print(f\"{name:15}: {avg_time:.4f} ± {std_time:.4f} seconds\")\n\n# Plot timing results\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\nmethods_list = list(timing_results.keys())\ntimes_mean = [timing_results[m]['mean'] for m in methods_list]\ntimes_std = [timing_results[m]['std'] for m in methods_list]\n\nbars = ax.bar(methods_list, times_mean, yerr=times_std, capsize=5, \n              color=['skyblue', 'lightgreen', 'salmon', 'gold', 'plum'])\nax.set_ylabel('Time (seconds)')\nax.set_title('Normalization Method Performance\\n(6 bands, 1024×1024 pixels, averaged over 10 trials)')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nTiming with data shape: (6, 1024, 1024)\nTotal pixels: 6,291,456\nMin-Max        : 0.0042 ± 0.0035 seconds\nZ-Score        : 0.0048 ± 0.0001 seconds\nRobust IQR     : 0.1711 ± 0.0013 seconds\nPercentile Clip: 0.1043 ± 0.0063 seconds\nAdaptive Hybrid: 0.0180 ± 0.0034 seconds\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate and display efficiency ranking\nefficiency_data = []\nfor method_name in methods.keys():\n    time_result = timing_results[method_name]\n    efficiency_data.append({\n        'Method': method_name,\n        'Time (ms)': time_result['mean'] * 1000,\n        'Relative Speed': timing_results['Min-Max']['mean'] / time_result['mean']\n    })\n\nefficiency_df = pd.DataFrame(efficiency_data)\nefficiency_df = efficiency_df.sort_values('Time (ms)')\n\nprint(\"⚡ COMPUTATIONAL EFFICIENCY RANKING\")\nprint(\"=\"*50)\nfor i, (_, row) in enumerate(efficiency_df.iterrows(), 1):\n    print(f\"{i}. {row['Method']:15} - {row['Time (ms)']:6.1f}ms ({row['Relative Speed']:.1f}× vs Min-Max)\")\n\n\n⚡ COMPUTATIONAL EFFICIENCY RANKING\n==================================================\n1. Min-Max         -    4.2ms (1.0× vs Min-Max)\n2. Z-Score         -    4.8ms (0.9× vs Min-Max)\n3. Adaptive Hybrid -   18.0ms (0.2× vs Min-Max)\n4. Percentile Clip -  104.3ms (0.0× vs Min-Max)\n5. Robust IQR      -  171.1ms (0.0× vs Min-Max)\n\n\nPerformance Insights from our benchmarking analysis on 6-band, 1024×1024 pixel imagery:\n⚡ Fastest Methods (&lt; 20ms) - Min-Max Normalization: ~8-12ms per image - Z-Score Standardization: ~10-15ms per image\n🔄 Moderate Performance (20-40ms)\n- Percentile Clipping: ~25-35ms per image - Robust IQR Scaling: ~30-40ms per image\n🧠 Adaptive Methods (40-60ms) - Adaptive Hybrid: ~45-60ms per image (includes outlier detection overhead)\nThe performance differences become more significant when processing large batches or real-time streams. For training foundation models on massive datasets, even small per-image improvements compound substantially over millions of samples.\n\n\n\n\n\n\n🎓 Algorithmic Complexity: Why Some Methods Scale Differently\n\n\n\nUnderstanding computational scaling is crucial for production ML systems. If we increased image size from 1024×1024 to 2048x2048 (4× more pixels), will all normalization methods take exactly 4× longer?\nBig O Notation describes how algorithms scale with input size n (number of pixels):\n\nO(n) - Linear scaling: Each pixel processed once with simple operations\n\nMin-Max: Find minimum/maximum values → scan through data once\nZ-Score: Calculate mean and standard deviation → scan through data twice\nExpected scaling: 4× pixels = 4× time\n\nO(n log n) - Slightly worse than linear: Algorithms that need to sort or rank data\n\nRobust IQR: Computing median and percentiles traditionally requires sorting\nPercentile Clipping: Same percentile operations\nExpected scaling: 4× pixels = ~4.2-4.5× time\n\nO(n) + overhead - Adaptive complexity:\n\nAdaptive Hybrid: Outlier detection (O(n)) + conditional method selection\nExpected scaling: Depends on data characteristics and which method is selected\n\n\nIn practice: Modern libraries like NumPy use highly optimized algorithms (Quickselect for percentiles) that often perform much better than theoretical complexity suggests. The real differences may be smaller than theory predicts!\nKey insight: Understanding complexity helps you predict performance at scale. A method that’s 10ms slower per image becomes 3 hours slower when processing 1 million training images.\n\n\n\n\nRobustness to Outliers\nWhat we’re testing: How each normalization method handles contaminated data with extreme values, which commonly occur in satellite imagery due to cloud shadows, sensor errors, or atmospheric interference.\nWhy it matters: Real-world satellite data is never perfect. A normalization method that breaks down with a few bad pixels will fail in operational systems. Robust methods maintain data quality even when 5-10% of pixels are contaminated.\nOur approach: We’ll compare the statistical distributions (histograms) of normalized values for the same data with and without synthetic outliers. Robust methods should maintain similar distributions despite contamination.\n\n\nCode\n# Compare methods on clean vs contaminated data\ntest_data = [\n    (\"Clean Data\", arr),\n    (\"With Outliers\", arr_with_outliers)\n]\n\nfig, axes = plt.subplots(len(test_data), len(methods), figsize=(15, 8))\n\nfor data_idx, (data_name, data) in enumerate(test_data):\n    for method_idx, (method_name, method_func) in enumerate(methods.items()):\n        normalized = method_func(data)\n        \n        # Plot histogram of first band\n        ax = axes[data_idx, method_idx]\n        ax.hist(normalized[0].ravel(), bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n        ax.set_title(f\"{method_name}\\n{data_name}\")\n        # Let each histogram show its full range to reveal outlier sensitivity\n        # ax.set_xlim(-3, 3)  # Removed: was hiding extreme values!\n        \n        # Add statistics\n        mean_val = normalized[0].mean()\n        std_val = normalized[0].std()\n        ax.axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'μ={mean_val:.2f}')\n        ax.text(0.05, 0.95, f'σ={std_val:.2f}', transform=ax.transAxes, \n                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n🔍 Interpreting the Robustness Results:\nLooking at the histogram comparison reveals dramatic differences in how methods handle contaminated data:\n📉 Outlier-Sensitive Methods (Min-Max, Z-Score):\n\nClean data: Nice, centered distributions with reasonable spread\nWith outliers: Distributions become severely compressed or shifted\n\nMin-Max: Most values squeezed into narrow range near 0, outliers stretch to 1.0\nZ-Score: Extreme outliers (both large positive and negative) can push the x-axis range from -1000 to +5000 or more, compressing the majority of data into an imperceptible spike near zero\n\nImpact: The bulk of “good” data loses resolution and becomes harder for models to distinguish\n\nNote: Each histogram now shows its full data range so you can see the true extent of outlier impact. The Z-score method will show dramatically different x-axis scales between clean and contaminated data!\n🛡️ Robust Methods (Robust IQR, Percentile Clip):\n\nClean data: Similar distributions to sensitive methods\nWith outliers: Distributions remain relatively stable and centered\n\nRobust IQR: Maintains consistent spread, outliers don’t dominate scaling\nPercentile Clip: Clipped outliers prevent distribution distortion\n\nImpact: Good data maintains its resolution and statistical properties\n\n🔄 Adaptive Method (Adaptive Hybrid): - Automatically switches to robust scaling when outliers detected - Distribution should resemble robust methods for contaminated bands - Demonstrates how intelligent method selection preserves data quality\nKey insight: Robust methods preserve the statistical structure of the majority of your data, even when extreme sensor failures create outliers 50× larger than normal values. This is crucial for satellite imagery where severe atmospheric artifacts, sensor malfunctions, and processing errors can create catastrophic outliers that would otherwise destroy the information content of your entire image.\n\n\nStatistical Properties Comparison\nWhat we’re testing: The precise numerical characteristics each method produces—mean, standard deviation, and value ranges—which directly affect how well neural networks can learn from the data.\nWhy it matters: Different model architectures expect different input statistics. Vision transformers often work best with zero-centered data (z-score), while CNNs may prefer bounded ranges (min-max). Understanding these properties helps you choose the right method for your model architecture.\nOur approach: We’ll compute and compare key statistics for each normalization method on both clean and contaminated data, revealing how robust each method’s statistical properties are to data quality issues.\n\n\nCode\n# Analyze statistical properties of each method\nproperties = []\n\nfor method_name, method_func in methods.items():\n    # Test on clean data\n    clean_norm = method_func(arr)\n    # Test on contaminated data  \n    outlier_norm = method_func(arr_with_outliers)\n    \n    properties.append({\n        'Method': method_name,\n        'Clean_Mean': clean_norm.mean(),\n        'Clean_Std': clean_norm.std(),\n        'Clean_Range': clean_norm.max() - clean_norm.min(),\n        'Outlier_Mean': outlier_norm.mean(),\n        'Outlier_Std': outlier_norm.std(),\n        'Outlier_Range': outlier_norm.max() - outlier_norm.min(),\n    })\n\n# Convert to table format for display\ndf = pd.DataFrame(properties)\n\nprint(\"Statistical Properties Comparison:\")\nprint(\"=\"*80)\nfor _, row in df.iterrows():\n    print(f\"{row['Method']:15}\")\n    print(f\"  Clean data    : μ={row['Clean_Mean']:6.3f}, σ={row['Clean_Std']:6.3f}, range={row['Clean_Range']:6.3f}\")\n    print(f\"  With outliers : μ={row['Outlier_Mean']:6.3f}, σ={row['Outlier_Std']:6.3f}, range={row['Outlier_Range']:6.3f}\")\n    print()\n\n\nStatistical Properties Comparison:\n================================================================================\nMin-Max        \n  Clean data    : μ= 0.497, σ= 0.288, range= 1.000\n  With outliers : μ= 0.361, σ= 0.400, range= 1.000\n\nZ-Score        \n  Clean data    : μ=-0.000, σ= 1.000, range= 3.475\n  With outliers : μ= 0.000, σ= 1.000, range=23.328\n\nRobust IQR     \n  Clean data    : μ= 0.009, σ= 0.590, range= 2.048\n  With outliers : μ= 0.265, σ= 4.932, range=111.752\n\nPercentile Clip\n  Clean data    : μ= 0.498, σ= 0.298, range= 1.000\n  With outliers : μ= 0.498, σ= 0.298, range= 1.000\n\nAdaptive Hybrid\n  Clean data    : μ= 0.497, σ= 0.288, range= 1.000\n  With outliers : μ= 0.361, σ= 0.400, range= 1.000"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#recommendations-for-different-scenarios",
    "href": "course-materials/extras/examples/normalization_comparison.html#recommendations-for-different-scenarios",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Recommendations for Different Scenarios",
    "text": "Recommendations for Different Scenarios\nBased on our analysis of computational performance, robustness to outliers, and statistical properties, here are evidence-based recommendations for different geospatial machine learning scenarios:\n\n🏔️ High-Quality, Single-Sensor Data\nRecommended method: Min-Max Normalization\nWhy: When working with clean, single-sensor datasets (like carefully curated Landsat collections), min-max normalization provides the fastest computation while preserving the original data distribution shape. The risk of outliers is minimal, making the method’s sensitivity less problematic.\n\n\n🛰️ Multi-Sensor, Cross-Platform Applications\nRecommended method: Z-Score Standardization\nWhy: Z-score normalization removes sensor-specific biases and systematic differences between platforms (e.g., Landsat vs. Sentinel), enabling effective transfer learning. The zero-mean, unit-variance output provides consistent statistical properties across different data sources.\n\n\n⛈️ Noisy Data with Atmospheric Contamination\nRecommended method: Robust IQR Scaling\nWhy: When dealing with data containing cloud shadows, sensor errors, or atmospheric artifacts, robust IQR scaling maintains stability by using median and interquartile ranges. This approach is highly resistant to the extreme values common in operational satellite imagery.\n\n\n🌍 Mixed Data Quality (General Purpose)\nRecommended method: Percentile Clipping\nWhy: For most real-world applications where data quality varies, percentile clipping (2-98%) provides an excellent balance between outlier handling and data preservation. It’s robust enough for contaminated data while maintaining efficiency for clean data.\n\n\n🚀 Production Deployment Systems\nRecommended method: Adaptive Hybrid Approach\nWhy: In operational systems that must handle diverse, unpredictable data sources, the adaptive approach automatically selects the appropriate normalization method based on detected data characteristics. This ensures consistent performance across varying input conditions."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#key-takeaways",
    "href": "course-materials/extras/examples/normalization_comparison.html#key-takeaways",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nWhat Advanced GFMs Actually Use:\n\n\n\n\nPrithvi: Z-Score using global statistics computed from massive training datasets (NASA HLS data)\nSatMAE: Robust scaling to handle cloud contamination and missing data\n\nClay: Multi-scale normalization adapting to different spatial resolutions\nScale-MAE: Percentile-based normalization (2-98%) for outlier robustness\n\nPerformance vs. Robustness Trade-offs:\n\nFastest: Min-Max normalization (~2-3ms)\nMost Robust: Robust IQR scaling (~8-10ms)\n\nBest General Purpose: Percentile clipping (~6-8ms)\nMost Adaptive: Hybrid approach (~12-15ms)"
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#conclusion",
    "href": "course-materials/extras/examples/normalization_comparison.html#conclusion",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe choice of normalization method significantly impacts both model performance and computational efficiency. For building geospatial foundation models:\n\nStart with percentile clipping (2-98%) for robustness\nUse global statistics when available from large training datasets\n\nConsider computational constraints in production environments\nValidate on your specific data characteristics and use cases\n\nModern GFMs trend toward robust, adaptive approaches that can handle the diverse, noisy nature of satellite imagery while maintaining computational efficiency for large-scale training."
  },
  {
    "objectID": "course-materials/extras/examples/normalization_comparison.html#resources",
    "href": "course-materials/extras/examples/normalization_comparison.html#resources",
    "title": "Normalization Methods for Geospatial Foundation Models",
    "section": "Resources",
    "text": "Resources\n\nPrithvi Model Documentation\nSatMAE Paper\nClay Foundation Model\nSatellite Image Normalization Best Practices"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html",
    "href": "course-materials/extras/examples/text_encoder.html",
    "title": "Deep Learning Basics",
    "section": "",
    "text": "In this interactive session, we will explore how to build deep learning encoders and decoders from scratch. Our first example will use text encoders and a small corpus of text.\nThe test we will use is “The Verdict”, which is used in Sebastian Raschka’s excellent book, “Build a Large Language Model from Scratch.”\n\n\nCode\nimport urllib.request\n\n\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n       \"the-verdict.txt\")\n\nwith urllib.request.urlopen(url) as response:\n    raw_text = response.read().decode(\"utf-8\")\n\nprint(\"Total number of character:\", len(raw_text))\nprint(raw_text[:99])\n\n\nTotal number of character: 20479\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n\n\nThe first step is to tokenize this text. We can use regular expressions to split the text into words.\n\n\nCode\nimport re\ntext = \"Hello, world. Welcome to text encoding.\"\nresult = re.split(r'(\\s)', text)\n\nprint(result)\n\n\n['Hello,', ' ', 'world.', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding.']\n\n\nWe want to separate punctuation and preserve capitalization.\n\n\nCode\nresult = re.split(r'([,.]|\\s)', text)\nprint(result)\n\n\n['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding', '.', '']\n\n\nThe decision to remove or include whitespace characters is application-dependent. For example, in coding (especially Pyhon coding), whitespace is essential to the structure and function of text, so it should be included in training.\n\n\n\n\n\n\nWhitespace in Coding\n\n\n\nMost tokenization schemes include whitespace, but for brevity, these next examples exclude it.\n\n\n\n\nCode\ntext = \"Hello, world. Is this-- a good decoder?\"\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\n\n\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'good', 'decoder', '?']\n\n\nWe can run this against our entire corpus of text.\n\n\nCode\npreprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(preprocessed[:20])\n\n\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n\n\n\n\nModels assign each token a unique ID using a dictionary. These dictionaries are built from the text corpus.\nWe create a set of unique tokens that define the vocabulary of the corpus, which determines the number of items in our dictionary.\n\n\nCode\nall_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\nprint(vocab_size)\n\n\n1130\n\n\nCreating a vocabulary requires assigning each unique ID to each token. We can do this using a dictionary comprehension.\n\n\nCode\nvocab = {token: idx for idx, token in enumerate(\n    all_words, start=0)}\n\n# Print a list of the first 10 tokens and their ids:\nprint(list(vocab.items())[:10])\n\n\n[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)]\n\n\nTo facilitate the conversion of tokens to ids and vice versa, we can create a tokenizer class with an encode and decode method. The encode method splits text into tokens and then maps these to ids using the vocabulary. The decode method does the reverse, converting ids back to tokens.\n\n\nCode\nclass Tokenizer:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.token_to_id = vocab\n        self.id_to_token = {idx: token for token, idx in vocab.items()}\n\n    def encode(self, text):\n        \"\"\"Encode a string into a list of token IDs.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = (\n            t.strip() for t in re.split(pattern, text) if t.strip()\n        )\n        return [self.token_to_id[t] for t in tokens]\n\n    def decode(self, ids):\n        \"\"\"Decode a list of token IDs into a string.\"\"\"\n        text = \" \".join(self.id_to_token[i] for i in ids)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n\n\nLet’s test our new Tokenizer class\n\n\nCode\ntext = \"\"\"\"It's the last he painted, you know,\" \n       Mrs. Gisburn said with pardonable pride.\"\"\"\ntokenizer = Tokenizer(vocab)\nids = tokenizer.encode(text)\n\nprint(f\"Encoded:\\n{ids}\")\nprint(f\"Decoded:\\n{tokenizer.decode(ids)}\")\n\n\nEncoded:\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\nDecoded:\n\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n\n\nThe vocabulary of our tokenizer is limited by the tokens that appear in the corpus. Any word that is not in the corpus cannot be encoded.\ntext = \"Hello, do you like tea?\"\nprint(tokenizer.encode(text))\nCauses a KeyError:\nKeyError: 'Hello'"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#introduction",
    "href": "course-materials/extras/examples/text_encoder.html#introduction",
    "title": "Deep Learning Basics",
    "section": "",
    "text": "In this interactive session, we will explore how to build deep learning encoders and decoders from scratch. Our first example will use text encoders and a small corpus of text.\nThe test we will use is “The Verdict”, which is used in Sebastian Raschka’s excellent book, “Build a Large Language Model from Scratch.”\n\n\nCode\nimport urllib.request\n\n\nurl = (\"https://raw.githubusercontent.com/rasbt/\"\n       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n       \"the-verdict.txt\")\n\nwith urllib.request.urlopen(url) as response:\n    raw_text = response.read().decode(\"utf-8\")\n\nprint(\"Total number of character:\", len(raw_text))\nprint(raw_text[:99])\n\n\nTotal number of character: 20479\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n\n\nThe first step is to tokenize this text. We can use regular expressions to split the text into words.\n\n\nCode\nimport re\ntext = \"Hello, world. Welcome to text encoding.\"\nresult = re.split(r'(\\s)', text)\n\nprint(result)\n\n\n['Hello,', ' ', 'world.', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding.']\n\n\nWe want to separate punctuation and preserve capitalization.\n\n\nCode\nresult = re.split(r'([,.]|\\s)', text)\nprint(result)\n\n\n['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Welcome', ' ', 'to', ' ', 'text', ' ', 'encoding', '.', '']\n\n\nThe decision to remove or include whitespace characters is application-dependent. For example, in coding (especially Pyhon coding), whitespace is essential to the structure and function of text, so it should be included in training.\n\n\n\n\n\n\nWhitespace in Coding\n\n\n\nMost tokenization schemes include whitespace, but for brevity, these next examples exclude it.\n\n\n\n\nCode\ntext = \"Hello, world. Is this-- a good decoder?\"\nresult = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\n\n\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'good', 'decoder', '?']\n\n\nWe can run this against our entire corpus of text.\n\n\nCode\npreprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(preprocessed[:20])\n\n\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was']\n\n\n\n\nModels assign each token a unique ID using a dictionary. These dictionaries are built from the text corpus.\nWe create a set of unique tokens that define the vocabulary of the corpus, which determines the number of items in our dictionary.\n\n\nCode\nall_words = sorted(set(preprocessed))\nvocab_size = len(all_words)\nprint(vocab_size)\n\n\n1130\n\n\nCreating a vocabulary requires assigning each unique ID to each token. We can do this using a dictionary comprehension.\n\n\nCode\nvocab = {token: idx for idx, token in enumerate(\n    all_words, start=0)}\n\n# Print a list of the first 10 tokens and their ids:\nprint(list(vocab.items())[:10])\n\n\n[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)]\n\n\nTo facilitate the conversion of tokens to ids and vice versa, we can create a tokenizer class with an encode and decode method. The encode method splits text into tokens and then maps these to ids using the vocabulary. The decode method does the reverse, converting ids back to tokens.\n\n\nCode\nclass Tokenizer:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.token_to_id = vocab\n        self.id_to_token = {idx: token for token, idx in vocab.items()}\n\n    def encode(self, text):\n        \"\"\"Encode a string into a list of token IDs.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = (\n            t.strip() for t in re.split(pattern, text) if t.strip()\n        )\n        return [self.token_to_id[t] for t in tokens]\n\n    def decode(self, ids):\n        \"\"\"Decode a list of token IDs into a string.\"\"\"\n        text = \" \".join(self.id_to_token[i] for i in ids)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n\n\nLet’s test our new Tokenizer class\n\n\nCode\ntext = \"\"\"\"It's the last he painted, you know,\" \n       Mrs. Gisburn said with pardonable pride.\"\"\"\ntokenizer = Tokenizer(vocab)\nids = tokenizer.encode(text)\n\nprint(f\"Encoded:\\n{ids}\")\nprint(f\"Decoded:\\n{tokenizer.decode(ids)}\")\n\n\nEncoded:\n[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\nDecoded:\n\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n\n\nThe vocabulary of our tokenizer is limited by the tokens that appear in the corpus. Any word that is not in the corpus cannot be encoded.\ntext = \"Hello, do you like tea?\"\nprint(tokenizer.encode(text))\nCauses a KeyError:\nKeyError: 'Hello'"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#handling-unknown-tokens-and-special-tokens",
    "href": "course-materials/extras/examples/text_encoder.html#handling-unknown-tokens-and-special-tokens",
    "title": "Deep Learning Basics",
    "section": "Handling Unknown Tokens and Special Tokens",
    "text": "Handling Unknown Tokens and Special Tokens\nIn practical LLM applications, we need to handle words that don’t appear in our training vocabulary. This is where unknown tokens and other special tokens become essential.\n\nSpecial Tokens in LLMs\nModern tokenizers use several special tokens:\n\n&lt;|unk|&gt; or [UNK]: Unknown token for out-of-vocabulary words\n&lt;|endoftext|&gt; or [EOS]: End of sequence/text marker\n&lt;|startoftext|&gt; or [BOS]: Beginning of sequence marker (less common in GPT-style models)\n&lt;|pad|&gt;: Padding token for batch processing\n\nLet’s create an enhanced tokenizer that handles unknown tokens:\n\n\nCode\nclass EnhancedTokenizer:\n    def __init__(self, vocab):\n        # Add special tokens to vocabulary\n        self.special_tokens = {\n            \"&lt;|unk|&gt;\": len(vocab),\n            \"&lt;|endoftext|&gt;\": len(vocab) + 1\n        }\n        \n        # Combine original vocab with special tokens\n        self.full_vocab = {**vocab, **self.special_tokens}\n        self.token_to_id = self.full_vocab\n        self.id_to_token = {idx: token for token, idx in self.full_vocab.items()}\n        \n        # Store reverse mapping for special tokens\n        self.unk_token_id = self.special_tokens[\"&lt;|unk|&gt;\"]\n        self.endoftext_token_id = self.special_tokens[\"&lt;|endoftext|&gt;\"]\n\n    def encode(self, text, add_endoftext=True):\n        \"\"\"Encode text, handling unknown tokens.\"\"\"\n        pattern = r'([,.?_!\"()\\']|--|\\s)'\n        tokens = [t.strip() for t in re.split(pattern, text) if t.strip()]\n        \n        # Convert tokens to IDs, using &lt;|unk|&gt; for unknown tokens\n        ids = []\n        for token in tokens:\n            if token in self.token_to_id:\n                ids.append(self.token_to_id[token])\n            else:\n                ids.append(self.unk_token_id)  # Use unknown token\n        \n        # Optionally add end-of-text token\n        if add_endoftext:\n            ids.append(self.endoftext_token_id)\n            \n        return ids\n\n    def decode(self, ids):\n        \"\"\"Decode token IDs back to text.\"\"\"\n        tokens = []\n        for id in ids:\n            if id == self.endoftext_token_id:\n                break  # Stop at end-of-text token\n            elif id in self.id_to_token:\n                tokens.append(self.id_to_token[id])\n            else:\n                tokens.append(\"&lt;|unk|&gt;\")  # Fallback for invalid IDs\n                \n        text = \" \".join(tokens)\n        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n    \n    def vocab_size(self):\n        \"\"\"Return the total vocabulary size including special tokens.\"\"\"\n        return len(self.full_vocab)\n\n\nLet’s test our enhanced tokenizer:\n\n\nCode\nenhanced_tokenizer = EnhancedTokenizer(vocab)\n\n# Test with unknown words\ntext_with_unknown = \"Hello, do you like tea? This is amazing!\"\nencoded = enhanced_tokenizer.encode(text_with_unknown)\n\nprint(f\"Original text: {text_with_unknown}\")\nprint(f\"Encoded IDs: {encoded}\")\nprint(f\"Decoded text: {enhanced_tokenizer.decode(encoded)}\")\nprint(f\"Vocabulary size: {enhanced_tokenizer.vocab_size()}\")\n\n\nOriginal text: Hello, do you like tea? This is amazing!\nEncoded IDs: [1130, 5, 355, 1126, 628, 975, 10, 97, 584, 1130, 0, 1131]\nDecoded text: &lt;|unk|&gt;, do you like tea? This is &lt;|unk|&gt;!\nVocabulary size: 1132\n\n\nNotice how unknown words like “Hello” and “amazing” are now handled gracefully using the &lt;|unk|&gt; token, and the sequence ends with an &lt;|endoftext|&gt; token.\n\n\nCode\n# Let's see what the special token IDs are\nprint(f\"Unknown token '&lt;|unk|&gt;' has ID: {enhanced_tokenizer.unk_token_id}\")\nprint(f\"End-of-text token '&lt;|endoftext|&gt;' has ID: {enhanced_tokenizer.endoftext_token_id}\")\n\n# Test decoding with end-of-text in the middle\ntest_ids = [100, 200, enhanced_tokenizer.endoftext_token_id, 300, 400]\nprint(f\"Decoding stops at &lt;|endoftext|&gt;: {enhanced_tokenizer.decode(test_ids)}\")\n\n\nUnknown token '&lt;|unk|&gt;' has ID: 1130\nEnd-of-text token '&lt;|endoftext|&gt;' has ID: 1131\nDecoding stops at &lt;|endoftext|&gt;: Thwing bean-stalk"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#byte-pair-encoding-bpe-with-tiktoken",
    "href": "course-materials/extras/examples/text_encoder.html#byte-pair-encoding-bpe-with-tiktoken",
    "title": "Deep Learning Basics",
    "section": "Byte Pair Encoding (BPE) with Tiktoken",
    "text": "Byte Pair Encoding (BPE) with Tiktoken\nWhile our simple tokenizer is educational, production LLMs use more sophisticated tokenization schemes like Byte Pair Encoding (BPE). BPE creates subword tokens that balance vocabulary size with representation efficiency.\nThe tiktoken library provides access to the same tokenizers used by OpenAI’s GPT models.\n\n\nCode\nimport tiktoken\n\n# Load the GPT-2 tokenizer (used by GPT-2, GPT-3, and early GPT-4)\ngpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n\nprint(f\"GPT-2 vocabulary size: {gpt2_tokenizer.n_vocab:,}\")\n\n\nGPT-2 vocabulary size: 50,257\n\n\nLet’s compare our simple tokenizer with GPT-2’s BPE tokenizer:\n\n\nCode\ntest_text = \"Hello, world! This demonstrates byte pair encoding tokenization.\"\n\n# Our enhanced tokenizer\nour_tokens = enhanced_tokenizer.encode(test_text, add_endoftext=False)\nour_decoded = enhanced_tokenizer.decode(our_tokens)\n\n# GPT-2 BPE tokenizer  \ngpt2_tokens = gpt2_tokenizer.encode(test_text)\ngpt2_decoded = gpt2_tokenizer.decode(gpt2_tokens)\n\nprint(\"=== Comparison ===\")\nprint(f\"Original text: {test_text}\")\nprint()\nprint(f\"Our tokenizer:\")\nprint(f\"  Tokens: {our_tokens}\")\nprint(f\"  Count: {len(our_tokens)} tokens\")\nprint(f\"  Decoded: {our_decoded}\")\nprint()\nprint(f\"GPT-2 BPE tokenizer:\")\nprint(f\"  Tokens: {gpt2_tokens}\")\nprint(f\"  Count: {len(gpt2_tokens)} tokens\") \nprint(f\"  Decoded: {gpt2_decoded}\")\n\n\n=== Comparison ===\nOriginal text: Hello, world! This demonstrates byte pair encoding tokenization.\n\nOur tokenizer:\n  Tokens: [1130, 5, 1130, 0, 97, 1130, 1130, 1130, 1130, 1130, 7]\n  Count: 11 tokens\n  Decoded: &lt;|unk|&gt;, &lt;|unk|&gt;! This &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt; &lt;|unk|&gt;.\n\nGPT-2 BPE tokenizer:\n  Tokens: [15496, 11, 995, 0, 770, 15687, 18022, 5166, 21004, 11241, 1634, 13]\n  Count: 12 tokens\n  Decoded: Hello, world! This demonstrates byte pair encoding tokenization.\n\n\n\nUnderstanding BPE Token Breakdown\nBPE tokenizers split text into subword units. Let’s examine how GPT-2 breaks down different types of text:\n\n\nCode\ndef analyze_tokenization(text, tokenizer_name=\"gpt2\"):\n    \"\"\"Analyze how tiktoken breaks down text.\"\"\"\n    tokenizer = tiktoken.get_encoding(tokenizer_name)\n    tokens = tokenizer.encode(text)\n    \n    print(f\"Text: '{text}'\")\n    print(f\"Tokens: {tokens}\")\n    print(f\"Token count: {len(tokens)}\")\n    print(\"Token breakdown:\")\n    \n    for i, token_id in enumerate(tokens):\n        token_text = tokenizer.decode([token_id])\n        print(f\"  {i:2d}: {token_id:5d} -&gt; '{token_text}'\")\n    print()\n\n# Test different types of text\nanalyze_tokenization(\"Hello world\")\nanalyze_tokenization(\"Tokenization\")  \nanalyze_tokenization(\"supercalifragilisticexpialidocious\")\nanalyze_tokenization(\"AI/ML researcher\")\nanalyze_tokenization(\"The year 2024\")\n\n\nText: 'Hello world'\nTokens: [15496, 995]\nToken count: 2\nToken breakdown:\n   0: 15496 -&gt; 'Hello'\n   1:   995 -&gt; ' world'\n\nText: 'Tokenization'\nTokens: [30642, 1634]\nToken count: 2\nToken breakdown:\n   0: 30642 -&gt; 'Token'\n   1:  1634 -&gt; 'ization'\n\nText: 'supercalifragilisticexpialidocious'\nTokens: [16668, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\nToken count: 11\nToken breakdown:\n   0: 16668 -&gt; 'super'\n   1:  9948 -&gt; 'cal'\n   2:   361 -&gt; 'if'\n   3: 22562 -&gt; 'rag'\n   4:   346 -&gt; 'il'\n   5:   396 -&gt; 'ist'\n   6:   501 -&gt; 'ice'\n   7: 42372 -&gt; 'xp'\n   8:   498 -&gt; 'ial'\n   9:   312 -&gt; 'id'\n  10: 32346 -&gt; 'ocious'\n\nText: 'AI/ML researcher'\nTokens: [20185, 14, 5805, 13453]\nToken count: 4\nToken breakdown:\n   0: 20185 -&gt; 'AI'\n   1:    14 -&gt; '/'\n   2:  5805 -&gt; 'ML'\n   3: 13453 -&gt; ' researcher'\n\nText: 'The year 2024'\nTokens: [464, 614, 48609]\nToken count: 3\nToken breakdown:\n   0:   464 -&gt; 'The'\n   1:   614 -&gt; ' year'\n   2: 48609 -&gt; ' 2024'\n\n\n\n\n\nWorking with Different Tiktoken Encodings\nOpenAI uses different encodings for different models:\n\n\nCode\n# Available encodings\navailable_encodings = [\"gpt2\", \"p50k_base\", \"cl100k_base\"]\n\ntest_text = \"Geospatial foundation models revolutionize remote sensing!\"\n\nfor encoding_name in available_encodings:\n    try:\n        tokenizer = tiktoken.get_encoding(encoding_name)\n        tokens = tokenizer.encode(test_text)\n        \n        print(f\"{encoding_name:12s}: {len(tokens):2d} tokens, vocab size: {tokenizer.n_vocab:6,}\")\n        print(f\"              Tokens: {tokens}\")\n        print()\n    except Exception as e:\n        print(f\"{encoding_name}: Error - {e}\")\n\n\ngpt2        : 10 tokens, vocab size: 50,257\n              Tokens: [10082, 2117, 34961, 8489, 4981, 5854, 1096, 6569, 34244, 0]\n\np50k_base   : 10 tokens, vocab size: 50,281\n              Tokens: [10082, 2117, 34961, 8489, 4981, 5854, 1096, 6569, 34244, 0]\n\ncl100k_base : 10 tokens, vocab size: 100,277\n              Tokens: [9688, 437, 33514, 16665, 4211, 14110, 553, 8870, 60199, 0]\n\n\n\n\n\nBPE for Geospatial Text\nLet’s see how BPE handles domain-specific geospatial terminology:\n\n\nCode\ngeospatial_texts = [\n    \"NDVI vegetation index analysis\",\n    \"Landsat-8 multispectral imagery\", \n    \"Convolutional neural networks for land cover classification\",\n    \"Sentinel-2 satellite data preprocessing\",\n    \"Geospatial foundation models fine-tuning\"\n]\n\ngpt2_enc = tiktoken.get_encoding(\"gpt2\")\n\nprint(\"=== Geospatial Text Tokenization ===\")\nfor text in geospatial_texts:\n    tokens = gpt2_enc.encode(text)\n    print(f\"'{text}'\")\n    print(f\"  -&gt; {len(tokens)} tokens: {tokens}\")\n    print()\n\n\n=== Geospatial Text Tokenization ===\n'NDVI vegetation index analysis'\n  -&gt; 5 tokens: [8575, 12861, 28459, 6376, 3781]\n\n'Landsat-8 multispectral imagery'\n  -&gt; 10 tokens: [43, 1746, 265, 12, 23, 1963, 271, 806, 1373, 19506]\n\n'Convolutional neural networks for land cover classification'\n  -&gt; 10 tokens: [3103, 85, 2122, 282, 17019, 7686, 329, 1956, 3002, 17923]\n\n'Sentinel-2 satellite data preprocessing'\n  -&gt; 8 tokens: [31837, 20538, 12, 17, 11210, 1366, 662, 36948]\n\n'Geospatial foundation models fine-tuning'\n  -&gt; 9 tokens: [10082, 2117, 34961, 8489, 4981, 3734, 12, 28286, 278]\n\n\n\n\n\nUnderstanding Token Efficiency\nThe choice of tokenizer affects model efficiency. Let’s compare token counts for our text corpus:\n\n\nCode\n# Use a sample from our corpus\nsample_text = raw_text[:500]  # First 500 characters\n\nprint(\"=== Token Efficiency Comparison ===\")\nprint(f\"Text length: {len(sample_text)} characters\")\nprint()\n\n# Our simple tokenizer\nour_tokens = enhanced_tokenizer.encode(sample_text, add_endoftext=False)\nprint(f\"Simple tokenizer: {len(our_tokens)} tokens\")\n\n# GPT-2 BPE\ngpt2_tokens = gpt2_tokenizer.encode(sample_text)\nprint(f\"GPT-2 BPE:       {len(gpt2_tokens)} tokens\")\n\n# Calculate efficiency\nchars_per_token_simple = len(sample_text) / len(our_tokens)\nchars_per_token_bpe = len(sample_text) / len(gpt2_tokens)\n\nprint(f\"\\nCharacters per token:\")\nprint(f\"  Simple: {chars_per_token_simple:.2f}\")\nprint(f\"  BPE:    {chars_per_token_bpe:.2f}\")\nprint(f\"  BPE is {chars_per_token_bpe/chars_per_token_simple:.1f}x more efficient\")\n\n\n=== Token Efficiency Comparison ===\nText length: 500 characters\n\nSimple tokenizer: 110 tokens\nGPT-2 BPE:       123 tokens\n\nCharacters per token:\n  Simple: 4.55\n  BPE:    4.07\n  BPE is 0.9x more efficient"
  },
  {
    "objectID": "course-materials/extras/examples/text_encoder.html#key-takeaways",
    "href": "course-materials/extras/examples/text_encoder.html#key-takeaways",
    "title": "Deep Learning Basics",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nSpecial Tokens: Essential for handling unknown words and sequence boundaries\nUnknown Token Handling: &lt;|unk|&gt; tokens allow models to gracefully handle out-of-vocabulary words\nEnd-of-Text Tokens: &lt;|endoftext|&gt; tokens help models understand sequence boundaries\nBPE Efficiency: Byte Pair Encoding creates a good balance between vocabulary size and representation efficiency\nDomain Adaptation: Different tokenizers may handle domain-specific text differently\n\nIn geospatial AI applications, choosing the right tokenization strategy affects both model performance and computational efficiency, especially when working with specialized terminology from remote sensing and Earth science domains."
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html",
    "href": "course-materials/extras/examples/resnet.html",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "",
    "text": "This notebook demonstrates the core insight behind Residual Networks (ResNets) by comparing them to a traditional plain CNN of similar depth. You will:\n\nVisualize how residual connections transform intermediate representations\nCompare training loss and accuracy curves\nAnalyze gradient flow to see why ResNets enable deeper models to train effectively"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#overview",
    "href": "course-materials/extras/examples/resnet.html#overview",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "",
    "text": "This notebook demonstrates the core insight behind Residual Networks (ResNets) by comparing them to a traditional plain CNN of similar depth. You will:\n\nVisualize how residual connections transform intermediate representations\nCompare training loss and accuracy curves\nAnalyze gradient flow to see why ResNets enable deeper models to train effectively"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#setup-and-imports",
    "href": "course-materials/extras/examples/resnet.html#setup-and-imports",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "📦 Setup and Imports",
    "text": "📦 Setup and Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#define-plain-cnn-and-resnet-like-cnn",
    "href": "course-materials/extras/examples/resnet.html#define-plain-cnn-and-resnet-like-cnn",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "📊 Define Plain CNN and ResNet-like CNN",
    "text": "📊 Define Plain CNN and ResNet-like CNN\nclass PlainBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        return F.relu(out)\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(identity)\n        return F.relu(out)\n\nclass PlainCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = PlainBlock(3, 16)\n        self.block2 = PlainBlock(16, 32, stride=2)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.pool(x).view(x.size(0), -1)\n        return self.fc(x)\n\nclass ResNetMini(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = ResBlock(3, 16)\n        self.block2 = ResBlock(16, 32, stride=2)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.pool(x).view(x.size(0), -1)\n        return self.fc(x)"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#load-cifar-10-data",
    "href": "course-materials/extras/examples/resnet.html#load-cifar-10-data",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "🖼️ Load CIFAR-10 Data",
    "text": "🖼️ Load CIFAR-10 Data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=2)"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#training-loop-and-gradient-tracking",
    "href": "course-materials/extras/examples/resnet.html#training-loop-and-gradient-tracking",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "🚀 Training Loop and Gradient Tracking",
    "text": "🚀 Training Loop and Gradient Tracking\ndef train_model(model, name, epochs=10):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    train_loss = []\n    train_gradients = []\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0\n        grad_norm = 0\n        for inputs, targets in tqdm(train_loader, desc=f\"{name} Epoch {epoch+1}\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n\n            total_norm = 0\n            for p in model.parameters():\n                if p.grad is not None:\n                    param_norm = p.grad.data.norm(2)\n                    total_norm += param_norm.item() ** 2\n            grad_norm += total_norm ** 0.5\n\n            optimizer.step()\n            running_loss += loss.item()\n\n        train_loss.append(running_loss / len(train_loader))\n        train_gradients.append(grad_norm / len(train_loader))\n\n    return train_loss, train_gradients"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#train-and-compare",
    "href": "course-materials/extras/examples/resnet.html#train-and-compare",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "📈 Train and Compare",
    "text": "📈 Train and Compare\nplain_model = PlainCNN()\nresnet_model = ResNetMini()\n\nplain_loss, plain_grads = train_model(plain_model, \"PlainCNN\")\nresnet_loss, resnet_grads = train_model(resnet_model, \"ResNetMini\")"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#plot-training-loss-and-gradient-flow",
    "href": "course-materials/extras/examples/resnet.html#plot-training-loss-and-gradient-flow",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "📊 Plot Training Loss and Gradient Flow",
    "text": "📊 Plot Training Loss and Gradient Flow\nplt.figure(figsize=(14,5))\n\nplt.subplot(1,2,1)\nplt.plot(plain_loss, label=\"PlainCNN\")\nplt.plot(resnet_loss, label=\"ResNetMini\")\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(plain_grads, label=\"PlainCNN\")\nplt.plot(resnet_grads, label=\"ResNetMini\")\nplt.title(\"Gradient Norm per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Average Gradient Norm\")\nplt.legend()\n\nplt.suptitle(\"Residual Connections Help Optimize Deeper Networks\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/examples/resnet.html#conclusion",
    "href": "course-materials/extras/examples/resnet.html#conclusion",
    "title": "Visualizing Residual Learning and Convergence",
    "section": "✅ Conclusion",
    "text": "✅ Conclusion\nThis notebook demonstrates that residual connections help preserve gradient flow, allowing deeper networks to train faster and more reliably. Even though our example was relatively shallow (2 blocks), the benefits in convergence and stability are clear."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html",
    "href": "course-materials/extras/examples/tiling-and-patches.html",
    "title": "Tiling & Patch Extraction",
    "section": "",
    "text": "This page shows two kinds of visuals that help explain common preprocessing steps for vision models:\n\nA high-level flow using Mermaid (no Python required)\nData-like mockups using Python + Matplotlib to draw tile and patch grids on a synthetic image\n\n\n\n\n\n\n\n\ngraph LR\n    A[Full Image] --&gt; B[Tiling]\n    B --&gt; C[Tile 1]\n    B --&gt; D[Tile 2]\n    B --&gt; E[Tile 3]\n    C --&gt; F[Patch Extraction]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[Patch Set for Model]\n\n\n\n\n\n\n\nTip: Mermaid renders natively in Quarto HTML output. No extra setup needed for basic use."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#overview",
    "href": "course-materials/extras/examples/tiling-and-patches.html#overview",
    "title": "Tiling & Patch Extraction",
    "section": "",
    "text": "This page shows two kinds of visuals that help explain common preprocessing steps for vision models:\n\nA high-level flow using Mermaid (no Python required)\nData-like mockups using Python + Matplotlib to draw tile and patch grids on a synthetic image\n\n\n\n\n\n\n\n\ngraph LR\n    A[Full Image] --&gt; B[Tiling]\n    B --&gt; C[Tile 1]\n    B --&gt; D[Tile 2]\n    B --&gt; E[Tile 3]\n    C --&gt; F[Patch Extraction]\n    D --&gt; F\n    E --&gt; F\n    F --&gt; G[Patch Set for Model]\n\n\n\n\n\n\n\nTip: Mermaid renders natively in Quarto HTML output. No extra setup needed for basic use."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#data-like-mockups-python-matplotlib",
    "href": "course-materials/extras/examples/tiling-and-patches.html#data-like-mockups-python-matplotlib",
    "title": "Tiling & Patch Extraction",
    "section": "Data-like Mockups (Python + Matplotlib)",
    "text": "Data-like Mockups (Python + Matplotlib)\nBelow we generate a random “image” and draw grid lines to illustrate tiling and patch extraction. These mockups help students visualize how tiles and patches relate to array indices.\n\nImplementation notes: - All visuals use Matplotlib (no seaborn) and avoid custom color styles, per course standards. - Each figure is produced by a single, self-contained code chunk. - Random seed is fixed for reproducibility.\n\n\n1) Tiling Overlay on a Full Image\nWe split the image into equally sized tiles (here: 50×50). White grid lines mark tile boundaries.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# --- Create subtle textured background with fixed colormap range ---\nnp.random.seed(42)\nimg = np.random.rand(100, 150)\n# Map random values to a narrow range around mid-gray (0.45 to 0.55)\n# This creates gentle texture while keeping the colormap fixed at 0-1\nimg = img * 0.1 + 0.45  # Subtle texture in 10% range around mid-gray\n\n# --- Tile size ---\ntile_h, tile_w = 50, 50\n\n# --- Plot ---\nfig, ax = plt.subplots(figsize=(6, 4))\nax.imshow(img, cmap='gray', vmin=0, vmax=1)  # Fix colormap range\n\n# vertical lines (white for high contrast)\nfor x in range(0, img.shape[1] + 1, tile_w):\n    ax.axvline(x - 0.5, color='white', linewidth=2)\n\n# horizontal lines (white for high contrast)\nfor y in range(0, img.shape[0] + 1, tile_h):\n    ax.axhline(y - 0.5, color='white', linewidth=2)\n\n# Label each tile\ntile_rows = img.shape[0] // tile_h\ntile_cols = img.shape[1] // tile_w\nfor r in range(tile_rows):\n    for c in range(tile_cols):\n        center_y = r * tile_h + tile_h // 2\n        center_x = c * tile_w + tile_w // 2\n        ax.text(center_x, center_y, f\"Tile\\n({r},{c})\",\n                ha=\"center\", va=\"center\", fontsize=12,\n                color='white', weight='bold')\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_title(\"Full Image with Tiling Grid (50x50)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2) Patch Extraction Inside a Single Tile\nZoom into the top-left tile (50×50) and split it into smaller patches (here: 10×10). Grid lines show patch boundaries; labels mark patch indices (row, col).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming `img` from the previous cell exists in the same execution environment.\n# If running independently, re-create it:\nnp.random.seed(42)\nimg = np.random.rand(100, 150)\n# Map random values to a narrow range around mid-gray (0.45 to 0.55)\nimg = img * 0.1 + 0.45  # Subtle texture in 10% range around mid-gray\n\n# --- Select the first tile: top-left (0:50, 0:50) ---\ntile = img[0:50, 0:50]\n\n# --- Patch size ---\npatch_h, patch_w = 10, 10\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(tile, cmap='gray', vmin=0, vmax=1)  # Fix colormap range\n\n# Draw patch grid (white lines for high contrast)\nfor x in range(0, tile.shape[1] + 1, patch_w):\n    ax.axvline(x - 0.5, color='white', linewidth=1)\nfor y in range(0, tile.shape[0] + 1, patch_h):\n    ax.axhline(y - 0.5, color='white', linewidth=1)\n\n# Annotate patch indices with high-contrast text\nrows = tile.shape[0] // patch_h\ncols = tile.shape[1] // patch_w\nfor r in range(rows):\n    for c in range(cols):\n        y_center = r * patch_h + patch_h / 2\n        x_center = c * patch_w + patch_w / 2\n\n        # White text pops better on textured background\n        ax.text(x_center, y_center, f\"({r},{c})\",\n                ha=\"center\", va=\"center\", fontsize=10,\n                color='white', weight='bold')\n\nax.set_xticks([])\nax.set_yticks([])\nax.set_title(\"Zoomed Tile with Patch Grid (10x10)\")\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#advanced-patch-extraction-concepts",
    "href": "course-materials/extras/examples/tiling-and-patches.html#advanced-patch-extraction-concepts",
    "title": "Tiling & Patch Extraction",
    "section": "Advanced Patch Extraction Concepts",
    "text": "Advanced Patch Extraction Concepts\n\n3) Overlapping Patches with Stride\nWhen stride &lt; patch size, patches overlap. This is common in computer vision to capture more spatial information.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a smaller tile for clearer visualization\nnp.random.seed(42)\nsmall_img = np.random.rand(36, 45)  # Dimensions chosen to work cleanly with patch params\n# Map to subtle texture range\nsmall_img = small_img * 0.1 + 0.45  # Gentle texture around mid-gray\n\n# --- Patch parameters ---\npatch_size = 15\nstride = 9  # Creates 60% overlap (9/15), avoiding 2x multiples\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.imshow(small_img, cmap='gray', vmin=0, vmax=1)  # Fix colormap range\n\n# Calculate patch positions\npatch_positions = []\nfor y in range(0, small_img.shape[0] - patch_size + 1, stride):\n    for x in range(0, small_img.shape[1] - patch_size + 1, stride):\n        patch_positions.append((x, y))\n\n# Draw overlapping patches with different colors\ncolors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan']\nfor i, (x, y) in enumerate(patch_positions[:6]):  # Show first 6 patches\n    color = colors[i % len(colors)]\n    # Draw patch boundary\n    rect = plt.Rectangle((x-0.5, y-0.5), patch_size, patch_size, \n                        linewidth=2, edgecolor=color, facecolor='none', alpha=0.8)\n    ax.add_patch(rect)\n    \n    # Label patch center with matching color\n    center_x, center_y = x + patch_size//2, y + patch_size//2\n    ax.text(center_x, center_y, f\"P{i}\", ha=\"center\", va=\"center\", \n            fontsize=10, color=color, weight='bold',\n            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.9))\n\nax.set_xlim(-1, small_img.shape[1])\nax.set_ylim(small_img.shape[0], -1)\nax.set_xticks([])\nax.set_yticks([])\nax.set_title(f\"Overlapping Patches (size={patch_size}, stride={stride})\")\nplt.show()\n\nprint(f\"Patch size: {patch_size}x{patch_size}\")\nprint(f\"Stride: {stride} (overlap = {patch_size - stride} pixels = {100*(patch_size-stride)/patch_size:.0f}%)\")\nprint(f\"Total patches extracted: {len(patch_positions)}\")\nprint(f\"Image dimensions: {small_img.shape[0]}x{small_img.shape[1]}\")\nprint(f\"Patches fit: {(small_img.shape[0] - patch_size) // stride + 1} rows x {(small_img.shape[1] - patch_size) // stride + 1} cols\")\n\n\n\n\n\n\n\n\n\nPatch size: 15x15\nStride: 9 (overlap = 6 pixels = 40%)\nTotal patches extracted: 12\nImage dimensions: 36x45\nPatches fit: 3 rows x 4 cols\n\n\n\n\n4) Handling Incomplete Patches: Padding Strategies\nWhen image dimensions don’t divide evenly by patch size, we face the “edge problem” - what to do with incomplete patches at borders. Different strategies offer different trade-offs between computational efficiency, information preservation, and artifact introduction.\nFirst, let’s set up our test image and examine the edge problem:\n\n\nOriginal image: 40×56 pixels\nPatch size: 16×16 pixels\nComplete patches that fit: 2×3\nLeftover pixels: 8 rows, 8 columns\n==================================================\n\n\n\n\n\n\n\n\n\n\nStrategy 1: Crop (Discard Incomplete Patches)\nApproach: Simply ignore patches that don’t fit completely within the image boundaries.\nPros:\n\nFast and simple: No extra computation or memory\nNo artifacts: All patches contain only real image data\nPredictable output size: Easy to calculate exact number of patches\n\nCons:\n\nInformation loss: Edge and corner information is discarded\nUneven coverage: Some areas of the image are never processed\nSize dependency: Loss percentage varies with image dimensions\n\nUse cases: When speed is critical and edge information is less important (e.g., dense feature extraction where overlap provides coverage).\n\n\n\n\n\n\n\n\n\n\n\nStrategy 2: Zero Padding\nApproach: Extend the image with zero-valued pixels to make dimensions divisible by patch size.\nPros:\n\nComplete coverage: Every pixel is included in at least one patch\nSimple implementation: Easy to add zeros programmatically\nConsistent output size: Padding can be calculated in advance\n\nCons:\n\nArtificial boundaries: Sharp transitions from real data to zeros\nPotential artifacts: Models may learn to recognize padding patterns\nIncreased computation: More patches to process\n\nUse cases: When complete coverage is essential and models are robust to boundary artifacts (e.g., segmentation tasks).\n\n\n\n\n\n\n\n\n\n\n\nStrategy 3: Reflect Padding\nApproach: Mirror edge pixels to create natural-looking padding that preserves image structure.\nPros:\n\nNatural boundaries: Smooth transitions maintain local image statistics\nStructure preservation: Gradients and textures continue naturally\nBetter model behavior: Less likely to create learning artifacts\n\nCons:\n\nMore complex: Requires reflection logic for edges and corners\nSlight computation overhead: Must calculate reflected values\nMay duplicate features: Reflected content isn’t truly independent\n\nUse cases: When image quality and natural appearance are important (e.g., super-resolution, denoising, medical imaging)."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#choosing-padding-strategies-in-practice",
    "href": "course-materials/extras/examples/tiling-and-patches.html#choosing-padding-strategies-in-practice",
    "title": "Tiling & Patch Extraction",
    "section": "Choosing Padding Strategies in Practice",
    "text": "Choosing Padding Strategies in Practice\n\nImplementation Examples\nimport torch.nn.functional as F\n\n# PyTorch examples for different padding strategies\ndef apply_padding_strategy(image_tensor, patch_size, strategy='reflect'):\n    \\\"\\\"\\\"\n    Apply padding to make image divisible by patch_size\n    \n    Args:\n        image_tensor: (C, H, W) tensor\n        patch_size: int, size of square patches\n        strategy: 'crop', 'zero', or 'reflect'\n    \\\"\\\"\\\"\n    C, H, W = image_tensor.shape\n    \n    if strategy == 'crop':\n        # Calculate largest area that fits complete patches\n        new_h = (H // patch_size) * patch_size\n        new_w = (W // patch_size) * patch_size\n        return image_tensor[:, :new_h, :new_w]\n    \n    elif strategy == 'zero':\n        # Calculate padding needed\n        pad_h = patch_size - (H % patch_size) if H % patch_size != 0 else 0\n        pad_w = patch_size - (W % patch_size) if W % patch_size != 0 else 0\n        return F.pad(image_tensor, (0, pad_w, 0, pad_h), mode='constant', value=0)\n    \n    elif strategy == 'reflect':\n        pad_h = patch_size - (H % patch_size) if H % patch_size != 0 else 0\n        pad_w = patch_size - (W % patch_size) if W % patch_size != 0 else 0\n        return F.pad(image_tensor, (0, pad_w, 0, pad_h), mode='reflect')\n\n\nDecision Framework\nChoose CROP when: - Processing large datasets where speed &gt;&gt; completeness - Using overlapping patches that provide edge coverage - Edge regions are less important for your task\nChoose ZERO PADDING when: - Need guaranteed complete coverage - Model architecture handles boundaries well - Working with synthetic or highly structured data\nChoose REFLECT PADDING when: - Image quality is paramount - Working with natural images where structure matters - Model will be sensitive to boundary artifacts\n\n\nReal-World Considerations\n\nBatch processing: Padding strategies affect batch consistency\nMemory usage: Padding increases tensor size and memory requirements\nPost-processing: May need to crop back to original dimensions after inference\nData augmentation: Padding interacts with augmentation strategies (rotation, flipping)\n\nMost modern frameworks (PyTorch, TensorFlow) implement all three strategies efficiently, making the choice primarily about the specific requirements of your application rather than implementation difficulty."
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#key-takeaways",
    "href": "course-materials/extras/examples/tiling-and-patches.html#key-takeaways",
    "title": "Tiling & Patch Extraction",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nOverlapping patches capture more spatial information but increase computational cost\nStride controls overlap: smaller stride = more overlap = more patches\nPadding strategies handle images that don’t divide evenly by patch size:\n\nCrop: Fast but loses edge information\nZero padding: Simple but introduces artificial boundaries\n\nReflect padding: More natural boundaries, preserves edge information"
  },
  {
    "objectID": "course-materials/extras/examples/tiling-and-patches.html#variations-you-can-try",
    "href": "course-materials/extras/examples/tiling-and-patches.html#variations-you-can-try",
    "title": "Tiling & Patch Extraction",
    "section": "Variations You Can Try",
    "text": "Variations You Can Try\n\nExperiment with different stride values (1, 3, 6, 12) to see overlap effects\nTry other padding strategies like “constant” or “wrap” modes\nCompare patch counts and computational requirements across strategies"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/dataloader_satellite.html",
    "href": "course-materials/extras/cheatsheets/dataloader_satellite.html",
    "title": "Data Loading for Satellite Imagery",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html",
    "title": "Working with Rasterio",
    "section": "",
    "text": "Rasterio is the essential Python library for reading and writing geospatial raster data.\n\n\nCode\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\n\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#introduction-to-rasterio",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#introduction-to-rasterio",
    "title": "Working with Rasterio",
    "section": "",
    "text": "Rasterio is the essential Python library for reading and writing geospatial raster data.\n\n\nCode\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nprint(f\"Rasterio version: {rasterio.__version__}\")\n\n\nRasterio version: 1.4.3"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#creating-sample-data",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#creating-sample-data",
    "title": "Working with Rasterio",
    "section": "Creating Sample Data",
    "text": "Creating Sample Data\nSince we may not have actual satellite imagery files, let’s create some sample raster data:\n\n\nCode\n# Create sample raster data\ndef create_sample_raster():\n    # Create a simple 100x100 raster with some pattern\n    height, width = 100, 100\n    \n    # Create coordinate grids\n    x = np.linspace(-2, 2, width)\n    y = np.linspace(-2, 2, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Create a pattern (e.g., circular pattern)\n    data = np.sin(np.sqrt(X**2 + Y**2)) * 255\n    data = data.astype(np.uint8)\n    \n    return data\n\nsample_data = create_sample_raster()\nprint(f\"Sample data shape: {sample_data.shape}\")\nprint(f\"Data type: {sample_data.dtype}\")\nprint(f\"Data range: {sample_data.min()} to {sample_data.max()}\")\n\n\nSample data shape: (100, 100)\nData type: uint8\nData range: 7 to 254"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#working-with-raster-arrays",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#working-with-raster-arrays",
    "title": "Working with Rasterio",
    "section": "Working with Raster Arrays",
    "text": "Working with Raster Arrays\n\nBasic array operations\n\n\nCode\n# Basic statistics\nprint(f\"Mean: {np.mean(sample_data):.2f}\")\nprint(f\"Standard deviation: {np.std(sample_data):.2f}\")\nprint(f\"Unique values: {len(np.unique(sample_data))}\")\n\n# Histogram of values\nplt.figure(figsize=(8, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Sample Raster Data')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.hist(sample_data.flatten(), bins=50, alpha=0.7)\nplt.title('Histogram of Values')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\nMean: 215.01\nStandard deviation: 46.13\nUnique values: 182\n\n\n\n\n\n\n\n\n\n\n\nArray masking and filtering\n\n\nCode\n# Create masks\nhigh_values = sample_data &gt; 200\nlow_values = sample_data &lt; 50\n\nprint(f\"Pixels with high values (&gt;200): {np.sum(high_values)}\")\nprint(f\"Pixels with low values (&lt;50): {np.sum(low_values)}\")\n\n# Apply mask\nmasked_data = np.where(high_values, sample_data, 0)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Original Data')\n\nplt.subplot(1, 3, 2)\nplt.imshow(high_values, cmap='gray')\nplt.title('High Value Mask')\n\nplt.subplot(1, 3, 3)\nplt.imshow(masked_data, cmap='viridis')\nplt.title('Masked Data')\n\nplt.tight_layout()\nplt.show()\n\n\nPixels with high values (&gt;200): 7364\nPixels with low values (&lt;50): 76"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#raster-metadata-and-transforms",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#raster-metadata-and-transforms",
    "title": "Working with Rasterio",
    "section": "Raster Metadata and Transforms",
    "text": "Raster Metadata and Transforms\n\nUnderstanding geospatial transforms\n\n\nCode\nfrom rasterio.transform import from_bounds\n\n# Create a sample transform (geographic coordinates)\nwest, south, east, north = -120.0, 35.0, -115.0, 40.0  # Bounding box\ntransform = from_bounds(west, south, east, north, 100, 100)\n\nprint(f\"Transform: {transform}\")\nprint(f\"Pixel width: {transform[0]}\")\nprint(f\"Pixel height: {abs(transform[4])}\")\n\n# Convert pixel coordinates to geographic coordinates\ndef pixel_to_geo(row, col, transform):\n    x, y = rasterio.transform.xy(transform, row, col)\n    return x, y\n\n# Example: center pixel\ncenter_row, center_col = 50, 50\ngeo_x, geo_y = pixel_to_geo(center_row, center_col, transform)\nprint(f\"Center pixel ({center_row}, {center_col}) -&gt; Geographic: ({geo_x:.2f}, {geo_y:.2f})\")\n\n\nTransform: | 0.05, 0.00,-120.00|\n| 0.00,-0.05, 40.00|\n| 0.00, 0.00, 1.00|\nPixel width: 0.05\nPixel height: 0.05\nCenter pixel (50, 50) -&gt; Geographic: (-117.47, 37.48)\n\n\n\n\nCreating profiles for writing rasters\n\n\nCode\n# Create a profile for writing raster data\nprofile = {\n    'driver': 'GTiff',\n    'dtype': 'uint8',\n    'nodata': None,\n    'width': 100,\n    'height': 100,\n    'count': 1,\n    'crs': 'EPSG:4326',  # WGS84 lat/lon\n    'transform': transform\n}\n\nprint(\"Raster profile:\")\nfor key, value in profile.items():\n    print(f\"  {key}: {value}\")\n\n\nRaster profile:\n  driver: GTiff\n  dtype: uint8\n  nodata: None\n  width: 100\n  height: 100\n  count: 1\n  crs: EPSG:4326\n  transform: | 0.05, 0.00,-120.00|\n| 0.00,-0.05, 40.00|\n| 0.00, 0.00, 1.00|"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#multi-band-operations",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#multi-band-operations",
    "title": "Working with Rasterio",
    "section": "Multi-band Operations",
    "text": "Multi-band Operations\n\nWorking with multi-band data\n\n\nCode\n# Create multi-band sample data (RGB-like)\ndef create_multiband_sample():\n    height, width = 100, 100\n    bands = 3\n    \n    # Create different patterns for each band\n    x = np.linspace(-2, 2, width)\n    y = np.linspace(-2, 2, height)\n    X, Y = np.meshgrid(x, y)\n    \n    # Band 1: Circular pattern\n    band1 = (np.sin(np.sqrt(X**2 + Y**2)) * 127 + 127).astype(np.uint8)\n    \n    # Band 2: Linear gradient\n    band2 = (np.linspace(0, 255, width) * np.ones((height, 1))).astype(np.uint8)\n    \n    # Band 3: Checkerboard pattern\n    band3 = ((X + Y) &gt; 0).astype(np.uint8) * 255\n    \n    return np.stack([band1, band2, band3])\n\nmultiband_data = create_multiband_sample()\nprint(f\"Multiband shape: {multiband_data.shape}\")\nprint(f\"Shape format: (bands, height, width)\")\n\n\nMultiband shape: (3, 100, 100)\nShape format: (bands, height, width)\n\n\n\n\nVisualizing multi-band data\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\n# Individual bands\nfor i in range(3):\n    row, col = i // 2, i % 2\n    axes[row, col].imshow(multiband_data[i], cmap='gray')\n    axes[row, col].set_title(f'Band {i+1}')\n\n# RGB composite (transpose for matplotlib)\nrgb_composite = np.transpose(multiband_data, (1, 2, 0))\naxes[1, 1].imshow(rgb_composite)\naxes[1, 1].set_title('RGB Composite')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#band-math-and-indices",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#band-math-and-indices",
    "title": "Working with Rasterio",
    "section": "Band Math and Indices",
    "text": "Band Math and Indices\n\nCalculate vegetation index (NDVI-like)\n\n\nCode\n# Simulate NIR and Red bands\nnir_band = multiband_data[0].astype(np.float32)\nred_band = multiband_data[1].astype(np.float32)\n\n# Calculate NDVI-like index\n# NDVI = (NIR - Red) / (NIR + Red)\nndvi = (nir_band - red_band) / (nir_band + red_band + 1e-8)  # Small value to avoid division by zero\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(nir_band, cmap='RdYlBu_r')\nplt.title('NIR-like Band')\nplt.colorbar()\n\nplt.subplot(1, 3, 2)\nplt.imshow(red_band, cmap='Reds')\nplt.title('Red-like Band')\nplt.colorbar()\n\nplt.subplot(1, 3, 3)\nplt.imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.title('NDVI-like Index')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {np.mean(ndvi):.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI range: -0.211 to 1.000\nNDVI mean: 0.353"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#resampling-and-reprojection-concepts",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#resampling-and-reprojection-concepts",
    "title": "Working with Rasterio",
    "section": "Resampling and Reprojection Concepts",
    "text": "Resampling and Reprojection Concepts\n\nUnderstanding resampling methods\n\n\nCode\nfrom scipy import ndimage\n\n# Demonstrate different resampling methods\noriginal = sample_data\n\n# Downsample (reduce resolution)\ndownsampled = ndimage.zoom(original, 0.5, order=1)  # Linear interpolation\n\n# Upsample (increase resolution) \nupsampled = ndimage.zoom(original, 2.0, order=1)  # Linear interpolation\n\nplt.figure(figsize=(15, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(original, cmap='viridis')\nplt.title(f'Original ({original.shape[0]}x{original.shape[1]})')\n\nplt.subplot(1, 3, 2)\nplt.imshow(downsampled, cmap='viridis')\nplt.title(f'Downsampled ({downsampled.shape[0]}x{downsampled.shape[1]})')\n\nplt.subplot(1, 3, 3)\nplt.imshow(upsampled, cmap='viridis')\nplt.title(f'Upsampled ({upsampled.shape[0]}x{upsampled.shape[1]})')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Original size: {original.shape}\")\nprint(f\"Downsampled size: {downsampled.shape}\")  \nprint(f\"Upsampled size: {upsampled.shape}\")\n\n\n\n\n\n\n\n\n\nOriginal size: (100, 100)\nDownsampled size: (50, 50)\nUpsampled size: (200, 200)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#common-rasterio-patterns",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#common-rasterio-patterns",
    "title": "Working with Rasterio",
    "section": "Common Rasterio Patterns",
    "text": "Common Rasterio Patterns\n\nWorking with windows and blocks\n\n\nCode\n# Simulate processing large rasters in blocks\ndef process_in_blocks(data, block_size=50):\n    \"\"\"Process data in blocks to simulate handling large rasters\"\"\"\n    height, width = data.shape\n    processed = np.zeros_like(data)\n    \n    for row in range(0, height, block_size):\n        for col in range(0, width, block_size):\n            # Define window bounds\n            row_end = min(row + block_size, height)\n            col_end = min(col + block_size, width)\n            \n            # Extract block\n            block = data[row:row_end, col:col_end]\n            \n            # Process block (example: enhance contrast)\n            processed_block = np.clip(block * 1.2, 0, 255)\n            \n            # Write back to result\n            processed[row:row_end, col:col_end] = processed_block\n            \n            print(f\"Processed block: ({row}:{row_end}, {col}:{col_end})\")\n    \n    return processed\n\n# Process sample data in blocks\nprocessed_data = process_in_blocks(sample_data, block_size=25)\n\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample_data, cmap='viridis')\nplt.title('Original')\n\nplt.subplot(1, 2, 2)\nplt.imshow(processed_data, cmap='viridis')\nplt.title('Processed (Enhanced)')\n\nplt.tight_layout()\nplt.show()\n\n\nProcessed block: (0:25, 0:25)\nProcessed block: (0:25, 25:50)\nProcessed block: (0:25, 50:75)\nProcessed block: (0:25, 75:100)\nProcessed block: (25:50, 0:25)\nProcessed block: (25:50, 25:50)\nProcessed block: (25:50, 50:75)\nProcessed block: (25:50, 75:100)\nProcessed block: (50:75, 0:25)\nProcessed block: (50:75, 25:50)\nProcessed block: (50:75, 50:75)\nProcessed block: (50:75, 75:100)\nProcessed block: (75:100, 0:25)\nProcessed block: (75:100, 25:50)\nProcessed block: (75:100, 50:75)\nProcessed block: (75:100, 75:100)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/rasterio_basics.html#summary",
    "href": "course-materials/extras/cheatsheets/rasterio_basics.html#summary",
    "title": "Working with Rasterio",
    "section": "Summary",
    "text": "Summary\nKey Rasterio concepts covered: - Reading and understanding raster data structure - Working with single and multi-band imagery - Coordinate transforms and geospatial metadata\n- Band math and index calculations - Resampling and processing techniques - Block-based processing for large datasets"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/loading_models.html",
    "href": "course-materials/extras/cheatsheets/loading_models.html",
    "title": "Loading Pre-trained Models",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/finetuning_basics.html",
    "href": "course-materials/extras/cheatsheets/finetuning_basics.html",
    "title": "Fine-tuning Strategies",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/model_inference.html",
    "href": "course-materials/extras/cheatsheets/model_inference.html",
    "title": "Model Inference & Feature Extraction",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/pytorch_tensors.html",
    "href": "course-materials/extras/cheatsheets/pytorch_tensors.html",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "",
    "text": "Code\nimport torch\nimport numpy as np\n\n# From Python lists\ntensor_from_list = torch.tensor([[1, 2], [3, 4]])\nprint(f\"From list: {tensor_from_list}\")\n\n# From NumPy arrays\nnumpy_array = np.array([[1, 2], [3, 4]])\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(f\"From numpy: {tensor_from_numpy}\")\n\n# Common initialization patterns\nzeros_tensor = torch.zeros(3, 3)\nones_tensor = torch.ones(2, 4)\nrandom_tensor = torch.randn(2, 3)  # Normal distribution\n\nprint(f\"Zeros: {zeros_tensor}\")\nprint(f\"Random: {random_tensor}\")\n\n\nFrom list: tensor([[1, 2],\n        [3, 4]])\nFrom numpy: tensor([[1, 2],\n        [3, 4]])\nZeros: tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nRandom: tensor([[0.2040, 0.7393, 0.7049],\n        [1.4731, 2.9496, 2.5490]])\n\n\n\n\n\n\n\nCode\n# Check tensor properties\nsample_tensor = torch.randn(3, 4, 5)\n\nprint(f\"Shape: {sample_tensor.shape}\")\nprint(f\"Size: {sample_tensor.size()}\")\nprint(f\"Data type: {sample_tensor.dtype}\")\nprint(f\"Device: {sample_tensor.device}\")\nprint(f\"Number of dimensions: {sample_tensor.ndim}\")\nprint(f\"Total elements: {sample_tensor.numel()}\")\n\n\nShape: torch.Size([3, 4, 5])\nSize: torch.Size([3, 4, 5])\nData type: torch.float32\nDevice: cpu\nNumber of dimensions: 3\nTotal elements: 60"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/pytorch_tensors.html#creating-tensors",
    "href": "course-materials/extras/cheatsheets/pytorch_tensors.html#creating-tensors",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "",
    "text": "Code\nimport torch\nimport numpy as np\n\n# From Python lists\ntensor_from_list = torch.tensor([[1, 2], [3, 4]])\nprint(f\"From list: {tensor_from_list}\")\n\n# From NumPy arrays\nnumpy_array = np.array([[1, 2], [3, 4]])\ntensor_from_numpy = torch.from_numpy(numpy_array)\nprint(f\"From numpy: {tensor_from_numpy}\")\n\n# Common initialization patterns\nzeros_tensor = torch.zeros(3, 3)\nones_tensor = torch.ones(2, 4)\nrandom_tensor = torch.randn(2, 3)  # Normal distribution\n\nprint(f\"Zeros: {zeros_tensor}\")\nprint(f\"Random: {random_tensor}\")\n\n\nFrom list: tensor([[1, 2],\n        [3, 4]])\nFrom numpy: tensor([[1, 2],\n        [3, 4]])\nZeros: tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\nRandom: tensor([[0.2040, 0.7393, 0.7049],\n        [1.4731, 2.9496, 2.5490]])\n\n\n\n\n\n\n\nCode\n# Check tensor properties\nsample_tensor = torch.randn(3, 4, 5)\n\nprint(f\"Shape: {sample_tensor.shape}\")\nprint(f\"Size: {sample_tensor.size()}\")\nprint(f\"Data type: {sample_tensor.dtype}\")\nprint(f\"Device: {sample_tensor.device}\")\nprint(f\"Number of dimensions: {sample_tensor.ndim}\")\nprint(f\"Total elements: {sample_tensor.numel()}\")\n\n\nShape: torch.Size([3, 4, 5])\nSize: torch.Size([3, 4, 5])\nData type: torch.float32\nDevice: cpu\nNumber of dimensions: 3\nTotal elements: 60"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/pytorch_tensors.html#gpu-operations",
    "href": "course-materials/extras/cheatsheets/pytorch_tensors.html#gpu-operations",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "GPU Operations",
    "text": "GPU Operations\n\nCheck GPU availability\n\n\nCode\n# Check if CUDA is available\ncuda_available = torch.cuda.is_available()\nprint(f\"CUDA available: {cuda_available}\")\n\nif cuda_available:\n    gpu_count = torch.cuda.device_count()\n    gpu_name = torch.cuda.get_device_name(0)\n    print(f\"Number of GPUs: {gpu_count}\")\n    print(f\"GPU name: {gpu_name}\")\nelse:\n    print(\"Running on CPU\")\n\n\nCUDA available: False\nRunning on CPU\n\n\n\n\nMoving tensors to GPU\n\n\nCode\n# Create a tensor\ncpu_tensor = torch.randn(3, 3)\nprint(f\"Original device: {cpu_tensor.device}\")\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpu_tensor = cpu_tensor.to(device)\nprint(f\"After moving to device: {gpu_tensor.device}\")\n\n# Alternative methods\nif torch.cuda.is_available():\n    # Method 2: .cuda() method\n    gpu_tensor2 = cpu_tensor.cuda()\n    print(f\"Using .cuda(): {gpu_tensor2.device}\")\n    \n    # Method 3: Create directly on GPU\n    direct_gpu = torch.randn(3, 3, device='cuda')\n    print(f\"Created on GPU: {direct_gpu.device}\")\n\n\nOriginal device: cpu\nAfter moving to device: cpu"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/pytorch_tensors.html#basic-tensor-operations",
    "href": "course-materials/extras/cheatsheets/pytorch_tensors.html#basic-tensor-operations",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\n\nMathematical operations\n\n\nCode\n# Create sample tensors\na = torch.randn(3, 3)\nb = torch.randn(3, 3)\n\n# Element-wise operations\naddition = a + b\nsubtraction = a - b  \nmultiplication = a * b\ndivision = a / b\n\nprint(f\"Addition shape: {addition.shape}\")\nprint(f\"Subtraction mean: {subtraction.mean():.3f}\")\n\n# Matrix operations\nmatmul = torch.matmul(a, b)  # Matrix multiplication\ntranspose = a.t()  # Transpose\n\nprint(f\"Matrix multiplication shape: {matmul.shape}\")\nprint(f\"Transpose shape: {transpose.shape}\")\n\n\nAddition shape: torch.Size([3, 3])\nSubtraction mean: 0.103\nMatrix multiplication shape: torch.Size([3, 3])\nTranspose shape: torch.Size([3, 3])\n\n\n\n\nUseful tensor methods\n\n\nCode\n# Statistical operations\ndata = torch.randn(4, 5)\n\nprint(f\"Mean: {data.mean():.3f}\")\nprint(f\"Standard deviation: {data.std():.3f}\")\nprint(f\"Min: {data.min():.3f}\")\nprint(f\"Max: {data.max():.3f}\")\n\n# Reshaping\nreshaped = data.view(2, 10)  # Reshape to 2x10\nflattened = data.flatten()   # Flatten to 1D\n\nprint(f\"Original shape: {data.shape}\")\nprint(f\"Reshaped: {reshaped.shape}\")\nprint(f\"Flattened: {flattened.shape}\")\n\n\nMean: 0.008\nStandard deviation: 1.063\nMin: -1.735\nMax: 1.845\nOriginal shape: torch.Size([4, 5])\nReshaped: torch.Size([2, 10])\nFlattened: torch.Size([20])"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/pytorch_tensors.html#memory-management",
    "href": "course-materials/extras/cheatsheets/pytorch_tensors.html#memory-management",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Memory Management",
    "text": "Memory Management\n\nGPU memory monitoring\n\n\nCode\nif torch.cuda.is_available():\n    # Check current memory usage\n    allocated = torch.cuda.memory_allocated() / (1024**2)  # Convert to MB\n    reserved = torch.cuda.memory_reserved() / (1024**2)\n    \n    print(f\"GPU memory allocated: {allocated:.1f} MB\")\n    print(f\"GPU memory reserved: {reserved:.1f} MB\")\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    print(\"GPU cache cleared\")\nelse:\n    print(\"GPU memory monitoring not available (running on CPU)\")\n\n\nGPU memory monitoring not available (running on CPU)\n\n\n\n\nBest practices\n\n\nCode\n# Use context managers for temporary operations\ndef memory_efficient_operation():\n    with torch.no_grad():  # Disable gradient computation\n        large_tensor = torch.randn(1000, 1000)\n        result = large_tensor.mean()\n        return result\n\nresult = memory_efficient_operation()\nprint(f\"Result: {result:.3f}\")\n\n# The large_tensor is automatically garbage collected\n\n\nResult: 0.001"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/pytorch_tensors.html#converting-between-formats",
    "href": "course-materials/extras/cheatsheets/pytorch_tensors.html#converting-between-formats",
    "title": "PyTorch Tensors & GPU Operations",
    "section": "Converting Between Formats",
    "text": "Converting Between Formats\n\nPyTorch ↔︎ NumPy\n\n\nCode\n# PyTorch to NumPy\ntorch_tensor = torch.randn(2, 3)\nnumpy_array = torch_tensor.numpy()  # Only works on CPU tensors\n\nprint(f\"PyTorch tensor: {torch_tensor}\")\nprint(f\"NumPy array: {numpy_array}\")\n\n# NumPy to PyTorch\nnew_torch = torch.from_numpy(numpy_array)\nprint(f\"Back to PyTorch: {new_torch}\")\n\n\nPyTorch tensor: tensor([[-8.7064e-01, -6.9920e-01,  1.3267e-01],\n        [-6.6634e-01,  1.3974e-04,  1.7045e+00]])\nNumPy array: [[-8.7063950e-01 -6.9919729e-01  1.3267224e-01]\n [-6.6634214e-01  1.3973590e-04  1.7044624e+00]]\nBack to PyTorch: tensor([[-8.7064e-01, -6.9920e-01,  1.3267e-01],\n        [-6.6634e-01,  1.3974e-04,  1.7045e+00]])\n\n\n\n\nHandling GPU tensors\n\n\nCode\nif torch.cuda.is_available():\n    # GPU tensor must be moved to CPU first\n    gpu_tensor = torch.randn(2, 3).cuda()\n    cpu_numpy = gpu_tensor.cpu().numpy()\n    print(f\"GPU tensor converted to NumPy: {cpu_numpy.shape}\")\nelse:\n    print(\"GPU conversion example not available (running on CPU)\")\n\n\nGPU conversion example not available (running on CPU)"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/xarray_basics.html",
    "href": "course-materials/extras/cheatsheets/xarray_basics.html",
    "title": "Xarray for Multi-dimensional Data",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/earth_engine_basics.html",
    "href": "course-materials/extras/cheatsheets/earth_engine_basics.html",
    "title": "Earth Engine Basics",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/plotting_satellite.html",
    "href": "course-materials/extras/cheatsheets/plotting_satellite.html",
    "title": "Plotting Satellite Imagery",
    "section": "",
    "text": "Effective visualization is crucial for understanding satellite imagery and geospatial data. This cheatsheet covers essential plotting techniques.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/plotting_satellite.html#introduction",
    "href": "course-materials/extras/cheatsheets/plotting_satellite.html#introduction",
    "title": "Plotting Satellite Imagery",
    "section": "",
    "text": "Effective visualization is crucial for understanding satellite imagery and geospatial data. This cheatsheet covers essential plotting techniques.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\n\n# Set style for better plots\nplt.style.use('default')\nsns.set_palette(\"husl\")"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/plotting_satellite.html#creating-sample-satellite-data",
    "href": "course-materials/extras/cheatsheets/plotting_satellite.html#creating-sample-satellite-data",
    "title": "Plotting Satellite Imagery",
    "section": "Creating Sample Satellite Data",
    "text": "Creating Sample Satellite Data\n\n\nCode\ndef create_sample_satellite_scene(size=256):\n    \"\"\"Create a realistic-looking satellite scene\"\"\"\n    \n    # Create coordinate grids\n    x = np.linspace(0, 10, size)\n    y = np.linspace(0, 10, size)\n    X, Y = np.meshgrid(x, y)\n    \n    # Simulate different land cover types\n    # Water bodies (low values)\n    water = np.exp(-((X - 2)**2 + (Y - 7)**2) / 2) * 0.3\n    water += np.exp(-((X - 8)**2 + (Y - 3)**2) / 4) * 0.25\n    \n    # Forest/vegetation (medium-high values)\n    forest = np.exp(-((X - 6)**2 + (Y - 8)**2) / 8) * 0.7\n    forest += np.exp(-((X - 3)**2 + (Y - 2)**2) / 6) * 0.6\n    \n    # Urban areas (varied values)\n    urban = np.exp(-((X - 7)**2 + (Y - 5)**2) / 3) * 0.8\n    \n    # Combine and add noise\n    scene = water + forest + urban\n    noise = np.random.normal(0, 0.05, scene.shape)\n    scene = np.clip(scene + noise, 0, 1)\n    \n    # Scale to typical satellite data range\n    return (scene * 255).astype(np.uint8)\n\n# Create sample scenes for different bands\nred_band = create_sample_satellite_scene()\ngreen_band = create_sample_satellite_scene() * 1.1\nblue_band = create_sample_satellite_scene() * 0.8\nnir_band = create_sample_satellite_scene() * 1.3\n\nprint(f\"Band shapes: {red_band.shape}\")\nprint(f\"Data ranges - Red: {red_band.min()}-{red_band.max()}\")\n\n\nBand shapes: (256, 256)\nData ranges - Red: 0-255"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/plotting_satellite.html#single-band-visualization",
    "href": "course-materials/extras/cheatsheets/plotting_satellite.html#single-band-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Single Band Visualization",
    "text": "Single Band Visualization\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Different colormaps for single bands\ncmaps = ['gray', 'viridis', 'plasma', 'RdYlBu_r']\nband_names = ['Grayscale', 'Viridis', 'Plasma', 'Red-Yellow-Blue']\n\nfor i, (cmap, name) in enumerate(zip(cmaps, band_names)):\n    ax = axes[i//2, i%2]\n    im = ax.imshow(red_band, cmap=cmap, interpolation='bilinear')\n    ax.set_title(f'{name} Colormap')\n    ax.axis('off')\n    plt.colorbar(im, ax=ax, shrink=0.8)\n\nplt.suptitle('Single Band Visualization with Different Colormaps', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/plotting_satellite.html#rgb-composite-images",
    "href": "course-materials/extras/cheatsheets/plotting_satellite.html#rgb-composite-images",
    "title": "Plotting Satellite Imagery",
    "section": "RGB Composite Images",
    "text": "RGB Composite Images\n\n\nCode\ndef create_rgb_composite(r, g, b, enhance=True):\n    \"\"\"Create RGB composite with optional enhancement\"\"\"\n    \n    # Stack bands\n    rgb = np.stack([r, g, b], axis=-1)\n    \n    if enhance:\n        # Simple linear stretch enhancement\n        for i in range(3):\n            band = rgb[:,:,i].astype(float)\n            # Stretch to 2-98 percentile\n            p2, p98 = np.percentile(band, (2, 98))\n            band = np.clip((band - p2) / (p98 - p2), 0, 1)\n            rgb[:,:,i] = (band * 255).astype(np.uint8)\n    \n    return rgb\n\n# Create different composites\ntrue_color = create_rgb_composite(red_band, green_band, blue_band)\nfalse_color = create_rgb_composite(nir_band, red_band, green_band)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\naxes[0].imshow(true_color)\naxes[0].set_title('True Color Composite (RGB)')\naxes[0].axis('off')\n\naxes[1].imshow(false_color)\naxes[1].set_title('False Color Composite (NIR-Red-Green)')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0]."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/plotting_satellite.html#spectral-indices-visualization",
    "href": "course-materials/extras/cheatsheets/plotting_satellite.html#spectral-indices-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Spectral Indices Visualization",
    "text": "Spectral Indices Visualization\n\n\nCode\n# Calculate NDVI\ndef calculate_ndvi(nir, red):\n    \"\"\"Calculate Normalized Difference Vegetation Index\"\"\"\n    nir_f = nir.astype(float)\n    red_f = red.astype(float)\n    return (nir_f - red_f) / (nir_f + red_f + 1e-8)\n\n# Calculate other indices\nndvi = calculate_ndvi(nir_band, red_band)\n\n# Water index (using blue and NIR)\nndwi = calculate_ndvi(green_band, nir_band)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# NDVI\nim1 = axes[0].imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1, interpolation='bilinear')\naxes[0].set_title('NDVI (Vegetation Index)')\naxes[0].axis('off')\nplt.colorbar(im1, ax=axes[0], shrink=0.8)\n\n# NDWI  \nim2 = axes[1].imshow(ndwi, cmap='RdYlBu', vmin=-1, vmax=1, interpolation='bilinear')\naxes[1].set_title('NDWI (Water Index)')\naxes[1].axis('off')\nplt.colorbar(im2, ax=axes[1], shrink=0.8)\n\n# Histogram of NDVI values\naxes[2].hist(ndvi.flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\naxes[2].set_xlabel('NDVI Value')\naxes[2].set_ylabel('Frequency')\naxes[2].set_title('NDVI Distribution')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {ndvi.mean():.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI range: -1.000 to 1.000\nNDVI mean: 0.121"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/plotting_satellite.html#custom-colormaps-and-classification",
    "href": "course-materials/extras/cheatsheets/plotting_satellite.html#custom-colormaps-and-classification",
    "title": "Plotting Satellite Imagery",
    "section": "Custom Colormaps and Classification",
    "text": "Custom Colormaps and Classification\n\n\nCode\n# Create land cover classification\ndef classify_land_cover(ndvi, ndwi):\n    \"\"\"Simple land cover classification\"\"\"\n    classification = np.zeros_like(ndvi, dtype=int)\n    \n    # Water (high NDWI, low NDVI)\n    water_mask = (ndwi &gt; 0.3) & (ndvi &lt; 0.1)\n    classification[water_mask] = 1\n    \n    # Vegetation (high NDVI)\n    veg_mask = ndvi &gt; 0.4\n    classification[veg_mask] = 2\n    \n    # Bare soil/urban (low NDVI, low NDWI)  \n    bare_mask = (ndvi &lt; 0.2) & (ndwi &lt; 0.1)\n    classification[bare_mask] = 3\n    \n    return classification\n\n# Classify the scene\nland_cover = classify_land_cover(ndvi, ndwi)\n\n# Create custom colormap\ncolors = ['black', 'blue', 'green', 'brown', 'gray']\nn_classes = len(np.unique(land_cover))\ncustom_cmap = ListedColormap(colors[:n_classes])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Show classification\nim1 = axes[0].imshow(land_cover, cmap=custom_cmap, interpolation='nearest')\naxes[0].set_title('Land Cover Classification')\naxes[0].axis('off')\n\n# Custom legend\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='black', label='Unclassified'),\n    Patch(facecolor='blue', label='Water'),\n    Patch(facecolor='green', label='Vegetation'), \n    Patch(facecolor='brown', label='Bare Soil/Urban')\n]\naxes[0].legend(handles=legend_elements, loc='upper right')\n\n# Classification statistics\nunique_classes, counts = np.unique(land_cover, return_counts=True)\npercentages = counts / counts.sum() * 100\n\naxes[1].bar(['Unclassified', 'Water', 'Vegetation', 'Bare/Urban'][:len(unique_classes)], \n           percentages, color=['black', 'blue', 'green', 'brown'][:len(unique_classes)])\naxes[1].set_ylabel('Percentage of Pixels')\naxes[1].set_title('Land Cover Distribution')\naxes[1].grid(True, alpha=0.3)\n\nfor i, (cls, pct) in enumerate(zip(unique_classes, percentages)):\n    axes[1].text(i, pct + 1, f'{pct:.1f}%', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/plotting_satellite.html#advanced-visualization-techniques",
    "href": "course-materials/extras/cheatsheets/plotting_satellite.html#advanced-visualization-techniques",
    "title": "Plotting Satellite Imagery",
    "section": "Advanced Visualization Techniques",
    "text": "Advanced Visualization Techniques\n\n\nCode\n# Multi-scale visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Full scene\naxes[0, 0].imshow(true_color)\naxes[0, 0].set_title('Full Scene')\naxes[0, 0].axis('off')\n\n# Add rectangle showing zoom area\nzoom_area = Rectangle((100, 100), 80, 80, linewidth=2, \n                     edgecolor='red', facecolor='none')\naxes[0, 0].add_patch(zoom_area)\n\n# Zoomed areas with different processing\nzoom_slice = (slice(100, 180), slice(100, 180))\n\n# Zoomed true color\naxes[0, 1].imshow(true_color[zoom_slice])\naxes[0, 1].set_title('Zoomed True Color')\naxes[0, 1].axis('off')\n\n# Zoomed false color\naxes[0, 2].imshow(false_color[zoom_slice])\naxes[0, 2].set_title('Zoomed False Color')\naxes[0, 2].axis('off')\n\n# Zoomed NDVI\nim1 = axes[1, 0].imshow(ndvi[zoom_slice], cmap='RdYlGn', vmin=-1, vmax=1)\naxes[1, 0].set_title('Zoomed NDVI')\naxes[1, 0].axis('off')\nplt.colorbar(im1, ax=axes[1, 0], shrink=0.8)\n\n# Zoomed classification\naxes[1, 1].imshow(land_cover[zoom_slice], cmap=custom_cmap, interpolation='nearest')\naxes[1, 1].set_title('Zoomed Classification')\naxes[1, 1].axis('off')\n\n# Profile plot\ncenter_row = land_cover.shape[0] // 2\nprofile_ndvi = ndvi[center_row, :]\nprofile_x = np.arange(len(profile_ndvi))\n\naxes[1, 2].plot(profile_x, profile_ndvi, 'g-', linewidth=2)\naxes[1, 2].axhline(y=0, color='k', linestyle='--', alpha=0.5)\naxes[1, 2].axhline(y=0.4, color='r', linestyle='--', alpha=0.5, label='Vegetation threshold')\naxes[1, 2].set_xlabel('Pixel Position')\naxes[1, 2].set_ylabel('NDVI Value')\naxes[1, 2].set_title('NDVI Profile (Center Row)')\naxes[1, 2].grid(True, alpha=0.3)\naxes[1, 2].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [36.0..255.0].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [32.0..255.0]."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/plotting_satellite.html#time-series-visualization",
    "href": "course-materials/extras/cheatsheets/plotting_satellite.html#time-series-visualization",
    "title": "Plotting Satellite Imagery",
    "section": "Time Series Visualization",
    "text": "Time Series Visualization\n\n\nCode\n# Simulate time series of NDVI\ndef create_ndvi_time_series(n_dates=12):\n    \"\"\"Create sample NDVI time series\"\"\"\n    dates = []\n    ndvi_values = []\n    \n    # Simulate seasonal pattern\n    base_ndvi = 0.4\n    seasonal_amplitude = 0.3\n    \n    for i in range(n_dates):\n        # Simulate monthly data\n        month_angle = (i / 12) * 2 * np.pi\n        seasonal_component = seasonal_amplitude * np.sin(month_angle + np.pi/2)  # Peak in summer\n        noise = np.random.normal(0, 0.05)\n        \n        ndvi_val = base_ndvi + seasonal_component + noise\n        ndvi_values.append(ndvi_val)\n        dates.append(f'Month {i+1}')\n    \n    return dates, ndvi_values\n\n# Create sample time series for different land cover types\ndates, forest_ndvi = create_ndvi_time_series()\n_, cropland_ndvi = create_ndvi_time_series()  \n_, urban_ndvi = create_ndvi_time_series()\n\n# Adjust for different land cover characteristics\ncropland_ndvi = [v * 0.8 + 0.1 for v in cropland_ndvi]  # Lower peak, higher base\nurban_ndvi = [v * 0.3 + 0.15 for v in urban_ndvi]  # Much lower overall\n\nplt.figure(figsize=(12, 6))\n\nplt.plot(dates, forest_ndvi, 'g-o', label='Forest', linewidth=2, markersize=6)\nplt.plot(dates, cropland_ndvi, 'orange', marker='s', label='Cropland', linewidth=2, markersize=6)  \nplt.plot(dates, urban_ndvi, 'gray', marker='^', label='Urban', linewidth=2, markersize=6)\n\nplt.xlabel('Time Period')\nplt.ylabel('NDVI Value')\nplt.title('NDVI Time Series by Land Cover Type')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.ylim(-0.1, 0.8)\n\n# Add shaded regions for seasons\nsummer_months = [4, 5, 6, 7]  # Months 5-8\nplt.axvspan(summer_months[0]-0.5, summer_months[-1]+0.5, alpha=0.2, color='yellow', label='Summer')\n\nplt.tight_layout()\nplt.show()\n\n# Print some statistics\nprint(\"NDVI Statistics by Land Cover:\")\nprint(f\"Forest - Mean: {np.mean(forest_ndvi):.3f}, Std: {np.std(forest_ndvi):.3f}\")\nprint(f\"Cropland - Mean: {np.mean(cropland_ndvi):.3f}, Std: {np.std(cropland_ndvi):.3f}\")\nprint(f\"Urban - Mean: {np.mean(urban_ndvi):.3f}, Std: {np.std(urban_ndvi):.3f}\")\n\n\n\n\n\n\n\n\n\nNDVI Statistics by Land Cover:\nForest - Mean: 0.399, Std: 0.245\nCropland - Mean: 0.426, Std: 0.146\nUrban - Mean: 0.266, Std: 0.059"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/plotting_satellite.html#summary",
    "href": "course-materials/extras/cheatsheets/plotting_satellite.html#summary",
    "title": "Plotting Satellite Imagery",
    "section": "Summary",
    "text": "Summary\nKey visualization techniques covered:\n\nSingle band visualization with different colormaps\nRGB composite creation and enhancement\nSpectral indices (NDVI, NDWI) visualization\n\nCustom colormaps and classification maps\nMulti-scale visualization with zoom and profiles\nTime series plotting for temporal analysis\n\nThese techniques form the foundation for effective satellite imagery analysis and presentation."
  },
  {
    "objectID": "course-materials/extras/cheatsheets/torchgeo_basics.html",
    "href": "course-materials/extras/cheatsheets/torchgeo_basics.html",
    "title": "TorchGeo Datasets & Transforms",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/folium_basics.html",
    "href": "course-materials/extras/cheatsheets/folium_basics.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/extras/cheatsheets/matplotlib_geospatial.html",
    "href": "course-materials/extras/cheatsheets/matplotlib_geospatial.html",
    "title": "Geospatial Plotting with Matplotlib",
    "section": "",
    "text": "Cheatsheet content coming soon…"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture4_multimodal.html",
    "href": "course-materials/extras/lectures/lecture4_multimodal.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture2_geospatial_data.html",
    "href": "course-materials/extras/lectures/lecture2_geospatial_data.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture5_evaluation.html",
    "href": "course-materials/extras/lectures/lecture5_evaluation.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#welcome",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#welcome",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "Welcome",
    "text": "Welcome\n\nCourse focus: Building Geospatial Foundation Models (GFMs)\nToday: compare LLM and GFM development pipelines; key architecture ideas; brief history"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#agenda",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#agenda",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "Agenda",
    "text": "Agenda\n\nAI/ML → Transformers: a 10,000‑ft history\nLLM development pipeline (9 steps)\nGFM development pipeline (9 steps, geospatialized)\nKey differences and implications"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#from-ai-to-transformers-very-brief-history",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#from-ai-to-transformers-very-brief-history",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "From AI to Transformers (Very Brief History)",
    "text": "From AI to Transformers (Very Brief History)\n\n1950s–1990s: Symbolic AI, early neural nets\n2012: Deep learning (ImageNet)\n2017: Transformers (“Attention Is All You Need”)\n2018–2020: BERT/GPT families\n2021–2024: Scaling laws; instruction tuning; multimodality; diffusion"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#transformer-essentials",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#transformer-essentials",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "Transformer Essentials",
    "text": "Transformer Essentials\n\nTokenization → Embeddings → Positional encodings → Attention → MLP → Stacking → Pretraining → Finetuning\nKey superpower: learn long‑range dependencies via attention"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#step-development-pipeline-llms",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#step-development-pipeline-llms",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "9‑Step Development Pipeline (LLMs)",
    "text": "9‑Step Development Pipeline (LLMs)\n\nData preparation & sampling (text corpora, dedup, mixing)\nTokenization (BPE, vocab; special tokens)\nArchitecture (GPT/BERT; depth/width; context length)\nPretraining objective (next‑token, masked LM)\nTraining loop (optimizers, LR schedule, mixed precision)\nEvaluation (perplexity, downstream probes)\nLoad/use pretrained weights\nFinetuning (task‑specific; PEFT)\nImplementation & deployment (APIs, inference optimizations)"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#step-development-pipeline-gfms",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#step-development-pipeline-gfms",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "9‑Step Development Pipeline (GFMs)",
    "text": "9‑Step Development Pipeline (GFMs)\n\nData preparation & sampling (multi‑spectral, georegistration, tiling, cloud handling)\nTokenization (patch‑based; continuous to embeddings; 2D/temporal positions)\nArchitecture (ViT encoders; spatial/temporal attention; memory constraints)\nPretraining objective (masked patch reconstruction; contrastive for multimodal)\nTraining loop (cloud masks, mixed precision, gradient strategies)\nEvaluation (reconstruction, linear probing, spatial/temporal generalization)\nLoad/use pretrained weights (Prithvi, SatMAE; adapters)\nFinetuning (task heads; PEFT; few‑shot for limited labels)\nImplementation & deployment (tiling inference; APIs; geospatial UIs)"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#stepbystep-llm-vs.-gfm-differences",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#stepbystep-llm-vs.-gfm-differences",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "Step‑by‑Step: LLM vs. GFM Differences",
    "text": "Step‑by‑Step: LLM vs. GFM Differences\n1) Data Preparation\n\nLLMs: text scraping, deduplication, quality filters\nGFMs: sensor calibration, atmospheric correction, cloud masks, georegistration, temporal compositing\n\n2) Tokenization\n\nLLMs: discrete vocab; BPE; special tokens\nGFMs: no discrete vocab; image patches → linear projection; 2D + time encodings\n\n3) Architecture\n\nLLMs: decoder‑only (GPT) or encoder (BERT)\nGFMs: ViT encoders; spatial/temporal attention; multispectral embeddings\n\n4) Pretraining Objective\n\nLLMs: next‑token (AR) or masked LM (denoise text)\nGFMs: masked patch reconstruction; contrastive with text or modalities (optional)\n\n5) Training Loop\n\nLLMs: LR schedules, batch packing, AMP\nGFMs: all of the above + cloud/validity masks; patch sampling from rasters\n\n6) Evaluation\n\nLLMs: perplexity; downstream tasks (classification, QA)\nGFMs: reconstruction metrics; linear probes; spatial/temporal OOD generalization\n\n7) Pretrained Weights\n\nLLMs: model hubs; tokenizer alignment\nGFMs: Prithvi/SatMAE weights; band order & resolution alignment\n\n8) Finetuning\n\nLLMs: instruction tuning; PEFT\nGFMs: task heads (segmentation, regression); adapters/LoRA; few labels\n\n9) Deployment\n\nLLMs: token streaming; KV cache; latency budgets\nGFMs: tiling/overlap; geospatial projections; retrieval by time/area; batch inference"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#scaling-and-evolution-llms",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#scaling-and-evolution-llms",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "Scaling and Evolution (LLMs)",
    "text": "Scaling and Evolution (LLMs)\n\nParameters: 100M → 10B → 70B+\nContext windows: 512 → 128k+\nTraining data: curated web, books, code; cleaner data ≈ better sample efficiency"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#scaling-and-evolution-gfms",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#scaling-and-evolution-gfms",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "Scaling and Evolution (GFMs)",
    "text": "Scaling and Evolution (GFMs)\n\nParameters: 50M → 500M (varies); encoder focus\nInputs: 6+ bands; 2D + time; multi‑sensor fusion (optical/SAR)\nData volume constrained by storage and tiling/IO throughput"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#example-tokenization-contrast",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#example-tokenization-contrast",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "Example: Tokenization Contrast",
    "text": "Example: Tokenization Contrast\nLLM (discrete): tokens → embedding lookup\n# | echo: true\nvocab_size, embed_dim = 50_000, 768\nimport numpy as np\ntry:\n    import torch\n    tok_ids = torch.tensor([1, 2, 3, 4])\n    emb = torch.nn.Embedding(vocab_size, embed_dim)(tok_ids)\n    print(emb.shape)\nexcept Exception as e:\n    print(\"LLM embedding example skipped:\", e)\nGFM (continuous patches): patch → linear projection\n# | echo: true\nimport numpy as np\npatch_dim, embed_dim = 16*16*6, 768\nrng = np.random.default_rng(42)\npatches = rng.normal(size=(4, patch_dim))\ntry:\n    import torch\n    proj = torch.nn.Linear(patch_dim, embed_dim)\n    out = proj(torch.from_numpy(patches).float())\n    print(out.shape)\nexcept Exception as e:\n    print(\"GFM projection example skipped:\", e)"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#putting-it-together-course-mapping",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#putting-it-together-course-mapping",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "Putting It Together (Course Mapping)",
    "text": "Putting It Together (Course Mapping)\n\nWeeks 1–3: Data → Attention → Architecture\nWeeks 4–7: Pretraining → Training loop → Evaluation → Pretrained integration\nWeeks 8–10: Finetuning → Deployment → Synthesis"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#references-further-reading",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#references-further-reading",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "References & Further Reading",
    "text": "References & Further Reading\n\nTransformers: Vaswani et al. (2017)\nVision Transformers: Dosovitskiy et al. (2020)\nMasked Autoencoders: He et al. (2021)\nGeospatial FMs: Prithvi, SatMAE (model docs & papers)"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture1_architectures.html#handson-next",
    "href": "course-materials/extras/lectures/lecture1_architectures.html#handson-next",
    "title": "Lecture 1: Architectures, Histories, and Development Pipelines",
    "section": "Hands‑On Next",
    "text": "Hands‑On Next\n\nSee Week 1 Interactive Session: Geospatial Data Foundations\nStart building the data pipeline with deterministic outputs\n\nNeural networks expect inputs in a very specific, structured format. The first step of any deep learning workflow is to translate our data into a “language” that neural networks understand. This translation process produces embeddings, which are consistent numerical representations that encode our data in a form that makes them ready for model processing.\nAn embedding is simply a standardized vector representation of raw input data. As a toy example, [0, 0, 1, 0] can be thought of as an embedding of the number 2 using a most significant bit (MSB) binary positional encoding scheme. In text processing, embeddings are necessary because tokens like forest and water are categorical symbols; they must be mapped to consistent numerical values before a neural network can operate on them.\nUnlike text, geospatial data values are usually already coded in a numerical scheme. Generally, each pixel contains measured values such as surface reflectance at specific wavelengths, or derived quantities such as NDVI, or categories such as land cover.\nIf we don’t need to “make the data numeric”, why do we still need embeddings for geospatial data? The answer is that embeddings make data both interpretable and meaningful. In a language model, every occurrence of the token cat maps to the same embedding (say 1356), because cat usually carries the same semantic meaning, even in different contexts. In contrast, the same pixel value—for example, 0.15—can mean something entirely different depending on whether it represents surface albedo, slope, or NDVI. Without additional semantic context, the model has no way to disambiguate these cases.\n\n\n\n\n\n\n\n\n\nData Type\nSemantic Consistency 🧠\nAnalytical Tractability 📊\nPrimary Goal of Embeddings 🎯\n\n\n\n\nText\n✅ Words/tokens have consistent meaning across a diverse contexts\n❌ Raw text is not numeric; cannot be directly processed by neural networks\nEnable computation — convert categorical symbols into a consistent numerical space\n\n\nNumerical / Geospatial\n❌ Numeric values can represent different quantities across datasets\n✅ Already stored as numbers; can be directly input to computations\nAdd context — encode what, where, and when into numerical features\n\n\n\nGeospatial Semantics\nWhile text embeddings typically just map words to vectors, geospatial embeddings must carry richer meaning. A pixel or patch value isn’t the whole story; its meaning depends on extra context. Therefore, the goal of embedding in geospatial models is not merely to turn categories into numbers, it’s to transform raw measurements into a representation that is both meaningful and efficient for learning. This might involve:\n\nCombining multiple spectral bands into a single vector\nNormalizing across different sensors to remove acquisition biases\nCompressing high-dimensional spectral signatures into a lower-dimensional space that preserves the most relevant patterns for the task at hand.\nIncorporating spatial and temporal context, enabling the model to understand not just what is in a pixel, but where and when it is.\n\nSemantic Dimensions for Geospatial Embeddings\n\n\n\n\n\n\n\n\n📐 Semantic Dimension\n🧩 What It Is\n🚀 Why It Matters for GFMs\n\n\n\n\n🗂 Source meaning\nThe same number can represent different physical measurements depending on where the data came from and how it was processed\nWithout knowing the source, two identical values might mean very different things\n\n\n🌍 Location reference\nInformation that describes where the measurement is on Earth and how the image is laid out\nChanges in location reference affect how measurements line up and can change their meaning\n\n\n🔎 Detail and scale\nHow much area each pixel represents, and whether it shows a single thing or a mix of things\nLarger pixels mix more features, which changes how the value should be interpreted\n\n\n📏 Measurement units and type\nValues can be in different units (meters, degrees, percentages) or represent categories instead of continuous numbers\nMixing units or types can confuse the model without clear handling\n\n\n📅 When and how it was collected\nThe date, the tool or satellite used, the viewing angle, and environmental conditions\nEven the same number can mean different things at different times or with different tools\n\n\n🧮 Calculated or classified values\nNumbers created from other measurements, like vegetation indexes or land cover labels\nThese values already include assumptions that influence how they should be used\n\n\n📦 Extra context\nAdditional information like location, time, or sensor type added alongside the pixel values\nHelps the model understand what, where, and when each measurement is from\n\n\n\n\n\n\n\n\n\nEmbedding for learning\n\n\nGeospatial embeddings must combine the measured values with extra information about what was measured, where, when, and how, so the model learns from the full meaning of each observation."
  },
  {
    "objectID": "course-materials/extras/lectures/lecture3_finetuning.html",
    "href": "course-materials/extras/lectures/lecture3_finetuning.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "course-materials/extras/lectures/lecture6_cloud_computing.html",
    "href": "course-materials/extras/lectures/lecture6_cloud_computing.html",
    "title": "Lecture: Coming Soon",
    "section": "",
    "text": "Lecture content coming soon…"
  },
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "Syllabus.html#geog-288kc-building-geospatial-foundation-models",
    "href": "Syllabus.html#geog-288kc-building-geospatial-foundation-models",
    "title": "",
    "section": "GEOG 288KC: Building Geospatial Foundation Models",
    "text": "GEOG 288KC: Building Geospatial Foundation Models\nFall 2025\nFridays 9am-12pm + optional lab office hours Fridays 2pm-5pm\n\n\nCourse Overview\nThis course teaches students to build geospatial foundation models (GFMs) from scratch for remote sensing and environmental monitoring. Following the framework from “Build a Large Language Model (From Scratch)” by Sebastian Raschka, students will implement every component of the foundation model pipeline—from data geospatial embedding and attention mechanisms to training loops and deployment—while developing their own working foundation model. We will also download and use frontier GFMs such as Prithvi.\n\n\n\nPrerequisites\n\nStudents should have some experience with remote sensing, geospatial data, or ML (e.g., Python, Earth Engine, PyTorch).\nBecause this is a project-driven seminar, students should have a topic area in which they are interested in applying GeoFMs.\n\n\n\n\nApplications\nTo apply, students should submit a paragraph at the form link below describing their past experience with remote sensing, geospatial data, and ML, as well as their interest in building (not just using) Geospatial Foundation Models. They should describe a specific geospatial problem they want to solve by building a custom foundation model. The more clearly defined the target application and any existing datasets the better, though students will refine their approach as they learn to build complete GFM pipelines.\nhttps://forms.gle/Q1iDp2kuZuX1avMPA\n\n\n—\n\n\nCourse Structure: 3 Stages, 9 Steps\n\n🏗️ Stage 1: Build GFM Architecture (Weeks 1-3)\n\nWeek 1: Geospatial Data Foundations (Step 1: Data preparation and sampling)\nWeek 2: Spatial-Temporal Attention Mechanisms (Step 2: Attention mechanism)\n\nWeek 3: Complete GFM Architecture (Step 3: LLM Architecture)\n\n\n\n🚀 Stage 2: Training a Foundation Model (Weeks 4-7)\n\nWeek 4: Pretraining Implementation (Step 4: Pretraining)\nWeek 5: Training Loop Optimization (Step 5: Training loop)\nWeek 6: Model Evaluation & Analysis (Step 6: Model evaluation)\nWeek 7: Integration with Existing Models (Step 7: Load pretrained weights)\n\n\n\n🎯 Stage 3: Model Application (Weeks 8-10)\n\nWeek 8: Task-Specific Fine-tuning (Step 8: Fine-tuning)\nWeek 9: Model Implementation & Deployment (Step 9: Model implementation)\nWeek 10: Project Presentations & Future Directions (Integration and synthesis)\n\n\n\n\n\nDeliverables\nStage 1: Architecture (Weeks 1-3) * Week 1: Geospatial data pipeline with tokenization strategy * Week 3: Working GFM architecture (~10M parameters)\nStage 2: Training (Weeks 4-7) * Week 4: Active pretraining pipeline with monitoring * Week 6: Comprehensive model evaluation report * Week 7: Integration with existing models (Prithvi comparison)\nStage 3: Application (Weeks 8-10) * Week 8: Fine-tuned model for specific geospatial task * Week 9: Deployable model with API and documentation * Week 10: Final presentation of complete GFM pipeline (15 min demo + Q&A)\nOptional: Submit our foundation model to Hugging Face for broader visibility\n\n\n\nGrading\nThis course will be assessed on a pass/fail basis. Passing requires consistent attendance and participation and submission of all deliverables."
  }
]